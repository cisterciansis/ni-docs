{
  "/neuralinternet/ni-compute/1-overview": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/1-overview",
    "title": "Overview",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/1-overview",
    "level": 0,
    "target_astro_path": "/",
    "main_markdown_content": "# Overview\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [compute/__init__.py](compute/__init__.py)\n- [compute/utils/parser.py](compute/utils/parser.py)\n\n</details>\n\n\n\nThe NI Compute Subnet is a decentralized GPU compute marketplace operating on the Bittensor network as subnet 27. It enables miners to contribute GPU resources and earn rewards based on their computational performance, while validators measure miner capabilities and allocate resources to clients through a trustless, permissionless system.\n\nThis document provides a high-level overview of the system architecture, core components, and their interactions. For detailed installation instructions, see [Installation and Setup](#1.2). For specific component documentation, refer to the [Validator System](#2), [Miner System](#3), and [Resource Allocation API](#4) sections.\n\n## System Architecture\n\nThe NI Compute Subnet consists of three primary components that interact through the Bittensor network and custom communication protocols:\n\n```mermaid\ngraph TB\n    subgraph \"Bittensor Network\"\n        BT[\"Subtensor Blockchain\"]\n        META[\"Metagraph State\"]\n    end\n    \n    subgraph \"Validator System\"\n        VAL[\"Validator Process<br/>neurons/validator.py\"]\n        POG[\"Proof-of-GPU Validation\"]\n        SCORE[\"Performance Scoring\"]\n        WEIGHTS[\"Weight Setting\"]\n    end\n    \n    subgraph \"Miner System\"\n        MIN[\"Miner Process<br/>neurons/miner.py\"]\n        DOCKER[\"Docker Container Management\"]\n        SSH[\"SSH Resource Access\"]\n        HASHCAT[\"Hashcat PoW Challenges\"]\n    end\n    \n    subgraph \"Resource Allocation API\"\n        API[\"RegisterAPI<br/>FastAPI Service\"]\n        ALLOC[\"Resource Discovery\"]\n        HEALTH[\"Health Monitoring\"]\n    end\n    \n    subgraph \"Data Layer\"\n        DB[\"ComputeDb<br/>SQLite Database\"]\n        WANDB[\"WandB Metrics\"]\n        CONFIG[\"GPU Performance Config\"]\n    end\n    \n    VAL -->|\"Validates Performance\"| MIN\n    VAL -->|\"Sets Network Weights\"| BT\n    VAL -->|\"Queries Metagraph\"| META\n    \n    MIN -->|\"Registers Hotkey\"| BT\n    MIN -->|\"Manages Containers\"| DOCKER\n    MIN -->|\"Provides SSH Access\"| SSH\n    \n    API -->|\"Allocates Resources\"| MIN\n    API -->|\"Health Checks\"| HEALTH\n    \n    VAL -->|\"Stores Results\"| DB\n    VAL -->|\"Logs Metrics\"| WANDB\n    MIN -->|\"Updates Status\"| WANDB\n    \n    CONFIG -->|\"Configures Scoring\"| VAL\n```\n\n**Sources:** [README.md:1-535](), [compute/__init__.py:1-93]()\n\n## Core Components\n\n### Validator System\nValidators are responsible for measuring miner performance and maintaining network integrity. They operate continuous validation cycles that include hardware specification queries, proof-of-GPU benchmarks, and cryptographic challenge verification.\n\nThe validator system implements a sophisticated scoring mechanism based on GPU performance metrics, with base scores assigned to different GPU models and scaling factors for multiple GPU configurations. Validators with sufficient stake (`validator_permit_stake = 1.0e4` TAO) can set network weights that determine miner rewards.\n\n**Key Classes and Constants:**\n- `neurons/validator.py` - Main validator process\n- `validator_permit_stake` - Minimum stake requirement for validators\n- `specs_timeout = 60` - Timeout for hardware specification requests\n- `pog_retry_limit = 30` - Maximum retries for proof-of-GPU validation\n\n### Miner System\nMiners contribute GPU computational resources to the network and respond to validator requests. They manage Docker containers for resource isolation, handle SSH-based resource allocation, and participate in proof-of-work challenges using Hashcat.\n\nThe miner system uses a priority-based request handling system where resource allocation requests (`miner_priority_allocate = 3`) take precedence over challenge responses (`miner_priority_challenge = 2`) and specification queries (`miner_priority_specs = 1`).\n\n**Key Classes and Constants:**\n- `neurons/miner.py` - Main miner process  \n- `miner_hashcat_location = \"hashcat\"` - Hashcat binary location\n- `miner_hashcat_workload_profile = \"3\"` - High performance profile\n- `pow_timeout = 30` - Proof-of-work challenge timeout\n\n### Resource Allocation API\nThe Resource Allocation API is a FastAPI-based web service that provides external access to the compute network. It handles resource discovery, allocation requests, and health monitoring of active allocations.\n\nThe API implements RSA encryption for secure communication and maintains state through both local database storage and distributed WandB synchronization.\n\n**Sources:** [compute/__init__.py:21-77](), [README.md:87-108]()\n\n## Validation and Challenge System\n\nThe subnet implements multiple validation mechanisms to ensure miner integrity and performance:\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator\"\n    participant M as \"Miner\"\n    participant D as \"Docker Container\"\n    participant BT as \"Bittensor Network\"\n    \n    Note over V,M: \"Hardware Specification Phase\"\n    V->>M: \"Send Specs Request\"\n    M->>V: \"Return GPU/CPU Specs\"\n    \n    Note over V,M: \"Proof-of-GPU Phase\"\n    V->>M: \"Request PoG Allocation\"\n    M->>D: \"Create Container\"\n    D->>V: \"Provide SSH Access\"\n    V->>D: \"Run GPU Benchmarks\"\n    D->>V: \"Return Benchmark Results\"\n    \n    Note over V,M: \"Challenge Phase\"\n    V->>M: \"Send PoW Challenge\"\n    M->>M: \"Execute Hashcat\"\n    M->>V: \"Return Merkle Proof\"\n    \n    Note over V,BT: \"Weight Setting Phase\"\n    V->>V: \"Calculate Scores\"\n    V->>BT: \"Set Network Weights\"\n```\n\n**Proof-of-Work Configuration:**\n- `pow_min_difficulty = 7` - Minimum challenge difficulty\n- `pow_max_difficulty = 12` - Maximum challenge difficulty  \n- `pow_default_mode = \"610\"` - BLAKE2b-512 hash mode\n- `pow_default_chars` - Challenge character set including alphanumeric and special characters\n\n**Sources:** [compute/__init__.py:37-49](), [README.md:437-450]()\n\n## Data Management and Monitoring\n\nThe system maintains state through multiple data persistence layers:\n\n| Component | Storage Type | Purpose |\n|-----------|--------------|---------|\n| ComputeDb | SQLite | Local miner stats, scores, allocations |\n| WandB | Distributed | Cross-validator metrics, distributed state |\n| Configuration | YAML/ENV | GPU performance benchmarks, API keys |\n\nThe monitoring system uses WandB for distributed state management and metrics collection, with separate runs for validators and miners to track performance and resource utilization.\n\n**Version Management:**\n- `__version__ = \"1.9.0\"` - Current subnet version\n- `__minimal_miner_version__ = \"1.8.5\"` - Minimum required miner version\n- `__minimal_validator_version__ = \"1.8.8\"` - Minimum required validator version\n\n**Sources:** [compute/__init__.py:20-26](), [README.md:297-303]()\n\n## Network Communication\n\nThe subnet uses custom Bittensor synapse protocols for component communication:\n\n```mermaid\ngraph LR\n    subgraph \"Protocol Layer\"\n        SPECS[\"Specs Protocol<br/>Hardware Queries\"]\n        ALLOC[\"Allocate Protocol<br/>Resource Requests\"]  \n        CHALLENGE[\"Challenge Protocol<br/>PoW Verification\"]\n    end\n    \n    subgraph \"Transport Layer\"\n        AXON[\"Custom Axon<br/>Miner Endpoints\"]\n        SUBTENSOR[\"Custom Subtensor<br/>Network Interface\"]\n    end\n    \n    SPECS --> AXON\n    ALLOC --> AXON\n    CHALLENGE --> AXON\n    \n    AXON --> SUBTENSOR\n```\n\nThe communication system includes blacklist management for suspected exploiters and trusted validator lists to maintain network security and integrity.\n\n**Sources:** [compute/__init__.py:59-92](), [compute/utils/parser.py:8-170]()\n\nFor detailed information about specific components, see the [Validator System](#2), [Miner System](#3), [Resource Allocation API](#4), and [Communication Protocols](#5) sections.",
    "resolved_links": [
      {
        "text": "README.md",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/README.md",
        "original_deepwiki_href": "README.md",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/__init__.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py",
        "original_deepwiki_href": "compute/__init__.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/utils/parser.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py",
        "original_deepwiki_href": "compute/utils/parser.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "Installation and Setup",
        "href": "/installation-and-setup#1.2",
        "original_deepwiki_href": "#1.2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Validator System",
        "href": "/validator-system#2",
        "original_deepwiki_href": "#2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Miner System",
        "href": "/miner-system#3",
        "original_deepwiki_href": "#3",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Resource Allocation API",
        "href": "/resource-allocation-api#4",
        "original_deepwiki_href": "#4",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Validator System",
        "href": "/validator-system#2",
        "original_deepwiki_href": "#2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Miner System",
        "href": "/miner-system#3",
        "original_deepwiki_href": "#3",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Resource Allocation API",
        "href": "/resource-allocation-api#4",
        "original_deepwiki_href": "#4",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Communication Protocols",
        "href": "/communication-protocols#5",
        "original_deepwiki_href": "#5",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"Bittensor Network\"\n        BT[\"Subtensor Blockchain\"]\n        META[\"Metagraph State\"]\n    end\n    \n    subgraph \"Validator System\"\n        VAL[\"Validator Process<br/>neurons/validator.py\"]\n        POG[\"Proof-of-GPU Validation\"]\n        SCORE[\"Performance Scoring\"]\n        WEIGHTS[\"Weight Setting\"]\n    end\n    \n    subgraph \"Miner System\"\n        MIN[\"Miner Process<br/>neurons/miner.py\"]\n        DOCKER[\"Docker Container Management\"]\n        SSH[\"SSH Resource Access\"]\n        HASHCAT[\"Hashcat PoW Challenges\"]\n    end\n    \n    subgraph \"Resource Allocation API\"\n        API[\"RegisterAPI<br/>FastAPI Service\"]\n        ALLOC[\"Resource Discovery\"]\n        HEALTH[\"Health Monitoring\"]\n    end\n    \n    subgraph \"Data Layer\"\n        DB[\"ComputeDb<br/>SQLite Database\"]\n        WANDB[\"WandB Metrics\"]\n        CONFIG[\"GPU Performance Config\"]\n    end\n    \n    VAL -->|\"Validates Performance\"| MIN\n    VAL -->|\"Sets Network Weights\"| BT\n    VAL -->|\"Queries Metagraph\"| META\n    \n    MIN -->|\"Registers Hotkey\"| BT\n    MIN -->|\"Manages Containers\"| DOCKER\n    MIN -->|\"Provides SSH Access\"| SSH\n    \n    API -->|\"Allocates Resources\"| MIN\n    API -->|\"Health Checks\"| HEALTH\n    \n    VAL -->|\"Stores Results\"| DB\n    VAL -->|\"Logs Metrics\"| WANDB\n    MIN -->|\"Updates Status\"| WANDB\n    \n    CONFIG -->|\"Configures Scoring\"| VAL",
      "sequenceDiagram\n    participant V as \"Validator\"\n    participant M as \"Miner\"\n    participant D as \"Docker Container\"\n    participant BT as \"Bittensor Network\"\n    \n    Note over V,M: \"Hardware Specification Phase\"\n    V->>M: \"Send Specs Request\"\n    M->>V: \"Return GPU/CPU Specs\"\n    \n    Note over V,M: \"Proof-of-GPU Phase\"\n    V->>M: \"Request PoG Allocation\"\n    M->>D: \"Create Container\"\n    D->>V: \"Provide SSH Access\"\n    V->>D: \"Run GPU Benchmarks\"\n    D->>V: \"Return Benchmark Results\"\n    \n    Note over V,M: \"Challenge Phase\"\n    V->>M: \"Send PoW Challenge\"\n    M->>M: \"Execute Hashcat\"\n    M->>V: \"Return Merkle Proof\"\n    \n    Note over V,BT: \"Weight Setting Phase\"\n    V->>V: \"Calculate Scores\"\n    V->>BT: \"Set Network Weights\"",
      "graph LR\n    subgraph \"Protocol Layer\"\n        SPECS[\"Specs Protocol<br/>Hardware Queries\"]\n        ALLOC[\"Allocate Protocol<br/>Resource Requests\"]  \n        CHALLENGE[\"Challenge Protocol<br/>PoW Verification\"]\n    end\n    \n    subgraph \"Transport Layer\"\n        AXON[\"Custom Axon<br/>Miner Endpoints\"]\n        SUBTENSOR[\"Custom Subtensor<br/>Network Interface\"]\n    end\n    \n    SPECS --> AXON\n    ALLOC --> AXON\n    CHALLENGE --> AXON\n    \n    AXON --> SUBTENSOR"
    ],
    "potential_frontmatter": {
      "title": "Overview"
    }
  },
  "/neuralinternet/ni-compute/1.1-architecture": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/1.1-architecture",
    "title": "Architecture",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/1.1-architecture",
    "level": 1,
    "target_astro_path": "/architecture",
    "main_markdown_content": "# Architecture\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [compute/utils/parser.py](compute/utils/parser.py)\n- [neurons/miner.py](neurons/miner.py)\n- [neurons/register_api.py](neurons/register_api.py)\n- [neurons/validator.py](neurons/validator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the high-level system architecture of the NI Compute Subnet, a decentralized GPU compute marketplace built on the Bittensor network. It covers the core system components, their interactions, communication protocols, and data flow patterns that enable validators to evaluate miner capabilities and allocate GPU resources to clients.\n\nFor detailed protocol specifications, see [Communication Protocols](#5). For database schema and operations, see [Database Operations](#2.3). For installation and deployment procedures, see [Installation and Setup](#1.2).\n\n## System Overview\n\nThe NI Compute Subnet implements a three-tier architecture consisting of validators that assess miner performance, miners that provide GPU resources, and a resource allocation API that manages client requests. The system operates on Bittensor's peer-to-peer network while providing traditional REST API access for external clients.\n\n```mermaid\ngraph TB\n    subgraph \"External Clients\"\n        WEB[\"Web Applications\"]\n        CLI[\"CLI Tools\"]\n        API_CLIENTS[\"API Clients\"]\n    end\n    \n    subgraph \"NI Compute Subnet Core\"\n        subgraph \"Validator Layer\"\n            VALIDATOR[\"Validator<br/>neurons/validator.py\"]\n            POG[\"ProofOfGPU<br/>Benchmarking Engine\"]\n            SCORING[\"Scoring System<br/>calc_score_pog()\"]\n        end\n        \n        subgraph \"Resource Allocation Layer\"\n            REGISTER_API[\"RegisterAPI<br/>neurons/register_api.py\"]\n            ALLOCATION_LOGIC[\"Resource Management<br/>_allocate_container()\"]\n            HEALTH_CHECK[\"Health Monitoring<br/>_check_allocation()\"]\n        end\n        \n        subgraph \"Miner Layer\"\n            MINER[\"Miner<br/>neurons/miner.py\"]\n            CONTAINER_MGR[\"Container Management<br/>neurons/Miner/container.py\"]\n            ALLOCATION_HANDLER[\"Allocation Handler<br/>register_allocation()\"]\n        end\n    end\n    \n    subgraph \"Bittensor Network\"\n        SUBTENSOR[\"ComputeSubnetSubtensor<br/>Blockchain Interface\"]\n        METAGRAPH[\"Metagraph<br/>Network State\"]\n        AXON_DENDRITE[\"Axon/Dendrite<br/>P2P Communication\"]\n    end\n    \n    subgraph \"Data Layer\"\n        COMPUTE_DB[(\"ComputeDb<br/>SQLite Database\")]\n        WANDB_STATE[(\"WandB<br/>Distributed State\")]\n        CONFIG_FILES[(\"Configuration<br/>config.yaml\")]\n    end\n    \n    %% External client interactions\n    WEB --> REGISTER_API\n    CLI --> REGISTER_API\n    API_CLIENTS --> REGISTER_API\n    \n    %% Core system interactions\n    VALIDATOR --> MINER\n    VALIDATOR --> SUBTENSOR\n    VALIDATOR --> POG\n    POG --> SCORING\n    \n    REGISTER_API --> ALLOCATION_LOGIC\n    ALLOCATION_LOGIC --> MINER\n    REGISTER_API --> HEALTH_CHECK\n    \n    MINER --> CONTAINER_MGR\n    MINER --> ALLOCATION_HANDLER\n    MINER --> AXON_DENDRITE\n    \n    %% Bittensor network interactions\n    VALIDATOR --> AXON_DENDRITE\n    MINER --> AXON_DENDRITE\n    AXON_DENDRITE --> SUBTENSOR\n    SUBTENSOR --> METAGRAPH\n    \n    %% Data layer interactions\n    VALIDATOR --> COMPUTE_DB\n    VALIDATOR --> WANDB_STATE\n    REGISTER_API --> COMPUTE_DB\n    MINER --> WANDB_STATE\n    VALIDATOR --> CONFIG_FILES\n```\n\nSources: [neurons/validator.py:70-89](), [neurons/miner.py:79-94](), [neurons/register_api.py:229-303](), [compute/axon.py](), [compute/protocol.py]()\n\n## Core Components\n\n### Validator System\n\nThe `Validator` class implements the core validation logic that maintains network integrity by evaluating miner performance and setting network weights.\n\n```mermaid\ngraph TB\n    subgraph \"Validator Core\"\n        VALIDATOR_MAIN[\"Validator.__init__()<br/>neurons/validator.py:130\"]\n        CONFIG_INIT[\"init_config()<br/>neurons/validator.py:211\"]\n        SCORE_SYNC[\"sync_scores()<br/>neurons/validator.py:312\"]\n    end\n    \n    subgraph \"Proof of GPU System\"\n        POG_MAIN[\"proof_of_gpu()<br/>neurons/validator.py:663\"]\n        TEST_MINER[\"test_miner_gpu()<br/>neurons/validator.py:799\"]\n        GPU_BENCHMARKS[\"GPU Benchmarking<br/>Merkle Proof Verification\"]\n        SCRIPT_EXECUTION[\"execute_script_on_miner()<br/>neurons/Validator/pog.py\"]\n    end\n    \n    subgraph \"Scoring Engine\"\n        CALC_SCORE[\"calc_score_pog()<br/>neurons/Validator/calculate_pow_score.py\"]\n        RELIABILITY_SCORE[\"Reliability Scoring<br/>Challenge Success Rate\"]\n        WEIGHT_SETTING[\"Network Weight Updates<br/>Bittensor Integration\"]\n    end\n    \n    subgraph \"Data Management\"\n        COMPUTE_DB_OPS[\"ComputeDb Operations<br/>SQLite Transactions\"]\n        WANDB_INTEGRATION[\"ComputeWandb<br/>Distributed Metrics\"]\n        MINER_STATS[\"Miner Statistics<br/>retrieve_stats()\"]\n    end\n    \n    VALIDATOR_MAIN --> CONFIG_INIT\n    VALIDATOR_MAIN --> POG_MAIN\n    VALIDATOR_MAIN --> SCORE_SYNC\n    \n    POG_MAIN --> TEST_MINER\n    TEST_MINER --> GPU_BENCHMARKS\n    TEST_MINER --> SCRIPT_EXECUTION\n    \n    SCORE_SYNC --> CALC_SCORE\n    CALC_SCORE --> RELIABILITY_SCORE\n    RELIABILITY_SCORE --> WEIGHT_SETTING\n    \n    VALIDATOR_MAIN --> COMPUTE_DB_OPS\n    VALIDATOR_MAIN --> WANDB_INTEGRATION\n    SCORE_SYNC --> MINER_STATS\n```\n\nThe validator operates on a continuous cycle, performing hardware verification every 360 blocks and updating scores based on GPU performance benchmarks and challenge response reliability.\n\nSources: [neurons/validator.py:70-200](), [neurons/Validator/pog.py](), [neurons/Validator/calculate_pow_score.py](), [neurons/Validator/database/]()\n\n### Miner System\n\nThe `Miner` class provides GPU compute resources to the network and handles allocation requests from validators and clients.\n\n```mermaid\ngraph TB\n    subgraph \"Miner Core\"\n        MINER_MAIN[\"Miner.__init__()<br/>neurons/miner.py:117\"]\n        AXON_INIT[\"init_axon()<br/>neurons/miner.py:222\"]\n        SYNC_STATUS[\"sync_status()<br/>neurons/miner.py:304\"]\n    end\n    \n    subgraph \"Request Handlers\"\n        ALLOCATE_HANDLER[\"allocate()<br/>neurons/miner.py:419\"]\n        CHALLENGE_HANDLER[\"challenge()<br/>neurons/miner.py:491\"]\n        BLACKLIST_LOGIC[\"base_blacklist()<br/>neurons/miner.py:330\"]\n    end\n    \n    subgraph \"Container Management\"\n        REGISTER_ALLOC[\"register_allocation()<br/>neurons/Miner/allocate.py\"]\n        CONTAINER_OPS[\"Container Operations<br/>neurons/Miner/container.py\"]\n        DOCKER_LIFECYCLE[\"Docker Lifecycle<br/>build_sample_container()\"]\n    end\n    \n    subgraph \"Resource Monitoring\"\n        ALLOCATION_STATUS[\"Allocation Status Tracking<br/>self.allocation_status\"]\n        WANDB_UPDATES[\"WandB State Updates<br/>update_allocated()\"]\n        HEALTH_CHECKS[\"Health Check Responses<br/>check_allocation()\"]\n    end\n    \n    MINER_MAIN --> AXON_INIT\n    MINER_MAIN --> SYNC_STATUS\n    \n    AXON_INIT --> ALLOCATE_HANDLER\n    AXON_INIT --> CHALLENGE_HANDLER\n    ALLOCATE_HANDLER --> BLACKLIST_LOGIC\n    \n    ALLOCATE_HANDLER --> REGISTER_ALLOC\n    REGISTER_ALLOC --> CONTAINER_OPS\n    CONTAINER_OPS --> DOCKER_LIFECYCLE\n    \n    ALLOCATE_HANDLER --> ALLOCATION_STATUS\n    ALLOCATION_STATUS --> WANDB_UPDATES\n    ALLOCATE_HANDLER --> HEALTH_CHECKS\n```\n\nThe miner continuously monitors for allocation opportunities while maintaining containerized environments for client workloads.\n\nSources: [neurons/miner.py:79-200](), [neurons/Miner/allocate.py](), [neurons/Miner/container.py](), [compute/wandb/wandb.py]()\n\n### Resource Allocation API\n\nThe `RegisterAPI` class exposes REST endpoints for external clients to allocate and manage GPU resources.\n\n```mermaid\ngraph TB\n    subgraph \"API Layer\"\n        REGISTER_API[\"RegisterAPI.__init__()<br/>neurons/register_api.py:230\"]\n        FASTAPI_APP[\"FastAPI Application<br/>self.app\"]\n        ROUTE_SETUP[\"_setup_routes()<br/>neurons/register_api.py:344\"]\n    end\n    \n    subgraph \"Allocation Endpoints\"\n        ALLOCATE_SPEC[\"allocate_spec()<br/>/service/allocate_spec\"]\n        ALLOCATE_HOTKEY[\"allocate_hotkey()<br/>/service/allocate_hotkey\"]\n        DEALLOCATE[\"deallocate()<br/>/service/deallocate\"]\n        CHECK_STATUS[\"check_miner_status()<br/>/service/check_miner_status\"]\n    end\n    \n    subgraph \"Resource Management\"\n        ALLOCATE_CONTAINER[\"_allocate_container()<br/>Resource Discovery\"]\n        ALLOCATION_DB[\"update_allocation_db()<br/>State Persistence\"]\n        HEALTH_MONITORING[\"_check_allocation()<br/>Timeout Management\"]\n    end\n    \n    subgraph \"External Integrations\"\n        DENDRITE_CLIENT[\"bt.dendrite<br/>Miner Communication\"]\n        WANDB_SYNC[\"_update_allocation_wandb()<br/>Distributed State\"]\n        WEBHOOK_NOTIFY[\"_notify_allocation_status()<br/>External Callbacks\"]\n    end\n    \n    REGISTER_API --> FASTAPI_APP\n    REGISTER_API --> ROUTE_SETUP\n    \n    ROUTE_SETUP --> ALLOCATE_SPEC\n    ROUTE_SETUP --> ALLOCATE_HOTKEY\n    ROUTE_SETUP --> DEALLOCATE\n    ROUTE_SETUP --> CHECK_STATUS\n    \n    ALLOCATE_SPEC --> ALLOCATE_CONTAINER\n    ALLOCATE_HOTKEY --> ALLOCATE_CONTAINER\n    ALLOCATE_CONTAINER --> ALLOCATION_DB\n    \n    REGISTER_API --> HEALTH_MONITORING\n    HEALTH_MONITORING --> ALLOCATION_DB\n    \n    ALLOCATE_CONTAINER --> DENDRITE_CLIENT\n    ALLOCATION_DB --> WANDB_SYNC\n    DEALLOCATE --> WEBHOOK_NOTIFY\n```\n\nThe API maintains allocation state in both local SQLite database and distributed WandB storage for cross-validator synchronization.\n\nSources: [neurons/register_api.py:229-350](), [neurons/register_api.py:407-850](), [neurons/Validator/database/allocate.py]()\n\n## Communication Architecture\n\nThe system implements a hybrid communication model combining Bittensor's peer-to-peer protocols with traditional REST APIs.\n\n```mermaid\ngraph TB\n    subgraph \"Protocol Layer\"\n        SPECS_PROTOCOL[\"Specs Protocol<br/>compute/protocol.py\"]\n        ALLOCATE_PROTOCOL[\"Allocate Protocol<br/>compute/protocol.py\"]\n        CHALLENGE_PROTOCOL[\"Challenge Protocol<br/>compute/protocol.py\"]\n    end\n    \n    subgraph \"Bittensor Network Layer\"\n        COMPUTE_AXON[\"ComputeSubnetAxon<br/>compute/axon.py\"]\n        COMPUTE_SUBTENSOR[\"ComputeSubnetSubtensor<br/>compute/axon.py\"]\n        DENDRITE_CLIENT[\"bt.dendrite<br/>RPC Client\"]\n    end\n    \n    subgraph \"REST API Layer\"\n        FASTAPI_ROUTES[\"FastAPI Routes<br/>HTTP/HTTPS\"]\n        WEBSOCKET_CONN[\"WebSocket Connection<br/>/connect\"]\n        API_MIDDLEWARE[\"IPWhitelistMiddleware<br/>Security Layer\"]\n    end\n    \n    subgraph \"Communication Flows\"\n        V_TO_M[\"Validator → Miner<br/>Specs/Challenge Queries\"]\n        API_TO_M[\"RegisterAPI → Miner<br/>Allocation Requests\"]\n        CLIENT_TO_API[\"External Client → API<br/>Resource Requests\"]\n    end\n    \n    SPECS_PROTOCOL --> COMPUTE_AXON\n    ALLOCATE_PROTOCOL --> COMPUTE_AXON\n    CHALLENGE_PROTOCOL --> COMPUTE_AXON\n    \n    COMPUTE_AXON --> COMPUTE_SUBTENSOR\n    COMPUTE_AXON --> DENDRITE_CLIENT\n    \n    FASTAPI_ROUTES --> API_MIDDLEWARE\n    WEBSOCKET_CONN --> FASTAPI_ROUTES\n    \n    V_TO_M --> SPECS_PROTOCOL\n    V_TO_M --> CHALLENGE_PROTOCOL\n    API_TO_M --> ALLOCATE_PROTOCOL\n    API_TO_M --> DENDRITE_CLIENT\n    CLIENT_TO_API --> FASTAPI_ROUTES\n```\n\n**Protocol Message Flow:**\n1. **Specs Query**: Validator requests hardware specifications from miners\n2. **Allocation Request**: RegisterAPI or Validator requests resource allocation \n3. **Challenge Response**: Validator sends proof-of-work challenges to miners\n4. **Health Check**: Periodic status verification of allocated resources\n\nSources: [compute/protocol.py](), [compute/axon.py](), [neurons/register_api.py:344-406](), [neurons/validator.py:594-662]()\n\n## Data Architecture\n\nThe system uses a multi-tier data storage approach combining local SQLite databases with distributed state management.\n\n```mermaid\ngraph TB\n    subgraph \"Local Data Storage\"\n        COMPUTE_DB[(\"ComputeDb<br/>SQLite Database\")]\n        MINER_TABLE[(\"miner table<br/>uid, ss58_address\")]\n        POG_STATS[(\"pog_stats table<br/>GPU performance data\")]\n        ALLOCATION_TABLE[(\"allocation table<br/>active reservations\")]\n        CHALLENGE_DETAILS[(\"challenge_details table<br/>success metrics\")]\n    end\n    \n    subgraph \"Distributed State\"\n        WANDB_RUNS[(\"WandB Validator Runs<br/>Aggregated metrics\")]\n        WANDB_MINERS[(\"WandB Miner Runs<br/>Hardware specifications\")]\n        WANDB_ALLOCATED[(\"Allocated Hotkeys<br/>Resource status\")]\n        WANDB_PENALIZED[(\"Penalized Hotkeys<br/>Blacklist data\")]\n    end\n    \n    subgraph \"Configuration Data\"\n        CONFIG_YAML[(\"config.yaml<br/>GPU performance benchmarks\")]\n        ENV_CONFIG[(\"Environment Variables<br/>API keys, endpoints\")]\n        PROTOCOL_SCHEMAS[(\"Protocol Definitions<br/>Message validation\")]\n    end\n    \n    subgraph \"Data Access Layer\"\n        DB_OPERATIONS[\"Database Operations<br/>compute/utils/db.py\"]\n        WANDB_CLIENT[\"ComputeWandb<br/>compute/wandb/wandb.py\"]\n        CONFIG_LOADER[\"Configuration Loader<br/>load_yaml_config()\"]\n    end\n    \n    COMPUTE_DB --> MINER_TABLE\n    COMPUTE_DB --> POG_STATS\n    COMPUTE_DB --> ALLOCATION_TABLE\n    COMPUTE_DB --> CHALLENGE_DETAILS\n    \n    WANDB_RUNS --> WANDB_MINERS\n    WANDB_RUNS --> WANDB_ALLOCATED\n    WANDB_RUNS --> WANDB_PENALIZED\n    \n    CONFIG_YAML --> ENV_CONFIG\n    CONFIG_YAML --> PROTOCOL_SCHEMAS\n    \n    DB_OPERATIONS --> COMPUTE_DB\n    WANDB_CLIENT --> WANDB_RUNS\n    CONFIG_LOADER --> CONFIG_YAML\n```\n\n**Data Synchronization Patterns:**\n- Local database stores operational state and query results\n- WandB provides cross-validator state synchronization\n- Configuration files define GPU performance baselines and system parameters\n\nSources: [compute/utils/db.py](), [compute/wandb/wandb.py](), [neurons/Validator/database/](), [config.yaml](), [neurons/validator.py:178-181]()\n\n## Deployment Architecture\n\nThe system supports distributed deployment across multiple validator and miner nodes with centralized API services.\n\n```mermaid\ngraph TB\n    subgraph \"Validator Nodes\"\n        V1[\"Validator Instance 1<br/>neurons/validator.py\"]\n        V2[\"Validator Instance 2<br/>neurons/validator.py\"]\n        VN[\"Validator Instance N<br/>neurons/validator.py\"]\n    end\n    \n    subgraph \"Miner Nodes\"\n        M1[\"Miner Instance 1<br/>neurons/miner.py + Docker\"]\n        M2[\"Miner Instance 2<br/>neurons/miner.py + Docker\"]\n        MN[\"Miner Instance N<br/>neurons/miner.py + Docker\"]\n    end\n    \n    subgraph \"API Services\"\n        API1[\"RegisterAPI Instance 1<br/>neurons/register_api.py\"]\n        API2[\"RegisterAPI Instance 2<br/>neurons/register_api.py\"]\n        LB[\"Load Balancer<br/>Optional\"]\n    end\n    \n    subgraph \"Infrastructure Services\"\n        BT_NETWORK[\"Bittensor Network<br/>Subtensor/Metagraph\"]\n        WANDB_SERVICE[\"WandB Service<br/>Distributed State\"]\n        MONITORING[\"System Monitoring<br/>PM2/Prometheus\"]\n    end\n    \n    subgraph \"Network Configuration\"\n        FIREWALL[\"UFW Firewall<br/>Ports 4444, 8091\"]\n        SSH_ACCESS[\"SSH Access<br/>Container Management\"]\n        DOCKER_RUNTIME[\"Docker + NVIDIA Runtime<br/>GPU Containers\"]\n    end\n    \n    V1 --> BT_NETWORK\n    V2 --> BT_NETWORK\n    VN --> BT_NETWORK\n    \n    M1 --> BT_NETWORK\n    M2 --> BT_NETWORK\n    MN --> BT_NETWORK\n    \n    API1 --> BT_NETWORK\n    API2 --> BT_NETWORK\n    LB --> API1\n    LB --> API2\n    \n    V1 --> WANDB_SERVICE\n    V2 --> WANDB_SERVICE\n    API1 --> WANDB_SERVICE\n    \n    M1 --> DOCKER_RUNTIME\n    M2 --> DOCKER_RUNTIME\n    MN --> DOCKER_RUNTIME\n    \n    FIREWALL --> SSH_ACCESS\n    SSH_ACCESS --> DOCKER_RUNTIME\n    \n    MONITORING --> V1\n    MONITORING --> M1\n    MONITORING --> API1\n```\n\n**Deployment Requirements:**\n- **Validators**: Require access to Subtensor endpoint and sufficient computational resources for GPU benchmarking\n- **Miners**: Need NVIDIA GPUs, Docker runtime, and open ports (4444 for SSH, 8091 for axon)\n- **RegisterAPI**: Can run on dedicated servers with database persistence and WandB integration\n\nSources: [README.md:110-340](), [compute/utils/parser.py:159-165](), [neurons/miner.py:154-167](), [neurons/register_api.py:86-95]()",
    "resolved_links": [
      {
        "text": "README.md",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/README.md",
        "original_deepwiki_href": "README.md",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/utils/parser.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py",
        "original_deepwiki_href": "compute/utils/parser.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/miner.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py",
        "original_deepwiki_href": "neurons/miner.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/register_api.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py",
        "original_deepwiki_href": "neurons/register_api.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/validator.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py",
        "original_deepwiki_href": "neurons/validator.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/validator.py:70-89",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:79-94",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:229-303",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/protocol.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:70-200",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/pog.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/calculate_pow_score.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:79-200",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/wandb/wandb.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:229-350",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:407-850",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/allocate.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:344-406",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:594-662",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "config.yaml",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:178-181",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "README.md:110-340",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/parser.py:159-165",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:154-167",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:86-95",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Communication Protocols",
        "href": "/communication-protocols#5",
        "original_deepwiki_href": "#5",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Database Operations",
        "href": "/validator-system/database-operations#2.3",
        "original_deepwiki_href": "#2.3",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Installation and Setup",
        "href": "/installation-and-setup#1.2",
        "original_deepwiki_href": "#1.2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"External Clients\"\n        WEB[\"Web Applications\"]\n        CLI[\"CLI Tools\"]\n        API_CLIENTS[\"API Clients\"]\n    end\n    \n    subgraph \"NI Compute Subnet Core\"\n        subgraph \"Validator Layer\"\n            VALIDATOR[\"Validator<br/>neurons/validator.py\"]\n            POG[\"ProofOfGPU<br/>Benchmarking Engine\"]\n            SCORING[\"Scoring System<br/>calc_score_pog()\"]\n        end\n        \n        subgraph \"Resource Allocation Layer\"\n            REGISTER_API[\"RegisterAPI<br/>neurons/register_api.py\"]\n            ALLOCATION_LOGIC[\"Resource Management<br/>_allocate_container()\"]\n            HEALTH_CHECK[\"Health Monitoring<br/>_check_allocation()\"]\n        end\n        \n        subgraph \"Miner Layer\"\n            MINER[\"Miner<br/>neurons/miner.py\"]\n            CONTAINER_MGR[\"Container Management<br/>neurons/Miner/container.py\"]\n            ALLOCATION_HANDLER[\"Allocation Handler<br/>register_allocation()\"]\n        end\n    end\n    \n    subgraph \"Bittensor Network\"\n        SUBTENSOR[\"ComputeSubnetSubtensor<br/>Blockchain Interface\"]\n        METAGRAPH[\"Metagraph<br/>Network State\"]\n        AXON_DENDRITE[\"Axon/Dendrite<br/>P2P Communication\"]\n    end\n    \n    subgraph \"Data Layer\"\n        COMPUTE_DB[(\"ComputeDb<br/>SQLite Database\")]\n        WANDB_STATE[(\"WandB<br/>Distributed State\")]\n        CONFIG_FILES[(\"Configuration<br/>config.yaml\")]\n    end\n    \n    %% External client interactions\n    WEB --> REGISTER_API\n    CLI --> REGISTER_API\n    API_CLIENTS --> REGISTER_API\n    \n    %% Core system interactions\n    VALIDATOR --> MINER\n    VALIDATOR --> SUBTENSOR\n    VALIDATOR --> POG\n    POG --> SCORING\n    \n    REGISTER_API --> ALLOCATION_LOGIC\n    ALLOCATION_LOGIC --> MINER\n    REGISTER_API --> HEALTH_CHECK\n    \n    MINER --> CONTAINER_MGR\n    MINER --> ALLOCATION_HANDLER\n    MINER --> AXON_DENDRITE\n    \n    %% Bittensor network interactions\n    VALIDATOR --> AXON_DENDRITE\n    MINER --> AXON_DENDRITE\n    AXON_DENDRITE --> SUBTENSOR\n    SUBTENSOR --> METAGRAPH\n    \n    %% Data layer interactions\n    VALIDATOR --> COMPUTE_DB\n    VALIDATOR --> WANDB_STATE\n    REGISTER_API --> COMPUTE_DB\n    MINER --> WANDB_STATE\n    VALIDATOR --> CONFIG_FILES",
      "graph TB\n    subgraph \"Validator Core\"\n        VALIDATOR_MAIN[\"Validator.__init__()<br/>neurons/validator.py:130\"]\n        CONFIG_INIT[\"init_config()<br/>neurons/validator.py:211\"]\n        SCORE_SYNC[\"sync_scores()<br/>neurons/validator.py:312\"]\n    end\n    \n    subgraph \"Proof of GPU System\"\n        POG_MAIN[\"proof_of_gpu()<br/>neurons/validator.py:663\"]\n        TEST_MINER[\"test_miner_gpu()<br/>neurons/validator.py:799\"]\n        GPU_BENCHMARKS[\"GPU Benchmarking<br/>Merkle Proof Verification\"]\n        SCRIPT_EXECUTION[\"execute_script_on_miner()<br/>neurons/Validator/pog.py\"]\n    end\n    \n    subgraph \"Scoring Engine\"\n        CALC_SCORE[\"calc_score_pog()<br/>neurons/Validator/calculate_pow_score.py\"]\n        RELIABILITY_SCORE[\"Reliability Scoring<br/>Challenge Success Rate\"]\n        WEIGHT_SETTING[\"Network Weight Updates<br/>Bittensor Integration\"]\n    end\n    \n    subgraph \"Data Management\"\n        COMPUTE_DB_OPS[\"ComputeDb Operations<br/>SQLite Transactions\"]\n        WANDB_INTEGRATION[\"ComputeWandb<br/>Distributed Metrics\"]\n        MINER_STATS[\"Miner Statistics<br/>retrieve_stats()\"]\n    end\n    \n    VALIDATOR_MAIN --> CONFIG_INIT\n    VALIDATOR_MAIN --> POG_MAIN\n    VALIDATOR_MAIN --> SCORE_SYNC\n    \n    POG_MAIN --> TEST_MINER\n    TEST_MINER --> GPU_BENCHMARKS\n    TEST_MINER --> SCRIPT_EXECUTION\n    \n    SCORE_SYNC --> CALC_SCORE\n    CALC_SCORE --> RELIABILITY_SCORE\n    RELIABILITY_SCORE --> WEIGHT_SETTING\n    \n    VALIDATOR_MAIN --> COMPUTE_DB_OPS\n    VALIDATOR_MAIN --> WANDB_INTEGRATION\n    SCORE_SYNC --> MINER_STATS",
      "graph TB\n    subgraph \"Miner Core\"\n        MINER_MAIN[\"Miner.__init__()<br/>neurons/miner.py:117\"]\n        AXON_INIT[\"init_axon()<br/>neurons/miner.py:222\"]\n        SYNC_STATUS[\"sync_status()<br/>neurons/miner.py:304\"]\n    end\n    \n    subgraph \"Request Handlers\"\n        ALLOCATE_HANDLER[\"allocate()<br/>neurons/miner.py:419\"]\n        CHALLENGE_HANDLER[\"challenge()<br/>neurons/miner.py:491\"]\n        BLACKLIST_LOGIC[\"base_blacklist()<br/>neurons/miner.py:330\"]\n    end\n    \n    subgraph \"Container Management\"\n        REGISTER_ALLOC[\"register_allocation()<br/>neurons/Miner/allocate.py\"]\n        CONTAINER_OPS[\"Container Operations<br/>neurons/Miner/container.py\"]\n        DOCKER_LIFECYCLE[\"Docker Lifecycle<br/>build_sample_container()\"]\n    end\n    \n    subgraph \"Resource Monitoring\"\n        ALLOCATION_STATUS[\"Allocation Status Tracking<br/>self.allocation_status\"]\n        WANDB_UPDATES[\"WandB State Updates<br/>update_allocated()\"]\n        HEALTH_CHECKS[\"Health Check Responses<br/>check_allocation()\"]\n    end\n    \n    MINER_MAIN --> AXON_INIT\n    MINER_MAIN --> SYNC_STATUS\n    \n    AXON_INIT --> ALLOCATE_HANDLER\n    AXON_INIT --> CHALLENGE_HANDLER\n    ALLOCATE_HANDLER --> BLACKLIST_LOGIC\n    \n    ALLOCATE_HANDLER --> REGISTER_ALLOC\n    REGISTER_ALLOC --> CONTAINER_OPS\n    CONTAINER_OPS --> DOCKER_LIFECYCLE\n    \n    ALLOCATE_HANDLER --> ALLOCATION_STATUS\n    ALLOCATION_STATUS --> WANDB_UPDATES\n    ALLOCATE_HANDLER --> HEALTH_CHECKS",
      "graph TB\n    subgraph \"API Layer\"\n        REGISTER_API[\"RegisterAPI.__init__()<br/>neurons/register_api.py:230\"]\n        FASTAPI_APP[\"FastAPI Application<br/>self.app\"]\n        ROUTE_SETUP[\"_setup_routes()<br/>neurons/register_api.py:344\"]\n    end\n    \n    subgraph \"Allocation Endpoints\"\n        ALLOCATE_SPEC[\"allocate_spec()<br/>/service/allocate_spec\"]\n        ALLOCATE_HOTKEY[\"allocate_hotkey()<br/>/service/allocate_hotkey\"]\n        DEALLOCATE[\"deallocate()<br/>/service/deallocate\"]\n        CHECK_STATUS[\"check_miner_status()<br/>/service/check_miner_status\"]\n    end\n    \n    subgraph \"Resource Management\"\n        ALLOCATE_CONTAINER[\"_allocate_container()<br/>Resource Discovery\"]\n        ALLOCATION_DB[\"update_allocation_db()<br/>State Persistence\"]\n        HEALTH_MONITORING[\"_check_allocation()<br/>Timeout Management\"]\n    end\n    \n    subgraph \"External Integrations\"\n        DENDRITE_CLIENT[\"bt.dendrite<br/>Miner Communication\"]\n        WANDB_SYNC[\"_update_allocation_wandb()<br/>Distributed State\"]\n        WEBHOOK_NOTIFY[\"_notify_allocation_status()<br/>External Callbacks\"]\n    end\n    \n    REGISTER_API --> FASTAPI_APP\n    REGISTER_API --> ROUTE_SETUP\n    \n    ROUTE_SETUP --> ALLOCATE_SPEC\n    ROUTE_SETUP --> ALLOCATE_HOTKEY\n    ROUTE_SETUP --> DEALLOCATE\n    ROUTE_SETUP --> CHECK_STATUS\n    \n    ALLOCATE_SPEC --> ALLOCATE_CONTAINER\n    ALLOCATE_HOTKEY --> ALLOCATE_CONTAINER\n    ALLOCATE_CONTAINER --> ALLOCATION_DB\n    \n    REGISTER_API --> HEALTH_MONITORING\n    HEALTH_MONITORING --> ALLOCATION_DB\n    \n    ALLOCATE_CONTAINER --> DENDRITE_CLIENT\n    ALLOCATION_DB --> WANDB_SYNC\n    DEALLOCATE --> WEBHOOK_NOTIFY",
      "graph TB\n    subgraph \"Protocol Layer\"\n        SPECS_PROTOCOL[\"Specs Protocol<br/>compute/protocol.py\"]\n        ALLOCATE_PROTOCOL[\"Allocate Protocol<br/>compute/protocol.py\"]\n        CHALLENGE_PROTOCOL[\"Challenge Protocol<br/>compute/protocol.py\"]\n    end\n    \n    subgraph \"Bittensor Network Layer\"\n        COMPUTE_AXON[\"ComputeSubnetAxon<br/>compute/axon.py\"]\n        COMPUTE_SUBTENSOR[\"ComputeSubnetSubtensor<br/>compute/axon.py\"]\n        DENDRITE_CLIENT[\"bt.dendrite<br/>RPC Client\"]\n    end\n    \n    subgraph \"REST API Layer\"\n        FASTAPI_ROUTES[\"FastAPI Routes<br/>HTTP/HTTPS\"]\n        WEBSOCKET_CONN[\"WebSocket Connection<br/>/connect\"]\n        API_MIDDLEWARE[\"IPWhitelistMiddleware<br/>Security Layer\"]\n    end\n    \n    subgraph \"Communication Flows\"\n        V_TO_M[\"Validator → Miner<br/>Specs/Challenge Queries\"]\n        API_TO_M[\"RegisterAPI → Miner<br/>Allocation Requests\"]\n        CLIENT_TO_API[\"External Client → API<br/>Resource Requests\"]\n    end\n    \n    SPECS_PROTOCOL --> COMPUTE_AXON\n    ALLOCATE_PROTOCOL --> COMPUTE_AXON\n    CHALLENGE_PROTOCOL --> COMPUTE_AXON\n    \n    COMPUTE_AXON --> COMPUTE_SUBTENSOR\n    COMPUTE_AXON --> DENDRITE_CLIENT\n    \n    FASTAPI_ROUTES --> API_MIDDLEWARE\n    WEBSOCKET_CONN --> FASTAPI_ROUTES\n    \n    V_TO_M --> SPECS_PROTOCOL\n    V_TO_M --> CHALLENGE_PROTOCOL\n    API_TO_M --> ALLOCATE_PROTOCOL\n    API_TO_M --> DENDRITE_CLIENT\n    CLIENT_TO_API --> FASTAPI_ROUTES",
      "graph TB\n    subgraph \"Local Data Storage\"\n        COMPUTE_DB[(\"ComputeDb<br/>SQLite Database\")]\n        MINER_TABLE[(\"miner table<br/>uid, ss58_address\")]\n        POG_STATS[(\"pog_stats table<br/>GPU performance data\")]\n        ALLOCATION_TABLE[(\"allocation table<br/>active reservations\")]\n        CHALLENGE_DETAILS[(\"challenge_details table<br/>success metrics\")]\n    end\n    \n    subgraph \"Distributed State\"\n        WANDB_RUNS[(\"WandB Validator Runs<br/>Aggregated metrics\")]\n        WANDB_MINERS[(\"WandB Miner Runs<br/>Hardware specifications\")]\n        WANDB_ALLOCATED[(\"Allocated Hotkeys<br/>Resource status\")]\n        WANDB_PENALIZED[(\"Penalized Hotkeys<br/>Blacklist data\")]\n    end\n    \n    subgraph \"Configuration Data\"\n        CONFIG_YAML[(\"config.yaml<br/>GPU performance benchmarks\")]\n        ENV_CONFIG[(\"Environment Variables<br/>API keys, endpoints\")]\n        PROTOCOL_SCHEMAS[(\"Protocol Definitions<br/>Message validation\")]\n    end\n    \n    subgraph \"Data Access Layer\"\n        DB_OPERATIONS[\"Database Operations<br/>compute/utils/db.py\"]\n        WANDB_CLIENT[\"ComputeWandb<br/>compute/wandb/wandb.py\"]\n        CONFIG_LOADER[\"Configuration Loader<br/>load_yaml_config()\"]\n    end\n    \n    COMPUTE_DB --> MINER_TABLE\n    COMPUTE_DB --> POG_STATS\n    COMPUTE_DB --> ALLOCATION_TABLE\n    COMPUTE_DB --> CHALLENGE_DETAILS\n    \n    WANDB_RUNS --> WANDB_MINERS\n    WANDB_RUNS --> WANDB_ALLOCATED\n    WANDB_RUNS --> WANDB_PENALIZED\n    \n    CONFIG_YAML --> ENV_CONFIG\n    CONFIG_YAML --> PROTOCOL_SCHEMAS\n    \n    DB_OPERATIONS --> COMPUTE_DB\n    WANDB_CLIENT --> WANDB_RUNS\n    CONFIG_LOADER --> CONFIG_YAML",
      "graph TB\n    subgraph \"Validator Nodes\"\n        V1[\"Validator Instance 1<br/>neurons/validator.py\"]\n        V2[\"Validator Instance 2<br/>neurons/validator.py\"]\n        VN[\"Validator Instance N<br/>neurons/validator.py\"]\n    end\n    \n    subgraph \"Miner Nodes\"\n        M1[\"Miner Instance 1<br/>neurons/miner.py + Docker\"]\n        M2[\"Miner Instance 2<br/>neurons/miner.py + Docker\"]\n        MN[\"Miner Instance N<br/>neurons/miner.py + Docker\"]\n    end\n    \n    subgraph \"API Services\"\n        API1[\"RegisterAPI Instance 1<br/>neurons/register_api.py\"]\n        API2[\"RegisterAPI Instance 2<br/>neurons/register_api.py\"]\n        LB[\"Load Balancer<br/>Optional\"]\n    end\n    \n    subgraph \"Infrastructure Services\"\n        BT_NETWORK[\"Bittensor Network<br/>Subtensor/Metagraph\"]\n        WANDB_SERVICE[\"WandB Service<br/>Distributed State\"]\n        MONITORING[\"System Monitoring<br/>PM2/Prometheus\"]\n    end\n    \n    subgraph \"Network Configuration\"\n        FIREWALL[\"UFW Firewall<br/>Ports 4444, 8091\"]\n        SSH_ACCESS[\"SSH Access<br/>Container Management\"]\n        DOCKER_RUNTIME[\"Docker + NVIDIA Runtime<br/>GPU Containers\"]\n    end\n    \n    V1 --> BT_NETWORK\n    V2 --> BT_NETWORK\n    VN --> BT_NETWORK\n    \n    M1 --> BT_NETWORK\n    M2 --> BT_NETWORK\n    MN --> BT_NETWORK\n    \n    API1 --> BT_NETWORK\n    API2 --> BT_NETWORK\n    LB --> API1\n    LB --> API2\n    \n    V1 --> WANDB_SERVICE\n    V2 --> WANDB_SERVICE\n    API1 --> WANDB_SERVICE\n    \n    M1 --> DOCKER_RUNTIME\n    M2 --> DOCKER_RUNTIME\n    MN --> DOCKER_RUNTIME\n    \n    FIREWALL --> SSH_ACCESS\n    SSH_ACCESS --> DOCKER_RUNTIME\n    \n    MONITORING --> V1\n    MONITORING --> M1\n    MONITORING --> API1"
    ],
    "potential_frontmatter": {
      "title": "Architecture"
    }
  },
  "/neuralinternet/ni-compute/1.2-installation-and-setup": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/1.2-installation-and-setup",
    "title": "Installation and Setup",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/1.2-installation-and-setup",
    "level": 1,
    "target_astro_path": "/installation-and-setup",
    "main_markdown_content": "# Installation and Setup\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [scripts/installation_script/README.md](scripts/installation_script/README.md)\n- [scripts/installation_script/compute_subnet_installer.sh](scripts/installation_script/compute_subnet_installer.sh)\n\n</details>\n\n\n\nThis document provides comprehensive installation instructions for setting up the NI Compute Subnet on Ubuntu systems. It covers system requirements, dependency installation, wallet configuration, and miner deployment using the automated installation script.\n\nFor information about validator-specific setup procedures, see [Validator System](#2). For details about the API service deployment, see [Resource Allocation API](#4).\n\n## Installation Architecture\n\nThe installation process follows a two-pass approach with clear separation between system-level dependencies and application-level configuration:\n\n```mermaid\ngraph TD\n    START[\"Installation Start\"] --> SYSCHECK[\"System Requirements Check\"]\n    SYSCHECK --> DOCKER[\"Docker Installation\"]\n    DOCKER --> NVIDIA[\"NVIDIA Drivers & Container Runtime\"]\n    NVIDIA --> CUDA[\"CUDA 12.8 Installation\"]\n    CUDA --> BITTENSOR[\"Bittensor Framework\"]\n    BITTENSOR --> REBOOT{\"Reboot Required?\"}\n    \n    REBOOT -->|Yes| REBOOT_SYS[\"System Reboot\"]\n    REBOOT -->|No| WALLET_CHECK[\"Wallet Check\"]\n    REBOOT_SYS --> WALLET_CHECK\n    \n    WALLET_CHECK --> WALLET_EXISTS{\"Wallet Exists?\"}\n    WALLET_EXISTS -->|No| WALLET_CREATE[\"Manual Wallet Creation\"]\n    WALLET_EXISTS -->|Yes| VENV_SETUP[\"Virtual Environment Setup\"]\n    WALLET_CREATE --> VENV_SETUP\n    \n    VENV_SETUP --> DEPS[\"Python Dependencies\"]\n    DEPS --> FIREWALL[\"UFW Firewall Configuration\"]\n    FIREWALL --> ENV_CONFIG[\"Environment Configuration\"]\n    ENV_CONFIG --> PM2_SETUP[\"PM2 Process Manager\"]\n    PM2_SETUP --> MINER_START[\"Miner Process Launch\"]\n    MINER_START --> COMPLETE[\"Installation Complete\"]\n```\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:1-805]()\n\n## System Requirements\n\nThe installation script supports Ubuntu 22.04 and 24.04 with the following requirements:\n\n| Component | Requirement | Detection Method |\n|-----------|-------------|------------------|\n| Operating System | Ubuntu 22.04/24.04 | `uname` check |\n| GPU | NVIDIA GPU | Driver installation |\n| Docker | Latest stable | `docker --version` |\n| CUDA | Version 12.8 | `nvcc --version` |\n| Python | 3.8+ | System package |\n| Node.js | 18.x | Package manager |\n\nThe installer performs automatic detection and installation of missing components through the `docker_installed()`, `nvidia_docker_installed()`, and `cuda_version_installed()` functions.\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:93-95](), [scripts/installation_script/compute_subnet_installer.sh:113-118](), [scripts/installation_script/compute_subnet_installer.sh:154-160](), [scripts/installation_script/compute_subnet_installer.sh:165-188]()\n\n## Installation Script Components\n\nThe `compute_subnet_installer.sh` script orchestrates the entire installation process through modular functions:\n\n```mermaid\ngraph TB\n    subgraph \"System Dependencies\"\n        DOCKER_FUNC[\"docker_installed()\"]\n        NVIDIA_FUNC[\"nvidia_docker_installed()\"] \n        CUDA_FUNC[\"cuda_version_installed()\"]\n        BTCLI_FUNC[\"btcli_installed()\"]\n    end\n    \n    subgraph \"Configuration Functions\"\n        SETUP_NEEDRESTART[\"setup_needrestart()\"]\n        RUN_APT[\"run_apt_get()\"]\n        INSTALL_PKG[\"install_package()\"]\n        PAUSE_USER[\"pause_for_user()\"]\n    end\n    \n    subgraph \"Wallet Management\"\n        WALLET_DIR[\"~/.bittensor/wallets\"]\n        WALLET_CHECK[\"Wallet Existence Check\"]\n    end\n    \n    subgraph \"Environment Setup\"\n        VENV_DIR[\"~/venv\"]\n        ENV_FILE[\".env Configuration\"]\n        WANDB_CONFIG[\"WANDB Integration\"]\n    end\n    \n    subgraph \"Process Management\"\n        PM2_START[\"PM2 Process Launch\"]\n        FIREWALL_CONFIG[\"UFW Firewall Rules\"]\n        MINER_PROCESS[\"neurons/miner.py\"]\n    end\n    \n    DOCKER_FUNC --> NVIDIA_FUNC\n    NVIDIA_FUNC --> CUDA_FUNC\n    CUDA_FUNC --> BTCLI_FUNC\n    BTCLI_FUNC --> WALLET_CHECK\n    WALLET_CHECK --> VENV_DIR\n    VENV_DIR --> ENV_FILE\n    ENV_FILE --> WANDB_CONFIG\n    WANDB_CONFIG --> FIREWALL_CONFIG\n    FIREWALL_CONFIG --> PM2_START\n    PM2_START --> MINER_PROCESS\n```\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:113-203](), [scripts/installation_script/compute_subnet_installer.sh:566-570](), [scripts/installation_script/compute_subnet_installer.sh:681-732](), [scripts/installation_script/compute_subnet_installer.sh:750-770]()\n\n## Installation Process\n\n### Phase 1: System Dependencies\n\nThe installer first establishes the execution environment and installs core system dependencies:\n\n**Environment Detection**\n```bash\n# User and home directory detection\nREAL_USER=$(whoami)\nUSER_NAME=\"$REAL_USER\"\nHOME_DIR=\"$(eval echo \"~$REAL_USER\")\"\n```\n\n**Docker Installation**\nThe script installs Docker CE with NVIDIA container runtime support when not present:\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:97-110](), [scripts/installation_script/compute_subnet_installer.sh:223-249](), [scripts/installation_script/compute_subnet_installer.sh:251-272]()\n\n**CUDA 12.8 Installation**\nVersion-specific CUDA installation based on Ubuntu release:\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:274-330](), [scripts/installation_script/compute_subnet_installer.sh:287-298](), [scripts/installation_script/compute_subnet_installer.sh:299-313]()\n\n### Phase 2: Repository Setup\n\nThe installer locates or clones the compute subnet repository:\n\n```mermaid\ngraph LR\n    GIT_CHECK[\"Git Repository Check\"] --> REPO_ROOT[\"git rev-parse --show-toplevel\"]\n    REPO_ROOT --> SETUP_CHECK[\"setup.py/pyproject.toml Check\"]\n    SETUP_CHECK --> CLONE_REPO[\"git clone ni-compute\"]\n    CLONE_REPO --> CS_PATH[\"CS_PATH Variable Set\"]\n    CS_PATH --> VENV_CREATE[\"Virtual Environment Creation\"]\n```\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:334-395](), [scripts/installation_script/compute_subnet_installer.sh:427-444]()\n\n### Phase 3: Python Environment\n\nVirtual environment creation and dependency installation:\n\n| Step | Command | Configuration File |\n|------|---------|-------------------|\n| Virtual Environment | `python3 -m venv ~/venv` | N/A |\n| Base Dependencies | `pip install -r requirements.txt` | `requirements.txt` |\n| Compute Dependencies | `pip install --no-deps -r requirements-compute.txt` | `requirements-compute.txt` |\n| Editable Install | `pip install -e .` | `setup.py`/`pyproject.toml` |\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:446-464](), [scripts/installation_script/compute_subnet_installer.sh:453-461]()\n\n### Phase 4: Wallet Configuration\n\nThe installer checks for existing Bittensor wallets and provides guidance for wallet creation:\n\n```bash\nWALLET_DIR=\"${HOME}/.bittensor/wallets\"\nhave_wallets=false\nif [ -d \"${WALLET_DIR}\" ] && [ -n \"$(ls -A \"${WALLET_DIR}\" 2>/dev/null)\" ]; then\n  have_wallets=true\nfi\n```\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:566-600]()\n\n### Phase 5: Network Configuration\n\n**Firewall Setup**\nUFW configuration with required ports:\n\n| Port | Protocol | Purpose |\n|------|----------|---------|\n| 22 | TCP | SSH Access |\n| 4444 | TCP | Validator Communication |\n| 8091 | TCP | Default Axon Port |\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:610-655]()\n\n**Environment Configuration**\nThe installer configures the `.env` file with database and API settings:\n\n```bash\n# SQLITE_DB_PATH configuration\nsed -i \"s@^SQLITE_DB_PATH=.*@SQLITE_DB_PATH=\\\"${CS_PATH}/database.db\\\"@\" \"$env_path\"\n\n# WANDB_API_KEY configuration\nsed -i \"s@^WANDB_API_KEY=.*@WANDB_API_KEY=\\\"$WANDB_API_KEY\\\"@\" \"$env_path\"\n```\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:699-732](), [scripts/installation_script/compute_subnet_installer.sh:718-720]()\n\n### Phase 6: Process Management\n\n**PM2 Configuration**\nThe miner process is launched using PM2 with comprehensive environment variables:\n\n```bash\npm2 start \"${VENV_DIR}/bin/python3\" \\\n  --name \"subnet${NETUID}_miner\" \\\n  -- \\\n  \"${CS_PATH}/neurons/miner.py\" \\\n  --netuid \"${NETUID}\" \\\n  --subtensor.network \"${SUBTENSOR_NETWORK}\" \\\n  --wallet.name \"default\" \\\n  --wallet.hotkey \"default\" \\\n  --axon.port \"${AXON_PORT}\"\n```\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:750-770](), [scripts/installation_script/compute_subnet_installer.sh:776-797]()\n\n## Configuration Options\n\n### Network Selection\n\nThe installer supports both mainnet and testnet configurations:\n\n| Network | NetUID | Subtensor Network | Default Port |\n|---------|--------|-------------------|--------------|\n| Mainnet | 27 | `subvortex.info:9944` | 8091 |\n| Testnet | 15 | `test` | 8091 |\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:618-649]()\n\n### Automated Mode\n\nThe installer supports non-interactive execution:\n\n```bash\n./compute_subnet_installer.sh --automated\n```\n\nThis mode uses environment variables for configuration and skips user prompts.\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:24-29](), [scripts/installation_script/compute_subnet_installer.sh:539-561]()\n\n## Verification\n\n### Process Status\n```bash\n# Check PM2 process status\npm2 list\npm2 logs subnet${NETUID}_miner\n\n# Check log files\ntail -f ${CS_PATH}/pm2_out.log\ntail -f ${CS_PATH}/pm2_error.log\n```\n\n### Network Registration\n```bash\n# Register hotkey on network\nbtcli subnet register --wallet.name default --wallet.hotkey default --netuid ${NETUID}\n```\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:773-774](), [scripts/installation_script/README.md:118-122]()\n\n## Troubleshooting\n\n### Common Issues\n\n**Docker Permission Errors**\n```bash\nsudo usermod -aG docker $USER\nsudo systemctl restart docker\n```\n\n**CUDA Environment Issues**\nThe installer automatically configures CUDA paths in `.bashrc`:\n```bash\nexport PATH=/usr/local/cuda-12.8/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH\n```\n\n**Wallet Not Found**\nEnsure wallet exists at `~/.bittensor/wallets/<coldkey>/hotkeys/<hotkey>` before running the second installation pass.\n\nSources: [scripts/installation_script/compute_subnet_installer.sh:131-142](), [scripts/installation_script/compute_subnet_installer.sh:315-326](), [scripts/installation_script/README.md:127-140]()",
    "resolved_links": [
      {
        "text": "scripts/installation_script/README.md",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/README.md",
        "original_deepwiki_href": "scripts/installation_script/README.md",
        "context": "collapsible_aside_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh",
        "original_deepwiki_href": "scripts/installation_script/compute_subnet_installer.sh",
        "context": "collapsible_aside_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:1-805",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:93-95",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:113-118",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:154-160",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:165-188",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:113-203",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:566-570",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:681-732",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:750-770",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:97-110",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:223-249",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:251-272",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:274-330",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:287-298",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:299-313",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:334-395",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:427-444",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:446-464",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:453-461",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:566-600",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:610-655",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:699-732",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:718-720",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:776-797",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:618-649",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:24-29",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:539-561",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:773-774",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/README.md:118-122",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:131-142",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/compute_subnet_installer.sh:315-326",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/installation_script/README.md:127-140",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Validator System",
        "href": "/validator-system#2",
        "original_deepwiki_href": "#2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Resource Allocation API",
        "href": "/resource-allocation-api#4",
        "original_deepwiki_href": "#4",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TD\n    START[\"Installation Start\"] --> SYSCHECK[\"System Requirements Check\"]\n    SYSCHECK --> DOCKER[\"Docker Installation\"]\n    DOCKER --> NVIDIA[\"NVIDIA Drivers & Container Runtime\"]\n    NVIDIA --> CUDA[\"CUDA 12.8 Installation\"]\n    CUDA --> BITTENSOR[\"Bittensor Framework\"]\n    BITTENSOR --> REBOOT{\"Reboot Required?\"}\n    \n    REBOOT -->|Yes| REBOOT_SYS[\"System Reboot\"]\n    REBOOT -->|No| WALLET_CHECK[\"Wallet Check\"]\n    REBOOT_SYS --> WALLET_CHECK\n    \n    WALLET_CHECK --> WALLET_EXISTS{\"Wallet Exists?\"}\n    WALLET_EXISTS -->|No| WALLET_CREATE[\"Manual Wallet Creation\"]\n    WALLET_EXISTS -->|Yes| VENV_SETUP[\"Virtual Environment Setup\"]\n    WALLET_CREATE --> VENV_SETUP\n    \n    VENV_SETUP --> DEPS[\"Python Dependencies\"]\n    DEPS --> FIREWALL[\"UFW Firewall Configuration\"]\n    FIREWALL --> ENV_CONFIG[\"Environment Configuration\"]\n    ENV_CONFIG --> PM2_SETUP[\"PM2 Process Manager\"]\n    PM2_SETUP --> MINER_START[\"Miner Process Launch\"]\n    MINER_START --> COMPLETE[\"Installation Complete\"]",
      "graph TB\n    subgraph \"System Dependencies\"\n        DOCKER_FUNC[\"docker_installed()\"]\n        NVIDIA_FUNC[\"nvidia_docker_installed()\"] \n        CUDA_FUNC[\"cuda_version_installed()\"]\n        BTCLI_FUNC[\"btcli_installed()\"]\n    end\n    \n    subgraph \"Configuration Functions\"\n        SETUP_NEEDRESTART[\"setup_needrestart()\"]\n        RUN_APT[\"run_apt_get()\"]\n        INSTALL_PKG[\"install_package()\"]\n        PAUSE_USER[\"pause_for_user()\"]\n    end\n    \n    subgraph \"Wallet Management\"\n        WALLET_DIR[\"~/.bittensor/wallets\"]\n        WALLET_CHECK[\"Wallet Existence Check\"]\n    end\n    \n    subgraph \"Environment Setup\"\n        VENV_DIR[\"~/venv\"]\n        ENV_FILE[\".env Configuration\"]\n        WANDB_CONFIG[\"WANDB Integration\"]\n    end\n    \n    subgraph \"Process Management\"\n        PM2_START[\"PM2 Process Launch\"]\n        FIREWALL_CONFIG[\"UFW Firewall Rules\"]\n        MINER_PROCESS[\"neurons/miner.py\"]\n    end\n    \n    DOCKER_FUNC --> NVIDIA_FUNC\n    NVIDIA_FUNC --> CUDA_FUNC\n    CUDA_FUNC --> BTCLI_FUNC\n    BTCLI_FUNC --> WALLET_CHECK\n    WALLET_CHECK --> VENV_DIR\n    VENV_DIR --> ENV_FILE\n    ENV_FILE --> WANDB_CONFIG\n    WANDB_CONFIG --> FIREWALL_CONFIG\n    FIREWALL_CONFIG --> PM2_START\n    PM2_START --> MINER_PROCESS",
      "graph LR\n    GIT_CHECK[\"Git Repository Check\"] --> REPO_ROOT[\"git rev-parse --show-toplevel\"]\n    REPO_ROOT --> SETUP_CHECK[\"setup.py/pyproject.toml Check\"]\n    SETUP_CHECK --> CLONE_REPO[\"git clone ni-compute\"]\n    CLONE_REPO --> CS_PATH[\"CS_PATH Variable Set\"]\n    CS_PATH --> VENV_CREATE[\"Virtual Environment Creation\"]"
    ],
    "potential_frontmatter": {
      "title": "Installation and Setup"
    }
  },
  "/neuralinternet/ni-compute/2-validator-system": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/2-validator-system",
    "title": "Validator System",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/2-validator-system",
    "level": 0,
    "target_astro_path": "/validator-system",
    "main_markdown_content": "# Validator System\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/validator.py](neurons/validator.py)\n\n</details>\n\n\n\nThe Validator System is the core component responsible for evaluating miner capabilities, performing proof-of-GPU validation, and setting network weights in the NI Compute Subnet. It orchestrates the entire validation process including hardware verification, performance scoring, and blockchain weight updates.\n\nFor information about the specific proof-of-GPU validation algorithms, see [Proof of GPU](#2.1). For details about the scoring mechanisms, see [Scoring System](#2.2). For database schema and operations, see [Database Operations](#2.3).\n\n## Architecture Overview\n\nThe validator system operates as a continuous validation loop that queries miners, validates their GPU capabilities, calculates performance scores, and updates network weights. The system is built around the `Validator` class which coordinates all validation activities.\n\n### Validator System Components\n\n```mermaid\ngraph TB\n    subgraph \"Validator Core\"\n        ValidatorClass[\"Validator Class<br/>neurons/validator.py\"]\n        Config[\"Configuration<br/>init_config()\"]\n        Prometheus[\"Prometheus Setup<br/>init_prometheus()\"]\n    end\n    \n    subgraph \"Blockchain Interface\"\n        Subtensor[\"ComputeSubnetSubtensor<br/>subtensor connection\"]\n        Metagraph[\"bt.metagraph<br/>network state\"]\n        Wallet[\"bt.wallet<br/>validator identity\"]\n    end\n    \n    subgraph \"Data Layer\"\n        ComputeDb[\"ComputeDb<br/>local database\"]\n        ComputeWandb[\"ComputeWandb<br/>metrics & monitoring\"]\n        ConfigData[\"config.yaml<br/>GPU performance data\"]\n    end\n    \n    subgraph \"Validation Engine\"\n        PoGEngine[\"proof_of_gpu()<br/>GPU validation\"]\n        ScoringEngine[\"sync_scores()<br/>performance scoring\"]\n        WeightSetter[\"set_weights()<br/>blockchain updates\"]\n    end\n    \n    subgraph \"Miner Communication\"\n        AllocateProtocol[\"Allocate Protocol<br/>resource allocation\"]\n        SpecsProtocol[\"Specs Protocol<br/>hardware queries\"]\n        ChallengeProtocol[\"Challenge Protocol<br/>PoW verification\"]\n    end\n    \n    ValidatorClass --> Config\n    ValidatorClass --> Prometheus\n    ValidatorClass --> Subtensor\n    ValidatorClass --> Metagraph\n    ValidatorClass --> Wallet\n    ValidatorClass --> ComputeDb\n    ValidatorClass --> ComputeWandb\n    ValidatorClass --> ConfigData\n    \n    ValidatorClass --> PoGEngine\n    ValidatorClass --> ScoringEngine\n    ValidatorClass --> WeightSetter\n    \n    PoGEngine --> AllocateProtocol\n    ValidatorClass --> SpecsProtocol\n    ValidatorClass --> ChallengeProtocol\n    \n    ScoringEngine --> ComputeDb\n    ScoringEngine --> ComputeWandb\n    WeightSetter --> Subtensor\n```\n\nSources: [neurons/validator.py:70-209](), [neurons/validator.py:130-175]()\n\n### Validation Process Flow\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator\"\n    participant DB as \"ComputeDb\"\n    participant W as \"ComputeWandb\"\n    participant BT as \"Subtensor\"\n    participant M as \"Miner\"\n    \n    Note over V: Initialization Phase\n    V->>DB: Initialize database connection\n    V->>W: Setup WandB monitoring\n    V->>BT: Connect to blockchain\n    V->>V: init_scores()\n    \n    Note over V: Main Validation Loop\n    loop Every Block\n        V->>BT: sync_local() - Update metagraph\n        V->>V: get_queryable() - Filter valid miners\n        \n        alt Every 360 blocks (PoG)\n            V->>V: proof_of_gpu()\n            V->>M: allocate_miner()\n            V->>M: test_miner_gpu()\n            V->>M: deallocate_miner()\n            V->>DB: update_pog_stats()\n        end\n        \n        alt Every 150 blocks (Specs)\n            V->>W: get_specs_wandb()\n            V->>DB: update_miner_details()\n        end\n        \n        alt Every 25 blocks (Status)\n            V->>V: sync_status()\n            V->>W: log_chain_data()\n        end\n        \n        alt Every 100 blocks (Weights)\n            V->>V: sync_scores()\n            V->>V: set_burn_weights()\n            V->>BT: Submit weights to blockchain\n        end\n    end\n```\n\nSources: [neurons/validator.py:1161-1273](), [neurons/validator.py:1192-1202](), [neurons/validator.py:1240-1247]()\n\n## Core Components\n\n### Validator Class\n\nThe `Validator` class is the main orchestrator that manages all validation activities. It maintains state for queryable miners, scores, and validation results.\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `scores` | `torch.Tensor` | Current performance scores for all miners |\n| `stats` | `dict` | Detailed statistics for each miner |\n| `_queryable_uids` | `Dict[int, bt.AxonInfo]` | Valid miners available for validation |\n| `allocated_hotkeys` | `list` | Currently allocated miner hotkeys |\n| `penalized_hotkeys` | `list` | Penalized miner hotkeys |\n\nSources: [neurons/validator.py:70-91](), [neurons/validator.py:84-91]()\n\n### Configuration Management\n\nThe validator uses multiple configuration sources to manage validation parameters:\n\n```mermaid\ngraph LR\n    subgraph \"Configuration Sources\"\n        ArgParser[\"ComputeArgPaser<br/>CLI arguments\"]\n        ConfigYaml[\"config.yaml<br/>GPU performance data\"]\n        EnvVars[\"Environment Variables<br/>system settings\"]\n    end\n    \n    subgraph \"Configuration Properties\"\n        BatchSize[\"validator_specs_batch_size<br/>validator_challenge_batch_size\"]\n        HardwareQuery[\"validator_perform_hardware_query\"]\n        Thresholds[\"validator_whitelist_updated_threshold\"]\n        Blacklists[\"blacklist_hotkeys<br/>blacklist_coldkeys\"]\n    end\n    \n    ArgParser --> BatchSize\n    ArgParser --> HardwareQuery\n    ArgParser --> Thresholds\n    ArgParser --> Blacklists\n    ConfigYaml --> BatchSize\n    EnvVars --> HardwareQuery\n```\n\nSources: [neurons/validator.py:210-235](), [neurons/validator.py:132-147]()\n\n### Queryable Miners Management\n\nThe validator maintains a filtered list of queryable miners based on multiple criteria:\n\n```mermaid\ngraph TD\n    AllMiners[\"All Network Miners<br/>metagraph.neurons\"]\n    \n    subgraph \"Filtering Pipeline\"\n        ValidTensors[\"get_valid_tensors()<br/>IP & blacklist filter\"]\n        FilterAxons[\"filter_axons()<br/>unique IP addresses\"]\n        FilterVersion[\"filter_axon_version()<br/>minimum version check\"]\n    end\n    \n    QueryableMiners[\"_queryable_uids<br/>Dict[int, bt.AxonInfo]\"]\n    \n    AllMiners --> ValidTensors\n    ValidTensors --> FilterAxons\n    FilterAxons --> FilterVersion\n    FilterVersion --> QueryableMiners\n    \n    subgraph \"Blacklist Checks\"\n        BlacklistColdkeys[\"blacklist_coldkeys\"]\n        BlacklistHotkeys[\"blacklist_hotkeys\"]\n        ExploiterKeys[\"exploiters_hotkeys<br/>exploiters_coldkeys\"]\n    end\n    \n    ValidTensors --> BlacklistColdkeys\n    ValidTensors --> BlacklistHotkeys\n    ValidTensors --> ExploiterKeys\n```\n\nSources: [neurons/validator.py:571-579](), [neurons/validator.py:487-515](), [neurons/validator.py:517-544]()\n\n## Validation Process\n\n### Proof-of-GPU Validation\n\nThe proof-of-GPU system allocates miners, tests their GPU capabilities, and verifies performance through cryptographic proofs:\n\n```mermaid\ngraph TD\n    subgraph \"PoG Initialization\"\n        GetQueryable[\"get_queryable()<br/>filter available miners\"]\n        GetAllocated[\"wandb.get_allocated_hotkeys()<br/>skip allocated miners\"]\n        CreateQueue[\"asyncio.Queue<br/>miner processing queue\"]\n    end\n    \n    subgraph \"Miner Testing Pipeline\"\n        AllocateMiner[\"allocate_miner()<br/>RSA key generation\"]\n        SSHConnect[\"paramiko.SSHClient<br/>secure connection\"]\n        HashCheck[\"compute_script_hash()<br/>integrity verification\"]\n        GPUInfo[\"get_remote_gpu_info()<br/>nvidia-smi query\"]\n        Benchmark[\"execute_script_on_miner('benchmark')<br/>performance test\"]\n        MerkleProof[\"execute_script_on_miner('compute')<br/>cryptographic proof\"]\n        VerifyProof[\"verify_responses()<br/>proof validation\"]\n    end\n    \n    subgraph \"Result Processing\"\n        UpdatePoGStats[\"update_pog_stats()<br/>database update\"]\n        SyncScores[\"sync_scores()<br/>recalculate scores\"]\n    end\n    \n    GetQueryable --> GetAllocated\n    GetAllocated --> CreateQueue\n    CreateQueue --> AllocateMiner\n    AllocateMiner --> SSHConnect\n    SSHConnect --> HashCheck\n    HashCheck --> GPUInfo\n    GPUInfo --> Benchmark\n    Benchmark --> MerkleProof\n    MerkleProof --> VerifyProof\n    VerifyProof --> UpdatePoGStats\n    UpdatePoGStats --> SyncScores\n```\n\nSources: [neurons/validator.py:663-787](), [neurons/validator.py:799-948]()\n\n### Scoring System\n\nThe scoring system calculates performance scores based on GPU specifications and reliability metrics:\n\n```mermaid\ngraph LR\n    subgraph \"Score Calculation\"\n        PoGSpecs[\"get_pog_specs()<br/>local GPU data\"]\n        CalcScore[\"calc_score_pog()<br/>performance calculation\"]\n        StatsAllocated[\"stats_allocated<br/>external scores\"]\n        PenalizedCheck[\"penalized_hotkeys<br/>penalty filter\"]\n    end\n    \n    subgraph \"Score Sources\"\n        LocalDB[\"Local Database<br/>own_score: true\"]\n        ExternalWandb[\"WandB Stats<br/>own_score: false\"]\n    end\n    \n    subgraph \"Final Score\"\n        FinalScore[\"stats[uid]['score']<br/>final miner score\"]\n        ReliabilityScore[\"reliability_score<br/>historical performance\"]\n    end\n    \n    PoGSpecs --> CalcScore\n    CalcScore --> LocalDB\n    StatsAllocated --> ExternalWandb\n    LocalDB --> FinalScore\n    ExternalWandb --> FinalScore\n    PenalizedCheck --> FinalScore\n    FinalScore --> ReliabilityScore\n```\n\nSources: [neurons/validator.py:312-402](), [neurons/validator.py:360-386]()\n\n## Database Operations\n\nThe validator uses `ComputeDb` for persistent storage of miner information, validation results, and statistics:\n\n| Table | Purpose | Key Operations |\n|-------|---------|----------------|\n| `miners` | Miner registration data | `select_miners()`, `update_miners()`, `purge_miner_entries()` |\n| `pog_stats` | Proof-of-GPU results | `get_pog_specs()`, `update_pog_stats()` |\n| `stats` | Performance statistics | `retrieve_stats()`, `write_stats()` |\n| `allocation` | Resource allocations | `update_miner_details()`, `get_miner_details()` |\n\nSources: [neurons/validator.py:171-172](), [compute/utils/db.py]()\n\n## Monitoring and Metrics\n\n### WandB Integration\n\nThe validator integrates with Weights & Biases for distributed state management and metrics collection:\n\n```mermaid\ngraph TD\n    subgraph \"WandB Operations\"\n        AllocatedHotkeys[\"update_allocated_hotkeys()<br/>track resource usage\"]\n        MinerSpecs[\"get_miner_specs()<br/>hardware specifications\"]\n        ChainData[\"log_chain_data()<br/>blockchain metrics\"]\n        PenalizedHotkeys[\"get_penalized_hotkeys_checklist_bak()\"]\n    end\n    \n    subgraph \"Metrics Collection\"\n        BlockData[\"Block, Stake, Rank<br/>vTrust, Emission\"]\n        ValidatorStats[\"Validator performance<br/>validation results\"]\n        MinerStats[\"Miner capabilities<br/>GPU specifications\"]\n    end\n    \n    AllocatedHotkeys --> MinerStats\n    MinerSpecs --> MinerStats\n    ChainData --> BlockData\n    PenalizedHotkeys --> ValidatorStats\n```\n\nSources: [neurons/validator.py:290-311](), [neurons/validator.py:594-661](), [neurons/validator.py:1229-1237]()\n\n### Weight Setting\n\nThe validator periodically updates network weights based on calculated scores:\n\n```mermaid\ngraph LR\n    subgraph \"Weight Calculation\"\n        Scores[\"self.scores<br/>miner performance\"]\n        ClampNegative[\"scores[scores < 0] = 0<br/>remove negative scores\"]\n        Normalize[\"torch.nn.functional.normalize()<br/>L1 normalization\"]\n    end\n    \n    subgraph \"Weight Submission\"\n        SetWeights[\"subtensor.set_weights()<br/>blockchain submission\"]\n        BurnWeights[\"set_burn_weights()<br/>burn account allocation\"]\n        VersionKey[\"version_key<br/>__version_as_int__\"]\n    end\n    \n    Scores --> ClampNegative\n    ClampNegative --> Normalize\n    Normalize --> SetWeights\n    Normalize --> BurnWeights\n    SetWeights --> VersionKey\n    BurnWeights --> VersionKey\n```\n\nSources: [neurons/validator.py:1132-1153](), [neurons/validator.py:1101-1131]()",
    "resolved_links": [
      {
        "text": "neurons/validator.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py",
        "original_deepwiki_href": "neurons/validator.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/validator.py:70-209",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:130-175",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1161-1273",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1192-1202",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1240-1247",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:70-91",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:84-91",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:210-235",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:132-147",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:571-579",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:487-515",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:517-544",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:663-787",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:799-948",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:312-402",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:360-386",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:171-172",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:290-311",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:594-661",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1229-1237",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1132-1153",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1101-1131",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Proof of GPU",
        "href": "/validator-system/proof-of-gpu#2.1",
        "original_deepwiki_href": "#2.1",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Scoring System",
        "href": "/validator-system/scoring-system#2.2",
        "original_deepwiki_href": "#2.2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Database Operations",
        "href": "/validator-system/database-operations#2.3",
        "original_deepwiki_href": "#2.3",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"Validator Core\"\n        ValidatorClass[\"Validator Class<br/>neurons/validator.py\"]\n        Config[\"Configuration<br/>init_config()\"]\n        Prometheus[\"Prometheus Setup<br/>init_prometheus()\"]\n    end\n    \n    subgraph \"Blockchain Interface\"\n        Subtensor[\"ComputeSubnetSubtensor<br/>subtensor connection\"]\n        Metagraph[\"bt.metagraph<br/>network state\"]\n        Wallet[\"bt.wallet<br/>validator identity\"]\n    end\n    \n    subgraph \"Data Layer\"\n        ComputeDb[\"ComputeDb<br/>local database\"]\n        ComputeWandb[\"ComputeWandb<br/>metrics & monitoring\"]\n        ConfigData[\"config.yaml<br/>GPU performance data\"]\n    end\n    \n    subgraph \"Validation Engine\"\n        PoGEngine[\"proof_of_gpu()<br/>GPU validation\"]\n        ScoringEngine[\"sync_scores()<br/>performance scoring\"]\n        WeightSetter[\"set_weights()<br/>blockchain updates\"]\n    end\n    \n    subgraph \"Miner Communication\"\n        AllocateProtocol[\"Allocate Protocol<br/>resource allocation\"]\n        SpecsProtocol[\"Specs Protocol<br/>hardware queries\"]\n        ChallengeProtocol[\"Challenge Protocol<br/>PoW verification\"]\n    end\n    \n    ValidatorClass --> Config\n    ValidatorClass --> Prometheus\n    ValidatorClass --> Subtensor\n    ValidatorClass --> Metagraph\n    ValidatorClass --> Wallet\n    ValidatorClass --> ComputeDb\n    ValidatorClass --> ComputeWandb\n    ValidatorClass --> ConfigData\n    \n    ValidatorClass --> PoGEngine\n    ValidatorClass --> ScoringEngine\n    ValidatorClass --> WeightSetter\n    \n    PoGEngine --> AllocateProtocol\n    ValidatorClass --> SpecsProtocol\n    ValidatorClass --> ChallengeProtocol\n    \n    ScoringEngine --> ComputeDb\n    ScoringEngine --> ComputeWandb\n    WeightSetter --> Subtensor",
      "sequenceDiagram\n    participant V as \"Validator\"\n    participant DB as \"ComputeDb\"\n    participant W as \"ComputeWandb\"\n    participant BT as \"Subtensor\"\n    participant M as \"Miner\"\n    \n    Note over V: Initialization Phase\n    V->>DB: Initialize database connection\n    V->>W: Setup WandB monitoring\n    V->>BT: Connect to blockchain\n    V->>V: init_scores()\n    \n    Note over V: Main Validation Loop\n    loop Every Block\n        V->>BT: sync_local() - Update metagraph\n        V->>V: get_queryable() - Filter valid miners\n        \n        alt Every 360 blocks (PoG)\n            V->>V: proof_of_gpu()\n            V->>M: allocate_miner()\n            V->>M: test_miner_gpu()\n            V->>M: deallocate_miner()\n            V->>DB: update_pog_stats()\n        end\n        \n        alt Every 150 blocks (Specs)\n            V->>W: get_specs_wandb()\n            V->>DB: update_miner_details()\n        end\n        \n        alt Every 25 blocks (Status)\n            V->>V: sync_status()\n            V->>W: log_chain_data()\n        end\n        \n        alt Every 100 blocks (Weights)\n            V->>V: sync_scores()\n            V->>V: set_burn_weights()\n            V->>BT: Submit weights to blockchain\n        end\n    end",
      "graph LR\n    subgraph \"Configuration Sources\"\n        ArgParser[\"ComputeArgPaser<br/>CLI arguments\"]\n        ConfigYaml[\"config.yaml<br/>GPU performance data\"]\n        EnvVars[\"Environment Variables<br/>system settings\"]\n    end\n    \n    subgraph \"Configuration Properties\"\n        BatchSize[\"validator_specs_batch_size<br/>validator_challenge_batch_size\"]\n        HardwareQuery[\"validator_perform_hardware_query\"]\n        Thresholds[\"validator_whitelist_updated_threshold\"]\n        Blacklists[\"blacklist_hotkeys<br/>blacklist_coldkeys\"]\n    end\n    \n    ArgParser --> BatchSize\n    ArgParser --> HardwareQuery\n    ArgParser --> Thresholds\n    ArgParser --> Blacklists\n    ConfigYaml --> BatchSize\n    EnvVars --> HardwareQuery",
      "graph TD\n    AllMiners[\"All Network Miners<br/>metagraph.neurons\"]\n    \n    subgraph \"Filtering Pipeline\"\n        ValidTensors[\"get_valid_tensors()<br/>IP & blacklist filter\"]\n        FilterAxons[\"filter_axons()<br/>unique IP addresses\"]\n        FilterVersion[\"filter_axon_version()<br/>minimum version check\"]\n    end\n    \n    QueryableMiners[\"_queryable_uids<br/>Dict[int, bt.AxonInfo]\"]\n    \n    AllMiners --> ValidTensors\n    ValidTensors --> FilterAxons\n    FilterAxons --> FilterVersion\n    FilterVersion --> QueryableMiners\n    \n    subgraph \"Blacklist Checks\"\n        BlacklistColdkeys[\"blacklist_coldkeys\"]\n        BlacklistHotkeys[\"blacklist_hotkeys\"]\n        ExploiterKeys[\"exploiters_hotkeys<br/>exploiters_coldkeys\"]\n    end\n    \n    ValidTensors --> BlacklistColdkeys\n    ValidTensors --> BlacklistHotkeys\n    ValidTensors --> ExploiterKeys",
      "graph TD\n    subgraph \"PoG Initialization\"\n        GetQueryable[\"get_queryable()<br/>filter available miners\"]\n        GetAllocated[\"wandb.get_allocated_hotkeys()<br/>skip allocated miners\"]\n        CreateQueue[\"asyncio.Queue<br/>miner processing queue\"]\n    end\n    \n    subgraph \"Miner Testing Pipeline\"\n        AllocateMiner[\"allocate_miner()<br/>RSA key generation\"]\n        SSHConnect[\"paramiko.SSHClient<br/>secure connection\"]\n        HashCheck[\"compute_script_hash()<br/>integrity verification\"]\n        GPUInfo[\"get_remote_gpu_info()<br/>nvidia-smi query\"]\n        Benchmark[\"execute_script_on_miner('benchmark')<br/>performance test\"]\n        MerkleProof[\"execute_script_on_miner('compute')<br/>cryptographic proof\"]\n        VerifyProof[\"verify_responses()<br/>proof validation\"]\n    end\n    \n    subgraph \"Result Processing\"\n        UpdatePoGStats[\"update_pog_stats()<br/>database update\"]\n        SyncScores[\"sync_scores()<br/>recalculate scores\"]\n    end\n    \n    GetQueryable --> GetAllocated\n    GetAllocated --> CreateQueue\n    CreateQueue --> AllocateMiner\n    AllocateMiner --> SSHConnect\n    SSHConnect --> HashCheck\n    HashCheck --> GPUInfo\n    GPUInfo --> Benchmark\n    Benchmark --> MerkleProof\n    MerkleProof --> VerifyProof\n    VerifyProof --> UpdatePoGStats\n    UpdatePoGStats --> SyncScores",
      "graph LR\n    subgraph \"Score Calculation\"\n        PoGSpecs[\"get_pog_specs()<br/>local GPU data\"]\n        CalcScore[\"calc_score_pog()<br/>performance calculation\"]\n        StatsAllocated[\"stats_allocated<br/>external scores\"]\n        PenalizedCheck[\"penalized_hotkeys<br/>penalty filter\"]\n    end\n    \n    subgraph \"Score Sources\"\n        LocalDB[\"Local Database<br/>own_score: true\"]\n        ExternalWandb[\"WandB Stats<br/>own_score: false\"]\n    end\n    \n    subgraph \"Final Score\"\n        FinalScore[\"stats[uid]['score']<br/>final miner score\"]\n        ReliabilityScore[\"reliability_score<br/>historical performance\"]\n    end\n    \n    PoGSpecs --> CalcScore\n    CalcScore --> LocalDB\n    StatsAllocated --> ExternalWandb\n    LocalDB --> FinalScore\n    ExternalWandb --> FinalScore\n    PenalizedCheck --> FinalScore\n    FinalScore --> ReliabilityScore",
      "graph TD\n    subgraph \"WandB Operations\"\n        AllocatedHotkeys[\"update_allocated_hotkeys()<br/>track resource usage\"]\n        MinerSpecs[\"get_miner_specs()<br/>hardware specifications\"]\n        ChainData[\"log_chain_data()<br/>blockchain metrics\"]\n        PenalizedHotkeys[\"get_penalized_hotkeys_checklist_bak()\"]\n    end\n    \n    subgraph \"Metrics Collection\"\n        BlockData[\"Block, Stake, Rank<br/>vTrust, Emission\"]\n        ValidatorStats[\"Validator performance<br/>validation results\"]\n        MinerStats[\"Miner capabilities<br/>GPU specifications\"]\n    end\n    \n    AllocatedHotkeys --> MinerStats\n    MinerSpecs --> MinerStats\n    ChainData --> BlockData\n    PenalizedHotkeys --> ValidatorStats",
      "graph LR\n    subgraph \"Weight Calculation\"\n        Scores[\"self.scores<br/>miner performance\"]\n        ClampNegative[\"scores[scores < 0] = 0<br/>remove negative scores\"]\n        Normalize[\"torch.nn.functional.normalize()<br/>L1 normalization\"]\n    end\n    \n    subgraph \"Weight Submission\"\n        SetWeights[\"subtensor.set_weights()<br/>blockchain submission\"]\n        BurnWeights[\"set_burn_weights()<br/>burn account allocation\"]\n        VersionKey[\"version_key<br/>__version_as_int__\"]\n    end\n    \n    Scores --> ClampNegative\n    ClampNegative --> Normalize\n    Normalize --> SetWeights\n    Normalize --> BurnWeights\n    SetWeights --> VersionKey\n    BurnWeights --> VersionKey"
    ],
    "potential_frontmatter": {
      "title": "Validator System"
    }
  },
  "/neuralinternet/ni-compute/2.1-proof-of-gpu": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/2.1-proof-of-gpu",
    "title": "Proof of GPU",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/2.1-proof-of-gpu",
    "level": 1,
    "target_astro_path": "/validator-system/proof-of-gpu",
    "main_markdown_content": "# Proof of GPU\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/utils/math.py](compute/utils/math.py)\n- [config.yaml](config.yaml)\n- [neurons/Miner/specs.py](neurons/Miner/specs.py)\n- [neurons/Validator/app_generator.py](neurons/Validator/app_generator.py)\n- [neurons/Validator/calculate_score.py](neurons/Validator/calculate_score.py)\n- [neurons/Validator/database/miner.py](neurons/Validator/database/miner.py)\n- [neurons/Validator/database/pog.py](neurons/Validator/database/pog.py)\n- [neurons/Validator/miner_script_m_merkletree.py](neurons/Validator/miner_script_m_merkletree.py)\n- [neurons/Validator/pog.py](neurons/Validator/pog.py)\n- [neurons/Validator/script.py](neurons/Validator/script.py)\n- [neurons/validator.py](neurons/validator.py)\n\n</details>\n\n\n\n## Overview\n\nProof of GPU (PoG) is a critical verification mechanism in the NI Compute system (Subnet 27) that enables validators to verify the GPU hardware capabilities of miners in the network. This verification is essential to ensure miners possess the computational resources they claim, maintaining the integrity of the decentralized GPU marketplace.\n\nThis document details how validators implement Proof of GPU verification, the benchmarking process, and how these results affect miner scoring within the subnet.\n\n*For information about how scores are calculated based on PoG results, see [Scoring System](#2.2).*\n\n## Core Concepts\n\nProof of GPU employs a series of technical tests to verify:\n\n1. Actual existence of GPU hardware\n2. Number of GPUs available on the miner\n3. Type/model of GPUs (e.g., RTX 3090, A100, etc.)\n4. Performance capabilities of the GPUs\n\nThe system uses a combination of direct hardware information queries, benchmarking performance tests, and cryptographic verification methods to ensure miners cannot falsify their hardware capabilities.\n\nSources: [neurons/validator.py:638-762](), [neurons/Validator/pog.py]()\n\n## Verification Process\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator\"\n    participant M as \"Miner\"\n    participant SSH as \"SSH Connection\"\n    participant GPU as \"Miner's GPU\"\n    \n    V->>M: Request allocation (for testing)\n    M-->>V: Return SSH connection details\n    V->>SSH: Establish connection\n    \n    Note over V,SSH: Integrity Verification Phase\n    V->>SSH: Hash script\n    SSH-->>V: Return script hash\n    V->>V: Verify script integrity\n    \n    Note over V,SSH: Hardware Detection Phase\n    V->>SSH: Request GPU information\n    SSH->>GPU: Query with nvidia-smi\n    GPU-->>SSH: Return GPU count and types\n    SSH-->>V: Return GPU information\n    \n    Note over V,SSH: Benchmarking Phase\n    V->>SSH: Execute benchmark operations\n    SSH->>GPU: Run matrix operations (FP16/FP32)\n    GPU-->>SSH: Return computation timings\n    SSH-->>V: Return benchmark results\n    V->>V: Calculate TFLOPS\n    V->>V: Identify GPU model based on performance\n    \n    Note over V,SSH: Merkle Proof Verification\n    V->>SSH: Send random seeds\n    SSH->>GPU: Run matrix calculations with seeds\n    GPU-->>SSH: Calculate Merkle tree of results\n    SSH-->>V: Return root hashes\n    V->>SSH: Challenge with random indices\n    SSH->>GPU: Generate proofs for indices\n    GPU-->>SSH: Return proofs\n    SSH-->>V: Return proofs\n    V->>V: Verify Merkle proofs\n    \n    V->>M: Deallocate resources\n    V->>V: Record GPU specifications and score\n```\n\nThe above diagram illustrates the complete PoG verification process between a validator and miner.\n\nSources: [neurons/validator.py:774-923](), [neurons/Validator/pog.py]()\n\n## Technical Implementation\n\n### 1. Allocation and Connection\n\nThe validator first allocates the miner's resources temporarily for testing purposes:\n\n1. The validator generates an RSA key pair for secure communication\n2. It requests allocation from the miner with minimal resource requirements\n3. If allocation succeeds, the validator receives SSH connection details\n4. An SSH connection is established to the miner's container\n\nThis process ensures validators can perform tests in a controlled environment.\n\nSources: [neurons/validator.py:924-987]()\n\n### 2. Script Integrity Verification\n\nTo prevent miners from tampering with the benchmarking script:\n\n1. The validator computes a hash of the local benchmarking script\n2. The script is sent to the miner and a hash is computed remotely\n3. The validator compares the local and remote hashes\n4. If they don't match, the verification fails immediately\n\n```mermaid\nflowchart TD\n    A[\"Compute local hash\n    (compute_script_hash)\"] --> B[\"Send script to miner\n    (send_script_and_request_hash)\"]\n    B --> C[\"Receive remote hash\"]\n    C --> D{\"Hashes match?\"}\n    D -->|\"Yes\"| E[\"Continue verification\"]\n    D -->|\"No\"| F[\"Fail verification\"]\n```\n\nSources: [neurons/validator.py:820-827](), [neurons/Validator/pog.py]()\n\n### 3. GPU Detection and Benchmarking\n\nThe validator performs direct hardware detection and benchmarking tests:\n\n1. Query GPU information using NVIDIA tools on the miner\n2. Execute matrix multiplication benchmarks in both FP16 and FP32 precision\n3. Measure execution time and calculate TFLOPS (Tera Floating-Point Operations Per Second)\n4. Identify GPU model based on performance metrics and reported hardware information\n\nThe benchmarking uses specially designed tests that:\n- Must run on GPUs (cannot be efficiently faked with CPUs)\n- Produce consistent results for specific GPU models\n- Scale with the available GPU memory\n\nSources: [neurons/validator.py:828-858](), [neurons/Validator/pog.py]()\n\n### 4. Merkle Proof Verification\n\nTo cryptographically verify that the benchmarking was actually performed:\n\n1. The validator sends random seeds to the miner\n2. The miner computes large matrices using these seeds\n3. The miner builds a Merkle tree from the computation results\n4. The miner returns the Merkle root hashes\n5. The validator requests proofs for random elements in the matrices\n6. The miner provides Merkle proofs for these elements\n7. The validator verifies the proofs against the root hashes\n\n```mermaid\nflowchart TD\n    A[\"Send random seeds to miner\n    (send_seeds)\"] --> B[\"Miner computes matrices\n    (execute_script_on_miner)\"]\n    B --> C[\"Miner builds Merkle trees\"]\n    C --> D[\"Receive root hashes\n    (parse_merkle_output)\"]\n    D --> E[\"Send challenge indices\n    (send_challenge_indices)\"]\n    E --> F[\"Receive Merkle proofs\n    (receive_responses)\"]\n    F --> G[\"Verify proofs\n    (verify_responses)\"]\n    G --> H{\"Verification successful?\"}\n    H -->|\"Yes\"| I[\"Record GPU specifications\"]\n    H -->|\"No\"| J[\"Fail verification\"]\n```\n\nThis cryptographic verification ensures the miner cannot precompute results or falsify benchmarks.\n\nSources: [neurons/validator.py:859-908](), [neurons/Validator/pog.py]()\n\n## System Architecture\n\nThe PoG system is implemented across several components in the codebase:\n\n```mermaid\ngraph TD\n    subgraph \"Validator System\"\n        V[\"validator.py\"] --> POG[\"Validator/pog.py\"]\n        V --> CALC[\"Validator/calculate_pow_score.py\"]\n        POG --> SH[\"send_script_and_request_hash()\"]\n        POG --> BM[\"parse_benchmark_output()\"]\n        POG --> MK[\"verify_merkle_proof_row()\"]\n        CALC --> CS[\"calc_score_pog()\"]\n    end\n    \n    subgraph \"Miner Node\"\n        MS[\"Miner Script\"] --> GPU[\"GPU Hardware\"]\n        MS --> CUDA[\"CUDA Operations\"]\n        MS --> MT[\"Merkle Tree Generation\"]\n    end\n    \n    subgraph \"Database\"\n        DB[\"ComputeDb\"] --> PST[\"POG Stats Table\"]\n        V --> DB\n    end\n    \n    V <--> SSH[\"SSH Connection\"]\n    SSH <--> MS\n```\n\nSources: [neurons/validator.py:638-762](), [neurons/Validator/pog.py](), [neurons/Validator/database/pog.py]()\n\n## GPU Scoring and Identification\n\nThe system identifies GPU models based on their performance characteristics:\n\n1. Benchmark results produce FP16 and FP32 TFLOPS measurements\n2. VRAM capacity is detected and reported\n3. These metrics are compared against known values for different GPU models\n4. A tolerance system allows for some variation in benchmark results\n5. The identified GPU type and count are stored in the database\n\nThe identification process uses a configuration file that defines performance expectations for different GPU models. The `identify_gpu` function matches the measured performance against these known profiles.\n\nSources: [neurons/Validator/pog.py](), [neurons/Validator/calculate_pow_score.py]()\n\n## Database Integration\n\nProof of GPU results are stored in a database for:\n\n1. Persistent tracking of miner capabilities\n2. Input into the scoring system\n3. Historical analysis of network hardware\n\nKey database functions:\n- `get_pog_specs`: Retrieves stored GPU specifications for a specific miner\n- `update_pog_stats`: Updates the database with new proof results\n- `retrieve_stats`: Gets statistics for all miners\n\nSources: [neurons/validator.py:343-349](), [neurons/Validator/database/pog.py]()\n\n## Scheduling and Resource Management\n\nThe PoG system includes intelligent scheduling to avoid overwhelming the network:\n\n1. Tests are performed periodically (approximately every ~360 blocks)\n2. Random delays are added to prevent network congestion\n3. Concurrent testing is limited to a configurable number of miners\n4. Allocated miners are excluded from testing to avoid disrupting active services\n5. Failed tests can be retried a configurable number of times\n\n```mermaid\nflowchart TD\n    A[\"Validator Start\"] --> B[\"Block check \n    (current_block % block_next_pog == 0)\"]\n    B -->|\"Yes\"| C[\"Schedule next POG\n    (block_next_pog = current_block + 360)\"]\n    C --> D[\"Create async task\n    (proof_of_gpu)\"]\n    D --> E[\"Random delay\n    (0-1200 seconds)\"]\n    E --> F[\"Initialize worker pool\"]\n    F --> G[\"Queue miners for testing\"]\n    G --> H{\"Queue empty?\"}\n    H -->|\"No\"| I[\"Test miner GPU\n    (test_miner_gpu)\"]\n    I --> J{\"Test successful?\"}\n    J -->|\"Yes\"| K[\"Record results\"]\n    J -->|\"No\"| L{\"Retry limit reached?\"}\n    L -->|\"No\"| M[\"Add to retry queue\"]\n    L -->|\"Yes\"| N[\"Record failure\"]\n    M --> H\n    K --> H\n    N --> H\n    H -->|\"Yes\"| O[\"Complete POG cycle\"]\n    O --> P[\"Sync scores\"]\n```\n\nSources: [neurons/validator.py:638-762](), [neurons/validator.py:1169-1176]()\n\n## Integration with Scoring System\n\nThe PoG results directly influence miner scoring:\n\n1. Successful verification stores GPU type and count in the database\n2. The scoring system retrieves this data when calculating miner scores\n3. Miners with more powerful/numerous GPUs receive higher scores\n4. These scores influence the weights set on the blockchain\n5. Weights determine reward distribution in the subnet\n\nIf a miner fails PoG verification or has no GPUs detected, they receive a score of 0 for GPU capabilities.\n\nSources: [neurons/validator.py:343-366](), [neurons/Validator/calculate_pow_score.py]()\n\n## Security Considerations\n\nThe PoG system includes several security measures:\n\n1. Script integrity verification prevents tampering with the benchmarking code\n2. Random seeds prevent precomputation of results\n3. Merkle proofs cryptographically verify computational results\n4. Performance-based verification makes it difficult to simulate GPUs with CPUs\n5. SSH connections are secured with proper authentication\n\nThese measures collectively ensure that miners cannot easily falsify their hardware capabilities.\n\nSources: [neurons/validator.py:774-923](), [neurons/Validator/pog.py]()\n\n## Table of GPU Identification Parameters\n\nThe following table illustrates examples of how different GPU models might be identified (actual values may vary):\n\n| GPU Model | Typical FP16 TFLOPS | Typical FP32 TFLOPS | VRAM (GB) | Score Multiplier |\n|-----------|---------------------|---------------------|-----------|------------------|\n| RTX 3090  | 35-40               | 18-22               | 24        | High             |\n| RTX 3080  | 28-33               | 14-18               | 10        | Medium-High      |\n| A100      | 75-85               | 18-22               | 40/80     | Very High        |\n| V100      | 55-65               | 14-18               | 16/32     | High             |\n| T4        | 8-12                | 4-6                 | 16        | Medium           |\n| K80       | 4-6                 | 2-3                 | 12        | Low              |\n\nThe system uses tolerance pairs to account for variations in benchmark results across different environments and configurations.\n\nSources: [neurons/Validator/calculate_pow_score.py](), [neurons/validator.py:856-858]()\n\n## Conclusion\n\nProof of GPU is a critical component of the NI Compute subnet that provides cryptographic assurance of miners' hardware capabilities. By combining hardware detection, performance benchmarking, and cryptographic verification, the system maintains the integrity of the marketplace and ensures that rewards are distributed fairly based on actual GPU resources contributed to the network.",
    "resolved_links": [
      {
        "text": "compute/utils/math.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/math.py",
        "original_deepwiki_href": "compute/utils/math.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "config.yaml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml",
        "original_deepwiki_href": "config.yaml",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Miner/specs.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/specs.py",
        "original_deepwiki_href": "neurons/Miner/specs.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/app_generator.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/app_generator.py",
        "original_deepwiki_href": "neurons/Validator/app_generator.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/calculate_score.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_score.py",
        "original_deepwiki_href": "neurons/Validator/calculate_score.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/database/miner.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/miner.py",
        "original_deepwiki_href": "neurons/Validator/database/miner.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/database/pog.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py",
        "original_deepwiki_href": "neurons/Validator/database/pog.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/miner_script_m_merkletree.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/miner_script_m_merkletree.py",
        "original_deepwiki_href": "neurons/Validator/miner_script_m_merkletree.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/pog.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py",
        "original_deepwiki_href": "neurons/Validator/pog.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/script.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/script.py",
        "original_deepwiki_href": "neurons/Validator/script.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/validator.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py",
        "original_deepwiki_href": "neurons/validator.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/validator.py:638-762",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/pog.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:774-923",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:924-987",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:820-827",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:828-858",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:859-908",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/pog.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/calculate_pow_score.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:343-349",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1169-1176",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:343-366",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:856-858",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Scoring System",
        "href": "/validator-system/scoring-system#2.2",
        "original_deepwiki_href": "#2.2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "sequenceDiagram\n    participant V as \"Validator\"\n    participant M as \"Miner\"\n    participant SSH as \"SSH Connection\"\n    participant GPU as \"Miner's GPU\"\n    \n    V->>M: Request allocation (for testing)\n    M-->>V: Return SSH connection details\n    V->>SSH: Establish connection\n    \n    Note over V,SSH: Integrity Verification Phase\n    V->>SSH: Hash script\n    SSH-->>V: Return script hash\n    V->>V: Verify script integrity\n    \n    Note over V,SSH: Hardware Detection Phase\n    V->>SSH: Request GPU information\n    SSH->>GPU: Query with nvidia-smi\n    GPU-->>SSH: Return GPU count and types\n    SSH-->>V: Return GPU information\n    \n    Note over V,SSH: Benchmarking Phase\n    V->>SSH: Execute benchmark operations\n    SSH->>GPU: Run matrix operations (FP16/FP32)\n    GPU-->>SSH: Return computation timings\n    SSH-->>V: Return benchmark results\n    V->>V: Calculate TFLOPS\n    V->>V: Identify GPU model based on performance\n    \n    Note over V,SSH: Merkle Proof Verification\n    V->>SSH: Send random seeds\n    SSH->>GPU: Run matrix calculations with seeds\n    GPU-->>SSH: Calculate Merkle tree of results\n    SSH-->>V: Return root hashes\n    V->>SSH: Challenge with random indices\n    SSH->>GPU: Generate proofs for indices\n    GPU-->>SSH: Return proofs\n    SSH-->>V: Return proofs\n    V->>V: Verify Merkle proofs\n    \n    V->>M: Deallocate resources\n    V->>V: Record GPU specifications and score",
      "flowchart TD\n    A[\"Compute local hash\n    (compute_script_hash)\"] --> B[\"Send script to miner\n    (send_script_and_request_hash)\"]\n    B --> C[\"Receive remote hash\"]\n    C --> D{\"Hashes match?\"}\n    D -->|\"Yes\"| E[\"Continue verification\"]\n    D -->|\"No\"| F[\"Fail verification\"]",
      "flowchart TD\n    A[\"Send random seeds to miner\n    (send_seeds)\"] --> B[\"Miner computes matrices\n    (execute_script_on_miner)\"]\n    B --> C[\"Miner builds Merkle trees\"]\n    C --> D[\"Receive root hashes\n    (parse_merkle_output)\"]\n    D --> E[\"Send challenge indices\n    (send_challenge_indices)\"]\n    E --> F[\"Receive Merkle proofs\n    (receive_responses)\"]\n    F --> G[\"Verify proofs\n    (verify_responses)\"]\n    G --> H{\"Verification successful?\"}\n    H -->|\"Yes\"| I[\"Record GPU specifications\"]\n    H -->|\"No\"| J[\"Fail verification\"]",
      "graph TD\n    subgraph \"Validator System\"\n        V[\"validator.py\"] --> POG[\"Validator/pog.py\"]\n        V --> CALC[\"Validator/calculate_pow_score.py\"]\n        POG --> SH[\"send_script_and_request_hash()\"]\n        POG --> BM[\"parse_benchmark_output()\"]\n        POG --> MK[\"verify_merkle_proof_row()\"]\n        CALC --> CS[\"calc_score_pog()\"]\n    end\n    \n    subgraph \"Miner Node\"\n        MS[\"Miner Script\"] --> GPU[\"GPU Hardware\"]\n        MS --> CUDA[\"CUDA Operations\"]\n        MS --> MT[\"Merkle Tree Generation\"]\n    end\n    \n    subgraph \"Database\"\n        DB[\"ComputeDb\"] --> PST[\"POG Stats Table\"]\n        V --> DB\n    end\n    \n    V <--> SSH[\"SSH Connection\"]\n    SSH <--> MS",
      "flowchart TD\n    A[\"Validator Start\"] --> B[\"Block check \n    (current_block % block_next_pog == 0)\"]\n    B -->|\"Yes\"| C[\"Schedule next POG\n    (block_next_pog = current_block + 360)\"]\n    C --> D[\"Create async task\n    (proof_of_gpu)\"]\n    D --> E[\"Random delay\n    (0-1200 seconds)\"]\n    E --> F[\"Initialize worker pool\"]\n    F --> G[\"Queue miners for testing\"]\n    G --> H{\"Queue empty?\"}\n    H -->|\"No\"| I[\"Test miner GPU\n    (test_miner_gpu)\"]\n    I --> J{\"Test successful?\"}\n    J -->|\"Yes\"| K[\"Record results\"]\n    J -->|\"No\"| L{\"Retry limit reached?\"}\n    L -->|\"No\"| M[\"Add to retry queue\"]\n    L -->|\"Yes\"| N[\"Record failure\"]\n    M --> H\n    K --> H\n    N --> H\n    H -->|\"Yes\"| O[\"Complete POG cycle\"]\n    O --> P[\"Sync scores\"]"
    ],
    "potential_frontmatter": {
      "title": "Proof of GPU"
    }
  },
  "/neuralinternet/ni-compute/2.2-scoring-system": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/2.2-scoring-system",
    "title": "Scoring System",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/2.2-scoring-system",
    "level": 1,
    "target_astro_path": "/validator-system/scoring-system",
    "main_markdown_content": "# Scoring System\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/Validator/calculate_pow_score.py](neurons/Validator/calculate_pow_score.py)\n- [neurons/validator.py](neurons/validator.py)\n\n</details>\n\n\n\nThe scoring system evaluates miner performance based on their GPU capabilities and proof verification results. It calculates normalized scores that determine each miner's weight in the network's incentive mechanism. The system integrates Proof-of-GPU validation results with configurable GPU performance metrics to assign fair scores across the network.\n\nFor information about the Proof-of-GPU validation process that generates the data used in scoring, see [Proof of GPU](#2.1). For details about how scores are stored and retrieved, see [Database Operations](#2.3).\n\n## Score Calculation Architecture\n\nThe scoring system operates through several interconnected components that collect GPU performance data, calculate scores, and synchronize results across the network.\n\n```mermaid\nflowchart TD\n    subgraph \"Data Sources\"\n        POG[\"Proof-of-GPU Results<br/>test_miner_gpu()\"]\n        WANDB[\"WandB Distributed State<br/>get_stats_allocated()\"]\n        CONFIG[\"GPU Performance Config<br/>config.yaml\"]\n        LOCALDB[\"Local Database<br/>pog_stats table\"]\n    end\n    \n    subgraph \"Score Calculation Engine\"\n        CALC[\"calc_score_pog()<br/>calculate_pow_score.py\"]\n        SYNC[\"sync_scores()<br/>validator.py\"]\n        NORM[\"Score Normalization<br/>normalize()\"]\n    end\n    \n    subgraph \"Score Storage & Distribution\"\n        STATS[\"self.stats dict<br/>Miner Statistics\"]\n        SCORES[\"self.scores Tensor<br/>PyTorch Tensor\"]\n        WEIGHTS[\"Network Weights<br/>set_weights()\"]\n    end\n    \n    POG --> CALC\n    CONFIG --> CALC\n    LOCALDB --> SYNC\n    WANDB --> SYNC\n    \n    CALC --> NORM\n    SYNC --> STATS\n    STATS --> SCORES\n    \n    SCORES --> WEIGHTS\n    \n    NORM -.-> STATS\n```\n\n**Score Calculation Flow**\nThe system processes GPU specifications through multiple stages to produce final network weights that determine miner rewards.\n\nSources: [neurons/validator.py:312-442](), [neurons/Validator/calculate_pow_score.py:35-63]()\n\n## GPU Performance Scoring\n\nThe core scoring mechanism evaluates miners based on their GPU hardware capabilities using predefined performance benchmarks and real-time validation results.\n\n### Score Calculation Formula\n\nThe `calc_score_pog` function implements the primary scoring algorithm:\n\n```mermaid\nflowchart LR\n    subgraph \"Input Parameters\"\n        GPU_SPECS[\"gpu_specs<br/>{gpu_name, num_gpus}\"]\n        CONFIG[\"config_data<br/>gpu_performance.gpu_scores\"]\n        ALLOCATED[\"allocated_hotkeys<br/>Active Allocations\"]\n    end\n    \n    subgraph \"Score Calculation\"\n        MAX_CALC[\"Calculate max_score<br/>max_gpu * 8\"]\n        FACTOR[\"score_factor = 100/max_score\"]\n        BASE_SCORE[\"base_score = gpu_score * num_gpus * factor\"]\n        ALLOCATION[\"Allocation Multiplier<br/>1.0 (no bonus)\"]\n        NORMALIZE[\"normalize(score, 0, 100)\"]\n    end\n    \n    GPU_SPECS --> BASE_SCORE\n    CONFIG --> MAX_CALC\n    CONFIG --> BASE_SCORE\n    MAX_CALC --> FACTOR\n    FACTOR --> BASE_SCORE\n    BASE_SCORE --> ALLOCATION\n    ALLOCATED --> ALLOCATION\n    ALLOCATION --> NORMALIZE\n```\n\n**GPU Score Calculation Pipeline**\nThe scoring formula normalizes GPU performance against the maximum possible score in the network.\n\n| Parameter | Description | Range |\n|-----------|-------------|-------|\n| `gpu_name` | GPU model identifier | String key from config |\n| `num_gpus` | Number of GPUs (capped) | 1-8 |\n| `gpu_score` | Base performance score | From config.yaml |\n| `score_factor` | Normalization factor | 100/max_possible_score |\n\nSources: [neurons/Validator/calculate_pow_score.py:35-63]()\n\n### GPU Performance Configuration\n\nThe system uses a configuration-driven approach to define GPU performance scores:\n\n```mermaid\ngraph TD\n    subgraph \"config.yaml Structure\"\n        GPU_PERF[\"gpu_performance\"]\n        GPU_SCORES[\"gpu_scores<br/>{GPU_MODEL: score}\"]\n        GPU_TOL[\"gpu_tolerance_pairs<br/>Performance Tolerances\"]\n    end\n    \n    subgraph \"Score Processing\"\n        MAX_GPU[\"max(gpu_scores.values())\"]\n        MAX_SCORE[\"max_score = max_gpu * 8\"]\n        FACTOR[\"score_factor = 100 / max_score\"]\n    end\n    \n    GPU_PERF --> GPU_SCORES\n    GPU_PERF --> GPU_TOL\n    GPU_SCORES --> MAX_GPU\n    MAX_GPU --> MAX_SCORE\n    MAX_SCORE --> FACTOR\n```\n\n**Configuration-Based Scoring**\nGPU performance scores are defined in configuration files and used to normalize miner capabilities.\n\nSources: [neurons/Validator/calculate_pow_score.py:37-42]()\n\n## Score Synchronization Process\n\nThe validator synchronizes scores across multiple data sources to maintain consistent network state and handle distributed validation scenarios.\n\n```mermaid\nsequenceDiagram\n    participant V as Validator\n    participant DB as ComputeDb\n    participant W as WandB\n    participant S as self.stats\n    participant T as self.scores\n    \n    Note over V,T: Score Synchronization Cycle\n    V->>DB: retrieve_stats()\n    V->>W: get_allocated_hotkeys()\n    V->>W: get_stats_allocated()\n    V->>W: get_penalized_hotkeys_checklist_bak()\n    \n    loop For each UID\n        V->>V: Check if uid in queryable_uids\n        alt UID not queryable\n            V->>S: Set score = 0, own_score = True\n            V->>T: scores[uid] = 0\n            V->>DB: DELETE FROM pog_stats\n        else UID queryable\n            V->>DB: get_pog_specs(hotkey)\n            alt Local specs found\n                V->>V: calc_score_pog(gpu_specs)\n                V->>S: Set own_score = True\n            else No local specs\n                V->>W: Use stats_allocated fallback\n                V->>S: Set own_score = False\n            end\n            V->>S: Apply penalty if hotkey penalized\n            V->>T: scores[uid] = calculated_score\n        end\n    end\n    \n    V->>DB: write_stats(stats)\n```\n\n**Score Synchronization Sequence**\nThe validator coordinates between local database, WandB distributed state, and in-memory score tracking.\n\nSources: [neurons/validator.py:312-442]()\n\n### Local vs External Score Sources\n\nThe system handles both local Proof-of-GPU results and external score data from other validators:\n\n| Score Source | Priority | Indicator | Data Source |\n|--------------|----------|-----------|-------------|\n| Local PoG Results | High | `own_score: True` | Local `pog_stats` table |\n| External Validator Data | Low | `own_score: False` | WandB `stats_allocated` |\n| Penalized Miners | Override | `score: 0` | WandB penalized list |\n| Non-queryable Miners | Override | `score: 0` | Filtered queryable set |\n\nSources: [neurons/validator.py:360-382]()\n\n## Weight Setting and Network Integration\n\nThe scoring system converts calculated scores into network weights that determine miner rewards in the Bittensor incentive mechanism.\n\n```mermaid\nflowchart TD\n    subgraph \"Score Processing\"\n        SCORES[\"self.scores<br/>PyTorch Tensor\"]\n        CLAMP[\"scores[scores < 0] = 0<br/>Remove Negatives\"]\n        NORMALIZE[\"torch.nn.functional.normalize<br/>p=1.0, dim=0\"]\n    end\n    \n    subgraph \"Weight Setting\"\n        WEIGHTS[\"Normalized Weights<br/>Sum = 1.0\"]\n        SUBTENSOR[\"subtensor.set_weights()\"]\n        BLOCKCHAIN[\"Bittensor Blockchain<br/>Network State\"]\n    end\n    \n    subgraph \"Alternative: Burn Weights\"\n        BURN_UID[\"get_burn_uid()<br/>Subnet Owner UID\"]\n        BURN_WEIGHT[\"Single Weight = 1.0<br/>100% to Burn Account\"]\n        BURN_SET[\"set_burn_weights()\"]\n    end\n    \n    SCORES --> CLAMP\n    CLAMP --> NORMALIZE\n    NORMALIZE --> WEIGHTS\n    WEIGHTS --> SUBTENSOR\n    SUBTENSOR --> BLOCKCHAIN\n    \n    BURN_UID --> BURN_WEIGHT\n    BURN_WEIGHT --> BURN_SET\n    BURN_SET --> BLOCKCHAIN\n```\n\n**Weight Setting Architecture**\nThe system converts scores to blockchain weights through normalization and clamping operations.\n\n### Weight Setting Process\n\nThe validator implements two weight setting strategies:\n\n1. **Standard Weight Setting** (`set_weights`):\n   - Clamps negative scores to zero\n   - L1-normalizes scores to sum to 1.0\n   - Distributes weights across all miners\n\n2. **Burn Weight Setting** (`set_burn_weights`):\n   - Assigns 100% weight to subnet owner (burn account)\n   - Used as alternative to standard distribution\n\nSources: [neurons/validator.py:1132-1154](), [neurons/validator.py:1101-1131]()\n\n### Weight Update Schedule\n\nWeights are updated periodically based on the `weights_rate_limit` configuration:\n\n| Event | Block Interval | Description |\n|-------|----------------|-------------|\n| Weight Setting | `weights_rate_limit` | Update network weights |\n| Score Sync | Every weight update | Recalculate all scores |\n| Block Tracking | Continuous | Prevent duplicate operations |\n\nThe validator tracks the last update block and ensures weights are only set once per interval to comply with network rate limits.\n\nSources: [neurons/validator.py:1240-1247]()\n\n## Score Statistics and Monitoring\n\nThe system provides comprehensive statistics and monitoring for score calculation and distribution across the network.\n\n```mermaid\ngraph TD\n    subgraph \"Statistics Collection\"\n        STATS_DICT[\"self.stats Dictionary<br/>Per-UID Statistics\"]\n        MINER_DETAILS[\"Miner Details<br/>{hotkey, allocated, score, gpu_specs}\"]\n        RELIABILITY[\"reliability_score<br/>Historical Performance\"]\n    end\n    \n    subgraph \"Monitoring Output\"\n        LOG_SUMMARY[\"Miner Stats Summary<br/>Formatted Log Output\"]\n        WANDB_METRICS[\"WandB Metrics<br/>Chain Data Logging\"]\n        ALLOCATION_SYNC[\"Allocation Synchronization<br/>update_allocation_wandb()\"]\n    end\n    \n    subgraph \"Score Distribution Analysis\"\n        GPU_PARSING[\"GPU Spec Display<br/>num_gpus x gpu_name\"]\n        SCORE_FORMAT[\"Score Formatting<br/>2 decimal places\"]\n        SOURCE_TRACKING[\"Source Identification<br/>Local vs External\"]\n    end\n    \n    STATS_DICT --> MINER_DETAILS\n    MINER_DETAILS --> LOG_SUMMARY\n    MINER_DETAILS --> GPU_PARSING\n    MINER_DETAILS --> SCORE_FORMAT\n    \n    LOG_SUMMARY --> MONITORING_OUTPUT\n    WANDB_METRICS --> MONITORING_OUTPUT\n    ALLOCATION_SYNC --> MONITORING_OUTPUT\n```\n\n**Statistics and Monitoring Pipeline**\nThe system provides detailed visibility into score calculation and distribution across miners.\n\n### Statistics Structure\n\nEach miner's statistics include comprehensive performance and allocation data:\n\n```python\nself.stats[uid] = {\n    \"hotkey\": hotkey,\n    \"allocated\": hotkey in allocated_hotkeys,\n    \"own_score\": bool,  # True if local PoG, False if external\n    \"score\": calculated_score * 100,  # Scaled to 0-100\n    \"gpu_specs\": gpu_specifications,\n    \"reliability_score\": historical_performance\n}\n```\n\nThe system outputs formatted statistics showing miner performance across the network with fixed-width columns for easy reading.\n\nSources: [neurons/validator.py:406-442]()",
    "resolved_links": [
      {
        "text": "neurons/Validator/calculate_pow_score.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py",
        "original_deepwiki_href": "neurons/Validator/calculate_pow_score.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/validator.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py",
        "original_deepwiki_href": "neurons/validator.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/validator.py:312-442",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/calculate_pow_score.py:35-63",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/calculate_pow_score.py:37-42",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:360-382",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1132-1154",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1101-1131",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:1240-1247",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:406-442",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Proof of GPU",
        "href": "/validator-system/proof-of-gpu#2.1",
        "original_deepwiki_href": "#2.1",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Database Operations",
        "href": "/validator-system/database-operations#2.3",
        "original_deepwiki_href": "#2.3",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "flowchart TD\n    subgraph \"Data Sources\"\n        POG[\"Proof-of-GPU Results<br/>test_miner_gpu()\"]\n        WANDB[\"WandB Distributed State<br/>get_stats_allocated()\"]\n        CONFIG[\"GPU Performance Config<br/>config.yaml\"]\n        LOCALDB[\"Local Database<br/>pog_stats table\"]\n    end\n    \n    subgraph \"Score Calculation Engine\"\n        CALC[\"calc_score_pog()<br/>calculate_pow_score.py\"]\n        SYNC[\"sync_scores()<br/>validator.py\"]\n        NORM[\"Score Normalization<br/>normalize()\"]\n    end\n    \n    subgraph \"Score Storage & Distribution\"\n        STATS[\"self.stats dict<br/>Miner Statistics\"]\n        SCORES[\"self.scores Tensor<br/>PyTorch Tensor\"]\n        WEIGHTS[\"Network Weights<br/>set_weights()\"]\n    end\n    \n    POG --> CALC\n    CONFIG --> CALC\n    LOCALDB --> SYNC\n    WANDB --> SYNC\n    \n    CALC --> NORM\n    SYNC --> STATS\n    STATS --> SCORES\n    \n    SCORES --> WEIGHTS\n    \n    NORM -.-> STATS",
      "flowchart LR\n    subgraph \"Input Parameters\"\n        GPU_SPECS[\"gpu_specs<br/>{gpu_name, num_gpus}\"]\n        CONFIG[\"config_data<br/>gpu_performance.gpu_scores\"]\n        ALLOCATED[\"allocated_hotkeys<br/>Active Allocations\"]\n    end\n    \n    subgraph \"Score Calculation\"\n        MAX_CALC[\"Calculate max_score<br/>max_gpu * 8\"]\n        FACTOR[\"score_factor = 100/max_score\"]\n        BASE_SCORE[\"base_score = gpu_score * num_gpus * factor\"]\n        ALLOCATION[\"Allocation Multiplier<br/>1.0 (no bonus)\"]\n        NORMALIZE[\"normalize(score, 0, 100)\"]\n    end\n    \n    GPU_SPECS --> BASE_SCORE\n    CONFIG --> MAX_CALC\n    CONFIG --> BASE_SCORE\n    MAX_CALC --> FACTOR\n    FACTOR --> BASE_SCORE\n    BASE_SCORE --> ALLOCATION\n    ALLOCATED --> ALLOCATION\n    ALLOCATION --> NORMALIZE",
      "graph TD\n    subgraph \"config.yaml Structure\"\n        GPU_PERF[\"gpu_performance\"]\n        GPU_SCORES[\"gpu_scores<br/>{GPU_MODEL: score}\"]\n        GPU_TOL[\"gpu_tolerance_pairs<br/>Performance Tolerances\"]\n    end\n    \n    subgraph \"Score Processing\"\n        MAX_GPU[\"max(gpu_scores.values())\"]\n        MAX_SCORE[\"max_score = max_gpu * 8\"]\n        FACTOR[\"score_factor = 100 / max_score\"]\n    end\n    \n    GPU_PERF --> GPU_SCORES\n    GPU_PERF --> GPU_TOL\n    GPU_SCORES --> MAX_GPU\n    MAX_GPU --> MAX_SCORE\n    MAX_SCORE --> FACTOR",
      "sequenceDiagram\n    participant V as Validator\n    participant DB as ComputeDb\n    participant W as WandB\n    participant S as self.stats\n    participant T as self.scores\n    \n    Note over V,T: Score Synchronization Cycle\n    V->>DB: retrieve_stats()\n    V->>W: get_allocated_hotkeys()\n    V->>W: get_stats_allocated()\n    V->>W: get_penalized_hotkeys_checklist_bak()\n    \n    loop For each UID\n        V->>V: Check if uid in queryable_uids\n        alt UID not queryable\n            V->>S: Set score = 0, own_score = True\n            V->>T: scores[uid] = 0\n            V->>DB: DELETE FROM pog_stats\n        else UID queryable\n            V->>DB: get_pog_specs(hotkey)\n            alt Local specs found\n                V->>V: calc_score_pog(gpu_specs)\n                V->>S: Set own_score = True\n            else No local specs\n                V->>W: Use stats_allocated fallback\n                V->>S: Set own_score = False\n            end\n            V->>S: Apply penalty if hotkey penalized\n            V->>T: scores[uid] = calculated_score\n        end\n    end\n    \n    V->>DB: write_stats(stats)",
      "flowchart TD\n    subgraph \"Score Processing\"\n        SCORES[\"self.scores<br/>PyTorch Tensor\"]\n        CLAMP[\"scores[scores < 0] = 0<br/>Remove Negatives\"]\n        NORMALIZE[\"torch.nn.functional.normalize<br/>p=1.0, dim=0\"]\n    end\n    \n    subgraph \"Weight Setting\"\n        WEIGHTS[\"Normalized Weights<br/>Sum = 1.0\"]\n        SUBTENSOR[\"subtensor.set_weights()\"]\n        BLOCKCHAIN[\"Bittensor Blockchain<br/>Network State\"]\n    end\n    \n    subgraph \"Alternative: Burn Weights\"\n        BURN_UID[\"get_burn_uid()<br/>Subnet Owner UID\"]\n        BURN_WEIGHT[\"Single Weight = 1.0<br/>100% to Burn Account\"]\n        BURN_SET[\"set_burn_weights()\"]\n    end\n    \n    SCORES --> CLAMP\n    CLAMP --> NORMALIZE\n    NORMALIZE --> WEIGHTS\n    WEIGHTS --> SUBTENSOR\n    SUBTENSOR --> BLOCKCHAIN\n    \n    BURN_UID --> BURN_WEIGHT\n    BURN_WEIGHT --> BURN_SET\n    BURN_SET --> BLOCKCHAIN",
      "graph TD\n    subgraph \"Statistics Collection\"\n        STATS_DICT[\"self.stats Dictionary<br/>Per-UID Statistics\"]\n        MINER_DETAILS[\"Miner Details<br/>{hotkey, allocated, score, gpu_specs}\"]\n        RELIABILITY[\"reliability_score<br/>Historical Performance\"]\n    end\n    \n    subgraph \"Monitoring Output\"\n        LOG_SUMMARY[\"Miner Stats Summary<br/>Formatted Log Output\"]\n        WANDB_METRICS[\"WandB Metrics<br/>Chain Data Logging\"]\n        ALLOCATION_SYNC[\"Allocation Synchronization<br/>update_allocation_wandb()\"]\n    end\n    \n    subgraph \"Score Distribution Analysis\"\n        GPU_PARSING[\"GPU Spec Display<br/>num_gpus x gpu_name\"]\n        SCORE_FORMAT[\"Score Formatting<br/>2 decimal places\"]\n        SOURCE_TRACKING[\"Source Identification<br/>Local vs External\"]\n    end\n    \n    STATS_DICT --> MINER_DETAILS\n    MINER_DETAILS --> LOG_SUMMARY\n    MINER_DETAILS --> GPU_PARSING\n    MINER_DETAILS --> SCORE_FORMAT\n    \n    LOG_SUMMARY --> MONITORING_OUTPUT\n    WANDB_METRICS --> MONITORING_OUTPUT\n    ALLOCATION_SYNC --> MONITORING_OUTPUT"
    ],
    "potential_frontmatter": {
      "title": "Scoring System"
    }
  },
  "/neuralinternet/ni-compute/2.3-database-operations": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/2.3-database-operations",
    "title": "Database Operations",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/2.3-database-operations",
    "level": 1,
    "target_astro_path": "/validator-system/database-operations",
    "main_markdown_content": "# Database Operations\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/utils/db.py](compute/utils/db.py)\n- [neurons/Validator/database/allocate.py](neurons/Validator/database/allocate.py)\n- [neurons/Validator/database/challenge.py](neurons/Validator/database/challenge.py)\n- [neurons/validator.py](neurons/validator.py)\n\n</details>\n\n\n\nThis document covers the database operations used by the NI Compute Subnet validator system to persist miner statistics, challenge results, allocation records, and proof-of-GPU data. The system uses SQLite for local data persistence with structured schemas for tracking network state and performance metrics.\n\nFor information about the scoring algorithms that use this data, see [Scoring System](#2.2). For details about proof-of-GPU validation that writes to these tables, see [Proof of GPU](#2.1).\n\n## Database Architecture\n\nThe compute subnet uses a centralized SQLite database managed by the `ComputeDb` class to store all validator-related data. The database serves as the primary persistence layer for validator operations, storing everything from miner registration details to performance benchmarks.\n\n```mermaid\ngraph TB\n    subgraph \"Database Layer\"\n        ComputeDb[\"ComputeDb<br/>SQLite Connection Manager\"]\n        SQLiteDB[(\"SQLite Database<br/>database.db\")]\n    end\n    \n    subgraph \"Core Tables\"\n        MinerTable[\"miner<br/>(uid, ss58_address)\"]\n        MinerDetailsTable[\"miner_details<br/>(hotkey, details, no_specs_count)\"]\n        ChallengeTable[\"challenge_details<br/>(uid, success, elapsed_time, difficulty)\"]\n        AllocationTable[\"allocation<br/>(hotkey, details)\"]\n        PogStatsTable[\"pog_stats<br/>(hotkey, gpu_name, num_gpus)\"]\n        StatsTable[\"stats<br/>(uid, hotkey, gpu_specs, score)\"]\n        BlacklistTable[\"blacklist<br/>(hotkey, details)\"]\n        WandbTable[\"wandb_runs<br/>(hotkey, run_id)\"]\n    end\n    \n    subgraph \"Accessing Components\"\n        ValidatorProcess[\"Validator Process<br/>neurons/validator.py\"]\n        ChallengeOps[\"Challenge Operations<br/>database/challenge.py\"]\n        AllocateOps[\"Allocation Operations<br/>database/allocate.py\"]\n        PogOps[\"PoG Operations<br/>database/pog.py\"]\n        MinerOps[\"Miner Operations<br/>database/miner.py\"]\n    end\n    \n    ComputeDb --> SQLiteDB\n    \n    SQLiteDB --> MinerTable\n    SQLiteDB --> MinerDetailsTable\n    SQLiteDB --> ChallengeTable\n    SQLiteDB --> AllocationTable\n    SQLiteDB --> PogStatsTable\n    SQLiteDB --> StatsTable\n    SQLiteDB --> BlacklistTable\n    SQLiteDB --> WandbTable\n    \n    ValidatorProcess --> ComputeDb\n    ChallengeOps --> ComputeDb\n    AllocateOps --> ComputeDb\n    PogOps --> ComputeDb\n    MinerOps --> ComputeDb\n    \n    MinerTable -.->|\"Foreign Key\"| ChallengeTable\n    MinerDetailsTable -.->|\"Foreign Key\"| PogStatsTable\n    MinerDetailsTable -.->|\"Foreign Key\"| StatsTable\n```\n\nSources: [compute/utils/db.py:9-84](), [neurons/validator.py:170-172]()\n\n## Core Database Tables\n\nThe database schema consists of eight primary tables, each serving specific validator functions:\n\n| Table Name | Primary Key | Purpose | Key Relationships |\n|------------|-------------|---------|-------------------|\n| `miner` | `uid` | Basic miner registration | Referenced by `challenge_details` |\n| `miner_details` | `hotkey` | Hardware specifications and Docker status | Referenced by `pog_stats`, `stats` |\n| `challenge_details` | Auto-increment | Proof-of-Work challenge results | Foreign keys to `miner` table |\n| `allocation` | `hotkey` | Active resource allocations | Unique hotkey constraint |\n| `pog_stats` | Auto-increment | Proof-of-GPU benchmark results | Foreign key to `miner_details` |\n| `stats` | `uid` | Comprehensive miner scoring data | Foreign key to `miner_details` |\n| `blacklist` | Auto-increment | Penalized miner hotkeys | Unique hotkey constraint |\n| `wandb_runs` | `hotkey` | WandB run tracking | Links to external monitoring |\n\n### Miner Registration Tables\n\nThe `miner` table stores basic network registration data, while `miner_details` contains comprehensive hardware specifications:\n\n```mermaid\nerDiagram\n    miner {\n        INTEGER uid PK\n        TEXT ss58_address UK\n    }\n    \n    miner_details {\n        INTEGER id PK\n        TEXT hotkey UK\n        TEXT details\n        INTEGER no_specs_count\n    }\n    \n    challenge_details {\n        INTEGER uid FK\n        TEXT ss58_address FK\n        BOOLEAN success\n        REAL elapsed_time\n        INTEGER difficulty\n        TIMESTAMP created_at\n    }\n    \n    miner ||--o{ challenge_details : \"has challenges\"\n    miner_details ||--o{ pog_stats : \"has PoG results\"\n    miner_details ||--o{ stats : \"has statistics\"\n```\n\nSources: [compute/utils/db.py:29-30](), [compute/utils/db.py:30-31](), [compute/utils/db.py:33-45]()\n\n### Performance and Allocation Tracking\n\nThe system maintains detailed performance metrics and resource allocation state through specialized tables:\n\n```mermaid\ngraph LR\n    subgraph \"Performance Tracking\"\n        PogStats[\"pog_stats<br/>GPU benchmarking results\"]\n        ChallengeDetails[\"challenge_details<br/>PoW challenge outcomes\"]\n        Stats[\"stats<br/>Aggregated scoring data\"]\n    end\n    \n    subgraph \"Resource Management\"\n        Allocation[\"allocation<br/>Active resource assignments\"]\n        Blacklist[\"blacklist<br/>Penalized miners\"]\n        WandbRuns[\"wandb_runs<br/>External monitoring links\"]\n    end\n    \n    subgraph \"Data Sources\"\n        ValidatorProcess[\"Validator Process\"]\n        PogValidation[\"PoG Validation\"]\n        AllocationAPI[\"Allocation API\"]\n    end\n    \n    ValidatorProcess --> Stats\n    PogValidation --> PogStats\n    ValidatorProcess --> ChallengeDetails\n    AllocationAPI --> Allocation\n    ValidatorProcess --> Blacklist\n    ValidatorProcess --> WandbRuns\n```\n\nSources: [compute/utils/db.py:53-62](), [compute/utils/db.py:64-77](), [compute/utils/db.py:46-47]()\n\n## Database Operations by Component\n\n### Challenge Management Operations\n\nThe challenge system tracks proof-of-work validation results with comprehensive statistical analysis:\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator Process\"\n    participant CD as \"ChallengeOps<br/>challenge.py\"\n    participant DB as \"ComputeDb\"\n    participant CT as \"challenge_details table\"\n    \n    V->>CD: update_challenge_details(pow_benchmarks)\n    CD->>DB: get_cursor()\n    CD->>CT: INSERT challenge results\n    Note over CD,CT: Bulk insert with executemany\n    CD->>DB: commit()\n    \n    V->>CD: select_challenge_stats()\n    CD->>CT: Complex query with CTEs\n    Note over CD,CT: Analyzes last 60 attempts<br/>calculates success rates\n    CD->>CD: Process statistics\n    CD-->>V: Return aggregated stats dict\n```\n\nThe `select_challenge_stats` function uses Common Table Expressions (CTEs) to analyze challenge performance over rolling windows, calculating success rates and average difficulties for the most recent 20 and 60 attempts.\n\nSources: [neurons/Validator/database/challenge.py:24-125](), [neurons/Validator/database/challenge.py:128-176]()\n\n### Miner Details and Allocation Management\n\nAllocation operations manage hardware specifications and resource assignment state:\n\n```mermaid\ngraph TD\n    subgraph \"Miner Specification Flow\"\n        GetSpecs[\"get_miner_details()<br/>Retrieve all miner specs\"]\n        UpdateSpecs[\"update_miner_details()<br/>Batch update from WandB\"]\n        CheckDocker[\"select_has_docker_miners_hotkey()<br/>Filter Docker-capable miners\"]\n    end\n    \n    subgraph \"Allocation Management\"\n        AllocateCheck[\"select_allocate_miners_hotkey()<br/>Find miners meeting requirements\"]\n        UpdateAllocation[\"update_allocation_db()<br/>Track active allocations\"]\n        UpdateBlacklist[\"update_blacklist_db()<br/>Manage penalized miners\"]\n    end\n    \n    subgraph \"Database Tables\"\n        MinerDetailsTable[(\"miner_details\")]\n        AllocationTable[(\"allocation\")]\n        BlacklistTable[(\"blacklist\")]\n    end\n    \n    GetSpecs --> MinerDetailsTable\n    UpdateSpecs --> MinerDetailsTable\n    CheckDocker --> MinerDetailsTable\n    \n    AllocateCheck --> MinerDetailsTable\n    UpdateAllocation --> AllocationTable\n    UpdateBlacklist --> BlacklistTable\n    \n    MinerDetailsTable -.->|\"JSON details parsing\"| AllocateCheck\n```\n\nThe `update_miner_details` function includes automatic schema migration logic to handle database structure changes while preserving existing data.\n\nSources: [neurons/Validator/database/allocate.py:26-45](), [neurons/Validator/database/allocate.py:93-176](), [neurons/Validator/database/allocate.py:178-206]()\n\n### Proof-of-GPU Statistics Management\n\nPoG operations maintain GPU benchmarking results and performance metrics:\n\n```mermaid\nflowchart LR\n    subgraph \"PoG Database Operations\"\n        UpdatePogStats[\"update_pog_stats()<br/>Store GPU benchmark results\"]\n        GetPogSpecs[\"get_pog_specs()<br/>Retrieve GPU specifications\"]\n        RetrieveStats[\"retrieve_stats()<br/>Load scoring statistics\"]\n        WriteStats[\"write_stats()<br/>Update comprehensive stats\"]\n    end\n    \n    subgraph \"Data Tables\"\n        PogStatsTable[(\"pog_stats<br/>hotkey, gpu_name, num_gpus\")]\n        StatsTable[(\"stats<br/>uid, score, gpu_specs\")]\n    end\n    \n    subgraph \"Processing Flow\"\n        PogValidation[\"PoG Validation Process\"]\n        ScoreCalculation[\"Score Calculation\"]\n        NetworkWeights[\"Network Weight Setting\"]\n    end\n    \n    PogValidation --> UpdatePogStats\n    UpdatePogStats --> PogStatsTable\n    GetPogSpecs --> PogStatsTable\n    \n    RetrieveStats --> StatsTable\n    WriteStats --> StatsTable\n    \n    GetPogSpecs --> ScoreCalculation\n    RetrieveStats --> ScoreCalculation\n    ScoreCalculation --> WriteStats\n    WriteStats --> NetworkWeights\n```\n\nSources: [neurons/Validator/database/pog.py]() (referenced), [neurons/validator.py:361-362](), [neurons/validator.py:402-403]()\n\n## Data Persistence Workflow\n\n### Validator Database Integration\n\nThe validator process integrates with the database through multiple synchronized operations:\n\n```mermaid\nsequenceDiagram\n    participant VP as \"Validator Process<br/>validator.py\"\n    participant DB as \"ComputeDb\"\n    participant WB as \"WandB Integration\"\n    participant NS as \"Network State\"\n    \n    Note over VP: Initialization Phase\n    VP->>DB: ComputeDb()\n    VP->>DB: select_miners()\n    DB-->>VP: miners dict\n    \n    Note over VP: Scoring Synchronization\n    VP->>DB: retrieve_stats()\n    VP->>WB: get_allocated_hotkeys()\n    VP->>WB: get_stats_allocated()\n    VP->>VP: sync_scores()\n    VP->>DB: write_stats()\n    \n    Note over VP: Allocation Monitoring\n    VP->>DB: SELECT FROM allocation\n    VP->>WB: update_allocated_hotkeys()\n    \n    Note over VP: PoG Results Processing\n    VP->>VP: proof_of_gpu()\n    VP->>DB: update_pog_stats()\n    VP->>VP: sync_scores()\n    VP->>NS: Set network weights\n```\n\nThe validator maintains a continuous cycle of data synchronization between local database state, distributed WandB state, and blockchain network state.\n\nSources: [neurons/validator.py:170-172](), [neurons/validator.py:312-404](), [neurons/validator.py:663-787]()\n\n### Database Transaction Management\n\nAll database operations use transaction-safe patterns with proper error handling:\n\n| Operation Type | Transaction Pattern | Error Handling |\n|----------------|-------------------|----------------|\n| Single Inserts | `cursor.execute()` + `commit()` | `rollback()` on exception |\n| Bulk Operations | `cursor.executemany()` + `commit()` | `rollback()` + logging |\n| Complex Queries | Read-only, no transaction | Exception logging only |\n| Schema Changes | DDL statements + `commit()` | `rollback()` + preservation |\n\nThe database connection uses `check_same_thread=False` to support multi-threaded validator operations while maintaining thread safety through proper cursor management.\n\nSources: [compute/utils/db.py:13-17](), [neurons/Validator/database/challenge.py:140-176](), [neurons/Validator/database/allocate.py:211-229]()",
    "resolved_links": [
      {
        "text": "compute/utils/db.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py",
        "original_deepwiki_href": "compute/utils/db.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/database/allocate.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/allocate.py",
        "original_deepwiki_href": "neurons/Validator/database/allocate.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/database/challenge.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/challenge.py",
        "original_deepwiki_href": "neurons/Validator/database/challenge.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/validator.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py",
        "original_deepwiki_href": "neurons/validator.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/utils/db.py:9-84",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:170-172",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py:29-30",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py:30-31",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py:33-45",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py:53-62",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py:64-77",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py:46-47",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/challenge.py:24-125",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/challenge.py:128-176",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/allocate.py:26-45",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/allocate.py:93-176",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/allocate.py:178-206",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/pog.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:361-362",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:402-403",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:312-404",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:663-787",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/db.py:13-17",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/challenge.py:140-176",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/allocate.py:211-229",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Scoring System",
        "href": "/validator-system/scoring-system#2.2",
        "original_deepwiki_href": "#2.2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Proof of GPU",
        "href": "/validator-system/proof-of-gpu#2.1",
        "original_deepwiki_href": "#2.1",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"Database Layer\"\n        ComputeDb[\"ComputeDb<br/>SQLite Connection Manager\"]\n        SQLiteDB[(\"SQLite Database<br/>database.db\")]\n    end\n    \n    subgraph \"Core Tables\"\n        MinerTable[\"miner<br/>(uid, ss58_address)\"]\n        MinerDetailsTable[\"miner_details<br/>(hotkey, details, no_specs_count)\"]\n        ChallengeTable[\"challenge_details<br/>(uid, success, elapsed_time, difficulty)\"]\n        AllocationTable[\"allocation<br/>(hotkey, details)\"]\n        PogStatsTable[\"pog_stats<br/>(hotkey, gpu_name, num_gpus)\"]\n        StatsTable[\"stats<br/>(uid, hotkey, gpu_specs, score)\"]\n        BlacklistTable[\"blacklist<br/>(hotkey, details)\"]\n        WandbTable[\"wandb_runs<br/>(hotkey, run_id)\"]\n    end\n    \n    subgraph \"Accessing Components\"\n        ValidatorProcess[\"Validator Process<br/>neurons/validator.py\"]\n        ChallengeOps[\"Challenge Operations<br/>database/challenge.py\"]\n        AllocateOps[\"Allocation Operations<br/>database/allocate.py\"]\n        PogOps[\"PoG Operations<br/>database/pog.py\"]\n        MinerOps[\"Miner Operations<br/>database/miner.py\"]\n    end\n    \n    ComputeDb --> SQLiteDB\n    \n    SQLiteDB --> MinerTable\n    SQLiteDB --> MinerDetailsTable\n    SQLiteDB --> ChallengeTable\n    SQLiteDB --> AllocationTable\n    SQLiteDB --> PogStatsTable\n    SQLiteDB --> StatsTable\n    SQLiteDB --> BlacklistTable\n    SQLiteDB --> WandbTable\n    \n    ValidatorProcess --> ComputeDb\n    ChallengeOps --> ComputeDb\n    AllocateOps --> ComputeDb\n    PogOps --> ComputeDb\n    MinerOps --> ComputeDb\n    \n    MinerTable -.->|\"Foreign Key\"| ChallengeTable\n    MinerDetailsTable -.->|\"Foreign Key\"| PogStatsTable\n    MinerDetailsTable -.->|\"Foreign Key\"| StatsTable",
      "erDiagram\n    miner {\n        INTEGER uid PK\n        TEXT ss58_address UK\n    }\n    \n    miner_details {\n        INTEGER id PK\n        TEXT hotkey UK\n        TEXT details\n        INTEGER no_specs_count\n    }\n    \n    challenge_details {\n        INTEGER uid FK\n        TEXT ss58_address FK\n        BOOLEAN success\n        REAL elapsed_time\n        INTEGER difficulty\n        TIMESTAMP created_at\n    }\n    \n    miner ||--o{ challenge_details : \"has challenges\"\n    miner_details ||--o{ pog_stats : \"has PoG results\"\n    miner_details ||--o{ stats : \"has statistics\"",
      "graph LR\n    subgraph \"Performance Tracking\"\n        PogStats[\"pog_stats<br/>GPU benchmarking results\"]\n        ChallengeDetails[\"challenge_details<br/>PoW challenge outcomes\"]\n        Stats[\"stats<br/>Aggregated scoring data\"]\n    end\n    \n    subgraph \"Resource Management\"\n        Allocation[\"allocation<br/>Active resource assignments\"]\n        Blacklist[\"blacklist<br/>Penalized miners\"]\n        WandbRuns[\"wandb_runs<br/>External monitoring links\"]\n    end\n    \n    subgraph \"Data Sources\"\n        ValidatorProcess[\"Validator Process\"]\n        PogValidation[\"PoG Validation\"]\n        AllocationAPI[\"Allocation API\"]\n    end\n    \n    ValidatorProcess --> Stats\n    PogValidation --> PogStats\n    ValidatorProcess --> ChallengeDetails\n    AllocationAPI --> Allocation\n    ValidatorProcess --> Blacklist\n    ValidatorProcess --> WandbRuns",
      "sequenceDiagram\n    participant V as \"Validator Process\"\n    participant CD as \"ChallengeOps<br/>challenge.py\"\n    participant DB as \"ComputeDb\"\n    participant CT as \"challenge_details table\"\n    \n    V->>CD: update_challenge_details(pow_benchmarks)\n    CD->>DB: get_cursor()\n    CD->>CT: INSERT challenge results\n    Note over CD,CT: Bulk insert with executemany\n    CD->>DB: commit()\n    \n    V->>CD: select_challenge_stats()\n    CD->>CT: Complex query with CTEs\n    Note over CD,CT: Analyzes last 60 attempts<br/>calculates success rates\n    CD->>CD: Process statistics\n    CD-->>V: Return aggregated stats dict",
      "graph TD\n    subgraph \"Miner Specification Flow\"\n        GetSpecs[\"get_miner_details()<br/>Retrieve all miner specs\"]\n        UpdateSpecs[\"update_miner_details()<br/>Batch update from WandB\"]\n        CheckDocker[\"select_has_docker_miners_hotkey()<br/>Filter Docker-capable miners\"]\n    end\n    \n    subgraph \"Allocation Management\"\n        AllocateCheck[\"select_allocate_miners_hotkey()<br/>Find miners meeting requirements\"]\n        UpdateAllocation[\"update_allocation_db()<br/>Track active allocations\"]\n        UpdateBlacklist[\"update_blacklist_db()<br/>Manage penalized miners\"]\n    end\n    \n    subgraph \"Database Tables\"\n        MinerDetailsTable[(\"miner_details\")]\n        AllocationTable[(\"allocation\")]\n        BlacklistTable[(\"blacklist\")]\n    end\n    \n    GetSpecs --> MinerDetailsTable\n    UpdateSpecs --> MinerDetailsTable\n    CheckDocker --> MinerDetailsTable\n    \n    AllocateCheck --> MinerDetailsTable\n    UpdateAllocation --> AllocationTable\n    UpdateBlacklist --> BlacklistTable\n    \n    MinerDetailsTable -.->|\"JSON details parsing\"| AllocateCheck",
      "flowchart LR\n    subgraph \"PoG Database Operations\"\n        UpdatePogStats[\"update_pog_stats()<br/>Store GPU benchmark results\"]\n        GetPogSpecs[\"get_pog_specs()<br/>Retrieve GPU specifications\"]\n        RetrieveStats[\"retrieve_stats()<br/>Load scoring statistics\"]\n        WriteStats[\"write_stats()<br/>Update comprehensive stats\"]\n    end\n    \n    subgraph \"Data Tables\"\n        PogStatsTable[(\"pog_stats<br/>hotkey, gpu_name, num_gpus\")]\n        StatsTable[(\"stats<br/>uid, score, gpu_specs\")]\n    end\n    \n    subgraph \"Processing Flow\"\n        PogValidation[\"PoG Validation Process\"]\n        ScoreCalculation[\"Score Calculation\"]\n        NetworkWeights[\"Network Weight Setting\"]\n    end\n    \n    PogValidation --> UpdatePogStats\n    UpdatePogStats --> PogStatsTable\n    GetPogSpecs --> PogStatsTable\n    \n    RetrieveStats --> StatsTable\n    WriteStats --> StatsTable\n    \n    GetPogSpecs --> ScoreCalculation\n    RetrieveStats --> ScoreCalculation\n    ScoreCalculation --> WriteStats\n    WriteStats --> NetworkWeights",
      "sequenceDiagram\n    participant VP as \"Validator Process<br/>validator.py\"\n    participant DB as \"ComputeDb\"\n    participant WB as \"WandB Integration\"\n    participant NS as \"Network State\"\n    \n    Note over VP: Initialization Phase\n    VP->>DB: ComputeDb()\n    VP->>DB: select_miners()\n    DB-->>VP: miners dict\n    \n    Note over VP: Scoring Synchronization\n    VP->>DB: retrieve_stats()\n    VP->>WB: get_allocated_hotkeys()\n    VP->>WB: get_stats_allocated()\n    VP->>VP: sync_scores()\n    VP->>DB: write_stats()\n    \n    Note over VP: Allocation Monitoring\n    VP->>DB: SELECT FROM allocation\n    VP->>WB: update_allocated_hotkeys()\n    \n    Note over VP: PoG Results Processing\n    VP->>VP: proof_of_gpu()\n    VP->>DB: update_pog_stats()\n    VP->>VP: sync_scores()\n    VP->>NS: Set network weights"
    ],
    "potential_frontmatter": {
      "title": "Database Operations"
    }
  },
  "/neuralinternet/ni-compute/3-miner-system": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/3-miner-system",
    "title": "Miner System",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/3-miner-system",
    "level": 0,
    "target_astro_path": "/miner-system",
    "main_markdown_content": "# Miner System\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/miner.py](neurons/miner.py)\n\n</details>\n\n\n\nThe Miner System provides compute resources to the NI Compute Subnet by responding to resource allocation requests and validation challenges from validators. It manages Docker containers for secure compute workloads, handles proof-of-work challenges, and maintains network connectivity through the Bittensor protocol.\n\nFor information about validator-side operations, see [Validator System](#2). For details about the resource allocation API that coordinates with miners, see [Resource Allocation API](#4). For container lifecycle management specifics, see [Container Management](#3.1).\n\n## Architecture Overview\n\nThe miner system is implemented as a single `Miner` class that operates as a Bittensor axon server, handling three primary types of requests from validators: resource allocation, challenge-response, and system monitoring.\n\n### Core System Components\n\n```mermaid\ngraph TB\n    subgraph \"Miner Class (neurons/miner.py)\"\n        MINER[\"Miner\"]\n        ALLOCATE[\"allocate()\"]\n        CHALLENGE[\"challenge()\"]\n        BLACKLIST[\"base_blacklist()\"]\n        PRIORITY[\"base_priority()\"]\n    end\n    \n    subgraph \"Request Processing\"\n        ALLOC_REQ[\"Allocate Synapse\"]\n        CHALL_REQ[\"Challenge Synapse\"]\n        BLACKLIST_CHECK[\"Blacklist Check\"]\n        PRIORITY_CALC[\"Priority Calculation\"]\n    end\n    \n    subgraph \"Container Management\"\n        REGISTER_ALLOC[\"register_allocation()\"]\n        DEREGISTER_ALLOC[\"deregister_allocation()\"]\n        CHECK_ALLOC[\"check_allocation()\"]\n        CONTAINER_OPS[\"Container Operations\"]\n    end\n    \n    subgraph \"Network Layer\"\n        AXON[\"ComputeSubnetAxon\"]\n        SUBTENSOR[\"ComputeSubnetSubtensor\"]\n        METAGRAPH[\"bt.metagraph\"]\n        WALLET[\"bt.wallet\"]\n    end\n    \n    subgraph \"Monitoring & State\"\n        WANDB[\"ComputeWandb\"]\n        SPECS_UPDATE[\"update_specs()\"]\n        ALLOCATED_UPDATE[\"update_allocated()\"]\n    end\n    \n    subgraph \"Infrastructure\"\n        DOCKER[\"Docker Runtime\"]\n        SSH_SERVER[\"SSH Access\"]\n        HASHCAT[\"Hashcat (PoW)\"]\n    end\n    \n    %% Request flow\n    ALLOC_REQ --> BLACKLIST_CHECK\n    CHALL_REQ --> BLACKLIST_CHECK\n    BLACKLIST_CHECK --> PRIORITY_CALC\n    PRIORITY_CALC --> ALLOCATE\n    PRIORITY_CALC --> CHALLENGE\n    \n    %% Allocation flow\n    ALLOCATE --> REGISTER_ALLOC\n    ALLOCATE --> DEREGISTER_ALLOC\n    ALLOCATE --> CHECK_ALLOC\n    REGISTER_ALLOC --> CONTAINER_OPS\n    CONTAINER_OPS --> DOCKER\n    CONTAINER_OPS --> SSH_SERVER\n    \n    %% Challenge flow\n    CHALLENGE --> HASHCAT\n    \n    %% Network integration\n    MINER --> AXON\n    AXON --> SUBTENSOR\n    AXON --> METAGRAPH\n    MINER --> WALLET\n    \n    %% Monitoring\n    MINER --> WANDB\n    WANDB --> SPECS_UPDATE\n    WANDB --> ALLOCATED_UPDATE\n    \n    %% Security\n    MINER --> BLACKLIST\n    MINER --> PRIORITY\n```\n\nSources: [neurons/miner.py:79-714]()\n\n### Miner Lifecycle and Main Loop\n\n```mermaid\nsequenceDiagram\n    participant MAIN as \"main()\"\n    participant MINER as \"Miner.__init__()\"\n    participant AXON as \"ComputeSubnetAxon\"\n    participant WANDB as \"ComputeWandb\"\n    participant LOOP as \"start() Loop\"\n    participant VALIDATOR as \"Validator\"\n    \n    MAIN->>MINER: \"Initialize miner\"\n    MINER->>MINER: \"init_config()\"\n    MINER->>MINER: \"init_black_and_white_list()\"\n    MINER->>AXON: \"Initialize axon server\"\n    MINER->>WANDB: \"Initialize WandB monitoring\"\n    MINER->>MINER: \"build_check_container()\"\n    \n    MINER->>AXON: \"axon.attach(allocate, challenge)\"\n    MINER->>AXON: \"axon.serve(netuid, subtensor)\"\n    MINER->>AXON: \"axon.start()\"\n    \n    MINER->>LOOP: \"asyncio.run(start())\"\n    \n    loop \"Every 5 seconds\"\n        LOOP->>LOOP: \"sync_local()\"\n        \n        alt \"Every 30 blocks (~6 min)\"\n            LOOP->>LOOP: \"get_updated_validator()\"\n        end\n        \n        alt \"Every 150 blocks (~30 min)\"\n            LOOP->>WANDB: \"update_specs()\"\n        end\n        \n        alt \"Every 75 blocks (~15 min)\"\n            LOOP->>LOOP: \"sync_status()\"\n            LOOP->>WANDB: \"log_chain_data()\"\n        end\n    end\n    \n    Note over VALIDATOR,AXON: \"Concurrent request handling\"\n    VALIDATOR->>AXON: \"Allocate/Challenge requests\"\n    AXON->>MINER: \"Route to handler methods\"\n```\n\nSources: [neurons/miner.py:702-714](), [neurons/miner.py:606-700](), [neurons/miner.py:117-189]()\n\n## Request Handling System\n\nThe miner processes two primary types of requests from validators through the Bittensor axon protocol: `Allocate` requests for compute resource management and `Challenge` requests for proof-of-work validation.\n\n### Request Processing Pipeline\n\nAll incoming requests go through a standardized processing pipeline that includes blacklisting, priority calculation, and request-specific handling.\n\n```mermaid\ngraph LR\n    subgraph \"Incoming Request\"\n        REQ[\"Synapse Request\"]\n        HOTKEY[\"dendrite.hotkey\"]\n        STAKE[\"Validator Stake\"]\n    end\n    \n    subgraph \"Security Layer\"\n        BLACKLIST_FN[\"blacklist_allocate()\"]\n        BLACKLIST_BASE[\"base_blacklist()\"]\n        WHITELIST_CHECK[\"whitelist_hotkeys\"]\n        STAKE_CHECK[\"validator_permit_stake\"]\n        EXPLOITER_CHECK[\"exploiters_hotkeys_set\"]\n    end\n    \n    subgraph \"Priority Layer\"\n        PRIORITY_FN[\"priority_allocate()\"]\n        PRIORITY_BASE[\"base_priority()\"]\n        STAKE_PRIORITY[\"metagraph.S[caller_uid]\"]\n        MINER_PRIORITY[\"miner_priority_allocate\"]\n    end\n    \n    subgraph \"Handler Layer\"\n        ALLOCATE_HANDLER[\"allocate()\"]\n        CHALLENGE_HANDLER[\"challenge()\"]\n    end\n    \n    REQ --> BLACKLIST_FN\n    HOTKEY --> BLACKLIST_BASE\n    STAKE --> STAKE_CHECK\n    \n    BLACKLIST_FN --> BLACKLIST_BASE\n    BLACKLIST_BASE --> WHITELIST_CHECK\n    BLACKLIST_BASE --> STAKE_CHECK\n    BLACKLIST_BASE --> EXPLOITER_CHECK\n    \n    REQ --> PRIORITY_FN\n    PRIORITY_FN --> PRIORITY_BASE\n    PRIORITY_BASE --> STAKE_PRIORITY\n    PRIORITY_FN --> MINER_PRIORITY\n    \n    REQ --> ALLOCATE_HANDLER\n    REQ --> CHALLENGE_HANDLER\n```\n\nSources: [neurons/miner.py:330-374](), [neurons/miner.py:375-385](), [neurons/miner.py:397-403]()\n\n### Blacklist and Security Controls\n\nThe `base_blacklist()` method implements comprehensive security controls to prevent unauthorized access and abuse:\n\n| Security Check | Implementation | Purpose |\n|---|---|---|\n| Whitelist Check | `hotkey not in self.whitelist_hotkeys` | Allow trusted validators regardless of stake |\n| Network Recognition | `hotkey not in self.metagraph.hotkeys` | Reject unregistered entities |\n| Stake Requirement | `stake < validator_permit_stake` | Ensure minimum validator stake |\n| Explicit Blacklist | `hotkey in self.blacklist_hotkeys` | Block specific problematic validators |\n| Exploiter Detection | `hotkey in self.exploiters_hotkeys_set` | Block known malicious actors |\n\nSources: [neurons/miner.py:330-374]()\n\n## Resource Allocation Handler\n\nThe `allocate()` method manages the complete lifecycle of compute resource allocation, from initial availability checks to container provisioning and deallocation.\n\n### Allocation Request Types\n\n```mermaid\ngraph TD\n    subgraph \"Allocate Request Processing\"\n        ALLOCATE_REQ[\"Allocate Synapse\"]\n        CHECKING_FLAG[\"synapse.checking\"]\n        TIMELINE[\"synapse.timeline\"]\n        DOCKER_CHANGE[\"synapse.docker_change\"]\n    end\n    \n    subgraph \"Checking Mode (checking=True)\"\n        CHECK_POSITIVE[\"timeline > 0\"]\n        CHECK_ALLOCATION[\"check_allocation()\"]\n        CHECK_NEGATIVE[\"timeline = 0\"]\n        CHECK_IF_ALLOCATED[\"check_if_allocated()\"]\n    end\n    \n    subgraph \"Docker Change Mode (docker_change=True)\"\n        EXCHANGE_KEY[\"exchange_key_container()\"]\n        RESTART_CONTAINER[\"restart_container()\"]\n        PAUSE_CONTAINER[\"pause_container()\"]\n        UNPAUSE_CONTAINER[\"unpause_container()\"]\n    end\n    \n    subgraph \"Allocation Mode (Normal)\"\n        ALLOC_POSITIVE[\"timeline > 0\"]\n        REGISTER_ALLOCATION[\"register_allocation()\"]\n        ALLOC_NEGATIVE[\"timeline = 0\"] \n        DEREGISTER_ALLOCATION[\"deregister_allocation()\"]\n    end\n    \n    ALLOCATE_REQ --> CHECKING_FLAG\n    CHECKING_FLAG -->|\"True\"| CHECK_POSITIVE\n    CHECKING_FLAG -->|\"True\"| CHECK_NEGATIVE\n    CHECK_POSITIVE --> CHECK_ALLOCATION\n    CHECK_NEGATIVE --> CHECK_IF_ALLOCATED\n    \n    ALLOCATE_REQ --> DOCKER_CHANGE\n    DOCKER_CHANGE -->|\"True\"| EXCHANGE_KEY\n    DOCKER_CHANGE -->|\"True\"| RESTART_CONTAINER\n    DOCKER_CHANGE -->|\"True\"| PAUSE_CONTAINER\n    DOCKER_CHANGE -->|\"True\"| UNPAUSE_CONTAINER\n    \n    ALLOCATE_REQ --> ALLOC_POSITIVE\n    ALLOCATE_REQ --> ALLOC_NEGATIVE\n    ALLOC_POSITIVE --> REGISTER_ALLOCATION\n    ALLOC_NEGATIVE --> DEREGISTER_ALLOCATION\n```\n\nSources: [neurons/miner.py:419-479]()\n\n### Allocation State Management\n\nThe miner tracks allocation state through multiple mechanisms to ensure consistency and prevent conflicts:\n\n- **WandB Integration**: `self.wandb.update_allocated()` synchronizes allocation status across the network\n- **File-based State**: `allocation_key` file stores the current allocation's public key\n- **Container State**: Docker container lifecycle tied to allocation status\n- **Concurrency Control**: `self.allocate_action` flag prevents concurrent allocations\n\nSources: [neurons/miner.py:405-417](), [neurons/miner.py:190-221]()\n\n## Challenge Response System\n\nThe `challenge()` method handles proof-of-work validation requests from validators, though the actual proof-of-work execution is currently disabled in the implementation.\n\n### Challenge Processing Flow\n\n```mermaid\ngraph LR\n    subgraph \"Challenge Request\"\n        CHALL_REQ[\"Challenge Synapse\"]\n        DIFFICULTY[\"challenge_difficulty\"]\n        HASH[\"challenge_hash\"]\n        SALT[\"challenge_salt\"]\n        MODE[\"challenge_mode\"]\n    end\n    \n    subgraph \"Validation\"\n        DIFFICULTY_CHECK[\"difficulty <= 0\"]\n        VALIDATOR_ID[\"dendrite.hotkey[:8]\"]\n        RUN_ID[\"run_id generation\"]\n    end\n    \n    subgraph \"PoW Execution (Disabled)\"\n        HASHCAT_PATH[\"hashcat_path\"]\n        WORKLOAD_PROFILE[\"hashcat_workload_profile\"]\n        EXTENDED_OPTIONS[\"hashcat_extended_options\"]\n        RUN_MINER_POW[\"run_miner_pow()\"]\n    end\n    \n    CHALL_REQ --> DIFFICULTY_CHECK\n    CHALL_REQ --> VALIDATOR_ID\n    VALIDATOR_ID --> RUN_ID\n    \n    CHALL_REQ --> HASHCAT_PATH\n    CHALL_REQ --> WORKLOAD_PROFILE\n    CHALL_REQ --> EXTENDED_OPTIONS\n    RUN_ID --> RUN_MINER_POW\n```\n\nSources: [neurons/miner.py:491-515]()\n\n## Network Integration and Monitoring\n\nThe miner maintains continuous integration with the Bittensor network through periodic synchronization and state updates.\n\n### Network Synchronization Schedule\n\n| Operation | Frequency | Purpose |\n|---|---|---|\n| `sync_local()` | Every 5 seconds | Update local metagraph state |\n| `get_updated_validator()` | Every 30 blocks (~6 min) | Refresh validator whitelist |\n| `update_specs()` | Every 150 blocks (~30 min) | Sync hardware specs to WandB |\n| `sync_status()` | Every 75 blocks (~15 min) | Update registration status and log metrics |\n\n### WandB Integration Points\n\nThe miner integrates with Weights & Biases for distributed state management and monitoring:\n\n- **Specs Management**: `self.wandb.update_specs()` publishes hardware specifications\n- **Allocation Tracking**: `self.wandb.update_allocated()` maintains allocation state\n- **Chain Data Logging**: `self.wandb.log_chain_data()` records network metrics\n- **Validator Discovery**: `self.wandb.get_allocated_hotkeys()` queries network state\n\nSources: [neurons/miner.py:606-700](), [neurons/miner.py:179-181]()\n\n### Initialization and Configuration\n\nThe miner initialization process follows a structured sequence to establish network connectivity and prepare for operation:\n\n```mermaid\ngraph TD\n    subgraph \"Configuration Phase\"\n        INIT_CONFIG[\"init_config()\"]\n        PARSE_ARGS[\"ComputeArgPaser\"]\n        LOGGING_SETUP[\"bt.logging setup\"]\n    end\n    \n    subgraph \"Network Objects\"\n        WALLET_INIT[\"bt.wallet(config)\"]\n        SUBTENSOR_INIT[\"ComputeSubnetSubtensor(config)\"]\n        METAGRAPH_INIT[\"subtensor.metagraph(netuid)\"]\n    end\n    \n    subgraph \"Infrastructure Setup\"\n        DOCKER_CHECK[\"check_docker_availability()\"]\n        BUILD_CONTAINER[\"build_check_container()\"]\n        BUILD_SAMPLE[\"build_sample_container()\"]\n        CUDA_CHECK[\"check_cuda_availability()\"]\n    end\n    \n    subgraph \"Security Setup\"\n        BLACKLIST_INIT[\"init_black_and_white_list()\"]\n        WHITELIST_SETUP[\"TRUSTED_VALIDATORS_HOTKEYS\"]\n        EXPLOITER_SETUP[\"SUSPECTED_EXPLOITERS_HOTKEYS\"]\n    end\n    \n    subgraph \"Service Initialization\"\n        AXON_INIT[\"init_axon()\"]\n        WANDB_INIT[\"ComputeWandb initialization\"]\n        ALLOCATION_CHECK[\"__check_alloaction_errors()\"]\n    end\n    \n    INIT_CONFIG --> PARSE_ARGS\n    PARSE_ARGS --> LOGGING_SETUP\n    \n    LOGGING_SETUP --> WALLET_INIT\n    WALLET_INIT --> SUBTENSOR_INIT\n    SUBTENSOR_INIT --> METAGRAPH_INIT\n    \n    METAGRAPH_INIT --> DOCKER_CHECK\n    DOCKER_CHECK --> BUILD_CONTAINER\n    BUILD_CONTAINER --> BUILD_SAMPLE\n    BUILD_SAMPLE --> CUDA_CHECK\n    \n    CUDA_CHECK --> BLACKLIST_INIT\n    BLACKLIST_INIT --> WHITELIST_SETUP\n    WHITELIST_SETUP --> EXPLOITER_SETUP\n    \n    EXPLOITER_SETUP --> AXON_INIT\n    AXON_INIT --> WANDB_INIT\n    WANDB_INIT --> ALLOCATION_CHECK\n```\n\nSources: [neurons/miner.py:117-189](), [neurons/miner.py:254-281](), [neurons/miner.py:222-252]()",
    "resolved_links": [
      {
        "text": "neurons/miner.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py",
        "original_deepwiki_href": "neurons/miner.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/miner.py:79-714",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:702-714",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:606-700",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:117-189",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:330-374",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:375-385",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:397-403",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:419-479",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:405-417",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:190-221",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:491-515",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:179-181",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:254-281",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:222-252",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Validator System",
        "href": "/validator-system#2",
        "original_deepwiki_href": "#2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Resource Allocation API",
        "href": "/resource-allocation-api#4",
        "original_deepwiki_href": "#4",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Container Management",
        "href": "/miner-system/container-management#3.1",
        "original_deepwiki_href": "#3.1",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"Miner Class (neurons/miner.py)\"\n        MINER[\"Miner\"]\n        ALLOCATE[\"allocate()\"]\n        CHALLENGE[\"challenge()\"]\n        BLACKLIST[\"base_blacklist()\"]\n        PRIORITY[\"base_priority()\"]\n    end\n    \n    subgraph \"Request Processing\"\n        ALLOC_REQ[\"Allocate Synapse\"]\n        CHALL_REQ[\"Challenge Synapse\"]\n        BLACKLIST_CHECK[\"Blacklist Check\"]\n        PRIORITY_CALC[\"Priority Calculation\"]\n    end\n    \n    subgraph \"Container Management\"\n        REGISTER_ALLOC[\"register_allocation()\"]\n        DEREGISTER_ALLOC[\"deregister_allocation()\"]\n        CHECK_ALLOC[\"check_allocation()\"]\n        CONTAINER_OPS[\"Container Operations\"]\n    end\n    \n    subgraph \"Network Layer\"\n        AXON[\"ComputeSubnetAxon\"]\n        SUBTENSOR[\"ComputeSubnetSubtensor\"]\n        METAGRAPH[\"bt.metagraph\"]\n        WALLET[\"bt.wallet\"]\n    end\n    \n    subgraph \"Monitoring & State\"\n        WANDB[\"ComputeWandb\"]\n        SPECS_UPDATE[\"update_specs()\"]\n        ALLOCATED_UPDATE[\"update_allocated()\"]\n    end\n    \n    subgraph \"Infrastructure\"\n        DOCKER[\"Docker Runtime\"]\n        SSH_SERVER[\"SSH Access\"]\n        HASHCAT[\"Hashcat (PoW)\"]\n    end\n    \n    %% Request flow\n    ALLOC_REQ --> BLACKLIST_CHECK\n    CHALL_REQ --> BLACKLIST_CHECK\n    BLACKLIST_CHECK --> PRIORITY_CALC\n    PRIORITY_CALC --> ALLOCATE\n    PRIORITY_CALC --> CHALLENGE\n    \n    %% Allocation flow\n    ALLOCATE --> REGISTER_ALLOC\n    ALLOCATE --> DEREGISTER_ALLOC\n    ALLOCATE --> CHECK_ALLOC\n    REGISTER_ALLOC --> CONTAINER_OPS\n    CONTAINER_OPS --> DOCKER\n    CONTAINER_OPS --> SSH_SERVER\n    \n    %% Challenge flow\n    CHALLENGE --> HASHCAT\n    \n    %% Network integration\n    MINER --> AXON\n    AXON --> SUBTENSOR\n    AXON --> METAGRAPH\n    MINER --> WALLET\n    \n    %% Monitoring\n    MINER --> WANDB\n    WANDB --> SPECS_UPDATE\n    WANDB --> ALLOCATED_UPDATE\n    \n    %% Security\n    MINER --> BLACKLIST\n    MINER --> PRIORITY",
      "sequenceDiagram\n    participant MAIN as \"main()\"\n    participant MINER as \"Miner.__init__()\"\n    participant AXON as \"ComputeSubnetAxon\"\n    participant WANDB as \"ComputeWandb\"\n    participant LOOP as \"start() Loop\"\n    participant VALIDATOR as \"Validator\"\n    \n    MAIN->>MINER: \"Initialize miner\"\n    MINER->>MINER: \"init_config()\"\n    MINER->>MINER: \"init_black_and_white_list()\"\n    MINER->>AXON: \"Initialize axon server\"\n    MINER->>WANDB: \"Initialize WandB monitoring\"\n    MINER->>MINER: \"build_check_container()\"\n    \n    MINER->>AXON: \"axon.attach(allocate, challenge)\"\n    MINER->>AXON: \"axon.serve(netuid, subtensor)\"\n    MINER->>AXON: \"axon.start()\"\n    \n    MINER->>LOOP: \"asyncio.run(start())\"\n    \n    loop \"Every 5 seconds\"\n        LOOP->>LOOP: \"sync_local()\"\n        \n        alt \"Every 30 blocks (~6 min)\"\n            LOOP->>LOOP: \"get_updated_validator()\"\n        end\n        \n        alt \"Every 150 blocks (~30 min)\"\n            LOOP->>WANDB: \"update_specs()\"\n        end\n        \n        alt \"Every 75 blocks (~15 min)\"\n            LOOP->>LOOP: \"sync_status()\"\n            LOOP->>WANDB: \"log_chain_data()\"\n        end\n    end\n    \n    Note over VALIDATOR,AXON: \"Concurrent request handling\"\n    VALIDATOR->>AXON: \"Allocate/Challenge requests\"\n    AXON->>MINER: \"Route to handler methods\"",
      "graph LR\n    subgraph \"Incoming Request\"\n        REQ[\"Synapse Request\"]\n        HOTKEY[\"dendrite.hotkey\"]\n        STAKE[\"Validator Stake\"]\n    end\n    \n    subgraph \"Security Layer\"\n        BLACKLIST_FN[\"blacklist_allocate()\"]\n        BLACKLIST_BASE[\"base_blacklist()\"]\n        WHITELIST_CHECK[\"whitelist_hotkeys\"]\n        STAKE_CHECK[\"validator_permit_stake\"]\n        EXPLOITER_CHECK[\"exploiters_hotkeys_set\"]\n    end\n    \n    subgraph \"Priority Layer\"\n        PRIORITY_FN[\"priority_allocate()\"]\n        PRIORITY_BASE[\"base_priority()\"]\n        STAKE_PRIORITY[\"metagraph.S[caller_uid]\"]\n        MINER_PRIORITY[\"miner_priority_allocate\"]\n    end\n    \n    subgraph \"Handler Layer\"\n        ALLOCATE_HANDLER[\"allocate()\"]\n        CHALLENGE_HANDLER[\"challenge()\"]\n    end\n    \n    REQ --> BLACKLIST_FN\n    HOTKEY --> BLACKLIST_BASE\n    STAKE --> STAKE_CHECK\n    \n    BLACKLIST_FN --> BLACKLIST_BASE\n    BLACKLIST_BASE --> WHITELIST_CHECK\n    BLACKLIST_BASE --> STAKE_CHECK\n    BLACKLIST_BASE --> EXPLOITER_CHECK\n    \n    REQ --> PRIORITY_FN\n    PRIORITY_FN --> PRIORITY_BASE\n    PRIORITY_BASE --> STAKE_PRIORITY\n    PRIORITY_FN --> MINER_PRIORITY\n    \n    REQ --> ALLOCATE_HANDLER\n    REQ --> CHALLENGE_HANDLER",
      "graph TD\n    subgraph \"Allocate Request Processing\"\n        ALLOCATE_REQ[\"Allocate Synapse\"]\n        CHECKING_FLAG[\"synapse.checking\"]\n        TIMELINE[\"synapse.timeline\"]\n        DOCKER_CHANGE[\"synapse.docker_change\"]\n    end\n    \n    subgraph \"Checking Mode (checking=True)\"\n        CHECK_POSITIVE[\"timeline > 0\"]\n        CHECK_ALLOCATION[\"check_allocation()\"]\n        CHECK_NEGATIVE[\"timeline = 0\"]\n        CHECK_IF_ALLOCATED[\"check_if_allocated()\"]\n    end\n    \n    subgraph \"Docker Change Mode (docker_change=True)\"\n        EXCHANGE_KEY[\"exchange_key_container()\"]\n        RESTART_CONTAINER[\"restart_container()\"]\n        PAUSE_CONTAINER[\"pause_container()\"]\n        UNPAUSE_CONTAINER[\"unpause_container()\"]\n    end\n    \n    subgraph \"Allocation Mode (Normal)\"\n        ALLOC_POSITIVE[\"timeline > 0\"]\n        REGISTER_ALLOCATION[\"register_allocation()\"]\n        ALLOC_NEGATIVE[\"timeline = 0\"] \n        DEREGISTER_ALLOCATION[\"deregister_allocation()\"]\n    end\n    \n    ALLOCATE_REQ --> CHECKING_FLAG\n    CHECKING_FLAG -->|\"True\"| CHECK_POSITIVE\n    CHECKING_FLAG -->|\"True\"| CHECK_NEGATIVE\n    CHECK_POSITIVE --> CHECK_ALLOCATION\n    CHECK_NEGATIVE --> CHECK_IF_ALLOCATED\n    \n    ALLOCATE_REQ --> DOCKER_CHANGE\n    DOCKER_CHANGE -->|\"True\"| EXCHANGE_KEY\n    DOCKER_CHANGE -->|\"True\"| RESTART_CONTAINER\n    DOCKER_CHANGE -->|\"True\"| PAUSE_CONTAINER\n    DOCKER_CHANGE -->|\"True\"| UNPAUSE_CONTAINER\n    \n    ALLOCATE_REQ --> ALLOC_POSITIVE\n    ALLOCATE_REQ --> ALLOC_NEGATIVE\n    ALLOC_POSITIVE --> REGISTER_ALLOCATION\n    ALLOC_NEGATIVE --> DEREGISTER_ALLOCATION",
      "graph LR\n    subgraph \"Challenge Request\"\n        CHALL_REQ[\"Challenge Synapse\"]\n        DIFFICULTY[\"challenge_difficulty\"]\n        HASH[\"challenge_hash\"]\n        SALT[\"challenge_salt\"]\n        MODE[\"challenge_mode\"]\n    end\n    \n    subgraph \"Validation\"\n        DIFFICULTY_CHECK[\"difficulty <= 0\"]\n        VALIDATOR_ID[\"dendrite.hotkey[:8]\"]\n        RUN_ID[\"run_id generation\"]\n    end\n    \n    subgraph \"PoW Execution (Disabled)\"\n        HASHCAT_PATH[\"hashcat_path\"]\n        WORKLOAD_PROFILE[\"hashcat_workload_profile\"]\n        EXTENDED_OPTIONS[\"hashcat_extended_options\"]\n        RUN_MINER_POW[\"run_miner_pow()\"]\n    end\n    \n    CHALL_REQ --> DIFFICULTY_CHECK\n    CHALL_REQ --> VALIDATOR_ID\n    VALIDATOR_ID --> RUN_ID\n    \n    CHALL_REQ --> HASHCAT_PATH\n    CHALL_REQ --> WORKLOAD_PROFILE\n    CHALL_REQ --> EXTENDED_OPTIONS\n    RUN_ID --> RUN_MINER_POW",
      "graph TD\n    subgraph \"Configuration Phase\"\n        INIT_CONFIG[\"init_config()\"]\n        PARSE_ARGS[\"ComputeArgPaser\"]\n        LOGGING_SETUP[\"bt.logging setup\"]\n    end\n    \n    subgraph \"Network Objects\"\n        WALLET_INIT[\"bt.wallet(config)\"]\n        SUBTENSOR_INIT[\"ComputeSubnetSubtensor(config)\"]\n        METAGRAPH_INIT[\"subtensor.metagraph(netuid)\"]\n    end\n    \n    subgraph \"Infrastructure Setup\"\n        DOCKER_CHECK[\"check_docker_availability()\"]\n        BUILD_CONTAINER[\"build_check_container()\"]\n        BUILD_SAMPLE[\"build_sample_container()\"]\n        CUDA_CHECK[\"check_cuda_availability()\"]\n    end\n    \n    subgraph \"Security Setup\"\n        BLACKLIST_INIT[\"init_black_and_white_list()\"]\n        WHITELIST_SETUP[\"TRUSTED_VALIDATORS_HOTKEYS\"]\n        EXPLOITER_SETUP[\"SUSPECTED_EXPLOITERS_HOTKEYS\"]\n    end\n    \n    subgraph \"Service Initialization\"\n        AXON_INIT[\"init_axon()\"]\n        WANDB_INIT[\"ComputeWandb initialization\"]\n        ALLOCATION_CHECK[\"__check_alloaction_errors()\"]\n    end\n    \n    INIT_CONFIG --> PARSE_ARGS\n    PARSE_ARGS --> LOGGING_SETUP\n    \n    LOGGING_SETUP --> WALLET_INIT\n    WALLET_INIT --> SUBTENSOR_INIT\n    SUBTENSOR_INIT --> METAGRAPH_INIT\n    \n    METAGRAPH_INIT --> DOCKER_CHECK\n    DOCKER_CHECK --> BUILD_CONTAINER\n    BUILD_CONTAINER --> BUILD_SAMPLE\n    BUILD_SAMPLE --> CUDA_CHECK\n    \n    CUDA_CHECK --> BLACKLIST_INIT\n    BLACKLIST_INIT --> WHITELIST_SETUP\n    WHITELIST_SETUP --> EXPLOITER_SETUP\n    \n    EXPLOITER_SETUP --> AXON_INIT\n    AXON_INIT --> WANDB_INIT\n    WANDB_INIT --> ALLOCATION_CHECK"
    ],
    "potential_frontmatter": {
      "title": "Miner System"
    }
  },
  "/neuralinternet/ni-compute/3.1-container-management": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/3.1-container-management",
    "title": "Container Management",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/3.1-container-management",
    "level": 1,
    "target_astro_path": "/miner-system/container-management",
    "main_markdown_content": "# Container Management\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [min_compute.yml](min_compute.yml)\n- [neurons/Miner/allocate.py](neurons/Miner/allocate.py)\n- [neurons/Miner/container.py](neurons/Miner/container.py)\n- [neurons/miner_checker.py](neurons/miner_checker.py)\n- [tests/test_miner_container.py](tests/test_miner_container.py)\n- [tests/test_rsa_encryption.py](tests/test_rsa_encryption.py)\n\n</details>\n\n\n\nContainer Management is the core Docker container lifecycle system used by miners to provide isolated compute resources to validators and clients. This system handles the creation, configuration, monitoring, and termination of SSH-enabled containers that serve as the compute environments for resource allocation requests.\n\nFor information about how containers integrate with the broader resource allocation workflow, see [Resource Allocation](#3.3). For details about the communication protocols used during container provisioning, see [Specs, Allocate, and Challenge Protocols](#5.1).\n\n## Container Lifecycle Overview\n\nThe container management system orchestrates Docker containers through a complete lifecycle from base image preparation to final cleanup. Each container is configured with SSH access, GPU capabilities, and custom software environments based on allocation requirements.\n\n```mermaid\nstateDiagram-v2\n    [*] --> ImageBuilding: \"build_sample_container()\"\n    ImageBuilding --> ImageReady: \"ssh-image-base created\"\n    ImageReady --> ContainerCreation: \"run_container()\"\n    ContainerCreation --> ContainerRunning: \"status: created\"\n    ContainerRunning --> ContainerPaused: \"pause_container()\"\n    ContainerPaused --> ContainerRunning: \"unpause_container()\"\n    ContainerRunning --> ContainerStopped: \"kill_container()\"\n    ContainerStopped --> [*]: \"cleanup complete\"\n    ContainerRunning --> KeyExchange: \"exchange_key_container()\"\n    KeyExchange --> ContainerRunning: \"SSH keys updated\"\n    ContainerRunning --> ContainerRestarted: \"restart_container()\"\n    ContainerRestarted --> ContainerRunning: \"restart complete\"\n```\n\nSources: [neurons/Miner/container.py:57-103](), [neurons/Miner/container.py:384-420](), [neurons/Miner/container.py:421-473]()\n\n## Base Image Management\n\nThe system maintains a base container image `ssh-image-base` that provides the foundation for all allocated containers. This image includes SSH server configuration, Python runtime, and GPU support.\n\n### Base Image Construction\n\n```mermaid\nflowchart TD\n    A[\"build_sample_container()\"] --> B[\"Check existing images\"]\n    B --> C{Base image exists?}\n    C -->|Yes| D[\"Return existing\"]\n    C -->|No| E[\"Build from pytorch/pytorch:2.7.0-cuda12.6-cudnn9-runtime\"]\n    E --> F[\"Install SSH server\"]\n    F --> G[\"Configure SSH settings\"]\n    G --> H[\"Install Python packages\"]\n    H --> I[\"Tag as ssh-image-base:latest\"]\n    I --> J[\"Base image ready\"]\n```\n\nThe base image includes:\n- PyTorch CUDA runtime environment\n- SSH server with root access enabled\n- Python 3 and pip package manager\n- Essential build tools and libraries\n- Conda environment configuration\n\nSources: [neurons/Miner/container.py:280-368]()\n\n## Container Creation and Configuration\n\nWhen a resource allocation request is received, the system creates a customized container based on the base image and specific requirements.\n\n### Container Creation Process\n\n```mermaid\nsequenceDiagram\n    participant AC as \"Allocation Controller\"\n    participant CM as \"Container Manager\"\n    participant DC as \"Docker Client\"\n    participant FS as \"File System\"\n    \n    AC->>CM: \"run_container(cpu_usage, ram_usage, gpu_usage, public_key, docker_requirement, testing)\"\n    CM->>CM: \"kill_container()\" \n    CM->>CM: \"password_generator(10)\"\n    CM->>CM: \"build_sample_container()\"\n    CM->>FS: \"Create Dockerfile with custom requirements\"\n    CM->>DC: \"images.build(path, dockerfile, tag=ssh-image)\"\n    CM->>DC: \"containers.run(image=ssh-image, device_requests=[GPU], ports={22: ssh_port})\"\n    DC-->>CM: \"Container created\"\n    CM->>CM: \"rsa.encrypt_data(public_key, connection_info)\"\n    CM->>FS: \"Write allocation_key file\"\n    CM-->>AC: \"Return encrypted connection info\"\n```\n\n### Container Configuration Parameters\n\n| Parameter | Description | Example |\n|-----------|-------------|---------|\n| `cpu_assignment` | CPU cores allocated | `\"0-1\"` for 2 cores |\n| `ram_limit` | Memory limit | `\"5g\"` for 5GB |\n| `hard_disk_capacity` | Storage limit | `\"100g\"` for 100GB |\n| `gpu_capacity` | GPU allocation | `\"all\"` for all GPUs |\n| `ssh_port` | SSH access port | `4444` |\n| `shm_size` | Shared memory size | `\"7g\"` (90% of available) |\n\nSources: [neurons/Miner/container.py:105-207]()\n\n## Security and Access Control\n\nThe container management system implements a multi-layered security model using RSA encryption and allocation key verification.\n\n### Security Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Client Side\"\n        CL[\"Client\"] \n        PRV[\"Private Key\"]\n        PUB[\"Public Key\"]\n    end\n    \n    subgraph \"Miner Container System\"\n        AK[\"allocation_key file\"]\n        CM[\"Container Manager\"]\n        SSH[\"SSH Container\"]\n    end\n    \n    subgraph \"Authentication Flow\"\n        ENC[\"Encrypted Connection Info\"]\n        DEC[\"Decrypted Credentials\"]\n    end\n    \n    CL --> PUB\n    PUB --> CM\n    CM --> ENC\n    ENC --> DEC\n    PRV --> DEC\n    DEC --> SSH\n    \n    CM --> AK\n    AK --> CM\n```\n\n### Allocation Key Management\n\nThe system uses allocation keys to verify container access permissions:\n\n- **Key Storage**: Public keys are base64-encoded and stored in `allocation_key` file\n- **Key Verification**: All container operations require matching public key\n- **Key Rotation**: SSH keys can be updated through `exchange_key_container()`\n\n### Security Functions\n\n| Function | Purpose | Key Verification |\n|----------|---------|------------------|\n| `restart_container()` | Restart existing container | Required |\n| `pause_container()` | Pause container execution | Required |\n| `unpause_container()` | Resume container execution | Required |\n| `exchange_key_container()` | Update SSH keys | Required |\n\nSources: [neurons/Miner/container.py:370-382](), [neurons/Miner/container.py:384-420](), [neurons/Miner/container.py:475-521]()\n\n## Integration with Allocation System\n\nContainer management integrates with the resource allocation system through the `allocate.py` module, which orchestrates container lifecycle during allocation requests.\n\n### Allocation Integration Flow\n\n```mermaid\nsequenceDiagram\n    participant API as \"RegisterAPI\"\n    participant ALLOC as \"Allocation Controller\"\n    participant CONT as \"Container Manager\"\n    participant SCHED as \"Scheduler\"\n    \n    API->>ALLOC: \"register_allocation(timeline, device_requirement, public_key, docker_requirement)\"\n    ALLOC->>CONT: \"kill_container()\" \n    ALLOC->>CONT: \"run_container(cpu_usage, ram_usage, hard_disk_usage, gpu_usage, public_key, docker_requirement, testing)\"\n    CONT-->>ALLOC: \"Container info + encrypted credentials\"\n    ALLOC->>SCHED: \"start(timeline)\"\n    ALLOC-->>API: \"Allocation result\"\n    \n    Note over SCHED: \"Timeline expires\"\n    SCHED->>CONT: \"kill_container(deregister=True)\"\n```\n\n### Container Types\n\nThe system supports two container types:\n\n- **Production Containers** (`container_name`): Long-running containers for actual resource allocation\n- **Test Containers** (`container_name_test`): Short-lived containers for validation and health checks\n\nSources: [neurons/Miner/allocate.py:29-62](), [neurons/Miner/allocate.py:66-94]()\n\n## Container Monitoring and Health Checks\n\nThe system provides container status monitoring and health checking capabilities used by validators and the allocation system.\n\n### Health Check Functions\n\n```mermaid\nflowchart LR\n    subgraph \"Health Check Operations\"\n        A[\"check_container()\"] --> B{Container exists?}\n        B -->|Yes| C{Status == running?}\n        B -->|No| D[\"Return False\"]\n        C -->|Yes| E[\"Return True\"]\n        C -->|No| D\n    end\n    \n    subgraph \"Allocation Status\"\n        F[\"check_if_allocated(public_key)\"] --> G{allocation_key exists?}\n        G -->|Yes| H{Key matches?}\n        G -->|No| I[\"Return False\"]\n        H -->|Yes| J{Container running?}\n        H -->|No| I\n        J -->|Yes| K[\"Return True\"]\n        J -->|No| I\n    end\n```\n\n### Monitoring Integration\n\nThe container system integrates with validator monitoring through:\n\n- **Miner Checker**: Validators use `miner_checker.py` to test container allocation and SSH access\n- **Health Endpoints**: API endpoints query container status for resource availability\n- **Allocation Tracking**: Container state is synchronized with allocation records\n\nSources: [neurons/Miner/container.py:210-222](), [neurons/Miner/allocate.py:106-137](), [neurons/miner_checker.py:85-151]()\n\n## Container Cleanup and Resource Management\n\nThe system implements comprehensive cleanup procedures to prevent resource leaks and ensure proper container lifecycle management.\n\n### Cleanup Operations\n\n```mermaid\ngraph TD\n    A[\"kill_container(deregister)\"] --> B{deregister flag?}\n    B -->|True| C[\"Kill production container\"]\n    B -->|False| D[\"Kill test container only\"]\n    C --> E[\"Find container_name\"]\n    D --> F[\"Find container_name_test\"]\n    E --> G{Container running?}\n    F --> H{Container running?}\n    G -->|Yes| I[\"exec_run('kill -15 1')\"]\n    G -->|No| J[\"remove()\"]\n    H -->|Yes| K[\"exec_run('kill -15 1')\"]\n    H -->|No| L[\"remove()\"]\n    I --> M[\"wait()\"]\n    K --> N[\"wait()\"]\n    M --> J\n    N --> L\n    J --> O[\"images.prune(dangling=True)\"]\n    L --> O\n```\n\nThe cleanup process includes:\n- Graceful container termination using SIGTERM\n- Container removal from Docker\n- Dangling image cleanup\n- Allocation key file management\n\nSources: [neurons/Miner/container.py:57-103]()",
    "resolved_links": [
      {
        "text": "min_compute.yml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/min_compute.yml",
        "original_deepwiki_href": "min_compute.yml",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Miner/allocate.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py",
        "original_deepwiki_href": "neurons/Miner/allocate.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Miner/container.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py",
        "original_deepwiki_href": "neurons/Miner/container.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/miner_checker.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner_checker.py",
        "original_deepwiki_href": "neurons/miner_checker.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "tests/test_miner_container.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/tests/test_miner_container.py",
        "original_deepwiki_href": "tests/test_miner_container.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "tests/test_rsa_encryption.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/tests/test_rsa_encryption.py",
        "original_deepwiki_href": "tests/test_rsa_encryption.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Miner/container.py:57-103",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:384-420",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:421-473",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:280-368",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:105-207",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:370-382",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:475-521",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py:29-62",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py:66-94",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:210-222",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py:106-137",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner_checker.py:85-151",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Resource Allocation",
        "href": "/miner-system/resource-allocation#3.3",
        "original_deepwiki_href": "#3.3",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Specs, Allocate, and Challenge Protocols",
        "href": "/communication-protocols/specs-allocate-and-challenge-protocols#5.1",
        "original_deepwiki_href": "#5.1",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "stateDiagram-v2\n    [*] --> ImageBuilding: \"build_sample_container()\"\n    ImageBuilding --> ImageReady: \"ssh-image-base created\"\n    ImageReady --> ContainerCreation: \"run_container()\"\n    ContainerCreation --> ContainerRunning: \"status: created\"\n    ContainerRunning --> ContainerPaused: \"pause_container()\"\n    ContainerPaused --> ContainerRunning: \"unpause_container()\"\n    ContainerRunning --> ContainerStopped: \"kill_container()\"\n    ContainerStopped --> [*]: \"cleanup complete\"\n    ContainerRunning --> KeyExchange: \"exchange_key_container()\"\n    KeyExchange --> ContainerRunning: \"SSH keys updated\"\n    ContainerRunning --> ContainerRestarted: \"restart_container()\"\n    ContainerRestarted --> ContainerRunning: \"restart complete\"",
      "flowchart TD\n    A[\"build_sample_container()\"] --> B[\"Check existing images\"]\n    B --> C{Base image exists?}\n    C -->|Yes| D[\"Return existing\"]\n    C -->|No| E[\"Build from pytorch/pytorch:2.7.0-cuda12.6-cudnn9-runtime\"]\n    E --> F[\"Install SSH server\"]\n    F --> G[\"Configure SSH settings\"]\n    G --> H[\"Install Python packages\"]\n    H --> I[\"Tag as ssh-image-base:latest\"]\n    I --> J[\"Base image ready\"]",
      "sequenceDiagram\n    participant AC as \"Allocation Controller\"\n    participant CM as \"Container Manager\"\n    participant DC as \"Docker Client\"\n    participant FS as \"File System\"\n    \n    AC->>CM: \"run_container(cpu_usage, ram_usage, gpu_usage, public_key, docker_requirement, testing)\"\n    CM->>CM: \"kill_container()\" \n    CM->>CM: \"password_generator(10)\"\n    CM->>CM: \"build_sample_container()\"\n    CM->>FS: \"Create Dockerfile with custom requirements\"\n    CM->>DC: \"images.build(path, dockerfile, tag=ssh-image)\"\n    CM->>DC: \"containers.run(image=ssh-image, device_requests=[GPU], ports={22: ssh_port})\"\n    DC-->>CM: \"Container created\"\n    CM->>CM: \"rsa.encrypt_data(public_key, connection_info)\"\n    CM->>FS: \"Write allocation_key file\"\n    CM-->>AC: \"Return encrypted connection info\"",
      "graph TB\n    subgraph \"Client Side\"\n        CL[\"Client\"] \n        PRV[\"Private Key\"]\n        PUB[\"Public Key\"]\n    end\n    \n    subgraph \"Miner Container System\"\n        AK[\"allocation_key file\"]\n        CM[\"Container Manager\"]\n        SSH[\"SSH Container\"]\n    end\n    \n    subgraph \"Authentication Flow\"\n        ENC[\"Encrypted Connection Info\"]\n        DEC[\"Decrypted Credentials\"]\n    end\n    \n    CL --> PUB\n    PUB --> CM\n    CM --> ENC\n    ENC --> DEC\n    PRV --> DEC\n    DEC --> SSH\n    \n    CM --> AK\n    AK --> CM",
      "sequenceDiagram\n    participant API as \"RegisterAPI\"\n    participant ALLOC as \"Allocation Controller\"\n    participant CONT as \"Container Manager\"\n    participant SCHED as \"Scheduler\"\n    \n    API->>ALLOC: \"register_allocation(timeline, device_requirement, public_key, docker_requirement)\"\n    ALLOC->>CONT: \"kill_container()\" \n    ALLOC->>CONT: \"run_container(cpu_usage, ram_usage, hard_disk_usage, gpu_usage, public_key, docker_requirement, testing)\"\n    CONT-->>ALLOC: \"Container info + encrypted credentials\"\n    ALLOC->>SCHED: \"start(timeline)\"\n    ALLOC-->>API: \"Allocation result\"\n    \n    Note over SCHED: \"Timeline expires\"\n    SCHED->>CONT: \"kill_container(deregister=True)\"",
      "flowchart LR\n    subgraph \"Health Check Operations\"\n        A[\"check_container()\"] --> B{Container exists?}\n        B -->|Yes| C{Status == running?}\n        B -->|No| D[\"Return False\"]\n        C -->|Yes| E[\"Return True\"]\n        C -->|No| D\n    end\n    \n    subgraph \"Allocation Status\"\n        F[\"check_if_allocated(public_key)\"] --> G{allocation_key exists?}\n        G -->|Yes| H{Key matches?}\n        G -->|No| I[\"Return False\"]\n        H -->|Yes| J{Container running?}\n        H -->|No| I\n        J -->|Yes| K[\"Return True\"]\n        J -->|No| I\n    end",
      "graph TD\n    A[\"kill_container(deregister)\"] --> B{deregister flag?}\n    B -->|True| C[\"Kill production container\"]\n    B -->|False| D[\"Kill test container only\"]\n    C --> E[\"Find container_name\"]\n    D --> F[\"Find container_name_test\"]\n    E --> G{Container running?}\n    F --> H{Container running?}\n    G -->|Yes| I[\"exec_run('kill -15 1')\"]\n    G -->|No| J[\"remove()\"]\n    H -->|Yes| K[\"exec_run('kill -15 1')\"]\n    H -->|No| L[\"remove()\"]\n    I --> M[\"wait()\"]\n    K --> N[\"wait()\"]\n    M --> J\n    N --> L\n    J --> O[\"images.prune(dangling=True)\"]\n    L --> O"
    ],
    "potential_frontmatter": {
      "title": "Container Management"
    }
  },
  "/neuralinternet/ni-compute/3.2-challenge-response": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/3.2-challenge-response",
    "title": "Challenge Response",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/3.2-challenge-response",
    "level": 1,
    "target_astro_path": "/miner-system/challenge-response",
    "main_markdown_content": "# Challenge Response\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.env.example](.env.example)\n- [neurons/Miner/pow.py](neurons/Miner/pow.py)\n- [neurons/Validator/pow.py](neurons/Validator/pow.py)\n- [neurons/miner.py](neurons/miner.py)\n\n</details>\n\n\n\nThis document describes the Challenge Response system within NI Compute's miner implementation. The Challenge Response mechanism allows miners to respond to computational challenges issued by validators, primarily for Proof of GPU (PoG) verification. This is one of several verification methods used in the subnet to ensure miners have the hardware capabilities they claim and are operational. For details on how validators score miners based on challenge results, see [Scoring System](#2.2).\n\n## Overview\n\nThe Challenge Response system enables validators to verify miners' GPU capabilities by requiring them to solve cryptographic challenges using their hardware. These challenges are designed to:\n\n1. Verify that miners possess the GPU resources they claim\n2. Ensure miners have properly configured systems with working CUDA and hashcat\n3. Provide a standardized benchmark for comparison across different hardware\n4. Prevent fraudulent resource claims by requiring proof of computational power\n\n```mermaid\nflowchart TD\n    subgraph \"Challenge Response System\"\n        V[\"Validator\"] -->|\"1. Issues Challenge\"| M[\"Miner\"]\n        M -->|\"2. Processes with GPU\"| HC[\"Hashcat Process\"]\n        HC -->|\"3. Returns Solution\"| M\n        M -->|\"4. Returns Result\"| V\n        V -->|\"5. Verifies & Scores\"| V\n    end\n\n    subgraph \"Internal Components\"\n        M --- POW[\"run_miner_pow()\"]\n        POW --- RH[\"run_hashcat()\"]\n        RH --- Q[\"Challenge Queue\"]\n    end\n```\n\nSources: [neurons/miner.py:491-515](), [neurons/Miner/pow.py:175-205]()\n\n## Challenge Structure\n\nChallenges sent by validators to miners contain specific parameters that define the computational task:\n\n| Parameter | Description |\n|-----------|-------------|\n| `challenge_hash` | The hash that needs to be cracked |\n| `challenge_salt` | Salt value used in the hash generation |\n| `challenge_mode` | Hashcat mode identifier for the algorithm |\n| `challenge_chars` | Available characters for the password |\n| `challenge_mask` | Pattern mask defining password structure |\n| `challenge_difficulty` | Difficulty level of the challenge |\n\nThe challenges are cryptographic problems that require GPU acceleration to solve efficiently, typically involving password recovery for a hashed value.\n\nSources: [neurons/miner.py:491-500](), [neurons/Validator/pow.py:29-72]()\n\n## Challenge Response Flow\n\nThe Challenge Response process follows a specific sequence of operations between validators and miners:\n\n```mermaid\nsequenceDiagram\n    participant V as Validator\n    participant M as Miner\n    participant HC as Hashcat Process\n    participant Q as Challenge Queue\n\n    V->>M: Send Challenge(hash, salt, mode, chars, mask, difficulty)\n    note over M: Receive challenge with Challenge Protocol\n    M->>Q: Add challenge to queue\n    note over Q: Maintain FIFO order for challenges\n    Q->>HC: Process next challenge when available\n    note over HC: Execute hashcat with GPU acceleration\n    HC-->>M: Return result (password or error)\n    M-->>V: Return response with execution time and result\n    V->>V: Verify and score the response\n```\n\nSources: [neurons/miner.py:491-515](), [neurons/Miner/pow.py:175-205]()\n\n## Challenge Handling on Miner Side\n\nWhen a miner receives a challenge, it follows these steps:\n\n1. The miner receives the challenge through the `challenge()` method in the `Miner` class.\n2. The challenge is identified by a run ID constructed from validator ID, difficulty, and portion of the challenge hash.\n3. The challenge is queued in a FIFO manner, ensuring orderly processing of multiple challenges.\n4. When processing begins, the miner calls `run_miner_pow()` which executes `run_hashcat()` with the appropriate parameters.\n5. Hashcat is run on the miner's GPU to attempt to recover the original input from the hash.\n6. Results, including success/failure and execution time, are returned to the validator.\n\nThe miner's challenge handler includes validation to prevent invalid challenges (e.g., difficulty <= 0):\n\n```mermaid\nflowchart TD\n    subgraph \"Miner Challenge Handler\"\n        R[\"Receive Challenge\"] --> V[\"Validate Challenge Parameters\"]\n        V -->|\"Valid\"| Q[\"Queue Challenge\"]\n        V -->|\"Invalid\"| E[\"Return Error\"]\n        \n        Q --> P[\"Process with run_miner_pow()\"]\n        P --> HC[\"Execute Hashcat\"]\n        HC --> S[\"Send Results to Validator\"]\n    end\n\n    subgraph \"Challenge Queue Management\"\n        CR[\"Challenge Received\"] --> CQ[\"Check Queue\"]\n        CQ -->|\"Empty\"| PP[\"Process Immediately\"]\n        CQ -->|\"Not Empty\"| AQ[\"Add to Queue\"]\n        AQ --> WP[\"Wait Processing\"]\n    end\n```\n\nSources: [neurons/miner.py:491-515](), [neurons/Miner/pow.py:29-205]()\n\n## Hashcat Execution\n\nThe core of the challenge response system uses Hashcat, a popular password recovery tool that leverages GPU acceleration:\n\n1. The miner configures Hashcat with parameters from the challenge:\n   - Hash and salt combination\n   - Attack mode (mode 3 for mask attack)\n   - Device type (2 for CUDA/GPU)\n   - Hash mode (defining the hash algorithm)\n   - Character set and mask\n   - Workload profile and extended options\n\n2. Hashcat is executed as a subprocess with timeout protection to prevent hanging:\n   - Successful execution returns the recovered password\n   - Timeouts or errors are captured and returned as part of the response\n\n3. A queue system ensures challenges are processed sequentially, preventing resource contention:\n   - Challenges are added to a FIFO queue\n   - New challenges wait until currently running challenges complete\n   - Execution time is tracked and returned for performance evaluation\n\nSources: [neurons/Miner/pow.py:43-172]()\n\n## Challenge Generation (Validator Side)\n\nWhile this document focuses on the miner's perspective, understanding how validators generate challenges provides context:\n\nValidators create challenges using:\n1. Secure random password generation with cryptographic randomness\n2. Hash generation using BLAKE2b algorithm\n3. Configuration of difficulty levels and character sets\n4. Creation of formatted challenges with all necessary parameters\n\nThe generated challenge is designed to be:\n- Verifiable (validator knows the expected answer)\n- Hardware-intensive (requires GPU acceleration)\n- Time-sensitive (expected completion time correlates with hardware capability)\n\nSources: [neurons/Validator/pow.py:29-72]()\n\n## Priority and Blacklisting\n\nThe Challenge Response system includes priority and blacklisting mechanisms:\n\n1. **Priority Handling**: Challenge requests are prioritized based on:\n   - Validator stake (higher stake = higher priority)\n   - Base priority value (`miner_priority_challenge`)\n   - This ensures that challenges from trusted validators with more stake are processed first\n\n2. **Blacklisting**: Miners can reject challenge requests based on:\n   - Unrecognized hotkeys (validators not registered in the metagraph)\n   - Insufficient stake (below `validator_permit_stake`)\n   - Manually blacklisted validators\n   - Known exploiters (from `SUSPECTED_EXPLOITERS_HOTKEYS` list)\n\nThese mechanisms help protect miners from spam or malicious challenges while ensuring responsiveness to legitimate validators.\n\nSources: [neurons/miner.py:482-488](), [neurons/miner.py:330-373]()\n\n## Integration with Miner System\n\nThe Challenge Response system is integrated into the broader miner architecture:\n\n```mermaid\nflowchart TD\n    subgraph \"Miner System\"\n        AX[\"ComputeSubnetAxon\"] -->|\"Receives Requests\"| CR[\"Challenge Response\"]\n        AX -->|\"Receives Requests\"| AL[\"Allocate\"]\n        \n        CR -->|\"Uses\"| POW[\"PoW Functions\"]\n        POW -->|\"Executes\"| HC[\"Hashcat\"]\n        \n        CR -->|\"Reports\"| WB[\"WandB Monitoring\"]\n    end\n    \n    subgraph \"Challenge Response Components\"\n        CH[\"challenge() Method\"] --> BL[\"blacklist_challenge()\"]\n        CH --> PR[\"priority_challenge()\"]\n        CH --> RM[\"run_miner_pow()\"]\n        RM --> RH[\"run_hashcat()\"]\n    end\n```\n\nThe Challenge Response system attaches to the miner's axon to receive requests from validators, alongside other endpoints like allocation requests.\n\nSources: [neurons/miner.py:227-235](), [neurons/miner.py:491-515]()\n\n## CUDA Availability Check\n\nBefore processing challenges, miners verify CUDA availability to ensure GPU acceleration works:\n\n```python\ndef check_cuda_availability():\n    import torch\n\n    if torch.cuda.is_available():\n        device_count = torch.cuda.device_count()\n        bt.logging.info(f\"CUDA is available with {device_count} CUDA device(s)!\")\n    else:\n        bt.logging.warning(\n            \"CUDA is not available or not properly configured on this system.\"\n        )\n```\n\nThis check happens during miner initialization, ensuring that the challenge response system can utilize GPU acceleration.\n\nSources: [neurons/Miner/pow.py:31-40]()\n\n## Performance Considerations\n\nChallenge response performance is a critical factor in miner evaluation:\n\n1. **Execution Time**: The time taken to solve challenges directly influences scoring\n2. **Queue Management**: FIFO queuing ensures fair processing of challenges\n3. **Timeout Handling**: Challenges have a maximum execution time (`pow_timeout`)\n4. **Workload Profile**: Configurable workload profiles allow miners to balance performance vs. system load\n\nMiners can configure several parameters to optimize challenge response:\n- `miner_hashcat_path`: Path to the hashcat executable\n- `miner_hashcat_workload_profile`: Performance vs. system impact balance\n- `miner_hashcat_extended_options`: Additional hashcat options\n\nSources: [neurons/Miner/pow.py:51-172](), [neurons/miner.py:170-172]()\n\n## Security Considerations\n\nThe Challenge Response system includes several security features:\n\n1. **Timeout Protection**: Prevents validators from hanging miners with impossible challenges\n2. **Blacklisting**: Protects against malicious validators\n3. **Process Isolation**: Hashcat runs as a separate subprocess\n4. **Error Handling**: Robust error handling prevents system crashes\n5. **Queue System**: Prevents resource exhaustion from multiple simultaneous challenges\n\nSources: [neurons/Miner/pow.py:92-172](), [neurons/miner.py:330-373]()\n\n## Troubleshooting\n\nCommon issues with Challenge Response include:\n\n| Issue | Possible Causes | Solutions |\n|-------|-----------------|-----------|\n| Timeouts | GPU overload, insufficient hardware | Adjust workload profile, upgrade hardware |\n| Errors | Missing/incompatible hashcat, CUDA issues | Check hashcat installation, update drivers |\n| No response | Blacklisting, network issues | Check blacklist settings, network connectivity |\n| Poor performance | Hardware limitations, competing processes | Close other GPU applications, optimize settings |\n\nSources: [neurons/Miner/pow.py:147-165]()\n\n## Conclusion\n\nThe Challenge Response system is a critical component in NI Compute's validator-miner relationship, allowing objective verification of GPU capabilities through computational challenges. By requiring miners to solve cryptographic problems with their GPU hardware, validators can ensure that miners possess the resources they claim to have. This mechanism, combined with other verification methods, creates a trustworthy decentralized GPU marketplace.",
    "resolved_links": [
      {
        "text": ".env.example",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/.env.example",
        "original_deepwiki_href": ".env.example",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Miner/pow.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py",
        "original_deepwiki_href": "neurons/Miner/pow.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/pow.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pow.py",
        "original_deepwiki_href": "neurons/Validator/pow.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/miner.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py",
        "original_deepwiki_href": "neurons/miner.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/miner.py:491-515",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/pow.py:175-205",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:491-500",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/pow.py:29-72",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/pow.py:29-205",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/pow.py:43-172",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:482-488",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:330-373",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:227-235",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/pow.py:31-40",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/pow.py:51-172",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:170-172",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/pow.py:92-172",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/pow.py:147-165",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Scoring System",
        "href": "/validator-system/scoring-system#2.2",
        "original_deepwiki_href": "#2.2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "flowchart TD\n    subgraph \"Challenge Response System\"\n        V[\"Validator\"] -->|\"1. Issues Challenge\"| M[\"Miner\"]\n        M -->|\"2. Processes with GPU\"| HC[\"Hashcat Process\"]\n        HC -->|\"3. Returns Solution\"| M\n        M -->|\"4. Returns Result\"| V\n        V -->|\"5. Verifies & Scores\"| V\n    end\n\n    subgraph \"Internal Components\"\n        M --- POW[\"run_miner_pow()\"]\n        POW --- RH[\"run_hashcat()\"]\n        RH --- Q[\"Challenge Queue\"]\n    end",
      "sequenceDiagram\n    participant V as Validator\n    participant M as Miner\n    participant HC as Hashcat Process\n    participant Q as Challenge Queue\n\n    V->>M: Send Challenge(hash, salt, mode, chars, mask, difficulty)\n    note over M: Receive challenge with Challenge Protocol\n    M->>Q: Add challenge to queue\n    note over Q: Maintain FIFO order for challenges\n    Q->>HC: Process next challenge when available\n    note over HC: Execute hashcat with GPU acceleration\n    HC-->>M: Return result (password or error)\n    M-->>V: Return response with execution time and result\n    V->>V: Verify and score the response",
      "flowchart TD\n    subgraph \"Miner Challenge Handler\"\n        R[\"Receive Challenge\"] --> V[\"Validate Challenge Parameters\"]\n        V -->|\"Valid\"| Q[\"Queue Challenge\"]\n        V -->|\"Invalid\"| E[\"Return Error\"]\n        \n        Q --> P[\"Process with run_miner_pow()\"]\n        P --> HC[\"Execute Hashcat\"]\n        HC --> S[\"Send Results to Validator\"]\n    end\n\n    subgraph \"Challenge Queue Management\"\n        CR[\"Challenge Received\"] --> CQ[\"Check Queue\"]\n        CQ -->|\"Empty\"| PP[\"Process Immediately\"]\n        CQ -->|\"Not Empty\"| AQ[\"Add to Queue\"]\n        AQ --> WP[\"Wait Processing\"]\n    end",
      "flowchart TD\n    subgraph \"Miner System\"\n        AX[\"ComputeSubnetAxon\"] -->|\"Receives Requests\"| CR[\"Challenge Response\"]\n        AX -->|\"Receives Requests\"| AL[\"Allocate\"]\n        \n        CR -->|\"Uses\"| POW[\"PoW Functions\"]\n        POW -->|\"Executes\"| HC[\"Hashcat\"]\n        \n        CR -->|\"Reports\"| WB[\"WandB Monitoring\"]\n    end\n    \n    subgraph \"Challenge Response Components\"\n        CH[\"challenge() Method\"] --> BL[\"blacklist_challenge()\"]\n        CH --> PR[\"priority_challenge()\"]\n        CH --> RM[\"run_miner_pow()\"]\n        RM --> RH[\"run_hashcat()\"]\n    end"
    ],
    "potential_frontmatter": {
      "title": "Challenge Response"
    }
  },
  "/neuralinternet/ni-compute/3.3-resource-allocation": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/3.3-resource-allocation",
    "title": "Resource Allocation",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/3.3-resource-allocation",
    "level": 1,
    "target_astro_path": "/miner-system/resource-allocation",
    "main_markdown_content": "# Resource Allocation\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/Miner/allocate.py](neurons/Miner/allocate.py)\n- [neurons/Miner/container.py](neurons/Miner/container.py)\n- [neurons/miner.py](neurons/miner.py)\n- [neurons/miner_checker.py](neurons/miner_checker.py)\n\n</details>\n\n\n\nThis document covers how miners in the NI Compute Subnet handle resource allocation requests from validators and external clients. Resource allocation involves provisioning Docker containers with specified compute resources (CPU, RAM, GPU, storage) and providing secure SSH access to allocated environments.\n\nFor information about the Resource Allocation API that external clients use to request resources, see [Resource Allocation API](#4). For details about container lifecycle management, see [Container Management](#3.1).\n\n## Allocation Request Processing\n\nThe miner's resource allocation system is built around the `Allocate` synapse protocol. When a validator or client sends an allocation request, the miner processes it through several stages:\n\n```mermaid\nflowchart TD\n    A[\"Allocate Synapse Request\"] --> B[\"blacklist_allocate()\"]\n    B --> C{\"Blacklisted?\"}\n    C -->|Yes| D[\"Reject Request\"]\n    C -->|No| E[\"priority_allocate()\"]\n    E --> F[\"allocate() Method\"]\n    F --> G{\"Checking Mode?\"}\n    G -->|Yes| H[\"check_allocation()\"]\n    G -->|No| I{\"Timeline > 0?\"}\n    I -->|Yes| J[\"register_allocation()\"]\n    I -->|No| K[\"deregister_allocation()\"]\n    H --> L[\"Return Status\"]\n    J --> M[\"run_container()\"]\n    K --> N[\"kill_container()\"]\n    M --> O[\"Update WandB State\"]\n    N --> O\n    O --> L\n```\n\n**Allocation Request Flow**\n\nSources: [neurons/miner.py:419-479](), [neurons/miner.py:397-403]()\n\nThe `allocate` method in the `Miner` class handles three types of operations:\n\n| Operation Type | Condition | Action |\n|---------------|-----------|---------|\n| **Check Allocation** | `checking=True, timeline>0` | Verify resource availability without allocating |\n| **Register Allocation** | `checking=False, timeline>0` | Create new resource allocation |\n| **Deregister Allocation** | `checking=False, timeline=0` | Remove existing allocation |\n\n## Resource Registration Process\n\nWhen a miner receives a valid allocation request with `timeline > 0`, it initiates the resource registration process:\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator/Client\"\n    participant M as \"Miner.allocate()\"\n    participant A as \"register_allocation()\"\n    participant C as \"Container Management\"\n    participant D as \"Docker Engine\"\n    participant S as \"Schedule Manager\"\n    \n    V->>M: \"Allocate(timeline=3600, device_requirement={...})\"\n    M->>A: \"register_allocation(timeline, device_requirement, public_key)\"\n    A->>C: \"kill_container() - cleanup existing\"\n    C->>D: \"Remove existing containers\"\n    A->>C: \"run_container(cpu_usage, ram_usage, gpu_usage, ...)\"\n    C->>D: \"Create container with resource limits\"\n    D->>C: \"Return container info + encrypted SSH details\"\n    C->>A: \"Return allocation status + connection info\"\n    A->>S: \"start(timeline) - schedule auto-deallocation\"\n    A->>M: \"Return allocation result\"\n    M->>V: \"Return Allocate synapse with status + SSH info\"\n```\n\n**Resource Registration Sequence**\n\nSources: [neurons/Miner/allocate.py:29-62](), [neurons/miner.py:463-476]()\n\nThe registration process transforms device requirements into Docker container configurations:\n\n```mermaid\ngraph LR\n    A[\"Device Requirements\"] --> B[\"Resource Parsing\"]\n    B --> C[\"Docker Configuration\"]\n    C --> D[\"Container Creation\"]\n    \n    subgraph \"Resource Parsing\"\n        B1[\"CPU Count → CPU Assignment\"]\n        B2[\"RAM Capacity → Memory Limit\"]\n        B3[\"Disk Capacity → Storage Limit\"]\n        B4[\"GPU Capacity → Device Requests\"]\n    end\n    \n    subgraph \"Container Creation\"\n        D1[\"build_sample_container()\"]\n        D2[\"Generate SSH Credentials\"]\n        D3[\"Create Dockerfile\"]\n        D4[\"Run Container with Limits\"]\n        D5[\"Return Encrypted Connection Info\"]\n    end\n    \n    B --> B1\n    B --> B2\n    B --> B3\n    B --> B4\n    \n    C --> D1\n    C --> D2\n    C --> D3\n    C --> D4\n    C --> D5\n```\n\n**Resource Requirement Processing**\n\nSources: [neurons/Miner/allocate.py:34-51](), [neurons/Miner/container.py:105-207]()\n\n## Container Resource Management\n\nThe container management system translates abstract resource requirements into concrete Docker container limits:\n\n### Resource Limit Translation\n\n| Resource Type | Input Format | Docker Configuration | Implementation |\n|--------------|--------------|---------------------|----------------|\n| **CPU** | `{\"count\": 2}` | `cpuset_cpus=\"0-1\"` | [container.py:110-137]() |\n| **RAM** | `{\"capacity\": 5368709120}` | `mem_limit=\"5g\"` | [container.py:111-133]() |\n| **GPU** | `{\"capacity\": \"all\"}` | `device_requests=[DeviceRequest(...)]` | [container.py:167-175]() |\n| **Storage** | `{\"capacity\": 107374182400}` | Volume mount limits | [container.py:112-149]() |\n\n### Container Lifecycle Operations\n\nThe system supports several container management operations beyond basic allocation:\n\n```mermaid\nstateDiagram-v2\n    [*] --> Available: \"No Container\"\n    Available --> Creating: \"register_allocation()\"\n    Creating --> Running: \"Container Created\"\n    Running --> Paused: \"pause_container()\"\n    Paused --> Running: \"unpause_container()\"\n    Running --> Running: \"restart_container()\"\n    Running --> Available: \"deregister_allocation()\"\n    Running --> Available: \"Timeline Expired\"\n    \n    note right of Running: \"SSH Access Available\\nResource Limits Applied\"\n    note right of Paused: \"Container Suspended\\nResources Released\"\n```\n\n**Container State Management**\n\nSources: [neurons/Miner/container.py:421-520](), [neurons/miner.py:437-458]()\n\n## Security and Access Control\n\nResource allocation implements multiple security layers:\n\n### Public Key Authentication\n\nAll allocation operations require RSA public key authentication:\n\n```mermaid\nflowchart LR\n    A[\"Client Public Key\"] --> B[\"Allocation Request\"]\n    B --> C[\"Container Creation\"]\n    C --> D[\"SSH Key Setup\"]\n    D --> E[\"Connection Info Encryption\"]\n    E --> F[\"Encrypted Response\"]\n    \n    subgraph \"Storage\"\n        G[\"allocation_key file\"]\n        H[\"Base64 Encoded Public Key\"]\n    end\n    \n    A --> G\n    G --> H\n    H --> I[\"Access Validation\"]\n    I --> J[\"Container Operations\"]\n```\n\n**Public Key Authentication Flow**\n\nSources: [neurons/Miner/container.py:188-200](), [neurons/Miner/allocate.py:74-77]()\n\n### SSH Access Management\n\nThe system provides secure SSH access to allocated containers:\n\n| Operation | Function | Security Check |\n|-----------|----------|----------------|\n| **Key Exchange** | `exchange_key_container()` | Public key validation |\n| **Container Restart** | `restart_container()` | Allocation key verification |\n| **Container Pause** | `pause_container()` | Authentication required |\n\nSources: [neurons/Miner/container.py:475-520](), [neurons/Miner/container.py:384-419]()\n\n## Allocation State Management\n\nThe miner maintains allocation state through multiple mechanisms:\n\n### Local State Storage\n\n```mermaid\ngraph TD\n    A[\"Allocation Request\"] --> B[\"allocation_key File\"]\n    B --> C[\"Base64 Encoded Public Key\"]\n    C --> D[\"Access Validation\"]\n    \n    E[\"Container Status\"] --> F[\"Docker API Queries\"]\n    F --> G[\"check_container()\"]\n    \n    H[\"WandB Integration\"] --> I[\"update_allocated()\"]\n    I --> J[\"Distributed State Sync\"]\n    \n    subgraph \"State Validation\"\n        K[\"check_if_allocated()\"]\n        L[\"File Existence Check\"]\n        M[\"Key Comparison\"]\n        N[\"Container Running Check\"]\n    end\n    \n    D --> K\n    G --> K\n    K --> L\n    K --> M\n    K --> N\n```\n\n**Allocation State Management**\n\nSources: [neurons/Miner/allocate.py:106-137](), [neurons/miner.py:405-417]()\n\n### Automatic Deallocation\n\nThe system includes automatic resource cleanup through timeline-based scheduling:\n\n```mermaid\nsequenceDiagram\n    participant A as \"register_allocation()\"\n    participant S as \"Schedule Manager\"\n    participant T as \"Timer Thread\"\n    participant C as \"Container Manager\"\n    \n    A->>S: \"start(timeline=3600)\"\n    S->>T: \"Create timer for 3600 seconds\"\n    T-->>T: \"Wait for timeline expiry\"\n    T->>C: \"kill_container() after timeout\"\n    C->>C: \"Remove container and cleanup\"\n    \n    Note over T: \"Timeline-based auto-cleanup\\nPrevents resource leaks\"\n```\n\n**Automatic Deallocation Timeline**\n\nSources: [neurons/Miner/allocate.py:57](), [neurons/Miner/schedule.py]()\n\n## Docker Integration\n\nThe allocation system builds upon Docker containers with specific configurations:\n\n### Base Container Setup\n\nThe system uses a pre-built base image (`ssh-image-base`) for faster allocation:\n\n```mermaid\ngraph LR\n    A[\"build_sample_container()\"] --> B[\"pytorch/pytorch:2.7.0-cuda12.6-cudnn9-runtime\"]\n    B --> C[\"Install SSH Server\"]\n    C --> D[\"Configure SSH Settings\"]\n    D --> E[\"Install Python Dependencies\"]\n    E --> F[\"ssh-image-base:latest\"]\n    \n    subgraph \"Runtime Container\"\n        G[\"Custom Dockerfile\"]\n        H[\"User SSH Keys\"]\n        I[\"Resource Limits\"]\n        J[\"ssh-image:latest\"]\n    end\n    \n    F --> G\n    G --> H\n    H --> I\n    I --> J\n```\n\n**Container Image Pipeline**\n\nSources: [neurons/Miner/container.py:280-368](), [neurons/Miner/container.py:136-159]()\n\n### Container Configuration\n\nEach allocation creates a customized container with:\n\n- **SSH Access**: Root user with password and key-based authentication\n- **GPU Support**: NVIDIA GPU access through device requests\n- **Resource Limits**: CPU, memory, and storage constraints\n- **Custom Environment**: User-specified Docker commands and dependencies\n\nSources: [neurons/Miner/container.py:170-181](), [neurons/Miner/container.py:136-146]()",
    "resolved_links": [
      {
        "text": "neurons/Miner/allocate.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py",
        "original_deepwiki_href": "neurons/Miner/allocate.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Miner/container.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py",
        "original_deepwiki_href": "neurons/Miner/container.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/miner.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py",
        "original_deepwiki_href": "neurons/miner.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/miner_checker.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner_checker.py",
        "original_deepwiki_href": "neurons/miner_checker.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/miner.py:419-479",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:397-403",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py:29-62",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:463-476",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py:34-51",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:105-207",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:421-520",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:437-458",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:188-200",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py:74-77",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:475-520",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:384-419",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py:106-137",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/miner.py:405-417",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/allocate.py:57",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/schedule.py",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:280-368",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:136-159",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:170-181",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Miner/container.py:136-146",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Resource Allocation API",
        "href": "/resource-allocation-api#4",
        "original_deepwiki_href": "#4",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Container Management",
        "href": "/miner-system/container-management#3.1",
        "original_deepwiki_href": "#3.1",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "flowchart TD\n    A[\"Allocate Synapse Request\"] --> B[\"blacklist_allocate()\"]\n    B --> C{\"Blacklisted?\"}\n    C -->|Yes| D[\"Reject Request\"]\n    C -->|No| E[\"priority_allocate()\"]\n    E --> F[\"allocate() Method\"]\n    F --> G{\"Checking Mode?\"}\n    G -->|Yes| H[\"check_allocation()\"]\n    G -->|No| I{\"Timeline > 0?\"}\n    I -->|Yes| J[\"register_allocation()\"]\n    I -->|No| K[\"deregister_allocation()\"]\n    H --> L[\"Return Status\"]\n    J --> M[\"run_container()\"]\n    K --> N[\"kill_container()\"]\n    M --> O[\"Update WandB State\"]\n    N --> O\n    O --> L",
      "sequenceDiagram\n    participant V as \"Validator/Client\"\n    participant M as \"Miner.allocate()\"\n    participant A as \"register_allocation()\"\n    participant C as \"Container Management\"\n    participant D as \"Docker Engine\"\n    participant S as \"Schedule Manager\"\n    \n    V->>M: \"Allocate(timeline=3600, device_requirement={...})\"\n    M->>A: \"register_allocation(timeline, device_requirement, public_key)\"\n    A->>C: \"kill_container() - cleanup existing\"\n    C->>D: \"Remove existing containers\"\n    A->>C: \"run_container(cpu_usage, ram_usage, gpu_usage, ...)\"\n    C->>D: \"Create container with resource limits\"\n    D->>C: \"Return container info + encrypted SSH details\"\n    C->>A: \"Return allocation status + connection info\"\n    A->>S: \"start(timeline) - schedule auto-deallocation\"\n    A->>M: \"Return allocation result\"\n    M->>V: \"Return Allocate synapse with status + SSH info\"",
      "graph LR\n    A[\"Device Requirements\"] --> B[\"Resource Parsing\"]\n    B --> C[\"Docker Configuration\"]\n    C --> D[\"Container Creation\"]\n    \n    subgraph \"Resource Parsing\"\n        B1[\"CPU Count → CPU Assignment\"]\n        B2[\"RAM Capacity → Memory Limit\"]\n        B3[\"Disk Capacity → Storage Limit\"]\n        B4[\"GPU Capacity → Device Requests\"]\n    end\n    \n    subgraph \"Container Creation\"\n        D1[\"build_sample_container()\"]\n        D2[\"Generate SSH Credentials\"]\n        D3[\"Create Dockerfile\"]\n        D4[\"Run Container with Limits\"]\n        D5[\"Return Encrypted Connection Info\"]\n    end\n    \n    B --> B1\n    B --> B2\n    B --> B3\n    B --> B4\n    \n    C --> D1\n    C --> D2\n    C --> D3\n    C --> D4\n    C --> D5",
      "stateDiagram-v2\n    [*] --> Available: \"No Container\"\n    Available --> Creating: \"register_allocation()\"\n    Creating --> Running: \"Container Created\"\n    Running --> Paused: \"pause_container()\"\n    Paused --> Running: \"unpause_container()\"\n    Running --> Running: \"restart_container()\"\n    Running --> Available: \"deregister_allocation()\"\n    Running --> Available: \"Timeline Expired\"\n    \n    note right of Running: \"SSH Access Available\\nResource Limits Applied\"\n    note right of Paused: \"Container Suspended\\nResources Released\"",
      "flowchart LR\n    A[\"Client Public Key\"] --> B[\"Allocation Request\"]\n    B --> C[\"Container Creation\"]\n    C --> D[\"SSH Key Setup\"]\n    D --> E[\"Connection Info Encryption\"]\n    E --> F[\"Encrypted Response\"]\n    \n    subgraph \"Storage\"\n        G[\"allocation_key file\"]\n        H[\"Base64 Encoded Public Key\"]\n    end\n    \n    A --> G\n    G --> H\n    H --> I[\"Access Validation\"]\n    I --> J[\"Container Operations\"]",
      "graph TD\n    A[\"Allocation Request\"] --> B[\"allocation_key File\"]\n    B --> C[\"Base64 Encoded Public Key\"]\n    C --> D[\"Access Validation\"]\n    \n    E[\"Container Status\"] --> F[\"Docker API Queries\"]\n    F --> G[\"check_container()\"]\n    \n    H[\"WandB Integration\"] --> I[\"update_allocated()\"]\n    I --> J[\"Distributed State Sync\"]\n    \n    subgraph \"State Validation\"\n        K[\"check_if_allocated()\"]\n        L[\"File Existence Check\"]\n        M[\"Key Comparison\"]\n        N[\"Container Running Check\"]\n    end\n    \n    D --> K\n    G --> K\n    K --> L\n    K --> M\n    K --> N",
      "sequenceDiagram\n    participant A as \"register_allocation()\"\n    participant S as \"Schedule Manager\"\n    participant T as \"Timer Thread\"\n    participant C as \"Container Manager\"\n    \n    A->>S: \"start(timeline=3600)\"\n    S->>T: \"Create timer for 3600 seconds\"\n    T-->>T: \"Wait for timeline expiry\"\n    T->>C: \"kill_container() after timeout\"\n    C->>C: \"Remove container and cleanup\"\n    \n    Note over T: \"Timeline-based auto-cleanup\\nPrevents resource leaks\"",
      "graph LR\n    A[\"build_sample_container()\"] --> B[\"pytorch/pytorch:2.7.0-cuda12.6-cudnn9-runtime\"]\n    B --> C[\"Install SSH Server\"]\n    C --> D[\"Configure SSH Settings\"]\n    D --> E[\"Install Python Dependencies\"]\n    E --> F[\"ssh-image-base:latest\"]\n    \n    subgraph \"Runtime Container\"\n        G[\"Custom Dockerfile\"]\n        H[\"User SSH Keys\"]\n        I[\"Resource Limits\"]\n        J[\"ssh-image:latest\"]\n    end\n    \n    F --> G\n    G --> H\n    H --> I\n    I --> J"
    ],
    "potential_frontmatter": {
      "title": "Resource Allocation"
    }
  },
  "/neuralinternet/ni-compute/4-resource-allocation-api": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/4-resource-allocation-api",
    "title": "Resource Allocation API",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/4-resource-allocation-api",
    "level": 0,
    "target_astro_path": "/resource-allocation-api",
    "main_markdown_content": "# Resource Allocation API\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/register_api.py](neurons/register_api.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Resource Allocation API is a FastAPI-based web service that provides programmatic access to GPU compute resources within the NI Compute Subnet. It serves as the primary interface for external clients to discover, allocate, deallocate, and manage computational resources provided by subnet miners.\n\nThis system handles the complete lifecycle of resource allocation, from initial discovery through active management of allocated containers. For information about the underlying miner resource provisioning, see [Miner System](#3). For details about the validation and scoring of these resources, see [Validator System](#2).\n\n## Architecture Overview\n\nThe Resource Allocation API is implemented as the `RegisterAPI` class in [neurons/register_api.py:229-3251](). It operates as a standalone FastAPI application that interfaces with the Bittensor network, local databases, and distributed state management systems.\n\n```mermaid\ngraph TB\n    subgraph \"External Clients\"\n        CLI[\"CLI Client\"]\n        WEB[\"Web Applications\"]\n        API_CLIENTS[\"API Clients\"]\n    end\n    \n    subgraph \"RegisterAPI Core\"\n        FASTAPI[\"FastAPI Application\"]\n        ROUTES[\"Route Handlers\"]\n        MODELS[\"Pydantic Models\"]\n    end\n    \n    subgraph \"Resource Management\"\n        ALLOCATE[\"_allocate_container()\"]\n        HOTKEY_ALLOC[\"_allocate_container_hotkey()\"]\n        HEALTH_CHECK[\"_check_allocation()\"]\n    end\n    \n    subgraph \"State Management\"\n        LOCAL_DB[\"ComputeDb (SQLite)\"]\n        WANDB_STATE[\"WandB Integration\"]\n        METAGRAPH[\"Bittensor Metagraph\"]\n    end\n    \n    subgraph \"Network Communication\"\n        DENDRITE[\"Bittensor Dendrite\"]\n        MINERS[\"Subnet Miners\"]\n    end\n    \n    CLI --> FASTAPI\n    WEB --> FASTAPI\n    API_CLIENTS --> FASTAPI\n    \n    FASTAPI --> ROUTES\n    ROUTES --> MODELS\n    ROUTES --> ALLOCATE\n    ROUTES --> HOTKEY_ALLOC\n    \n    ALLOCATE --> DENDRITE\n    HOTKEY_ALLOC --> DENDRITE\n    DENDRITE --> MINERS\n    \n    ROUTES --> LOCAL_DB\n    ROUTES --> WANDB_STATE\n    HEALTH_CHECK --> METAGRAPH\n    \n    HEALTH_CHECK --> DENDRITE\n```\n\nSources: [neurons/register_api.py:229-3251]()\n\n## Core Components\n\n### FastAPI Application Setup\n\nThe `RegisterAPI` class initializes a FastAPI application with SSL support and middleware for IP whitelisting when enabled. The application runs on port 8903 by default and requires SSL certificates for secure communication.\n\n```mermaid\ngraph LR\n    subgraph \"RegisterAPI.__init__()\"\n        CONFIG[\"Configuration Setup\"]\n        WALLET[\"Bittensor Wallet\"]\n        SUBTENSOR[\"ComputeSubnetSubtensor\"]\n        DENDRITE[\"Bittensor Dendrite\"]\n        METAGRAPH[\"Network Metagraph\"]\n        WANDB[\"ComputeWandb\"]\n        FASTAPI_APP[\"FastAPI Application\"]\n    end\n    \n    CONFIG --> WALLET\n    CONFIG --> SUBTENSOR\n    WALLET --> DENDRITE\n    SUBTENSOR --> METAGRAPH\n    CONFIG --> WANDB\n    CONFIG --> FASTAPI_APP\n    \n    FASTAPI_APP --> ROUTES[\"_setup_routes()\"]\n    ROUTES --> MIDDLEWARE[\"IPWhitelistMiddleware\"]\n```\n\nSources: [neurons/register_api.py:229-342](), [neurons/register_api.py:314-323]()\n\n### Request/Response Models\n\nThe API uses Pydantic models to define request and response structures:\n\n| Model | Purpose | Key Fields |\n|-------|---------|------------|\n| `DeviceRequirement` | GPU resource specifications | `gpu_type`, `gpu_size`, `ram`, `timeline` |\n| `DockerRequirement` | Container configuration | `base_image`, `ssh_key`, `dockerfile` |\n| `Allocation` | Allocation response data | `hotkey`, `ssh_ip`, `ssh_port`, `uuid_key` |\n| `Resource` | Resource information | `gpu_name`, `gpu_capacity`, `allocate_status` |\n| `ResourceQuery` | Resource filtering | `gpu_name`, `cpu_count_min/max`, capacity ranges |\n\nSources: [neurons/register_api.py:147-214](), [neurons/register_api.py:156-168]()\n\n## API Endpoints\n\n### Resource Allocation Endpoints\n\n#### Allocate by Specification\n- **Endpoint**: `POST /service/allocate_spec`\n- **Handler**: [neurons/register_api.py:434-547]()\n- **Purpose**: Allocates resources based on GPU specifications and requirements\n- **Process**: Discovers suitable miners, validates availability, provisions container\n\n#### Allocate by Hotkey\n- **Endpoint**: `POST /service/allocate_hotkey`\n- **Handler**: [neurons/register_api.py:576-695]()\n- **Purpose**: Allocates a specific miner's resources by hotkey\n- **Process**: Direct allocation to specified miner with container provisioning\n\n#### Deallocate Resources\n- **Endpoint**: `POST /service/deallocate`\n- **Handler**: [neurons/register_api.py:725-850]()\n- **Purpose**: Releases allocated resources and cleans up containers\n- **Process**: Validates UUID, sends deallocation signal to miner, updates state\n\n### Container Management Endpoints\n\nThe API provides Docker container lifecycle management:\n\n| Endpoint | Purpose | Handler Location |\n|----------|---------|-------------------|\n| `/service/restart_docker` | Restart allocated container | [neurons/register_api.py:920-1012]() |\n| `/service/pause_docker` | Pause container execution | [neurons/register_api.py:1027-1114]() |\n| `/service/unpause_docker` | Resume paused container | [neurons/register_api.py:1129-1215]() |\n| `/service/exchange_docker_key` | Update SSH keys | [neurons/register_api.py:1230-1317]() |\n\n### Resource Discovery Endpoints\n\n```mermaid\ngraph TD\n    subgraph \"Resource Listing\"\n        SQL_LIST[\"/list/resources_sql\"]\n        WANDB_LIST[\"/list/resources_wandb\"]\n        ALLOC_LIST[\"/list/allocations_sql\"]\n    end\n    \n    subgraph \"Data Sources\"\n        COMPUTE_DB[\"ComputeDb (Local)\"]\n        WANDB_API[\"WandB API\"]\n        MINER_SPECS[\"get_miner_details()\"]\n    end\n    \n    subgraph \"Filtering & Pagination\"\n        RESOURCE_QUERY[\"ResourceQuery Model\"]\n        PAGINATE[\"_paginate_list()\"]\n    end\n    \n    SQL_LIST --> COMPUTE_DB\n    SQL_LIST --> MINER_SPECS\n    WANDB_LIST --> WANDB_API\n    WANDB_LIST --> MINER_SPECS\n    \n    SQL_LIST --> RESOURCE_QUERY\n    WANDB_LIST --> RESOURCE_QUERY\n    RESOURCE_QUERY --> PAGINATE\n    \n    ALLOC_LIST --> COMPUTE_DB\n```\n\nSources: [neurons/register_api.py:1441-1644](), [neurons/register_api.py:1847-2053](), [neurons/register_api.py:1346-1419]()\n\n## Resource Management Logic\n\n### Allocation Process\n\nThe allocation process involves candidate discovery, availability checking, and container provisioning:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant RegisterAPI\n    participant ComputeDb\n    participant Dendrite\n    participant Miner\n    participant WandB\n    \n    Client->>RegisterAPI: \"/service/allocate_spec\"\n    RegisterAPI->>ComputeDb: \"select_allocate_miners_hotkey()\"\n    ComputeDb-->>RegisterAPI: \"candidate_hotkeys[]\"\n    \n    RegisterAPI->>Dendrite: \"Allocate(checking=True)\"\n    Dendrite->>Miner: \"Check availability\"\n    Miner-->>Dendrite: \"availability_response\"\n    Dendrite-->>RegisterAPI: \"final_candidates[]\"\n    \n    RegisterAPI->>RegisterAPI: \"Sort by scores\"\n    RegisterAPI->>Dendrite: \"Allocate(checking=False)\"\n    Dendrite->>Miner: \"Provision container\"\n    Miner-->>Dendrite: \"ssh_credentials\"\n    Dendrite-->>RegisterAPI: \"allocation_response\"\n    \n    RegisterAPI->>ComputeDb: \"update_allocation_db()\"\n    RegisterAPI->>WandB: \"_update_allocation_wandb()\"\n    RegisterAPI-->>Client: \"Allocation details\"\n```\n\nSources: [neurons/register_api.py:2733-2805](), [neurons/register_api.py:2807-2889]()\n\n### Health Monitoring\n\nThe `_check_allocation()` method continuously monitors allocated resources:\n\n- **Frequency**: Every 180 seconds (`ALLOCATE_CHECK_PERIOD`)\n- **Timeout Handling**: Deallocates after 20 consecutive failures (`ALLOCATE_CHECK_COUNT`)\n- **Notifications**: Sends webhook notifications for status changes\n- **Implementation**: [neurons/register_api.py:3002-3100]()\n\n### State Synchronization\n\nResource state is maintained across multiple systems:\n\n| System | Update Method | Purpose |\n|--------|---------------|---------|\n| Local SQLite | `update_allocation_db()` | Persistent allocation tracking |\n| WandB | `_update_allocation_wandb()` | Distributed state sharing |\n| Metagraph | `_refresh_metagraph()` | Network topology updates |\n\nSources: [neurons/register_api.py:2891-2919](), [neurons/register_api.py:2921-2929]()\n\n## Integration Points\n\n### Bittensor Network Integration\n\nThe API integrates deeply with Bittensor network components:\n\n- **Subtensor**: Uses `ComputeSubnetSubtensor` for blockchain interaction\n- **Dendrite**: Communicates with miners via `Allocate` protocol messages\n- **Metagraph**: Maintains current network state and miner information\n- **Wallet**: Provides cryptographic identity for API operations\n\nSources: [neurons/register_api.py:264-276]()\n\n### Database Operations\n\nThe API uses `ComputeDb` for local state persistence with the following key operations:\n\n- Allocation tracking in `allocation` table\n- Miner details retrieval via `get_miner_details()`\n- Challenge statistics for miner filtering\n- Implementation: [neurons/Validator/database/allocate.py]()\n\n### WandB Integration\n\nWandB serves as distributed state management:\n\n- **Allocated Hotkeys**: Tracks resources across all validators\n- **Miner Specifications**: Hardware details and availability\n- **Penalized Hotkeys**: Blacklist management\n- **Implementation**: Via `ComputeWandb` class integration\n\nSources: [neurons/register_api.py:1646-1702](), [neurons/register_api.py:1870-1872]()\n\n## Configuration and Security\n\n### Authentication and Security\n\nThe API implements several security measures:\n\n- **SSL/TLS**: Required certificates for HTTPS communication\n- **IP Whitelisting**: Optional middleware for access control (`IPWhitelistMiddleware`)\n- **RSA Encryption**: Key pair generation for secure miner communication\n- **UUID Validation**: Prevents unauthorized resource access\n\nSources: [neurons/register_api.py:120-134](), [neurons/register_api.py:3214-3227]()\n\n### Constants and Configuration\n\nKey configuration constants defined in the module:\n\n| Constant | Value | Purpose |\n|----------|--------|---------|\n| `DEFAULT_API_PORT` | 8903 | Default API server port |\n| `DATA_SYNC_PERIOD` | 600 | Metagraph refresh interval |\n| `ALLOCATE_CHECK_PERIOD` | 180 | Health check frequency |\n| `ALLOCATE_CHECK_COUNT` | 20 | Max failures before deallocation |\n| `VALID_VALIDATOR_HOTKEYS` | Array | Authorized validator hotkeys |\n\nSources: [neurons/register_api.py:86-116]()\n\nThe API runs with SSL certificates located at `cert/server.key`, `cert/server.cer`, and `cert/ca.cer`, and terminates if these certificates are not found.",
    "resolved_links": [
      {
        "text": "neurons/register_api.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py",
        "original_deepwiki_href": "neurons/register_api.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/register_api.py:229-3251",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:229-342",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:314-323",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:147-214",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:156-168",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1441-1644",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1847-2053",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1346-1419",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2733-2805",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2807-2889",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2891-2919",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2921-2929",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:264-276",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1646-1702",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1870-1872",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:120-134",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:3214-3227",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:86-116",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Miner System",
        "href": "/miner-system#3",
        "original_deepwiki_href": "#3",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Validator System",
        "href": "/validator-system#2",
        "original_deepwiki_href": "#2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"External Clients\"\n        CLI[\"CLI Client\"]\n        WEB[\"Web Applications\"]\n        API_CLIENTS[\"API Clients\"]\n    end\n    \n    subgraph \"RegisterAPI Core\"\n        FASTAPI[\"FastAPI Application\"]\n        ROUTES[\"Route Handlers\"]\n        MODELS[\"Pydantic Models\"]\n    end\n    \n    subgraph \"Resource Management\"\n        ALLOCATE[\"_allocate_container()\"]\n        HOTKEY_ALLOC[\"_allocate_container_hotkey()\"]\n        HEALTH_CHECK[\"_check_allocation()\"]\n    end\n    \n    subgraph \"State Management\"\n        LOCAL_DB[\"ComputeDb (SQLite)\"]\n        WANDB_STATE[\"WandB Integration\"]\n        METAGRAPH[\"Bittensor Metagraph\"]\n    end\n    \n    subgraph \"Network Communication\"\n        DENDRITE[\"Bittensor Dendrite\"]\n        MINERS[\"Subnet Miners\"]\n    end\n    \n    CLI --> FASTAPI\n    WEB --> FASTAPI\n    API_CLIENTS --> FASTAPI\n    \n    FASTAPI --> ROUTES\n    ROUTES --> MODELS\n    ROUTES --> ALLOCATE\n    ROUTES --> HOTKEY_ALLOC\n    \n    ALLOCATE --> DENDRITE\n    HOTKEY_ALLOC --> DENDRITE\n    DENDRITE --> MINERS\n    \n    ROUTES --> LOCAL_DB\n    ROUTES --> WANDB_STATE\n    HEALTH_CHECK --> METAGRAPH\n    \n    HEALTH_CHECK --> DENDRITE",
      "graph LR\n    subgraph \"RegisterAPI.__init__()\"\n        CONFIG[\"Configuration Setup\"]\n        WALLET[\"Bittensor Wallet\"]\n        SUBTENSOR[\"ComputeSubnetSubtensor\"]\n        DENDRITE[\"Bittensor Dendrite\"]\n        METAGRAPH[\"Network Metagraph\"]\n        WANDB[\"ComputeWandb\"]\n        FASTAPI_APP[\"FastAPI Application\"]\n    end\n    \n    CONFIG --> WALLET\n    CONFIG --> SUBTENSOR\n    WALLET --> DENDRITE\n    SUBTENSOR --> METAGRAPH\n    CONFIG --> WANDB\n    CONFIG --> FASTAPI_APP\n    \n    FASTAPI_APP --> ROUTES[\"_setup_routes()\"]\n    ROUTES --> MIDDLEWARE[\"IPWhitelistMiddleware\"]",
      "graph TD\n    subgraph \"Resource Listing\"\n        SQL_LIST[\"/list/resources_sql\"]\n        WANDB_LIST[\"/list/resources_wandb\"]\n        ALLOC_LIST[\"/list/allocations_sql\"]\n    end\n    \n    subgraph \"Data Sources\"\n        COMPUTE_DB[\"ComputeDb (Local)\"]\n        WANDB_API[\"WandB API\"]\n        MINER_SPECS[\"get_miner_details()\"]\n    end\n    \n    subgraph \"Filtering & Pagination\"\n        RESOURCE_QUERY[\"ResourceQuery Model\"]\n        PAGINATE[\"_paginate_list()\"]\n    end\n    \n    SQL_LIST --> COMPUTE_DB\n    SQL_LIST --> MINER_SPECS\n    WANDB_LIST --> WANDB_API\n    WANDB_LIST --> MINER_SPECS\n    \n    SQL_LIST --> RESOURCE_QUERY\n    WANDB_LIST --> RESOURCE_QUERY\n    RESOURCE_QUERY --> PAGINATE\n    \n    ALLOC_LIST --> COMPUTE_DB",
      "sequenceDiagram\n    participant Client\n    participant RegisterAPI\n    participant ComputeDb\n    participant Dendrite\n    participant Miner\n    participant WandB\n    \n    Client->>RegisterAPI: \"/service/allocate_spec\"\n    RegisterAPI->>ComputeDb: \"select_allocate_miners_hotkey()\"\n    ComputeDb-->>RegisterAPI: \"candidate_hotkeys[]\"\n    \n    RegisterAPI->>Dendrite: \"Allocate(checking=True)\"\n    Dendrite->>Miner: \"Check availability\"\n    Miner-->>Dendrite: \"availability_response\"\n    Dendrite-->>RegisterAPI: \"final_candidates[]\"\n    \n    RegisterAPI->>RegisterAPI: \"Sort by scores\"\n    RegisterAPI->>Dendrite: \"Allocate(checking=False)\"\n    Dendrite->>Miner: \"Provision container\"\n    Miner-->>Dendrite: \"ssh_credentials\"\n    Dendrite-->>RegisterAPI: \"allocation_response\"\n    \n    RegisterAPI->>ComputeDb: \"update_allocation_db()\"\n    RegisterAPI->>WandB: \"_update_allocation_wandb()\"\n    RegisterAPI-->>Client: \"Allocation details\""
    ],
    "potential_frontmatter": {
      "title": "Resource Allocation API"
    }
  },
  "/neuralinternet/ni-compute/4.1-api-endpoints": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/4.1-api-endpoints",
    "title": "API Endpoints",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/4.1-api-endpoints",
    "level": 1,
    "target_astro_path": "/resource-allocation-api/api-endpoints",
    "main_markdown_content": "# API Endpoints\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/register_api.py](neurons/register_api.py)\n\n</details>\n\n\n\n## Overview\n\nThis document details the API endpoints available in the NI Compute (Subnet 27) RegisterAPI system. The API provides interfaces for allocating GPU resources, managing Docker containers, and retrieving information about available resources in the compute marketplace. For information about the overall resource management approach, see [Resource Management](#4.2).\n\nThe RegisterAPI runs as a FastAPI web service that exposes HTTP endpoints for client applications to interact with the compute subnet. It serves as the primary interface for allocating, deallocating, and checking the status of GPU resources.\n\nSources: [neurons/register_api.py:228-342]()\n\n## API Architecture\n\nThe RegisterAPI is implemented using FastAPI and follows a RESTful design pattern. It provides endpoints for different operations categorized by functionality.\n\n```mermaid\nflowchart TD\n    subgraph \"RegisterAPI\"\n        direction TB\n        API[\"FastAPI Service\"]\n        \n        subgraph \"Allocation Endpoints\"\n            AE1[\"POST /service/allocate_spec\"]\n            AE2[\"POST /service/allocate_hotkey\"]\n            AE3[\"POST /service/deallocate\"]\n        end\n        \n        subgraph \"Management Endpoints\"\n            ME1[\"POST /service/check_miner_status\"]\n            ME2[\"POST /service/restart_docker\"]\n            ME3[\"POST /service/pause_docker\"]\n            ME4[\"POST /service/unpause_docker\"]\n            ME5[\"POST /service/exchange_docker_key\"]\n        end\n        \n        subgraph \"Resource Listing Endpoints\"\n            RL1[\"POST /list/allocations_sql\"]\n            RL2[\"POST /list/resources_sql\"]\n            RL3[\"POST /list/resources_wandb\"]\n            RL4[\"POST /list/count_all_gpus\"]\n            RL5[\"POST /list/count_all_by_model\"]\n            RL6[\"POST /list/all_runs\"]\n        end\n        \n        API --> AE1 & AE2 & AE3\n        API --> ME1 & ME2 & ME3 & ME4 & ME5\n        API --> RL1 & RL2 & RL3 & RL4 & RL5 & RL6\n    end\n    \n    DB[(ComputeDb)] <-- \"Store/Retrieve\" --> API\n    WS[\"WebSocket\\n/connect\"] <-- \"Status updates\" --> API\n    Metagraph[\"Bittensor\\nMetagraph\"] <-- \"Retrieve miner info\" --> API\n    Dendrite[\"Bittensor\\nDendrite\"] <-- \"Communicate with miners\" --> API\n    WandB[\"Weights & Biases\"] <-- \"Resource metrics\" --> API\n```\n\nSources: [neurons/register_api.py:344-377](), [neurons/register_api.py:400-406]()\n\n## Data Models\n\nThe API uses several data models to structure request and response data:\n\n```mermaid\nclassDiagram\n    class DeviceRequirement {\n        +int cpu_count\n        +str gpu_type\n        +int gpu_size\n        +int ram\n        +int hard_disk\n        +int timeline\n    }\n    \n    class DockerRequirement {\n        +str base_image\n        +str ssh_key\n        +str volume_path\n        +str dockerfile\n    }\n    \n    class Allocation {\n        +str resource\n        +str hotkey\n        +str regkey\n        +str ssh_ip\n        +int ssh_port\n        +str ssh_username\n        +str ssh_password\n        +str ssh_command\n        +str ssh_key\n        +str uuid_key\n        +int miner_version\n    }\n    \n    class Resource {\n        +str hotkey\n        +int cpu_count\n        +str gpu_name\n        +str gpu_capacity\n        +int gpu_count\n        +str ram\n        +str hard_disk\n        +str allocate_status\n    }\n    \n    class ResourceQuery {\n        +str gpu_name\n        +int cpu_count_min\n        +int cpu_count_max\n        +float gpu_capacity_min\n        +float gpu_capacity_max\n        +float hard_disk_total_min\n        +float hard_disk_total_max\n        +float ram_total_min\n        +float ram_total_max\n    }\n    \n    class Response {\n        +bool success\n        +str message\n        +dict data\n    }\n```\n\nSources: [neurons/register_api.py:136-226]()\n\n## Allocation Endpoints\n\n### Allocate by Specification\n\nAllocates GPU resources based on hardware specifications.\n\n**Endpoint:** `POST /service/allocate_spec`\n\n**Request Parameters:**\n- `requirements`: DeviceRequirement - Hardware specifications for the allocation\n- `docker_requirement`: DockerRequirement - Docker container configuration\n\n**Response:** \n- Success: Returns an Allocation object with SSH access details\n- Error: Returns an error message if allocation fails\n\n**Example Flow:**\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant RegisterAPI\n    participant Dendrite\n    participant Miner\n    participant ComputeDb\n\n    Client->>RegisterAPI: POST /service/allocate_spec\n    RegisterAPI->>RegisterAPI: Generate RSA key pair\n    RegisterAPI->>RegisterAPI: Find suitable miner\n    RegisterAPI->>Dendrite: Send Allocate request\n    Dendrite->>Miner: Forward Allocate request\n    Miner->>Miner: Create Docker container\n    Miner->>Dendrite: Return SSH access details\n    Dendrite->>RegisterAPI: Forward SSH access details\n    RegisterAPI->>ComputeDb: Update allocation DB\n    RegisterAPI->>Client: Return allocation information\n```\n\nSources: [neurons/register_api.py:407-547]()\n\n### Allocate by Hotkey\n\nAllocates resources from a specific miner by its hotkey.\n\n**Endpoint:** `POST /service/allocate_hotkey`\n\n**Request Parameters:**\n- `hotkey`: String - The miner's hotkey\n- `ssh_key`: Optional[String] - SSH public key for secure access\n- `docker_requirement`: Optional[DockerRequirement] - Docker container configuration\n\n**Response:**\n- Success: Returns an Allocation object with SSH access details\n- Error: Returns an error message if allocation fails\n\nSources: [neurons/register_api.py:549-695]()\n\n### Deallocate\n\nDeallocates previously allocated GPU resources.\n\n**Endpoint:** `POST /service/deallocate`\n\n**Request Parameters:**\n- `hotkey`: String - The miner's hotkey\n- `uuid_key`: String - The UUID of the allocation\n- `notify_flag`: Boolean - Whether to send a notification about deallocation\n\n**Response:**\n- Success: Confirmation of successful deallocation\n- Error: Error message if deallocation fails\n\nSources: [neurons/register_api.py:697-850]()\n\n## Management Endpoints\n\n### Check Miner Status\n\nChecks the status of specified miners.\n\n**Endpoint:** `POST /service/check_miner_status`\n\n**Request Parameters:**\n- `hotkey_list`: List[String] - List of miner hotkeys to check\n- `query_version`: Boolean - Whether to query miner version\n\n**Response:**\n- List of status objects for each miner, containing hotkey and status information\n\nSources: [neurons/register_api.py:852-905]()\n\n### Docker Container Management\n\nThe following endpoints allow management of allocated Docker containers:\n\n1. **Restart Docker:**\n   - **Endpoint:** `POST /service/restart_docker`\n   - **Parameters:** `hotkey`, `uuid_key`\n\n2. **Pause Docker:**\n   - **Endpoint:** `POST /service/pause_docker`\n   - **Parameters:** `hotkey`, `uuid_key`\n\n3. **Unpause Docker:**\n   - **Endpoint:** `POST /service/unpause_docker`\n   - **Parameters:** `hotkey`, `uuid_key`\n\n4. **Exchange Docker SSH Key:**\n   - **Endpoint:** `POST /service/exchange_docker_key`\n   - **Parameters:** `hotkey`, `uuid_key`, `ssh_key`, `key_type`\n\nAll these endpoints follow a similar pattern:\n1. Verify the allocation exists and the UUID matches\n2. Send the appropriate command to the miner via Dendrite\n3. Return success/failure response\n\n```mermaid\nflowchart TB\n    Start[\"Docker Management Request\"] --> GetAllocation[\"Retrieve allocation\\ndetails from DB\"]\n    GetAllocation --> ValidateUUID{\"UUID valid?\"}\n    ValidateUUID -- \"Yes\" --> FindMiner[\"Find miner in metagraph\"]\n    ValidateUUID -- \"No\" --> Error1[\"Return Error:\\nInvalid UUID\"]\n    FindMiner --> SendCommand[\"Send command to miner\\nvia Dendrite\"]\n    SendCommand --> Response{\"Miner\\nresponded?\"}\n    Response -- \"Yes\" --> Success[\"Return Success\"]\n    Response -- \"No\" --> Error2[\"Return Error:\\nNo response\"]\n```\n\nSources: [neurons/register_api.py:907-1311]()\n\n## Resource Listing Endpoints\n\n### List Allocations\n\nLists all current resource allocations.\n\n**Endpoint:** `POST /list/allocations_sql`\n\n**Response:**\n- List of Allocation objects describing currently allocated resources\n- Error if retrieving allocations fails\n\nSources: [neurons/register_api.py:1313-1413]()\n\n### List Resources\n\nLists available GPU resources from the SQLite database.\n\n**Endpoint:** `POST /list/resources_sql`\n\n**Request Parameters:**\n- `query`: ResourceQuery - Optional filter criteria\n- `stats`: Boolean - Return statistics instead of full list\n- `page_size`: Optional[Integer] - Number of items per page\n- `page_number`: Optional[Integer] - Page number to return\n\n**Response:**\n- List of Resource objects or statistics about available resources\n- Error if retrieving resources fails\n\nSources: [neurons/register_api.py:1415-1638]()\n\n### List Resources from W&B\n\nLists available GPU resources from Weights & Biases data.\n\n**Endpoint:** `POST /list/resources_wandb`\n\n**Request Parameters:**\n- Same as `/list/resources_sql`\n\n**Response:**\n- Similar to `/list/resources_sql` but using W&B as data source\n\nSources: [neurons/register_api.py:1822-2047]()\n\n### Count GPUs\n\nCounts all available GPUs on the compute subnet.\n\n**Endpoint:** `POST /list/count_all_gpus`\n\n**Response:**\n- Total count of GPUs available on the subnet\n\nSources: [neurons/register_api.py:1698-1749]()\n\n### Count GPUs by Model\n\nCounts GPUs of a specific model with optional filtering.\n\n**Endpoint:** `POST /list/count_all_by_model`\n\n**Request Parameters:**\n- `model`: String - GPU model name\n- `cpu_count`: Optional[Integer] - CPU count filter\n- `ram_size`: Optional[Float] - RAM size filter\n\n**Response:**\n- Count of GPUs matching the specified criteria\n\nSources: [neurons/register_api.py:1750-1819]()\n\n### List All Runs\n\nLists all running miners from Weights & Biases.\n\n**Endpoint:** `POST /list/all_runs`\n\n**Request Parameters:**\n- `hotkey`: Optional[String] - Filter by specific hotkey\n- `page_size`: Optional[Integer] - Number of items per page\n- `page_number`: Optional[Integer] - Page number to return\n\n**Response:**\n- List of run information objects from W&B\n\nSources: [neurons/register_api.py:2049-2154]()\n\n## Resource Allocation Flow\n\nThe following diagram illustrates the complete flow of the resource allocation process:\n\n```mermaid\nflowchart TD\n    Client[\"Client\"] --> SpecRequest[\"POST /service/allocate_spec\\nor\\nPOST /service/allocate_hotkey\"]\n    \n    subgraph \"RegisterAPI Allocation Process\"\n        SpecRequest --> GenerateKey[\"Generate RSA Key Pair\"]\n        GenerateKey --> FindMiner[\"Find Suitable Miner\"]\n        FindMiner --> SendAlloc[\"Send Allocation Request\\nvia Dendrite\"]\n    end\n    \n    subgraph \"Miner Allocation Process\"\n        SendAlloc --> MinerProc[\"Miner Processes Request\"]\n        MinerProc --> CreateCont[\"Create Docker Container\"]\n        CreateCont --> GenerateSSH[\"Generate SSH Credentials\"]\n        GenerateSSH --> ReturnInfo[\"Return Container Info\"]\n    end\n    \n    ReturnInfo --> DecryptInfo[\"Decrypt Container Info\"]\n    DecryptInfo --> UpdateDB[\"Update Allocation Database\"]\n    UpdateDB --> UpdateWandB[\"Update WandB Metrics\"]\n    UpdateWandB --> ReturnDetails[\"Return Allocation Details\\nto Client\"]\n    \n    ReturnDetails --> Client\n    \n    Client --> DeallocRequest[\"POST /service/deallocate\"]\n    DeallocRequest --> VerifyUUID[\"Verify UUID\"]\n    VerifyUUID --> SendDealloc[\"Send Deallocation Request\"]\n    SendDealloc --> StopContainer[\"Stop Container\"]\n    StopContainer --> UpdateDBDealloc[\"Update Database\"]\n    UpdateDBDealloc --> NotifyDealloc[\"Send Deallocation Notification\"]\n    NotifyDealloc --> ReturnSuccess[\"Return Success to Client\"]\n    ReturnSuccess --> Client\n```\n\nSources: [neurons/register_api.py:407-547](), [neurons/register_api.py:549-695](), [neurons/register_api.py:697-850]()\n\n## API Connection Validation\n\nThe RegisterAPI includes a utility function to check port availability and connectivity:\n\n```mermaid\nflowchart LR\n    Start[\"check_port(host, port)\"] --> CreateSocket[\"Create Socket Connection\"]\n    CreateSocket --> Attempt[\"Attempt Connection\"]\n    Attempt --> Result{\"Connection\\nResult\"}\n    Result -- \"Success (0)\" --> ReturnTrue[\"Return True\"]\n    Result -- \"Failure (non-zero)\" --> ReturnFalse[\"Return False\"]\n    CreateSocket -- \"Socket Error\" --> ReturnNone1[\"Return None\"]\n    Attempt -- \"Hostname Error\" --> ReturnNone2[\"Return None\"]\n```\n\nSources: [compute/utils/socket.py:1-18]()\n\n## Authentication and Security\n\nThe RegisterAPI includes several security features:\n\n1. **IP Whitelisting**: Optional middleware to restrict access to specific IP addresses\n2. **UUID Validation**: All resource management endpoints require a valid UUID that matches the allocation record\n3. **RSA Encryption**: Container access credentials are encrypted with RSA\n4. **Miner Blacklist**: Prevents allocation to known problematic miners\n\nWhen enabled, IP whitelisting restricts API access to a predefined list of IPs, with configuration available through environment variables.\n\nSources: [neurons/register_api.py:85-97](), [neurons/register_api.py:119-134](), [neurons/register_api.py:321-322]()\n\n## Error Handling\n\nThe API implements custom error handling to provide clear, structured error responses:\n\n1. **Validation Errors**: Returns details about which fields failed validation\n2. **Not Found Errors**: When resources or allocations can't be found\n3. **Authorization Errors**: When security checks fail\n4. **General Errors**: For other unexpected failures\n\nAll error responses follow a consistent format with `success: false`, an error message, and optional details.\n\nSources: [neurons/register_api.py:345-358]()\n\n## Background Tasks\n\nThe RegisterAPI maintains two background tasks:\n\n1. **Metagraph Refresh**: Periodically updates the metagraph to keep miner information current\n2. **Allocation Check**: Regularly checks allocated resources to ensure they're still available and properly managed\n\nThese tasks run asynchronously in the background while the API handles requests.\n\nSources: [neurons/register_api.py:367-368]()",
    "resolved_links": [
      {
        "text": "neurons/register_api.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py",
        "original_deepwiki_href": "neurons/register_api.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/register_api.py:228-342",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:344-377",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:400-406",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:136-226",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:407-547",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:549-695",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:697-850",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:852-905",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:907-1311",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1313-1413",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1415-1638",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1822-2047",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1698-1749",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1750-1819",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2049-2154",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/socket.py:1-18",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:85-97",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:119-134",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:321-322",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:345-358",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:367-368",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Resource Management",
        "href": "/resource-allocation-api/resource-management#4.2",
        "original_deepwiki_href": "#4.2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "flowchart TD\n    subgraph \"RegisterAPI\"\n        direction TB\n        API[\"FastAPI Service\"]\n        \n        subgraph \"Allocation Endpoints\"\n            AE1[\"POST /service/allocate_spec\"]\n            AE2[\"POST /service/allocate_hotkey\"]\n            AE3[\"POST /service/deallocate\"]\n        end\n        \n        subgraph \"Management Endpoints\"\n            ME1[\"POST /service/check_miner_status\"]\n            ME2[\"POST /service/restart_docker\"]\n            ME3[\"POST /service/pause_docker\"]\n            ME4[\"POST /service/unpause_docker\"]\n            ME5[\"POST /service/exchange_docker_key\"]\n        end\n        \n        subgraph \"Resource Listing Endpoints\"\n            RL1[\"POST /list/allocations_sql\"]\n            RL2[\"POST /list/resources_sql\"]\n            RL3[\"POST /list/resources_wandb\"]\n            RL4[\"POST /list/count_all_gpus\"]\n            RL5[\"POST /list/count_all_by_model\"]\n            RL6[\"POST /list/all_runs\"]\n        end\n        \n        API --> AE1 & AE2 & AE3\n        API --> ME1 & ME2 & ME3 & ME4 & ME5\n        API --> RL1 & RL2 & RL3 & RL4 & RL5 & RL6\n    end\n    \n    DB[(ComputeDb)] <-- \"Store/Retrieve\" --> API\n    WS[\"WebSocket\\n/connect\"] <-- \"Status updates\" --> API\n    Metagraph[\"Bittensor\\nMetagraph\"] <-- \"Retrieve miner info\" --> API\n    Dendrite[\"Bittensor\\nDendrite\"] <-- \"Communicate with miners\" --> API\n    WandB[\"Weights & Biases\"] <-- \"Resource metrics\" --> API",
      "classDiagram\n    class DeviceRequirement {\n        +int cpu_count\n        +str gpu_type\n        +int gpu_size\n        +int ram\n        +int hard_disk\n        +int timeline\n    }\n    \n    class DockerRequirement {\n        +str base_image\n        +str ssh_key\n        +str volume_path\n        +str dockerfile\n    }\n    \n    class Allocation {\n        +str resource\n        +str hotkey\n        +str regkey\n        +str ssh_ip\n        +int ssh_port\n        +str ssh_username\n        +str ssh_password\n        +str ssh_command\n        +str ssh_key\n        +str uuid_key\n        +int miner_version\n    }\n    \n    class Resource {\n        +str hotkey\n        +int cpu_count\n        +str gpu_name\n        +str gpu_capacity\n        +int gpu_count\n        +str ram\n        +str hard_disk\n        +str allocate_status\n    }\n    \n    class ResourceQuery {\n        +str gpu_name\n        +int cpu_count_min\n        +int cpu_count_max\n        +float gpu_capacity_min\n        +float gpu_capacity_max\n        +float hard_disk_total_min\n        +float hard_disk_total_max\n        +float ram_total_min\n        +float ram_total_max\n    }\n    \n    class Response {\n        +bool success\n        +str message\n        +dict data\n    }",
      "sequenceDiagram\n    participant Client\n    participant RegisterAPI\n    participant Dendrite\n    participant Miner\n    participant ComputeDb\n\n    Client->>RegisterAPI: POST /service/allocate_spec\n    RegisterAPI->>RegisterAPI: Generate RSA key pair\n    RegisterAPI->>RegisterAPI: Find suitable miner\n    RegisterAPI->>Dendrite: Send Allocate request\n    Dendrite->>Miner: Forward Allocate request\n    Miner->>Miner: Create Docker container\n    Miner->>Dendrite: Return SSH access details\n    Dendrite->>RegisterAPI: Forward SSH access details\n    RegisterAPI->>ComputeDb: Update allocation DB\n    RegisterAPI->>Client: Return allocation information",
      "flowchart TB\n    Start[\"Docker Management Request\"] --> GetAllocation[\"Retrieve allocation\\ndetails from DB\"]\n    GetAllocation --> ValidateUUID{\"UUID valid?\"}\n    ValidateUUID -- \"Yes\" --> FindMiner[\"Find miner in metagraph\"]\n    ValidateUUID -- \"No\" --> Error1[\"Return Error:\\nInvalid UUID\"]\n    FindMiner --> SendCommand[\"Send command to miner\\nvia Dendrite\"]\n    SendCommand --> Response{\"Miner\\nresponded?\"}\n    Response -- \"Yes\" --> Success[\"Return Success\"]\n    Response -- \"No\" --> Error2[\"Return Error:\\nNo response\"]",
      "flowchart TD\n    Client[\"Client\"] --> SpecRequest[\"POST /service/allocate_spec\\nor\\nPOST /service/allocate_hotkey\"]\n    \n    subgraph \"RegisterAPI Allocation Process\"\n        SpecRequest --> GenerateKey[\"Generate RSA Key Pair\"]\n        GenerateKey --> FindMiner[\"Find Suitable Miner\"]\n        FindMiner --> SendAlloc[\"Send Allocation Request\\nvia Dendrite\"]\n    end\n    \n    subgraph \"Miner Allocation Process\"\n        SendAlloc --> MinerProc[\"Miner Processes Request\"]\n        MinerProc --> CreateCont[\"Create Docker Container\"]\n        CreateCont --> GenerateSSH[\"Generate SSH Credentials\"]\n        GenerateSSH --> ReturnInfo[\"Return Container Info\"]\n    end\n    \n    ReturnInfo --> DecryptInfo[\"Decrypt Container Info\"]\n    DecryptInfo --> UpdateDB[\"Update Allocation Database\"]\n    UpdateDB --> UpdateWandB[\"Update WandB Metrics\"]\n    UpdateWandB --> ReturnDetails[\"Return Allocation Details\\nto Client\"]\n    \n    ReturnDetails --> Client\n    \n    Client --> DeallocRequest[\"POST /service/deallocate\"]\n    DeallocRequest --> VerifyUUID[\"Verify UUID\"]\n    VerifyUUID --> SendDealloc[\"Send Deallocation Request\"]\n    SendDealloc --> StopContainer[\"Stop Container\"]\n    StopContainer --> UpdateDBDealloc[\"Update Database\"]\n    UpdateDBDealloc --> NotifyDealloc[\"Send Deallocation Notification\"]\n    NotifyDealloc --> ReturnSuccess[\"Return Success to Client\"]\n    ReturnSuccess --> Client",
      "flowchart LR\n    Start[\"check_port(host, port)\"] --> CreateSocket[\"Create Socket Connection\"]\n    CreateSocket --> Attempt[\"Attempt Connection\"]\n    Attempt --> Result{\"Connection\\nResult\"}\n    Result -- \"Success (0)\" --> ReturnTrue[\"Return True\"]\n    Result -- \"Failure (non-zero)\" --> ReturnFalse[\"Return False\"]\n    CreateSocket -- \"Socket Error\" --> ReturnNone1[\"Return None\"]\n    Attempt -- \"Hostname Error\" --> ReturnNone2[\"Return None\"]"
    ],
    "potential_frontmatter": {
      "title": "API Endpoints"
    }
  },
  "/neuralinternet/ni-compute/4.2-resource-management": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/4.2-resource-management",
    "title": "Resource Management",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/4.2-resource-management",
    "level": 1,
    "target_astro_path": "/resource-allocation-api/resource-management",
    "main_markdown_content": "# Resource Management\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/register_api.py](neurons/register_api.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Resource Management system implements the core allocation logic within the Resource Allocation API, handling resource discovery, allocation strategies, health monitoring, and state synchronization. This system operates within the `RegisterAPI` class and provides the intelligence for matching compute requirements with available miners, managing resource lifecycle, and maintaining distributed state.\n\nFor information about the API endpoints that expose these capabilities, see [API Endpoints](#4.1). For broader context about the validator system that validates miner capabilities, see [Validator System](#2).\n\n## Resource Discovery System\n\nThe resource discovery system identifies and evaluates candidate miners for allocation requests through a multi-stage process involving database queries, network validation, and scoring algorithms.\n\n### Candidate Selection Process\n\n```mermaid\nflowchart TD\n    DevReq[\"DeviceRequirement\"] --> SelectCandidates[\"select_allocate_miners_hotkey()\"]\n    SelectCandidates --> CandidateList[\"candidates_hotkey[]\"]\n    CandidateList --> FilterAxons[\"Filter metagraph.axons\"]\n    FilterAxons --> AxonCandidates[\"axon_candidates[]\"]\n    \n    AxonCandidates --> CheckAvailability[\"dendrite(Allocate(checking=True))\"]\n    CheckAvailability --> ValidateResponses[\"Validate responses\"]\n    ValidateResponses --> FinalCandidates[\"final_candidates_hotkey[]\"]\n    \n    FinalCandidates --> ScoreSort[\"Score-based sorting\"]\n    ScoreSort --> SortedCandidates[\"sorted_hotkeys[]\"]\n    SortedCandidates --> AttemptAllocation[\"Attempt allocation\"]\n```\n\n**Candidate Discovery Flow**\n\nSources: [neurons/register_api.py:2741-2805]()\n\nThe system uses a two-phase approach for candidate selection:\n\n1. **Database Filtering**: The `select_allocate_miners_hotkey()` function queries the local database to find miners matching hardware requirements\n2. **Network Validation**: Available candidates are validated through the Bittensor network using `Allocate` synapse with `checking=True`\n3. **Scoring and Prioritization**: Valid candidates are sorted by their network scores to prioritize higher-performing miners\n\n### WandB-Based Resource Discovery\n\n```mermaid\nflowchart TD\n    WandbQuery[\"get_wandb_running_miners()\"] --> FilterRuns[\"Filter running miner runs\"]\n    FilterRuns --> CheckPenalized[\"Check penalized_hotkeys\"]\n    CheckPenalized --> ValidateAge[\"miner_is_older_than(48h)\"]\n    ValidateAge --> ValidatePog[\"miner_pog_ok(2.5h)\"]\n    ValidatePog --> CheckActive[\"Verify in metagraph.axons\"]\n    CheckActive --> ExtractSpecs[\"Extract specs from run.config\"]\n    ExtractSpecs --> SpecsDetails[\"specs_details{}\"]\n    SpecsDetails --> ResourceList[\"resource_list[]\"]\n```\n\n**WandB Resource Discovery Flow**\n\nSources: [neurons/register_api.py:1646-1702](), [neurons/register_api.py:1881-1884]()\n\nThe system implements an alternative discovery mechanism using WandB for distributed miner information:\n\n| Validation Check | Function | Purpose |\n|------------------|----------|---------|\n| Age Verification | `miner_is_older_than()` | Ensures miners have been active for 48+ hours |\n| PoG Validation | `miner_pog_ok()` | Confirms recent Proof-of-GPU completion within 2.5 hours |\n| Penalty Check | `get_penalized_hotkeys_checklist()` | Excludes blacklisted or penalized miners |\n| Network Presence | `metagraph.axons` lookup | Verifies miner is active on network |\n\n## Allocation Strategies\n\nThe system implements two primary allocation strategies: specification-based allocation and hotkey-specific allocation, each optimized for different use cases.\n\n### Specification-Based Allocation\n\n```mermaid\nflowchart TD\n    AllocateSpec[\"_allocate_container()\"] --> BuildDevReq[\"Build device_requirement{}\"]\n    BuildDevReq --> FindCandidates[\"select_allocate_miners_hotkey()\"]\n    FindCandidates --> CheckAvailable[\"dendrite(checking=True)\"]\n    CheckAvailable --> ScoreSort[\"torch.ones_like(metagraph.S)\"]\n    ScoreSort --> TryAllocation[\"Loop through sorted_hotkeys\"]\n    TryAllocation --> SendAllocate[\"dendrite(Allocate(checking=False))\"]\n    SendAllocate --> ValidateResponse[\"Check response.status\"]\n    ValidateResponse --> Success[\"Return allocation info\"]\n    ValidateResponse --> NextCandidate[\"Try next candidate\"]\n    NextCandidate --> TryAllocation\n```\n\n**Specification-Based Allocation Flow**\n\nSources: [neurons/register_api.py:2733-2805]()\n\nThe specification-based strategy processes `DeviceRequirement` objects containing:\n- CPU count requirements\n- GPU type and memory specifications  \n- RAM and storage capacity needs\n- Timeline duration for allocation\n\n### Hotkey-Specific Allocation\n\n```mermaid\nflowchart TD\n    AllocateHotkey[\"_allocate_container_hotkey()\"] --> FindAxon[\"Locate axon by hotkey\"]\n    FindAxon --> SetDefaults[\"Set default device_requirement{}\"]\n    SetDefaults --> SetBaseImage[\"docker_requirement.base_image\"]\n    SetBaseImage --> DirectAllocate[\"dendrite(Allocate())\"]\n    DirectAllocate --> ValidateResponse[\"Check response.status\"]\n    ValidateResponse --> ReturnInfo[\"Return allocation + miner_version\"]\n    ValidateResponse --> ReturnError[\"Return error message\"]\n```\n\n**Hotkey-Specific Allocation Flow**\n\nSources: [neurons/register_api.py:2807-2889]()\n\nThis strategy targets specific miners by hotkey, bypassing the discovery phase and applying default resource requirements with configurable Docker base images.\n\n## Health Monitoring System\n\nThe health monitoring system continuously tracks allocated resources and manages their lifecycle through automated checks and notifications.\n\n### Allocation Health Check Process\n\n```mermaid\nflowchart TD\n    CheckTask[\"_check_allocation()\"] --> QueryDB[\"SELECT * FROM allocation\"]\n    QueryDB --> LoopAllocations[\"For each allocation\"]\n    LoopAllocations --> ValidateHotkey[\"Check hotkey in metagraph\"]\n    ValidateHotkey --> HealthCheck[\"dendrite_check(Allocate(checking=True))\"]\n    HealthCheck --> EvaluateResponse[\"Evaluate response\"]\n    \n    EvaluateResponse --> Online[\"response.status == False\"]\n    EvaluateResponse --> Offline[\"No response or timeout\"]\n    \n    Online --> NotifyOnline[\"_notify_allocation_status(ONLINE)\"]\n    Online --> RemoveFromCheck[\"Remove from checking_allocated[]\"]\n    \n    Offline --> AddToCheck[\"Add to checking_allocated[]\"]\n    Offline --> NotifyOffline[\"_notify_allocation_status(OFFLINE)\"]\n    Offline --> CountChecks[\"checking_allocated.count(hotkey)\"]\n    \n    CountChecks --> MaxReached[\"count >= ALLOCATE_CHECK_COUNT\"]\n    MaxReached --> Deallocate[\"update_allocation_db(False)\"]\n    Deallocate --> NotifyDealloc[\"_notify_allocation_status(DEALLOCATION)\"]\n```\n\n**Health Monitoring Process Flow**\n\nSources: [neurons/register_api.py:3002-3101]()\n\nThe health monitoring system operates with the following parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `ALLOCATE_CHECK_PERIOD` | 180 seconds | Interval between health checks |\n| `ALLOCATE_CHECK_COUNT` | 20 | Maximum failed checks before deallocation |\n| Health Check Timeout | 10 seconds | Maximum wait time for miner response |\n\n### Status Transition Management\n\nThe system tracks miner status transitions and maintains allocation state through the `checking_allocated` list and database updates.\n\nSources: [neurons/register_api.py:3039-3081]()\n\n## State Management\n\nThe Resource Management system maintains both local and distributed state through SQLite database operations and WandB synchronization.\n\n### Database State Operations\n\n```mermaid\nflowchart TD\n    AllocDB[\"allocation table\"] --> UpdateLocal[\"update_allocation_db()\"]\n    UpdateLocal --> ExtractHotkeys[\"Extract hotkey_list[]\"]\n    ExtractHotkeys --> UpdateWandB[\"_update_allocation_wandb()\"]\n    UpdateWandB --> WandbSync[\"wandb.update_allocated_hotkeys()\"]\n    \n    QueryState[\"List operations\"] --> QueryDB[\"SELECT hotkey, details FROM allocation\"]\n    QueryDB --> ParseJSON[\"json.loads(details)\"]\n    ParseJSON --> BuildAllocation[\"Build Allocation objects\"]\n    \n    HealthCheck[\"Health monitoring\"] --> StateUpdate[\"Allocation state changes\"]\n    StateUpdate --> NotifyRetry[\"notify_retry_table[]\"]\n    StateUpdate --> UpdateLocal\n```\n\n**State Management Architecture**\n\nSources: [neurons/register_api.py:2891-2919](), [neurons/register_api.py:1346-1419]()\n\n### Distributed State Synchronization\n\nThe system maintains consistency across validators through WandB-based state sharing:\n\n| State Component | Storage | Synchronization Method |\n|-----------------|---------|----------------------|\n| Active Allocations | SQLite `allocation` table | `_update_allocation_wandb()` |\n| Allocated Hotkeys | WandB runs | `wandb.update_allocated_hotkeys()` |\n| Retry Notifications | In-memory `notify_retry_table` | Periodic retry processing |\n\nSources: [neurons/register_api.py:2915-2919]()\n\n## Notification System\n\nThe notification system provides external webhook integration for allocation lifecycle events and status changes.\n\n### Notification Event Types\n\n```mermaid\nflowchart TD\n    EventTrigger[\"Allocation Event\"] --> EventType{\"Event Type\"}\n    \n    EventType --> Deallocation[\"DEALLOCATION\"]\n    EventType --> Online[\"ONLINE\"] \n    EventType --> Offline[\"OFFLINE\"]\n    \n    Deallocation --> BuildDeallocationMsg[\"Build deallocation message\"]\n    Online --> BuildStatusMsg[\"Build status change message\"]\n    Offline --> BuildStatusMsg\n    \n    BuildDeallocationMsg --> DeallocationURL[\"deallocation_notify_url\"]\n    BuildStatusMsg --> StatusURL[\"status_notify_url\"]\n    \n    DeallocationURL --> SignAndSend[\"HMAC signature + POST\"]\n    StatusURL --> SignAndSend\n    \n    SignAndSend --> RetryLogic[\"Retry up to MAX_NOTIFY_RETRY\"]\n    RetryLogic --> Success[\"200/201 response\"]\n    RetryLogic --> AddToRetryTable[\"Add to notify_retry_table[]\"]\n```\n\n**Notification System Flow**\n\nSources: [neurons/register_api.py:2940-3000]()\n\n### Notification Configuration\n\nThe notification system operates with these key parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `MAX_NOTIFY_RETRY` | 3 | Maximum notification attempts |\n| `NOTIFY_RETRY_PERIOD` | 15 seconds | Delay between retry attempts |\n| Webhook Signature | HMAC-SHA256 | Request authentication |\n| SSL Certificates | Required | Secure communication |\n\nSources: [neurons/register_api.py:92-93](), [neurons/register_api.py:2972-2976]()\n\n## Data Models and Configuration\n\nThe Resource Management system uses several key data models and configuration parameters to define resource requirements and allocation responses.\n\n### Core Data Models\n\n```mermaid\nclassDiagram\n    class DeviceRequirement {\n        +int cpu_count\n        +str gpu_type\n        +int gpu_size\n        +int ram\n        +int hard_disk\n        +int timeline\n    }\n    \n    class Allocation {\n        +str resource\n        +str hotkey\n        +str regkey\n        +str ssh_ip\n        +int ssh_port\n        +str ssh_username\n        +str ssh_password\n        +str ssh_command\n        +str ssh_key\n        +str uuid_key\n        +int miner_version\n    }\n    \n    class ResourceQuery {\n        +str gpu_name\n        +int cpu_count_min\n        +int cpu_count_max\n        +float gpu_capacity_min\n        +float gpu_capacity_max\n        +float hard_disk_total_min\n        +float hard_disk_total_max\n        +float ram_total_min\n        +float ram_total_max\n    }\n    \n    class DockerRequirement {\n        +str base_image\n        +str ssh_key\n        +str volume_path\n        +str dockerfile\n    }\n    \n    DeviceRequirement --> Allocation : \"Generates\"\n    ResourceQuery --> Allocation : \"Filters\"\n    DockerRequirement --> Allocation : \"Configures\"\n```\n\n**Resource Management Data Models**\n\nSources: [neurons/register_api.py:147-175](), [neurons/register_api.py:156-167](), [neurons/register_api.py:204-213]()\n\n### System Configuration Constants\n\nThe system behavior is controlled through key configuration constants:\n\n| Constant | Value | Purpose |\n|----------|-------|---------|\n| `DATA_SYNC_PERIOD` | 600 seconds | Metagraph refresh interval |\n| `ALLOCATE_CHECK_PERIOD` | 180 seconds | Health check frequency |\n| `ALLOCATE_CHECK_COUNT` | 20 | Max timeout count before deallocation |\n| `MAX_ALLOCATION_RETRY` | 3 | Maximum allocation attempt retries |\n| `VALID_VALIDATOR_HOTKEYS` | Array of 19 hotkeys | Authorized validator addresses |\n\nSources: [neurons/register_api.py:85-116]()\n\nThe Resource Management system integrates these components to provide robust, scalable compute resource allocation with comprehensive monitoring and state management capabilities.",
    "resolved_links": [
      {
        "text": "neurons/register_api.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py",
        "original_deepwiki_href": "neurons/register_api.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/register_api.py:2741-2805",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1646-1702",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1881-1884",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2733-2805",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2807-2889",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:3002-3101",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:3039-3081",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2891-2919",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:1346-1419",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2915-2919",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2940-3000",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:92-93",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:2972-2976",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:147-175",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:156-167",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:204-213",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register_api.py:85-116",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "API Endpoints",
        "href": "/resource-allocation-api/api-endpoints#4.1",
        "original_deepwiki_href": "#4.1",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Validator System",
        "href": "/validator-system#2",
        "original_deepwiki_href": "#2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "flowchart TD\n    DevReq[\"DeviceRequirement\"] --> SelectCandidates[\"select_allocate_miners_hotkey()\"]\n    SelectCandidates --> CandidateList[\"candidates_hotkey[]\"]\n    CandidateList --> FilterAxons[\"Filter metagraph.axons\"]\n    FilterAxons --> AxonCandidates[\"axon_candidates[]\"]\n    \n    AxonCandidates --> CheckAvailability[\"dendrite(Allocate(checking=True))\"]\n    CheckAvailability --> ValidateResponses[\"Validate responses\"]\n    ValidateResponses --> FinalCandidates[\"final_candidates_hotkey[]\"]\n    \n    FinalCandidates --> ScoreSort[\"Score-based sorting\"]\n    ScoreSort --> SortedCandidates[\"sorted_hotkeys[]\"]\n    SortedCandidates --> AttemptAllocation[\"Attempt allocation\"]",
      "flowchart TD\n    WandbQuery[\"get_wandb_running_miners()\"] --> FilterRuns[\"Filter running miner runs\"]\n    FilterRuns --> CheckPenalized[\"Check penalized_hotkeys\"]\n    CheckPenalized --> ValidateAge[\"miner_is_older_than(48h)\"]\n    ValidateAge --> ValidatePog[\"miner_pog_ok(2.5h)\"]\n    ValidatePog --> CheckActive[\"Verify in metagraph.axons\"]\n    CheckActive --> ExtractSpecs[\"Extract specs from run.config\"]\n    ExtractSpecs --> SpecsDetails[\"specs_details{}\"]\n    SpecsDetails --> ResourceList[\"resource_list[]\"]",
      "flowchart TD\n    AllocateSpec[\"_allocate_container()\"] --> BuildDevReq[\"Build device_requirement{}\"]\n    BuildDevReq --> FindCandidates[\"select_allocate_miners_hotkey()\"]\n    FindCandidates --> CheckAvailable[\"dendrite(checking=True)\"]\n    CheckAvailable --> ScoreSort[\"torch.ones_like(metagraph.S)\"]\n    ScoreSort --> TryAllocation[\"Loop through sorted_hotkeys\"]\n    TryAllocation --> SendAllocate[\"dendrite(Allocate(checking=False))\"]\n    SendAllocate --> ValidateResponse[\"Check response.status\"]\n    ValidateResponse --> Success[\"Return allocation info\"]\n    ValidateResponse --> NextCandidate[\"Try next candidate\"]\n    NextCandidate --> TryAllocation",
      "flowchart TD\n    AllocateHotkey[\"_allocate_container_hotkey()\"] --> FindAxon[\"Locate axon by hotkey\"]\n    FindAxon --> SetDefaults[\"Set default device_requirement{}\"]\n    SetDefaults --> SetBaseImage[\"docker_requirement.base_image\"]\n    SetBaseImage --> DirectAllocate[\"dendrite(Allocate())\"]\n    DirectAllocate --> ValidateResponse[\"Check response.status\"]\n    ValidateResponse --> ReturnInfo[\"Return allocation + miner_version\"]\n    ValidateResponse --> ReturnError[\"Return error message\"]",
      "flowchart TD\n    CheckTask[\"_check_allocation()\"] --> QueryDB[\"SELECT * FROM allocation\"]\n    QueryDB --> LoopAllocations[\"For each allocation\"]\n    LoopAllocations --> ValidateHotkey[\"Check hotkey in metagraph\"]\n    ValidateHotkey --> HealthCheck[\"dendrite_check(Allocate(checking=True))\"]\n    HealthCheck --> EvaluateResponse[\"Evaluate response\"]\n    \n    EvaluateResponse --> Online[\"response.status == False\"]\n    EvaluateResponse --> Offline[\"No response or timeout\"]\n    \n    Online --> NotifyOnline[\"_notify_allocation_status(ONLINE)\"]\n    Online --> RemoveFromCheck[\"Remove from checking_allocated[]\"]\n    \n    Offline --> AddToCheck[\"Add to checking_allocated[]\"]\n    Offline --> NotifyOffline[\"_notify_allocation_status(OFFLINE)\"]\n    Offline --> CountChecks[\"checking_allocated.count(hotkey)\"]\n    \n    CountChecks --> MaxReached[\"count >= ALLOCATE_CHECK_COUNT\"]\n    MaxReached --> Deallocate[\"update_allocation_db(False)\"]\n    Deallocate --> NotifyDealloc[\"_notify_allocation_status(DEALLOCATION)\"]",
      "flowchart TD\n    AllocDB[\"allocation table\"] --> UpdateLocal[\"update_allocation_db()\"]\n    UpdateLocal --> ExtractHotkeys[\"Extract hotkey_list[]\"]\n    ExtractHotkeys --> UpdateWandB[\"_update_allocation_wandb()\"]\n    UpdateWandB --> WandbSync[\"wandb.update_allocated_hotkeys()\"]\n    \n    QueryState[\"List operations\"] --> QueryDB[\"SELECT hotkey, details FROM allocation\"]\n    QueryDB --> ParseJSON[\"json.loads(details)\"]\n    ParseJSON --> BuildAllocation[\"Build Allocation objects\"]\n    \n    HealthCheck[\"Health monitoring\"] --> StateUpdate[\"Allocation state changes\"]\n    StateUpdate --> NotifyRetry[\"notify_retry_table[]\"]\n    StateUpdate --> UpdateLocal",
      "flowchart TD\n    EventTrigger[\"Allocation Event\"] --> EventType{\"Event Type\"}\n    \n    EventType --> Deallocation[\"DEALLOCATION\"]\n    EventType --> Online[\"ONLINE\"] \n    EventType --> Offline[\"OFFLINE\"]\n    \n    Deallocation --> BuildDeallocationMsg[\"Build deallocation message\"]\n    Online --> BuildStatusMsg[\"Build status change message\"]\n    Offline --> BuildStatusMsg\n    \n    BuildDeallocationMsg --> DeallocationURL[\"deallocation_notify_url\"]\n    BuildStatusMsg --> StatusURL[\"status_notify_url\"]\n    \n    DeallocationURL --> SignAndSend[\"HMAC signature + POST\"]\n    StatusURL --> SignAndSend\n    \n    SignAndSend --> RetryLogic[\"Retry up to MAX_NOTIFY_RETRY\"]\n    RetryLogic --> Success[\"200/201 response\"]\n    RetryLogic --> AddToRetryTable[\"Add to notify_retry_table[]\"]",
      "classDiagram\n    class DeviceRequirement {\n        +int cpu_count\n        +str gpu_type\n        +int gpu_size\n        +int ram\n        +int hard_disk\n        +int timeline\n    }\n    \n    class Allocation {\n        +str resource\n        +str hotkey\n        +str regkey\n        +str ssh_ip\n        +int ssh_port\n        +str ssh_username\n        +str ssh_password\n        +str ssh_command\n        +str ssh_key\n        +str uuid_key\n        +int miner_version\n    }\n    \n    class ResourceQuery {\n        +str gpu_name\n        +int cpu_count_min\n        +int cpu_count_max\n        +float gpu_capacity_min\n        +float gpu_capacity_max\n        +float hard_disk_total_min\n        +float hard_disk_total_max\n        +float ram_total_min\n        +float ram_total_max\n    }\n    \n    class DockerRequirement {\n        +str base_image\n        +str ssh_key\n        +str volume_path\n        +str dockerfile\n    }\n    \n    DeviceRequirement --> Allocation : \"Generates\"\n    ResourceQuery --> Allocation : \"Filters\"\n    DockerRequirement --> Allocation : \"Configures\""
    ],
    "potential_frontmatter": {
      "title": "Resource Management"
    }
  },
  "/neuralinternet/ni-compute/5-communication-protocols": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/5-communication-protocols",
    "title": "Communication Protocols",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/5-communication-protocols",
    "level": 0,
    "target_astro_path": "/communication-protocols",
    "main_markdown_content": "# Communication Protocols\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/axon.py](compute/axon.py)\n- [compute/protocol.py](compute/protocol.py)\n\n</details>\n\n\n\n## Overview\n\nThis page documents the network communication protocols and message formats used between system components in the NI Compute subnet. The system implements three core Bittensor synapse protocols (`Specs`, `Allocate`, `Challenge`) and custom extensions to Bittensor's communication infrastructure (`ComputeSubnetAxon`, `ComputeSubnetSubtensor`, `ComputeSubnetAxonMiddleware`).\n\nThese protocols enable hardware specification discovery, resource allocation, and cryptographic validation challenges between validators and miners. For information about the HTTP API endpoints, see [Resource Allocation API](#4).\n\nSources: [compute/protocol.py:1-136](), [compute/axon.py:1-487]()\n\n## Protocol Stack\n\nNI Compute extends Bittensor's communication infrastructure with custom protocols specific to the GPU compute marketplace. The system implements three main synapse protocols and uses customized versions of Bittensor's Axon and Subtensor components.\n\n**Protocol Stack Architecture**\n```mermaid\ngraph TD\n    subgraph \"Bittensor Base Classes\"\n        bt_Synapse[\"bt.Synapse\"]\n        bittensor_Axon[\"bittensor.core.axon.Axon\"]\n        bittensor_Subtensor[\"bittensor.core.subtensor.Subtensor\"]\n        AxonMiddleware[\"bittensor.core.axon.AxonMiddleware\"]\n    end\n    \n    subgraph \"compute/protocol.py\"\n        Specs[\"class Specs(bt.Synapse)\"] --> bt_Synapse\n        Allocate[\"class Allocate(bt.Synapse)\"] --> bt_Synapse\n        Challenge[\"class Challenge(bt.Synapse)\"] --> bt_Synapse\n    end\n    \n    subgraph \"compute/axon.py\"\n        ComputeSubnetAxon[\"class ComputeSubnetAxon(axon)\"] --> bittensor_Axon\n        ComputeSubnetSubtensor[\"class ComputeSubnetSubtensor(subtensor)\"] --> bittensor_Subtensor\n        ComputeSubnetAxonMiddleware[\"class ComputeSubnetAxonMiddleware(AxonMiddleware)\"] --> AxonMiddleware\n        custom_serve_extrinsic[\"custom_serve_extrinsic()\"]\n    end\n    \n    subgraph \"Communication Methods\"\n        preprocess[\"preprocess()\"] --> ComputeSubnetAxonMiddleware\n        serve_prometheus[\"serve_prometheus()\"] --> ComputeSubnetSubtensor\n        info[\"info()\"] --> ComputeSubnetAxon\n    end\n    \n    ComputeSubnetAxon --> ComputeSubnetAxonMiddleware\n```\n\nSources: [compute/protocol.py:23-136](), [compute/axon.py:152-487]()\n\n## Core Synapse Protocols\n\nNI Compute defines three primary protocols for communication between validators and miners, all extending Bittensor's Synapse base class.\n\n### Specs Protocol\n\nThe `Specs` class enables validators to query hardware specifications from miners, providing detailed information about CPU, GPU, RAM, and storage resources.\n\n**Specs Class Definition**\n```mermaid\nclassDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Specs {\n        +str specs_input\n        +str specs_output  \n        +deserialize() str\n    }\n    \n    bt_Synapse <|-- Specs\n    \n    note for Specs \"compute/protocol.py:23-57\"\n```\n\n| Attribute | Type | Description |\n|-----------|------|-------------|\n| `specs_input` | `str` | Input parameter (typically empty string) |\n| `specs_output` | `str` | JSON response containing detailed hardware specifications |\n\nThe `deserialize()` method returns `self.specs_output` containing hardware details in the format:\n```\n{\"CPU\":{'count' : 4, 'vendor_id_raw' : 'AuthenticAMD', ...}}\n```\n\nSources: [compute/protocol.py:23-57]()\n\n### Allocate Protocol\n\nThe `Allocate` class facilitates resource allocation requests from validators to miners, including parameters for container configuration, access credentials, and resource requirements.\n\n**Allocate Class Definition**\n```mermaid\nclassDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Allocate {\n        +int timeline\n        +dict device_requirement\n        +bool checking\n        +dict output\n        +str public_key\n        +dict docker_requirement\n        +bool docker_change\n        +dict docker_action\n        +deserialize() dict\n    }\n    \n    bt_Synapse <|-- Allocate\n    \n    note for Allocate \"compute/protocol.py:60-110\"\n```\n\n| Attribute | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `timeline` | `int` | `0` | Duration of allocation in seconds |\n| `device_requirement` | `dict` | `{}` | Hardware requirements specification |\n| `checking` | `bool` | `True` | Flag: `True` for availability check, `False` for actual allocation |\n| `public_key` | `str` | `\"\"` | Public key for secure communication |\n| `output` | `dict` | `{}` | Response containing allocation details from miner |\n| `docker_requirement` | `dict` | Default config | Container configuration with `base_image`, `ssh_key`, `ssh_port`, `volume_path`, `dockerfile` |\n| `docker_change` | `bool` | `False` | Flag indicating Docker configuration changes |\n| `docker_action` | `dict` | Action config | Docker actions with `action`, `ssh_key`, `key_type` |\n\nThe `deserialize()` method returns `self.output` containing the allocation response from the miner.\n\nSources: [compute/protocol.py:60-110]()\n\n### Challenge Protocol\n\nThe `Challenge` class enables validators to verify miner GPU capabilities through cryptographic proof-of-work challenges, ensuring that miners have the claimed GPU resources.\n\n**Challenge Class Definition**\n```mermaid\nclassDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Challenge {\n        +str challenge_hash\n        +str challenge_salt\n        +str challenge_mode\n        +str challenge_chars\n        +str challenge_mask\n        +int challenge_difficulty\n        +dict output\n        +deserialize() dict\n    }\n    \n    bt_Synapse <|-- Challenge\n    \n    note for Challenge \"compute/protocol.py:112-136\"\n```\n\n| Attribute | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `challenge_hash` | `str` | `\"\"` | Cryptographic hash to be solved |\n| `challenge_salt` | `str` | `\"\"` | Salt value for the challenge |\n| `challenge_mode` | `str` | `\"\"` | Challenge type (hashcat mode) |\n| `challenge_chars` | `str` | `\"\"` | Character set for brute force |\n| `challenge_mask` | `str` | `\"\"` | Mask pattern for challenge |\n| `challenge_difficulty` | `int` | `compute.pow_min_difficulty` | Difficulty level of the challenge |\n| `output` | `dict` | `{}` | Response containing solution or error |\n\nThe `deserialize()` method returns `self.output` which contains either the password solution or error information:\n```python\n{\"password\": None, \"error\": f\"Hashcat execution failed with code {process.returncode}: {stderr}\"}\n```\n\nSources: [compute/protocol.py:112-136]()\n\n## Custom Axon and Subtensor\n\nNI Compute extends Bittensor's communication infrastructure with customized components to support the specific needs of the compute subnet.\n\n### ComputeSubnetAxon\n\nThe `ComputeSubnetAxon` class extends `bittensor.core.axon.Axon` to provide compute subnet-specific functionality, including customized version tracking and middleware processing.\n\n**ComputeSubnetAxon Request Processing Flow**\n```mermaid\nsequenceDiagram\n    participant Validator\n    participant ComputeSubnetAxon\n    participant ComputeSubnetAxonMiddleware\n    participant SynapseProtocol[\"Specs/Allocate/Challenge\"]\n    participant MinerLogic[\"Miner Handler Functions\"]\n    \n    Validator->>ComputeSubnetAxon: \"HTTP POST /{protocol_name}\"\n    ComputeSubnetAxon->>ComputeSubnetAxonMiddleware: \"process_request()\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"preprocess(request)\"\n    Note over ComputeSubnetAxonMiddleware: \"request.url.path.split('/')[1]\"<br/>\"Extract protocol name\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"request_synapse.from_headers()\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"Fill axon.nonce, axon.signature\"\n    ComputeSubnetAxonMiddleware->>SynapseProtocol: \"Forward to registered handler\"\n    SynapseProtocol->>MinerLogic: \"Execute miner logic\"\n    MinerLogic->>SynapseProtocol: \"Fill synapse response fields\"\n    SynapseProtocol->>ComputeSubnetAxonMiddleware: \"Return completed synapse\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxon: \"Return HTTP response\"\n    ComputeSubnetAxon->>Validator: \"Return synapse response\"\n```\n\n**Key Methods and Attributes**:\n\n| Method/Attribute | Description |\n|------------------|-------------|\n| `__init__()` | Initializes with custom middleware class `ComputeSubnetAxonMiddleware` |\n| `info()` | Returns `bittensor.AxonInfo` with `get_local_version()` and compute-specific placeholders |\n| `middleware_cls` | Set to `ComputeSubnetAxonMiddleware` |\n\nThe `info()` method returns customized axon information including:\n- `version`: From `get_local_version()`\n- `protocol`: `4`\n- `placeholder1`: `1`\n- `placeholder2`: `2`\n\nSources: [compute/axon.py:285-390]()\n\n### ComputeSubnetSubtensor\n\nThe `ComputeSubnetSubtensor` class extends `bittensor.core.subtensor.Subtensor` with additional functionality for Prometheus metrics extrinsic submission.\n\n**Key Methods**:\n\n| Method | Parameters | Description |\n|--------|------------|-------------|\n| `serve_prometheus()` | `wallet`, `port`, `netuid`, `wait_for_inclusion`, `wait_for_finalization` | Serves Prometheus metrics by submitting extrinsic to blockchain |\n| `do_serve_prometheus()` | `wallet`, `call_params`, `wait_for_inclusion`, `wait_for_finalization` | Core method for submitting Prometheus extrinsics with retry logic |\n\n**Prometheus Extrinsic Flow**:\n```mermaid\nsequenceDiagram\n    participant Miner\n    participant ComputeSubnetSubtensor\n    participant SubstrateInterface[\"self.substrate\"]\n    participant Blockchain[\"Bittensor Blockchain\"]\n    \n    Miner->>ComputeSubnetSubtensor: \"serve_prometheus(wallet, port, netuid)\"\n    ComputeSubnetSubtensor->>ComputeSubnetSubtensor: \"do_serve_prometheus()\"\n    ComputeSubnetSubtensor->>ComputeSubnetSubtensor: \"@retry(delay=1, tries=3, backoff=2)\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"compose_call('SubtensorModule', 'serve_prometheus')\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"create_signed_extrinsic(call, wallet.hotkey)\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"submit_extrinsic()\"\n    SubstrateInterface->>Blockchain: \"Submit prometheus extrinsic\"\n    Blockchain->>SubstrateInterface: \"Extrinsic response\"\n    SubstrateInterface->>ComputeSubnetSubtensor: \"response.process_events()\"\n    ComputeSubnetSubtensor->>Miner: \"return (success, error_message)\"\n```\n\nThe implementation includes retry logic with exponential backoff and comprehensive error handling for substrate communication failures.\n\nSources: [compute/axon.py:152-283]()\n\n### ComputeSubnetAxonMiddleware\n\nThe `ComputeSubnetAxonMiddleware` class extends `bittensor.core.axon.AxonMiddleware` and handles the preprocessing of requests, extracting the appropriate Synapse protocol based on the URL path and preparing it for execution.\n\n**Middleware Processing Implementation**:\n```mermaid\nflowchart TD\n    Request[\"HTTP Request\"] --> ExtractName[\"request.url.path.split('/')[1]\"]\n    ExtractName --> GetSynapse[\"self.axon.forward_class_types.get(request_name)\"]\n    GetSynapse --> CreateSynapse[\"request_synapse.from_headers(request.headers)\"]\n    CreateSynapse --> FillAxon[\"synapse.axon.__dict__.update()\"]\n    FillAxon --> FillDendrite[\"synapse.dendrite.__dict__.update()\"]\n    FillDendrite --> Sign[\"self.axon.wallet.hotkey.sign(message)\"]\n    Sign --> ReturnSynapse[\"return synapse\"]\n    \n    GetSynapse -->|\"None\"| UnknownSynapseError[\"raise UnknownSynapseError\"]\n    CreateSynapse -->|\"Exception\"| SynapseParsingError[\"raise SynapseParsingError\"]\n    ExtractName -->|\"Exception\"| InvalidRequestNameError[\"raise InvalidRequestNameError\"]\n```\n\n**Key `preprocess()` Method Logic**:\n\n1. **Request Name Extraction**: `request.url.path.split(\"/\")[1]`\n2. **Synapse Type Resolution**: `self.axon.forward_class_types.get(request_name)`\n3. **Synapse Creation**: `request_synapse.from_headers(request.headers)`\n4. **Axon Info Population**:\n   - `version`: `__version_as_int__`\n   - `uuid`: `str(self.axon.uuid)`\n   - `nonce`: `time.monotonic_ns()`\n   - `status_code`: `\"100\"`\n5. **Dendrite Info Population**: Client `port` and `ip`\n6. **Signature Generation**: \n   ```python\n   message = f\"{synapse.axon.nonce}.{synapse.dendrite.hotkey}.{synapse.axon.hotkey}.{synapse.axon.uuid}\"\n   synapse.axon.signature = f\"0x{self.axon.wallet.hotkey.sign(message).hex()}\"\n   ```\n\nSources: [compute/axon.py:391-487]()\n\n## Communication Flow\n\nThe following diagram illustrates the end-to-end communication flow between validators and miners using the three core protocols.\n\n**Complete Protocol Communication Flow**\n```mermaid\nsequenceDiagram\n    participant Validator[\"neurons/validator.py\"]\n    participant Dendrite[\"bt.dendrite\"]\n    participant MinerAxon[\"ComputeSubnetAxon\"]\n    participant Middleware[\"ComputeSubnetAxonMiddleware\"]\n    participant MinerLogic[\"neurons/miner.py\"]\n    \n    Note over Validator,MinerLogic: 1. Hardware Discovery (Specs Protocol)\n    Validator->>Dendrite: \"query(Specs(specs_input=''))\"\n    Dendrite->>MinerAxon: \"POST /Specs\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Specs)\"\n    MinerLogic->>MinerLogic: \"Get hardware specifications\"\n    MinerLogic->>Middleware: \"specs_output=hardware_json\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + specs_output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"\n    \n    Note over Validator,MinerLogic: 2. GPU Verification (Challenge Protocol)\n    Validator->>Dendrite: \"query(Challenge(challenge_hash, challenge_difficulty))\"\n    Dendrite->>MinerAxon: \"POST /Challenge\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Challenge)\"\n    MinerLogic->>MinerLogic: \"Execute hashcat proof-of-work\"\n    MinerLogic->>Middleware: \"output={'password': solution}\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"\n    \n    Note over Validator,MinerLogic: 3. Resource Allocation (Allocate Protocol)\n    Validator->>Dendrite: \"query(Allocate(timeline, device_requirement, docker_requirement))\"\n    Dendrite->>MinerAxon: \"POST /Allocate\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Allocate)\"\n    MinerLogic->>MinerLogic: \"Create Docker container\"\n    MinerLogic->>Middleware: \"output={'ssh_address': address, 'ssh_port': port}\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"\n```\n\nSources: [compute/protocol.py:1-136](), [compute/axon.py:391-487]()\n\n## Protocol Version Management\n\nNI Compute implements version tracking through the `__version_as_int__` global variable and `get_local_version()` function to ensure protocol compatibility between nodes.\n\n**Version Integration Points**\n```mermaid\nflowchart TD\n    version_as_int[\"__version_as_int__\"] --> axon_info[\"ComputeSubnetAxon.info()\"]\n    version_as_int --> serve_params[\"AxonServeCallParams.version\"]\n    version_as_int --> middleware[\"ComputeSubnetAxonMiddleware.preprocess()\"]\n    \n    get_local_version[\"get_local_version()\"] --> axon_info\n    \n    axon_info --> bittensor_AxonInfo[\"bittensor.AxonInfo\"]\n    serve_params --> custom_serve_extrinsic[\"custom_serve_extrinsic()\"]\n    middleware --> synapse_axon[\"synapse.axon.version\"]\n    \n    bittensor_AxonInfo --> blockchain[\"Blockchain Registration\"]\n    custom_serve_extrinsic --> blockchain\n```\n\n**Version Usage in Communication**:\n\n| Component | Usage | Code Reference |\n|-----------|-------|----------------|\n| `ComputeSubnetAxonMiddleware.preprocess()` | Sets `synapse.axon.version = __version_as_int__` | [compute/axon.py:470]() |\n| `ComputeSubnetAxon.info()` | Returns `bittensor.AxonInfo(version=get_local_version())` | [compute/axon.py:379]() |\n| `custom_serve_extrinsic()` | Uses `version=__version_as_int__` in `AxonServeCallParams` | [compute/axon.py:104]() |\n\nThe version is encoded as an integer for efficient blockchain storage and comparison, ensuring that all communication includes version information for compatibility checking.\n\nSources: [compute/axon.py:47-104](), [compute/axon.py:376-388](), [compute/axon.py:470]()\n\n## Subtensor Integration\n\nNI Compute extends the standard Bittensor Subtensor to support custom extrinsics required for the compute subnet through the `custom_serve_extrinsic` function and `ComputeSubnetSubtensor` class.\n\n**Custom Serve Extrinsic Implementation**\n```mermaid\nflowchart TD\n    custom_serve_extrinsic[\"custom_serve_extrinsic()\"] --> unlock_key[\"unlock_key(wallet, 'hotkey')\"]\n    unlock_key --> create_params[\"AxonServeCallParams()\"]\n    create_params --> get_neuron[\"subtensor.get_neuron_for_pubkey_and_subnet()\"]\n    get_neuron --> check_updated{\"neuron_up_to_date?\"}\n    check_updated -->|\"True\"| return_true[\"return True\"]\n    check_updated -->|\"False\"| do_serve[\"do_serve_axon()\"]\n    do_serve --> wait_check{\"wait_for_inclusion or wait_for_finalization?\"}\n    wait_check -->|\"True\"| check_success{\"success?\"}\n    wait_check -->|\"False\"| return_true2[\"return True\"]\n    check_success -->|\"True\"| return_true3[\"return True\"]\n    check_success -->|\"False\"| return_false[\"return False\"]\n```\n\n**Key Customizations**:\n\n1. **Custom Version Integration**: Uses `__version_as_int__` from the compute subnet\n2. **Modified Parameters**: Includes compute-specific `placeholder1` and `placeholder2` values\n3. **Enhanced Logging**: Custom debug messages for axon serving status\n4. **Registration Override**: Replaces `bittensor.core.extrinsics.serving.serve_extrinsic` with custom implementation\n\n**AxonServeCallParams Structure**:\n```python\nparams = AxonServeCallParams(\n    version=__version_as_int__,           # Compute subnet version\n    ip=net.ip_to_int(ip),                # IP address as integer\n    port=port,                           # Port number\n    ip_type=net.ip_version(ip),          # IP version (4 or 6)\n    netuid=netuid,                       # Subnet ID\n    hotkey=wallet.hotkey.ss58_address,   # Hotkey address\n    coldkey=wallet.coldkeypub.ss58_address, # Coldkey address\n    protocol=protocol,                   # Protocol version\n    placeholder1=placeholder1,           # Reserved field\n    placeholder2=placeholder2,           # Reserved field\n    certificate=certificate,             # TLS certificate\n)\n```\n\nThe system patches the global Bittensor extrinsic function:\n```python\nbittensor.core.extrinsics.serving.serve_extrinsic = custom_serve_extrinsic\n```\n\nSources: [compute/axon.py:63-150]()\n\n## Summary\n\nThe communication protocols in NI Compute form a critical infrastructure layer that enables secure, efficient interaction between validators and miners in the decentralized GPU marketplace. By extending Bittensor's core communication components, NI Compute implements specialized protocols for hardware discovery, verification, and resource allocation.\n\nThis layered approach ensures:\n\n1. **Discoverability**: Validators can discover and verify miner hardware capabilities\n2. **Security**: Secure communication channels for sensitive operations\n3. **Resource Management**: Efficient allocation and management of GPU resources\n4. **Monitoring**: Integration with Prometheus for metrics collection\n\nUnderstanding these protocols is essential for developing and extending the NI Compute platform with new features and capabilities.",
    "resolved_links": [
      {
        "text": "compute/axon.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py",
        "original_deepwiki_href": "compute/axon.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/protocol.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py",
        "original_deepwiki_href": "compute/protocol.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/protocol.py:1-136",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:1-487",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/protocol.py:23-136",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:152-487",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/protocol.py:23-57",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/protocol.py:60-110",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/protocol.py:112-136",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:285-390",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:152-283",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:391-487",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:47-104",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:376-388",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:470",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:63-150",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Resource Allocation API",
        "href": "/resource-allocation-api#4",
        "original_deepwiki_href": "#4",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TD\n    subgraph \"Bittensor Base Classes\"\n        bt_Synapse[\"bt.Synapse\"]\n        bittensor_Axon[\"bittensor.core.axon.Axon\"]\n        bittensor_Subtensor[\"bittensor.core.subtensor.Subtensor\"]\n        AxonMiddleware[\"bittensor.core.axon.AxonMiddleware\"]\n    end\n    \n    subgraph \"compute/protocol.py\"\n        Specs[\"class Specs(bt.Synapse)\"] --> bt_Synapse\n        Allocate[\"class Allocate(bt.Synapse)\"] --> bt_Synapse\n        Challenge[\"class Challenge(bt.Synapse)\"] --> bt_Synapse\n    end\n    \n    subgraph \"compute/axon.py\"\n        ComputeSubnetAxon[\"class ComputeSubnetAxon(axon)\"] --> bittensor_Axon\n        ComputeSubnetSubtensor[\"class ComputeSubnetSubtensor(subtensor)\"] --> bittensor_Subtensor\n        ComputeSubnetAxonMiddleware[\"class ComputeSubnetAxonMiddleware(AxonMiddleware)\"] --> AxonMiddleware\n        custom_serve_extrinsic[\"custom_serve_extrinsic()\"]\n    end\n    \n    subgraph \"Communication Methods\"\n        preprocess[\"preprocess()\"] --> ComputeSubnetAxonMiddleware\n        serve_prometheus[\"serve_prometheus()\"] --> ComputeSubnetSubtensor\n        info[\"info()\"] --> ComputeSubnetAxon\n    end\n    \n    ComputeSubnetAxon --> ComputeSubnetAxonMiddleware",
      "classDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Specs {\n        +str specs_input\n        +str specs_output  \n        +deserialize() str\n    }\n    \n    bt_Synapse <|-- Specs\n    \n    note for Specs \"compute/protocol.py:23-57\"",
      "classDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Allocate {\n        +int timeline\n        +dict device_requirement\n        +bool checking\n        +dict output\n        +str public_key\n        +dict docker_requirement\n        +bool docker_change\n        +dict docker_action\n        +deserialize() dict\n    }\n    \n    bt_Synapse <|-- Allocate\n    \n    note for Allocate \"compute/protocol.py:60-110\"",
      "classDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Challenge {\n        +str challenge_hash\n        +str challenge_salt\n        +str challenge_mode\n        +str challenge_chars\n        +str challenge_mask\n        +int challenge_difficulty\n        +dict output\n        +deserialize() dict\n    }\n    \n    bt_Synapse <|-- Challenge\n    \n    note for Challenge \"compute/protocol.py:112-136\"",
      "sequenceDiagram\n    participant Validator\n    participant ComputeSubnetAxon\n    participant ComputeSubnetAxonMiddleware\n    participant SynapseProtocol[\"Specs/Allocate/Challenge\"]\n    participant MinerLogic[\"Miner Handler Functions\"]\n    \n    Validator->>ComputeSubnetAxon: \"HTTP POST /{protocol_name}\"\n    ComputeSubnetAxon->>ComputeSubnetAxonMiddleware: \"process_request()\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"preprocess(request)\"\n    Note over ComputeSubnetAxonMiddleware: \"request.url.path.split('/')[1]\"<br/>\"Extract protocol name\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"request_synapse.from_headers()\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"Fill axon.nonce, axon.signature\"\n    ComputeSubnetAxonMiddleware->>SynapseProtocol: \"Forward to registered handler\"\n    SynapseProtocol->>MinerLogic: \"Execute miner logic\"\n    MinerLogic->>SynapseProtocol: \"Fill synapse response fields\"\n    SynapseProtocol->>ComputeSubnetAxonMiddleware: \"Return completed synapse\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxon: \"Return HTTP response\"\n    ComputeSubnetAxon->>Validator: \"Return synapse response\"",
      "sequenceDiagram\n    participant Miner\n    participant ComputeSubnetSubtensor\n    participant SubstrateInterface[\"self.substrate\"]\n    participant Blockchain[\"Bittensor Blockchain\"]\n    \n    Miner->>ComputeSubnetSubtensor: \"serve_prometheus(wallet, port, netuid)\"\n    ComputeSubnetSubtensor->>ComputeSubnetSubtensor: \"do_serve_prometheus()\"\n    ComputeSubnetSubtensor->>ComputeSubnetSubtensor: \"@retry(delay=1, tries=3, backoff=2)\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"compose_call('SubtensorModule', 'serve_prometheus')\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"create_signed_extrinsic(call, wallet.hotkey)\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"submit_extrinsic()\"\n    SubstrateInterface->>Blockchain: \"Submit prometheus extrinsic\"\n    Blockchain->>SubstrateInterface: \"Extrinsic response\"\n    SubstrateInterface->>ComputeSubnetSubtensor: \"response.process_events()\"\n    ComputeSubnetSubtensor->>Miner: \"return (success, error_message)\"",
      "flowchart TD\n    Request[\"HTTP Request\"] --> ExtractName[\"request.url.path.split('/')[1]\"]\n    ExtractName --> GetSynapse[\"self.axon.forward_class_types.get(request_name)\"]\n    GetSynapse --> CreateSynapse[\"request_synapse.from_headers(request.headers)\"]\n    CreateSynapse --> FillAxon[\"synapse.axon.__dict__.update()\"]\n    FillAxon --> FillDendrite[\"synapse.dendrite.__dict__.update()\"]\n    FillDendrite --> Sign[\"self.axon.wallet.hotkey.sign(message)\"]\n    Sign --> ReturnSynapse[\"return synapse\"]\n    \n    GetSynapse -->|\"None\"| UnknownSynapseError[\"raise UnknownSynapseError\"]\n    CreateSynapse -->|\"Exception\"| SynapseParsingError[\"raise SynapseParsingError\"]\n    ExtractName -->|\"Exception\"| InvalidRequestNameError[\"raise InvalidRequestNameError\"]",
      "sequenceDiagram\n    participant Validator[\"neurons/validator.py\"]\n    participant Dendrite[\"bt.dendrite\"]\n    participant MinerAxon[\"ComputeSubnetAxon\"]\n    participant Middleware[\"ComputeSubnetAxonMiddleware\"]\n    participant MinerLogic[\"neurons/miner.py\"]\n    \n    Note over Validator,MinerLogic: 1. Hardware Discovery (Specs Protocol)\n    Validator->>Dendrite: \"query(Specs(specs_input=''))\"\n    Dendrite->>MinerAxon: \"POST /Specs\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Specs)\"\n    MinerLogic->>MinerLogic: \"Get hardware specifications\"\n    MinerLogic->>Middleware: \"specs_output=hardware_json\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + specs_output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"\n    \n    Note over Validator,MinerLogic: 2. GPU Verification (Challenge Protocol)\n    Validator->>Dendrite: \"query(Challenge(challenge_hash, challenge_difficulty))\"\n    Dendrite->>MinerAxon: \"POST /Challenge\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Challenge)\"\n    MinerLogic->>MinerLogic: \"Execute hashcat proof-of-work\"\n    MinerLogic->>Middleware: \"output={'password': solution}\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"\n    \n    Note over Validator,MinerLogic: 3. Resource Allocation (Allocate Protocol)\n    Validator->>Dendrite: \"query(Allocate(timeline, device_requirement, docker_requirement))\"\n    Dendrite->>MinerAxon: \"POST /Allocate\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Allocate)\"\n    MinerLogic->>MinerLogic: \"Create Docker container\"\n    MinerLogic->>Middleware: \"output={'ssh_address': address, 'ssh_port': port}\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"",
      "flowchart TD\n    version_as_int[\"__version_as_int__\"] --> axon_info[\"ComputeSubnetAxon.info()\"]\n    version_as_int --> serve_params[\"AxonServeCallParams.version\"]\n    version_as_int --> middleware[\"ComputeSubnetAxonMiddleware.preprocess()\"]\n    \n    get_local_version[\"get_local_version()\"] --> axon_info\n    \n    axon_info --> bittensor_AxonInfo[\"bittensor.AxonInfo\"]\n    serve_params --> custom_serve_extrinsic[\"custom_serve_extrinsic()\"]\n    middleware --> synapse_axon[\"synapse.axon.version\"]\n    \n    bittensor_AxonInfo --> blockchain[\"Blockchain Registration\"]\n    custom_serve_extrinsic --> blockchain",
      "flowchart TD\n    custom_serve_extrinsic[\"custom_serve_extrinsic()\"] --> unlock_key[\"unlock_key(wallet, 'hotkey')\"]\n    unlock_key --> create_params[\"AxonServeCallParams()\"]\n    create_params --> get_neuron[\"subtensor.get_neuron_for_pubkey_and_subnet()\"]\n    get_neuron --> check_updated{\"neuron_up_to_date?\"}\n    check_updated -->|\"True\"| return_true[\"return True\"]\n    check_updated -->|\"False\"| do_serve[\"do_serve_axon()\"]\n    do_serve --> wait_check{\"wait_for_inclusion or wait_for_finalization?\"}\n    wait_check -->|\"True\"| check_success{\"success?\"}\n    wait_check -->|\"False\"| return_true2[\"return True\"]\n    check_success -->|\"True\"| return_true3[\"return True\"]\n    check_success -->|\"False\"| return_false[\"return False\"]"
    ],
    "potential_frontmatter": {
      "title": "Communication Protocols"
    }
  },
  "/neuralinternet/ni-compute/5.1-specs-allocate-and-challenge-protocols": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/5.1-specs-allocate-and-challenge-protocols",
    "title": "Specs, Allocate, and Challenge Protocols",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/5.1-specs-allocate-and-challenge-protocols",
    "level": 1,
    "target_astro_path": "/communication-protocols/specs-allocate-and-challenge-protocols",
    "main_markdown_content": "# Specs, Allocate, and Challenge Protocols\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/protocol.py](compute/protocol.py)\n\n</details>\n\n\n\nThis document covers the three core Bittensor synapse protocols used for communication between validators and miners in the NI Compute Subnet: hardware specification queries, resource allocation requests, and challenge-response mechanisms. These protocols form the foundational communication layer for the compute marketplace.\n\nFor information about custom Bittensor extensions and Axon modifications, see [Custom Axon and Subtensor](#5.2).\n\n## Protocol Architecture\n\nThe NI Compute Subnet implements three primary communication protocols that extend Bittensor's `bt.Synapse` base class. Each protocol handles a specific aspect of the validator-miner interaction workflow.\n\n### Protocol Inheritance Structure\n\n```mermaid\nclassDiagram\n    class bt_Synapse {\n        <<abstract>>\n        +deserialize()\n    }\n    \n    class Specs {\n        +specs_input: str\n        +specs_output: str\n        +deserialize() str\n    }\n    \n    class Allocate {\n        +timeline: int\n        +device_requirement: dict\n        +checking: bool\n        +public_key: str\n        +docker_requirement: dict\n        +docker_change: bool\n        +docker_action: dict\n        +output: dict\n        +deserialize() dict\n    }\n    \n    class Challenge {\n        +challenge_hash: str\n        +challenge_salt: str\n        +challenge_mode: str\n        +challenge_chars: str\n        +challenge_mask: str\n        +challenge_difficulty: int\n        +output: dict\n        +deserialize() dict\n    }\n    \n    bt_Synapse <|-- Specs\n    bt_Synapse <|-- Allocate\n    bt_Synapse <|-- Challenge\n```\n\n**Sources:** [compute/protocol.py:18-136]()\n\n### Protocol Communication Flow\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator\"\n    participant BT as \"Bittensor Network\"\n    participant M as \"Miner\"\n    participant C as \"Container Runtime\"\n    \n    Note over V,M: Hardware Specification Query\n    V->>M: Specs(specs_input=\"\")\n    M->>M: Collect hardware info\n    M->>V: specs_output={\"CPU\": {...}, \"GPU\": {...}}\n    \n    Note over V,M: Resource Allocation Request\n    V->>M: Allocate(timeline=3600, device_requirement={...})\n    M->>C: Check resource availability\n    C->>M: Resource status\n    M->>V: output={\"status\": \"allocated\", \"ssh_details\": {...}}\n    \n    Note over V,M: Challenge Verification\n    V->>M: Challenge(challenge_hash=\"...\", challenge_difficulty=4)\n    M->>M: Execute hashcat proof-of-work\n    M->>V: output={\"password\": \"result\", \"error\": null}\n```\n\n**Sources:** [compute/protocol.py:23-135]()\n\n## Specs Protocol\n\nThe `Specs` protocol handles hardware specification queries between validators and miners. It allows validators to discover the computational capabilities of available miners.\n\n### Specs Protocol Structure\n\n| Attribute | Type | Description |\n|-----------|------|-------------|\n| `specs_input` | `str` | Input data sent to miner (typically empty) |\n| `specs_output` | `str` | Hardware specifications returned by miner |\n\n### Specs Message Format\n\nThe `specs_output` contains detailed hardware information in the following format:\n\n```json\n{\n  \"CPU\": {\n    \"count\": 4,\n    \"vendor_id_raw\": \"AuthenticAMD\",\n    \"brand_raw\": \"AMD Ryzen 7 3700X\",\n    \"hz_advertised_friendly\": \"3.6 GHz\"\n  },\n  \"GPU\": {\n    \"name\": \"NVIDIA GeForce RTX 3080\",\n    \"memory_total\": 10737418240,\n    \"compute_capability\": \"8.6\"\n  },\n  \"RAM\": {\n    \"total\": 34359738368,\n    \"available\": 28991029248\n  },\n  \"DISK\": {\n    \"total\": 1000204886016,\n    \"free\": 750153424896\n  }\n}\n```\n\n### Specs Protocol Implementation\n\n```mermaid\nflowchart TD\n    A[\"Validator calls dendrite.query()\"] --> B[\"Specs synapse created\"]\n    B --> C[\"specs_input = ''\"]\n    C --> D[\"Message sent to miner\"]\n    D --> E[\"Miner collects hardware info\"]\n    E --> F[\"specs_output populated\"]\n    F --> G[\"Response sent to validator\"]\n    G --> H[\"deserialize() returns specs_output\"]\n```\n\n**Sources:** [compute/protocol.py:23-57]()\n\n## Allocate Protocol\n\nThe `Allocate` protocol manages resource allocation requests and Docker container provisioning. It supports both allocation checking and actual resource reservation.\n\n### Allocate Protocol Attributes\n\n| Attribute | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `timeline` | `int` | `0` | Duration of allocation in seconds |\n| `device_requirement` | `dict` | `{}` | Required hardware specifications |\n| `checking` | `bool` | `True` | Flag for checking vs. actual allocation |\n| `public_key` | `str` | `\"\"` | RSA public key for encryption |\n| `output` | `dict` | `{}` | Miner response with allocation details |\n| `docker_requirement` | `dict` | See below | Docker container configuration |\n| `docker_change` | `bool` | `False` | Flag for Docker configuration changes |\n| `docker_action` | `dict` | See below | Docker action specifications |\n\n### Docker Configuration Structure\n\nThe `docker_requirement` dictionary contains container configuration:\n\n```python\n{\n    \"base_image\": \"ubuntu\",\n    \"ssh_key\": \"\",\n    \"ssh_port\": 4444,\n    \"volume_path\": \"/tmp\",\n    \"dockerfile\": \"\"\n}\n```\n\nThe `docker_action` dictionary specifies Docker operations:\n\n```python\n{\n    \"action\": \"\",\n    \"ssh_key\": \"\",\n    \"key_type\": \"\"\n}\n```\n\n### Allocate Protocol Workflow\n\n```mermaid\nflowchart TD\n    A[\"Allocation Request\"] --> B{\"checking == True?\"}\n    B -->|Yes| C[\"Check resource availability\"]\n    B -->|No| D[\"Perform actual allocation\"]\n    \n    C --> E[\"Return availability status\"]\n    D --> F[\"Create Docker container\"]\n    F --> G[\"Configure SSH access\"]\n    G --> H[\"Return allocation details\"]\n    \n    E --> I[\"output = {'available': bool}\"]\n    H --> J[\"output = {'status': 'allocated', 'ssh_details': {...}}\"]\n    \n    I --> K[\"deserialize() returns output\"]\n    J --> K\n```\n\n**Sources:** [compute/protocol.py:60-109]()\n\n## Challenge Protocol\n\nThe `Challenge` protocol implements proof-of-work verification using hashcat for GPU capability validation. It ensures miners have the computational resources they claim.\n\n### Challenge Protocol Attributes\n\n| Attribute | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `challenge_hash` | `str` | `\"\"` | Target hash for proof-of-work |\n| `challenge_salt` | `str` | `\"\"` | Salt value for hash computation |\n| `challenge_mode` | `str` | `\"\"` | Hashcat attack mode |\n| `challenge_chars` | `str` | `\"\"` | Character set for brute force |\n| `challenge_mask` | `str` | `\"\"` | Password mask pattern |\n| `challenge_difficulty` | `int` | `compute.pow_min_difficulty` | Minimum difficulty level |\n| `output` | `dict` | `{}` | Challenge response with results |\n\n### Challenge Response Format\n\nThe challenge response includes either a successful password result or error information:\n\n```json\n{\n  \"password\": \"found_password_or_null\",\n  \"error\": \"error_message_if_failed\"\n}\n```\n\n### Challenge Verification Process\n\n```mermaid\nflowchart TD\n    A[\"Validator generates challenge\"] --> B[\"Challenge parameters set\"]\n    B --> C[\"challenge_hash, challenge_salt, etc.\"]\n    C --> D[\"Synapse sent to miner\"]\n    D --> E[\"Miner executes hashcat\"]\n    E --> F{\"Hashcat successful?\"}\n    F -->|Yes| G[\"output = {'password': result, 'error': null}\"]\n    F -->|No| H[\"output = {'password': null, 'error': error_msg}\"]\n    G --> I[\"Response sent to validator\"]\n    H --> I\n    I --> J[\"deserialize() returns output\"]\n```\n\n**Sources:** [compute/protocol.py:112-135]()\n\n## Protocol Integration\n\nThese protocols integrate with the broader NI Compute Subnet architecture through the Bittensor network layer and are used by both validators and miners for different purposes:\n\n- **Validators** use these protocols to query miner capabilities, allocate resources, and verify computational claims\n- **Miners** implement protocol handlers to respond to specification queries, manage resource allocation, and execute proof-of-work challenges\n- **Resource Allocation API** leverages the `Allocate` protocol for external resource management requests\n\nThe protocols ensure secure, verifiable communication while maintaining compatibility with the Bittensor ecosystem.\n\n**Sources:** [compute/protocol.py:1-136]()",
    "resolved_links": [
      {
        "text": "compute/protocol.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py",
        "original_deepwiki_href": "compute/protocol.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "Custom Axon and Subtensor",
        "href": "/communication-protocols/custom-axon-and-subtensor#5.2",
        "original_deepwiki_href": "#5.2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "classDiagram\n    class bt_Synapse {\n        <<abstract>>\n        +deserialize()\n    }\n    \n    class Specs {\n        +specs_input: str\n        +specs_output: str\n        +deserialize() str\n    }\n    \n    class Allocate {\n        +timeline: int\n        +device_requirement: dict\n        +checking: bool\n        +public_key: str\n        +docker_requirement: dict\n        +docker_change: bool\n        +docker_action: dict\n        +output: dict\n        +deserialize() dict\n    }\n    \n    class Challenge {\n        +challenge_hash: str\n        +challenge_salt: str\n        +challenge_mode: str\n        +challenge_chars: str\n        +challenge_mask: str\n        +challenge_difficulty: int\n        +output: dict\n        +deserialize() dict\n    }\n    \n    bt_Synapse <|-- Specs\n    bt_Synapse <|-- Allocate\n    bt_Synapse <|-- Challenge",
      "sequenceDiagram\n    participant V as \"Validator\"\n    participant BT as \"Bittensor Network\"\n    participant M as \"Miner\"\n    participant C as \"Container Runtime\"\n    \n    Note over V,M: Hardware Specification Query\n    V->>M: Specs(specs_input=\"\")\n    M->>M: Collect hardware info\n    M->>V: specs_output={\"CPU\": {...}, \"GPU\": {...}}\n    \n    Note over V,M: Resource Allocation Request\n    V->>M: Allocate(timeline=3600, device_requirement={...})\n    M->>C: Check resource availability\n    C->>M: Resource status\n    M->>V: output={\"status\": \"allocated\", \"ssh_details\": {...}}\n    \n    Note over V,M: Challenge Verification\n    V->>M: Challenge(challenge_hash=\"...\", challenge_difficulty=4)\n    M->>M: Execute hashcat proof-of-work\n    M->>V: output={\"password\": \"result\", \"error\": null}",
      "flowchart TD\n    A[\"Validator calls dendrite.query()\"] --> B[\"Specs synapse created\"]\n    B --> C[\"specs_input = ''\"]\n    C --> D[\"Message sent to miner\"]\n    D --> E[\"Miner collects hardware info\"]\n    E --> F[\"specs_output populated\"]\n    F --> G[\"Response sent to validator\"]\n    G --> H[\"deserialize() returns specs_output\"]",
      "flowchart TD\n    A[\"Allocation Request\"] --> B{\"checking == True?\"}\n    B -->|Yes| C[\"Check resource availability\"]\n    B -->|No| D[\"Perform actual allocation\"]\n    \n    C --> E[\"Return availability status\"]\n    D --> F[\"Create Docker container\"]\n    F --> G[\"Configure SSH access\"]\n    G --> H[\"Return allocation details\"]\n    \n    E --> I[\"output = {'available': bool}\"]\n    H --> J[\"output = {'status': 'allocated', 'ssh_details': {...}}\"]\n    \n    I --> K[\"deserialize() returns output\"]\n    J --> K",
      "flowchart TD\n    A[\"Validator generates challenge\"] --> B[\"Challenge parameters set\"]\n    B --> C[\"challenge_hash, challenge_salt, etc.\"]\n    C --> D[\"Synapse sent to miner\"]\n    D --> E[\"Miner executes hashcat\"]\n    E --> F{\"Hashcat successful?\"}\n    F -->|Yes| G[\"output = {'password': result, 'error': null}\"]\n    F -->|No| H[\"output = {'password': null, 'error': error_msg}\"]\n    G --> I[\"Response sent to validator\"]\n    H --> I\n    I --> J[\"deserialize() returns output\"]"
    ],
    "potential_frontmatter": {
      "title": "Specs, Allocate, and Challenge Protocols"
    }
  },
  "/neuralinternet/ni-compute/5.2-custom-axon-and-subtensor": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/5.2-custom-axon-and-subtensor",
    "title": "Custom Axon and Subtensor",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/5.2-custom-axon-and-subtensor",
    "level": 1,
    "target_astro_path": "/communication-protocols/custom-axon-and-subtensor",
    "main_markdown_content": "# Custom Axon and Subtensor\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/axon.py](compute/axon.py)\n\n</details>\n\n\n\nThis document covers the compute subnet's custom extensions to Bittensor's core communication components: Axon and Subtensor. These extensions provide subnet-specific functionality including custom version handling, Prometheus metrics integration, and enhanced request preprocessing.\n\nFor information about the core communication protocols used by these components, see [Specs, Allocate, and Challenge Protocols](#5.1). For monitoring and metrics infrastructure, see [Monitoring and Metrics](#6).\n\n## Architecture Overview\n\nThe compute subnet extends Bittensor's base communication classes to add subnet-specific functionality. The main extensions include custom serve extrinsics, Prometheus metrics support, and enhanced middleware processing.\n\n```mermaid\ngraph TB\n    subgraph \"Bittensor Base Classes\"\n        BaseAxon[\"bittensor.core.axon.Axon\"]\n        BaseSubtensor[\"bittensor.core.subtensor.Subtensor\"]\n        BaseMiddleware[\"bittensor.core.axon.AxonMiddleware\"]\n    end\n    \n    subgraph \"Compute Subnet Extensions\"\n        ComputeAxon[\"ComputeSubnetAxon\"]\n        ComputeSubtensor[\"ComputeSubnetSubtensor\"]\n        ComputeMiddleware[\"ComputeSubnetAxonMiddleware\"]\n        CustomServe[\"custom_serve_extrinsic\"]\n    end\n    \n    subgraph \"Integration Components\"\n        VersionInt[\"__version_as_int__\"]\n        LocalVersion[\"get_local_version()\"]\n        PrometheusExtrinsic[\"prometheus_extrinsic\"]\n    end\n    \n    BaseAxon --> ComputeAxon\n    BaseSubtensor --> ComputeSubtensor\n    BaseMiddleware --> ComputeMiddleware\n    \n    ComputeAxon --> VersionInt\n    ComputeAxon --> LocalVersion\n    ComputeSubtensor --> PrometheusExtrinsic\n    ComputeMiddleware --> VersionInt\n    CustomServe --> VersionInt\n    \n    CustomServe -.->|\"Patches\"| BaseAxon\n```\n\n**Sources:** [compute/axon.py:1-488]()\n\n## Custom Serve Extrinsic\n\nThe `custom_serve_extrinsic` function replaces Bittensor's standard serve extrinsic functionality to incorporate compute subnet specific versioning.\n\n### Function Implementation\n\nThe custom serve extrinsic handles axon registration on the blockchain with subnet-specific parameters:\n\n```mermaid\ngraph TD\n    Start[\"custom_serve_extrinsic()\"]\n    Unlock[\"unlock_key(wallet)\"]\n    Params[\"Create AxonServeCallParams\"]\n    CheckNeuron[\"Check existing neuron state\"]\n    UpToDate{{\"Neuron up to date?\"}}\n    DoServe[\"do_serve_axon()\"]\n    Success{{\"Success?\"}}\n    Return[\"Return result\"]\n    \n    Start --> Unlock\n    Unlock --> Params\n    Params --> CheckNeuron\n    CheckNeuron --> UpToDate\n    UpToDate -->|\"Yes\"| Return\n    UpToDate -->|\"No\"| DoServe\n    DoServe --> Success\n    Success --> Return\n    \n    Params -.-> VersionInt[\"__version_as_int__\"]\n```\n\nKey characteristics:\n- Uses `__version_as_int__` for subnet version identification\n- Includes placeholder parameters for future extensibility\n- Patches the original Bittensor serve extrinsic at module level\n- Handles certificate-based TLS configuration\n\n**Sources:** [compute/axon.py:63-150]()\n\n## ComputeSubnetSubtensor\n\nThe `ComputeSubnetSubtensor` class extends the base Subtensor with Prometheus metrics functionality, allowing miners and validators to register metrics endpoints on the blockchain.\n\n### Prometheus Integration\n\n```mermaid\ngraph LR\n    Client[\"Wallet/Client\"]\n    SubtensorClass[\"ComputeSubnetSubtensor\"]\n    PrometheusMethod[\"serve_prometheus()\"]\n    ExtrinsicMethod[\"do_serve_prometheus()\"]\n    Blockchain[\"Bittensor Blockchain\"]\n    \n    Client --> SubtensorClass\n    SubtensorClass --> PrometheusMethod\n    PrometheusMethod --> ExtrinsicMethod\n    ExtrinsicMethod --> Blockchain\n    \n    PrometheusMethod -.-> PrometheusExtrinsicFunc[\"prometheus_extrinsic()\"]\n```\n\n### Key Methods\n\n| Method | Purpose | Parameters |\n|--------|---------|------------|\n| `serve_prometheus` | Public interface for Prometheus registration | `wallet`, `port`, `netuid`, wait flags |\n| `do_serve_prometheus` | Internal extrinsic submission handler | `wallet`, `call_params`, wait flags |\n\nThe implementation includes:\n- Retry logic with exponential backoff\n- Exception handling for substrate requests\n- Support for inclusion and finalization waiting\n- Integration with the compute subnet's Prometheus extrinsic function\n\n**Sources:** [compute/axon.py:152-283]()\n\n## ComputeSubnetAxon\n\nThe `ComputeSubnetAxon` class extends the base Axon with compute subnet specific configuration and information handling.\n\n### Architecture\n\n```mermaid\ngraph TB\n    subgraph \"ComputeSubnetAxon Components\"\n        Config[\"Configuration Management\"]\n        Wallet[\"Wallet Integration\"]\n        Network[\"Network Configuration\"]\n        Middleware[\"ComputeSubnetAxonMiddleware\"]\n        FastAPI[\"FastAPI Application\"]\n    end\n    \n    subgraph \"Custom Overrides\"\n        InfoMethod[\"info() method\"]\n        LocalVersionFunc[\"get_local_version()\"]\n        ProtocolValues[\"Protocol Values<br/>placeholder1=1<br/>placeholder2=2\"]\n    end\n    \n    Config --> Network\n    Wallet --> InfoMethod\n    InfoMethod --> LocalVersionFunc\n    InfoMethod --> ProtocolValues\n    FastAPI --> Middleware\n```\n\n### Info Method Override\n\nThe `info()` method returns subnet-specific axon information:\n\n- Uses `get_local_version()` instead of standard versioning\n- Sets protocol version to 4\n- Includes custom placeholder values for future extensibility\n- Returns properly formatted `AxonInfo` object\n\n### Configuration Parameters\n\n| Parameter | Purpose | Default Handling |\n|-----------|---------|------------------|\n| `external_ip` | External IP for network communication | Auto-detected if not provided |\n| `external_port` | External port for network communication | Uses internal port if not provided |\n| `max_workers` | Thread pool size | From configuration |\n\n**Sources:** [compute/axon.py:285-388]()\n\n## ComputeSubnetAxonMiddleware\n\nThe `ComputeSubnetAxonMiddleware` extends the base middleware with compute subnet specific request preprocessing.\n\n### Request Processing Flow\n\n```mermaid\nsequenceDiagram\n    participant Request as \"Incoming Request\"\n    participant Middleware as \"ComputeSubnetAxonMiddleware\"\n    participant Synapse as \"Synapse Object\"\n    participant Wallet as \"Wallet (Signing)\"\n    \n    Request ->> Middleware: HTTP Request\n    Middleware ->> Middleware: Extract request_name from URL\n    Middleware ->> Middleware: Get synapse class type\n    Middleware ->> Synapse: Create from headers\n    Middleware ->> Synapse: Fill axon info (__version_as_int__)\n    Middleware ->> Synapse: Fill dendrite info\n    Middleware ->> Wallet: Sign message\n    Wallet -->> Middleware: Signature\n    Middleware ->> Synapse: Set signature\n    Middleware -->> Request: Return processed synapse\n```\n\n### Custom Preprocessing\n\nThe `preprocess` method implements compute subnet specific logic:\n\n1. **Request Name Extraction**: Parses request name from URL path\n2. **Synapse Creation**: Instantiates appropriate synapse type from headers\n3. **Version Handling**: Sets axon version to `__version_as_int__`\n4. **Signature Generation**: Signs with wallet hotkey using custom message format\n\n### Error Handling\n\nThe middleware handles three main error types:\n- `InvalidRequestNameError`: Malformed URL paths\n- `UnknownSynapseError`: Unknown synapse types\n- `SynapseParsingError`: Header parsing failures\n\n**Sources:** [compute/axon.py:391-487]()\n\n## Integration Workflow\n\nThe custom Axon and Subtensor components integrate with the broader compute subnet architecture through specific workflows:\n\n```mermaid\ngraph TD\n    subgraph \"Miner/Validator Startup\"\n        Start[\"Process Start\"]\n        CreateAxon[\"Create ComputeSubnetAxon\"]\n        CreateSubtensor[\"Create ComputeSubnetSubtensor\"]\n        ServeAxon[\"Serve Axon (custom_serve_extrinsic)\"]\n        ServePrometheus[\"Serve Prometheus metrics\"]\n    end\n    \n    subgraph \"Runtime Operations\"\n        RequestProcessing[\"Process Incoming Requests\"]\n        MiddlewareHandling[\"ComputeSubnetAxonMiddleware\"]\n        SynapseProcessing[\"Synapse Processing\"]\n        ResponseGeneration[\"Response Generation\"]\n    end\n    \n    subgraph \"Blockchain Integration\"\n        BlockchainReg[\"Blockchain Registration\"]\n        NetworkState[\"Network State Updates\"]\n        MetricsReporting[\"Metrics Reporting\"]\n    end\n    \n    Start --> CreateAxon\n    Start --> CreateSubtensor\n    CreateAxon --> ServeAxon\n    CreateSubtensor --> ServePrometheus\n    \n    ServeAxon --> BlockchainReg\n    ServePrometheus --> MetricsReporting\n    \n    RequestProcessing --> MiddlewareHandling\n    MiddlewareHandling --> SynapseProcessing\n    SynapseProcessing --> ResponseGeneration\n    \n    BlockchainReg --> NetworkState\n```\n\nThis integration ensures that:\n- All network communication uses compute subnet versioning\n- Prometheus metrics are properly registered and accessible\n- Request processing includes subnet-specific preprocessing\n- Blockchain registration includes all necessary subnet parameters\n\n**Sources:** [compute/axon.py:1-488]()",
    "resolved_links": [
      {
        "text": "compute/axon.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py",
        "original_deepwiki_href": "compute/axon.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "Specs, Allocate, and Challenge Protocols",
        "href": "/communication-protocols/specs-allocate-and-challenge-protocols#5.1",
        "original_deepwiki_href": "#5.1",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Monitoring and Metrics",
        "href": "/monitoring-and-metrics#6",
        "original_deepwiki_href": "#6",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"Bittensor Base Classes\"\n        BaseAxon[\"bittensor.core.axon.Axon\"]\n        BaseSubtensor[\"bittensor.core.subtensor.Subtensor\"]\n        BaseMiddleware[\"bittensor.core.axon.AxonMiddleware\"]\n    end\n    \n    subgraph \"Compute Subnet Extensions\"\n        ComputeAxon[\"ComputeSubnetAxon\"]\n        ComputeSubtensor[\"ComputeSubnetSubtensor\"]\n        ComputeMiddleware[\"ComputeSubnetAxonMiddleware\"]\n        CustomServe[\"custom_serve_extrinsic\"]\n    end\n    \n    subgraph \"Integration Components\"\n        VersionInt[\"__version_as_int__\"]\n        LocalVersion[\"get_local_version()\"]\n        PrometheusExtrinsic[\"prometheus_extrinsic\"]\n    end\n    \n    BaseAxon --> ComputeAxon\n    BaseSubtensor --> ComputeSubtensor\n    BaseMiddleware --> ComputeMiddleware\n    \n    ComputeAxon --> VersionInt\n    ComputeAxon --> LocalVersion\n    ComputeSubtensor --> PrometheusExtrinsic\n    ComputeMiddleware --> VersionInt\n    CustomServe --> VersionInt\n    \n    CustomServe -.->|\"Patches\"| BaseAxon",
      "graph TD\n    Start[\"custom_serve_extrinsic()\"]\n    Unlock[\"unlock_key(wallet)\"]\n    Params[\"Create AxonServeCallParams\"]\n    CheckNeuron[\"Check existing neuron state\"]\n    UpToDate{{\"Neuron up to date?\"}}\n    DoServe[\"do_serve_axon()\"]\n    Success{{\"Success?\"}}\n    Return[\"Return result\"]\n    \n    Start --> Unlock\n    Unlock --> Params\n    Params --> CheckNeuron\n    CheckNeuron --> UpToDate\n    UpToDate -->|\"Yes\"| Return\n    UpToDate -->|\"No\"| DoServe\n    DoServe --> Success\n    Success --> Return\n    \n    Params -.-> VersionInt[\"__version_as_int__\"]",
      "graph LR\n    Client[\"Wallet/Client\"]\n    SubtensorClass[\"ComputeSubnetSubtensor\"]\n    PrometheusMethod[\"serve_prometheus()\"]\n    ExtrinsicMethod[\"do_serve_prometheus()\"]\n    Blockchain[\"Bittensor Blockchain\"]\n    \n    Client --> SubtensorClass\n    SubtensorClass --> PrometheusMethod\n    PrometheusMethod --> ExtrinsicMethod\n    ExtrinsicMethod --> Blockchain\n    \n    PrometheusMethod -.-> PrometheusExtrinsicFunc[\"prometheus_extrinsic()\"]",
      "graph TB\n    subgraph \"ComputeSubnetAxon Components\"\n        Config[\"Configuration Management\"]\n        Wallet[\"Wallet Integration\"]\n        Network[\"Network Configuration\"]\n        Middleware[\"ComputeSubnetAxonMiddleware\"]\n        FastAPI[\"FastAPI Application\"]\n    end\n    \n    subgraph \"Custom Overrides\"\n        InfoMethod[\"info() method\"]\n        LocalVersionFunc[\"get_local_version()\"]\n        ProtocolValues[\"Protocol Values<br/>placeholder1=1<br/>placeholder2=2\"]\n    end\n    \n    Config --> Network\n    Wallet --> InfoMethod\n    InfoMethod --> LocalVersionFunc\n    InfoMethod --> ProtocolValues\n    FastAPI --> Middleware",
      "sequenceDiagram\n    participant Request as \"Incoming Request\"\n    participant Middleware as \"ComputeSubnetAxonMiddleware\"\n    participant Synapse as \"Synapse Object\"\n    participant Wallet as \"Wallet (Signing)\"\n    \n    Request ->> Middleware: HTTP Request\n    Middleware ->> Middleware: Extract request_name from URL\n    Middleware ->> Middleware: Get synapse class type\n    Middleware ->> Synapse: Create from headers\n    Middleware ->> Synapse: Fill axon info (__version_as_int__)\n    Middleware ->> Synapse: Fill dendrite info\n    Middleware ->> Wallet: Sign message\n    Wallet -->> Middleware: Signature\n    Middleware ->> Synapse: Set signature\n    Middleware -->> Request: Return processed synapse",
      "graph TD\n    subgraph \"Miner/Validator Startup\"\n        Start[\"Process Start\"]\n        CreateAxon[\"Create ComputeSubnetAxon\"]\n        CreateSubtensor[\"Create ComputeSubnetSubtensor\"]\n        ServeAxon[\"Serve Axon (custom_serve_extrinsic)\"]\n        ServePrometheus[\"Serve Prometheus metrics\"]\n    end\n    \n    subgraph \"Runtime Operations\"\n        RequestProcessing[\"Process Incoming Requests\"]\n        MiddlewareHandling[\"ComputeSubnetAxonMiddleware\"]\n        SynapseProcessing[\"Synapse Processing\"]\n        ResponseGeneration[\"Response Generation\"]\n    end\n    \n    subgraph \"Blockchain Integration\"\n        BlockchainReg[\"Blockchain Registration\"]\n        NetworkState[\"Network State Updates\"]\n        MetricsReporting[\"Metrics Reporting\"]\n    end\n    \n    Start --> CreateAxon\n    Start --> CreateSubtensor\n    CreateAxon --> ServeAxon\n    CreateSubtensor --> ServePrometheus\n    \n    ServeAxon --> BlockchainReg\n    ServePrometheus --> MetricsReporting\n    \n    RequestProcessing --> MiddlewareHandling\n    MiddlewareHandling --> SynapseProcessing\n    SynapseProcessing --> ResponseGeneration\n    \n    BlockchainReg --> NetworkState"
    ],
    "potential_frontmatter": {
      "title": "Custom Axon and Subtensor"
    }
  },
  "/neuralinternet/ni-compute/6-monitoring-and-metrics": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/6-monitoring-and-metrics",
    "title": "Monitoring and Metrics",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/6-monitoring-and-metrics",
    "level": 0,
    "target_astro_path": "/monitoring-and-metrics",
    "main_markdown_content": "# Monitoring and Metrics\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/axon.py](compute/axon.py)\n- [compute/wandb/wandb.py](compute/wandb/wandb.py)\n\n</details>\n\n\n\nThis document covers the monitoring and observability infrastructure used in the NI Compute Subnet. The system employs two primary monitoring solutions: Weights & Biases (WandB) for distributed state management and experiment tracking, and Prometheus for network observability metrics.\n\nFor information about the database operations used for local state persistence, see [Database Operations](#2.3). For details about the communication protocols that generate monitored events, see [Communication Protocols](#5).\n\n## Architecture Overview\n\nThe monitoring system operates across three main components: validators, miners, and the resource allocation API. WandB serves as the primary distributed state store, while Prometheus provides real-time metrics collection.\n\n### Monitoring Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Validator Instances\"\n        VAL1[\"Validator Process<br/>neurons/validator.py\"]\n        VAL2[\"Validator Process<br/>neurons/validator.py\"]\n        VAL3[\"Validator Process<br/>neurons/validator.py\"]\n    end\n    \n    subgraph \"Miner Instances\"\n        MIN1[\"Miner Process<br/>neurons/miner.py\"]\n        MIN2[\"Miner Process<br/>neurons/miner.py\"]\n        MIN3[\"Miner Process<br/>neurons/miner.py\"]\n    end\n    \n    subgraph \"WandB Cloud Service\"\n        WANDB_PROJECT[\"opencompute Project<br/>neuralinternet/opencompute\"]\n        WANDB_RUNS[\"Individual Runs<br/>validator-{hotkey}<br/>miner-{hotkey}\"]\n        WANDB_CONFIG[\"Run Configurations<br/>Hardware Specs<br/>Allocation State<br/>Performance Stats\"]\n    end\n    \n    subgraph \"Prometheus Infrastructure\"\n        PROM_AXON[\"ComputeSubnetAxon<br/>Prometheus Endpoints\"]\n        PROM_METRICS[\"Network Metrics<br/>Performance Data\"]\n        PROM_EXTRINSIC[\"prometheus_extrinsic<br/>Blockchain Integration\"]\n    end\n    \n    subgraph \"Local State\"\n        COMPUTE_DB[\"ComputeDb<br/>SQLite Database\"]\n        WANDB_RUNS_TABLE[\"wandb_runs table<br/>hotkey -> run_id mapping\"]\n    end\n    \n    VAL1 --> WANDB_PROJECT\n    VAL2 --> WANDB_PROJECT\n    VAL3 --> WANDB_PROJECT\n    MIN1 --> WANDB_PROJECT\n    MIN2 --> WANDB_PROJECT\n    MIN3 --> WANDB_PROJECT\n    \n    WANDB_PROJECT --> WANDB_RUNS\n    WANDB_RUNS --> WANDB_CONFIG\n    \n    VAL1 --> PROM_AXON\n    MIN1 --> PROM_AXON\n    PROM_AXON --> PROM_METRICS\n    PROM_AXON --> PROM_EXTRINSIC\n    \n    VAL1 --> COMPUTE_DB\n    MIN1 --> COMPUTE_DB\n    COMPUTE_DB --> WANDB_RUNS_TABLE\n```\n\n**Sources:** [compute/wandb/wandb.py:1-648](), [compute/axon.py:152-284]()\n\n## WandB Integration\n\nThe `ComputeWandb` class provides the core monitoring infrastructure, managing distributed state across the network through individual WandB runs for each validator and miner.\n\n### WandB System Components\n\n```mermaid\ngraph TD\n    subgraph \"ComputeWandb Class\"\n        INIT[\"__init__<br/>Run Initialization\"]\n        CONFIG[\"update_config<br/>Configuration Management\"]\n        SIGN[\"sign_run<br/>Cryptographic Signing\"]\n        VERIFY[\"verify_run<br/>Signature Verification\"]\n    end\n    \n    subgraph \"Validator Operations\"\n        UPDATE_STATS[\"update_stats<br/>Challenge Results\"]\n        UPDATE_ALLOC[\"update_allocated_hotkeys<br/>Resource Allocation State\"]\n        UPDATE_PEN[\"update_penalized_hotkeys<br/>Blacklist Management\"]\n        GET_STATS[\"get_stats_allocated<br/>Cross-Validator Aggregation\"]\n    end\n    \n    subgraph \"Miner Operations\"\n        UPDATE_SPECS[\"update_specs<br/>Hardware Specifications\"]\n        UPDATE_MINER_ALLOC[\"update_allocated<br/>Allocation Status\"]\n        UPDATE_PORT[\"update_miner_port_open<br/>Network Accessibility\"]\n        SYNC_ALLOC[\"sync_allocated<br/>State Synchronization\"]\n    end\n    \n    subgraph \"Data Retrieval\"\n        GET_ALLOC[\"get_allocated_hotkeys<br/>Active Allocations\"]\n        GET_PEN[\"get_penalized_hotkeys<br/>Blacklisted Keys\"]\n        GET_SPECS[\"get_miner_specs<br/>Hardware Information\"]\n    end\n    \n    subgraph \"WandB API Layer\"\n        API_INSTANCE[\"wandb.Api<br/>API Client\"]\n        PROJECT_REF[\"api.project<br/>opencompute\"]\n        RUNS_QUERY[\"api.runs<br/>Query Interface\"]\n    end\n    \n    INIT --> CONFIG\n    CONFIG --> SIGN\n    UPDATE_STATS --> SIGN\n    UPDATE_ALLOC --> SIGN\n    UPDATE_PEN --> SIGN\n    UPDATE_SPECS --> SIGN\n    UPDATE_MINER_ALLOC --> SIGN\n    UPDATE_PORT --> SIGN\n    \n    GET_STATS --> VERIFY\n    GET_ALLOC --> VERIFY\n    GET_PEN --> VERIFY\n    GET_SPECS --> VERIFY\n    \n    API_INSTANCE --> PROJECT_REF\n    PROJECT_REF --> RUNS_QUERY\n    RUNS_QUERY --> GET_STATS\n    RUNS_QUERY --> GET_ALLOC\n    RUNS_QUERY --> GET_PEN\n    RUNS_QUERY --> GET_SPECS\n```\n\n**Sources:** [compute/wandb/wandb.py:19-648]()\n\n### Run Management and Initialization\n\nEach network participant creates a WandB run with a standardized naming convention: `{role}-{hotkey}`. The system handles run persistence through local database storage and automatic recovery.\n\n| Component | Description | Key Methods |\n|-----------|-------------|-------------|\n| Run Initialization | Creates or resumes WandB runs | `__init__`, `save_run_id`, `get_run_id` |\n| Configuration Management | Updates run configuration with network state | `update_config` |\n| State Persistence | Stores run IDs in local SQLite database | Database operations in `wandb_runs` table |\n\n**Sources:** [compute/wandb/wandb.py:52-88](), [compute/wandb/wandb.py:109-138]()\n\n### Hardware Specifications Tracking\n\nMiners upload hardware specifications to enable validators to make informed allocation decisions. The `update_specs` method integrates with the performance measurement system to provide encrypted hardware details.\n\n```mermaid\ngraph LR\n    subgraph \"Hardware Detection\"\n        PERF_INFO[\"get_perf_info<br/>neurons/Validator/script.py\"]\n        GPU_SPECS[\"GPU Specifications<br/>Name, Count, Memory\"]\n        CPU_SPECS[\"CPU Specifications<br/>Cores, Architecture\"]\n    end\n    \n    subgraph \"WandB Upload\"\n        UPDATE_SPECS[\"update_specs<br/>compute/wandb/wandb.py\"]\n        RUN_CONFIG[\"run.config<br/>specs field\"]\n        SIGNATURE[\"sign_run<br/>Cryptographic Verification\"]\n    end\n    \n    subgraph \"Validator Consumption\"\n        GET_MINER_SPECS[\"get_miner_specs<br/>Query Interface\"]\n        QUERYABLE_UIDS[\"queryable_uids<br/>Network Participants\"]\n        ALLOCATION_LOGIC[\"Resource Allocation<br/>Decision Making\"]\n    end\n    \n    PERF_INFO --> GPU_SPECS\n    PERF_INFO --> CPU_SPECS\n    GPU_SPECS --> UPDATE_SPECS\n    CPU_SPECS --> UPDATE_SPECS\n    UPDATE_SPECS --> RUN_CONFIG\n    RUN_CONFIG --> SIGNATURE\n    \n    GET_MINER_SPECS --> QUERYABLE_UIDS\n    QUERYABLE_UIDS --> ALLOCATION_LOGIC\n```\n\n**Sources:** [compute/wandb/wandb.py:140-159](), [compute/wandb/wandb.py:540-574]()\n\n## State Management and Synchronization\n\nThe WandB system maintains several critical state categories across the network, with built-in aggregation and conflict resolution mechanisms.\n\n### Allocation State Management\n\n```mermaid\ngraph TB\n    subgraph \"Validator State Updates\"\n        ALLOC_UPDATE[\"update_allocated_hotkeys<br/>List of Allocated Keys\"]\n        STATS_UPDATE[\"update_stats<br/>Performance Statistics\"]\n        PEN_UPDATE[\"update_penalized_hotkeys<br/>Blacklist Management\"]\n    end\n    \n    subgraph \"Cross-Validator Aggregation\"\n        GET_ALLOC_HOTKEYS[\"get_allocated_hotkeys<br/>Aggregate Across Validators\"]\n        GET_STATS_ALLOC[\"get_stats_allocated<br/>Statistics Aggregation\"]\n        CONFLICT_RESOLUTION[\"pick_dominant_dict<br/>Score-Based Resolution\"]\n    end\n    \n    subgraph \"Database Synchronization\"\n        RETRIEVE_STATS[\"retrieve_stats<br/>neurons/Validator/database/pog.py\"]\n        WRITE_STATS[\"write_stats<br/>neurons/Validator/database/pog.py\"]\n        COMPUTE_DB[\"ComputeDb<br/>Local State Storage\"]\n    end\n    \n    subgraph \"Verification Layer\"\n        VERIFY_RUN[\"verify_run<br/>Signature Verification\"]\n        VALID_VALIDATORS[\"valid_validator_hotkeys<br/>Authorized Validators\"]\n        SIGNATURE_CHECK[\"Cryptographic Validation\"]\n    end\n    \n    ALLOC_UPDATE --> GET_ALLOC_HOTKEYS\n    STATS_UPDATE --> GET_STATS_ALLOC\n    PEN_UPDATE --> GET_ALLOC_HOTKEYS\n    \n    GET_STATS_ALLOC --> CONFLICT_RESOLUTION\n    CONFLICT_RESOLUTION --> RETRIEVE_STATS\n    RETRIEVE_STATS --> WRITE_STATS\n    WRITE_STATS --> COMPUTE_DB\n    \n    GET_ALLOC_HOTKEYS --> VERIFY_RUN\n    GET_STATS_ALLOC --> VERIFY_RUN\n    VERIFY_RUN --> VALID_VALIDATORS\n    VALID_VALIDATORS --> SIGNATURE_CHECK\n```\n\n**Sources:** [compute/wandb/wandb.py:198-250](), [compute/wandb/wandb.py:291-333](), [compute/wandb/wandb.py:334-450]()\n\n### Statistics Aggregation Algorithm\n\nThe `get_stats_allocated` method implements a sophisticated aggregation algorithm that resolves conflicts between multiple validator reports for the same miner UID.\n\n| Step | Process | Implementation |\n|------|---------|----------------|\n| 1. Collection | Query all validator runs with stats | WandB API filters |\n| 2. Verification | Validate cryptographic signatures | `verify_run` method |\n| 3. Filtering | Select entries with `own_score=True` and `allocated=True` | Boolean filtering |\n| 4. Aggregation | Group by UID, collect multiple reports | Dictionary aggregation |\n| 5. Resolution | Use `pick_dominant_dict` for conflict resolution | Counter-based selection |\n| 6. Scoring | Prefer highest score in case of ties | Score comparison |\n\n**Sources:** [compute/wandb/wandb.py:334-450](), [compute/wandb/wandb.py:391-426]()\n\n## Security and Verification\n\nThe monitoring system implements cryptographic verification to ensure data integrity and prevent tampering.\n\n### Signature Verification Process\n\n```mermaid\ngraph TD\n    subgraph \"Signing Process\"\n        RUN_ID[\"run.id<br/>WandB Run Identifier\"]\n        DATA_HASH[\"SHA-256 Hash<br/>Computed from run_id\"]\n        WALLET_SIGN[\"wallet.hotkey.sign<br/>Cryptographic Signature\"]\n        CONFIG_UPDATE[\"run.config.update<br/>Store Signature\"]\n    end\n    \n    subgraph \"Verification Process\"\n        EXTRACT_SIG[\"Extract Signature<br/>From run.config\"]\n        RECREATE_HASH[\"Recreate Hash<br/>From run_id\"]\n        KEYPAIR_VERIFY[\"bt.Keypair.verify<br/>Signature Validation\"]\n        RESULT[\"Verification Result<br/>Boolean\"]\n    end\n    \n    subgraph \"Security Enforcement\"\n        VALID_HOTKEYS[\"valid_validator_hotkeys<br/>Authorized Keys\"]\n        ACCESS_CONTROL[\"Data Access Control<br/>Based on Verification\"]\n        FLAG_PARAM[\"flag Parameter<br/>Enforcement Toggle\"]\n    end\n    \n    RUN_ID --> DATA_HASH\n    DATA_HASH --> WALLET_SIGN\n    WALLET_SIGN --> CONFIG_UPDATE\n    \n    EXTRACT_SIG --> RECREATE_HASH\n    RECREATE_HASH --> KEYPAIR_VERIFY\n    KEYPAIR_VERIFY --> RESULT\n    \n    RESULT --> ACCESS_CONTROL\n    VALID_HOTKEYS --> ACCESS_CONTROL\n    FLAG_PARAM --> ACCESS_CONTROL\n```\n\n**Sources:** [compute/wandb/wandb.py:576-591](), [compute/wandb/wandb.py:592-616]()\n\n## Prometheus Integration\n\nThe Prometheus integration provides real-time metrics collection through custom Axon and Subtensor implementations.\n\n### Prometheus Architecture\n\n```mermaid\ngraph LR\n    subgraph \"Custom Implementations\"\n        COMPUTE_SUBTENSOR[\"ComputeSubnetSubtensor<br/>compute/axon.py\"]\n        SERVE_PROMETHEUS[\"serve_prometheus<br/>Method\"]\n        DO_SERVE[\"do_serve_prometheus<br/>Extrinsic Handler\"]\n    end\n    \n    subgraph \"Blockchain Integration\"\n        PROMETHEUS_EXTRINSIC[\"prometheus_extrinsic<br/>compute/prometheus.py\"]\n        SUBSTRATE_CALL[\"substrate.compose_call<br/>SubtensorModule\"]\n        SIGNED_EXTRINSIC[\"create_signed_extrinsic<br/>Blockchain Transaction\"]\n    end\n    \n    subgraph \"Network Deployment\"\n        AXON_SERVE[\"Custom Axon<br/>ComputeSubnetAxon\"]\n        METRICS_ENDPOINT[\"Prometheus Endpoint<br/>Port Configuration\"]\n        NETWORK_REGISTRATION[\"Blockchain Registration<br/>Network Visibility\"]\n    end\n    \n    COMPUTE_SUBTENSOR --> SERVE_PROMETHEUS\n    SERVE_PROMETHEUS --> DO_SERVE\n    DO_SERVE --> PROMETHEUS_EXTRINSIC\n    \n    PROMETHEUS_EXTRINSIC --> SUBSTRATE_CALL\n    SUBSTRATE_CALL --> SIGNED_EXTRINSIC\n    \n    AXON_SERVE --> METRICS_ENDPOINT\n    METRICS_ENDPOINT --> NETWORK_REGISTRATION\n```\n\n**Sources:** [compute/axon.py:166-201](), [compute/axon.py:203-283]()\n\n### Prometheus Extrinsic Submission\n\nThe `serve_prometheus` method submits blockchain extrinsics to register Prometheus endpoints with the network, enabling distributed metrics collection.\n\n| Component | Function | Parameters |\n|-----------|----------|------------|\n| `serve_prometheus` | Main entry point | `wallet`, `port`, `netuid`, `wait_for_inclusion`, `wait_for_finalization` |\n| `do_serve_prometheus` | Extrinsic handler | `call_params`, retry logic with exponential backoff |\n| `prometheus_extrinsic` | Blockchain integration | Custom prometheus integration (imported) |\n\n**Sources:** [compute/axon.py:166-201](), [compute/axon.py:203-283]()\n\n## Data Flow Integration\n\nThe monitoring system integrates with the core subnet operations through several key data flows.\n\n### Validator Monitoring Flow\n\n```mermaid\ngraph TD\n    subgraph \"Validation Process\"\n        POG_VALIDATION[\"Proof-of-GPU<br/>Validation\"]\n        CHALLENGE_RESULTS[\"Challenge Results<br/>Success/Failure\"]\n        SCORING[\"Score Calculation<br/>Performance Metrics\"]\n    end\n    \n    subgraph \"Local Storage\"\n        COMPUTE_DB_WRITE[\"ComputeDb Write<br/>Local Statistics\"]\n        POG_STATS[\"pog_stats table<br/>GPU Performance\"]\n        STATS_TABLE[\"stats table<br/>Miner Scores\"]\n    end\n    \n    subgraph \"WandB Logging\"\n        UPDATE_STATS_CALL[\"update_stats<br/>Aggregate Results\"]\n        WANDB_LOG[\"wandb.log<br/>Time Series Data\"]\n        CONFIG_UPDATE_STATS[\"run.config.update<br/>Latest State\"]\n    end\n    \n    subgraph \"Cross-Network Sync\"\n        ALLOCATED_UPDATE[\"update_allocated_hotkeys<br/>Resource State\"]\n        PENALIZED_UPDATE[\"update_penalized_hotkeys<br/>Blacklist State\"]\n        SIGNATURE_APPLY[\"sign_run<br/>Cryptographic Proof\"]\n    end\n    \n    POG_VALIDATION --> CHALLENGE_RESULTS\n    CHALLENGE_RESULTS --> SCORING\n    SCORING --> COMPUTE_DB_WRITE\n    \n    COMPUTE_DB_WRITE --> POG_STATS\n    COMPUTE_DB_WRITE --> STATS_TABLE\n    \n    SCORING --> UPDATE_STATS_CALL\n    UPDATE_STATS_CALL --> WANDB_LOG\n    WANDB_LOG --> CONFIG_UPDATE_STATS\n    \n    CONFIG_UPDATE_STATS --> ALLOCATED_UPDATE\n    ALLOCATED_UPDATE --> PENALIZED_UPDATE\n    PENALIZED_UPDATE --> SIGNATURE_APPLY\n```\n\n**Sources:** [compute/wandb/wandb.py:186-196](), [compute/wandb/wandb.py:198-230](), [compute/wandb/wandb.py:232-249]()\n\n## Configuration and Environment\n\nThe monitoring system requires specific configuration for proper operation, including API credentials and network parameters.\n\n### Environment Requirements\n\n| Requirement | Configuration | Source |\n|-------------|---------------|---------|\n| WandB API Key | `WANDB_API_KEY` environment variable or `.netrc` file | Environment setup |\n| Project Configuration | `PUBLIC_WANDB_NAME = \"opencompute\"` | Hard-coded constant |\n| Entity Configuration | `PUBLIC_WANDB_ENTITY = \"neuralinternet\"` | Hard-coded constant |\n| Database Connection | `ComputeDb()` instance | Local SQLite database |\n\n**Sources:** [compute/wandb/wandb.py:15-16](), [compute/wandb/wandb.py:38-45]()",
    "resolved_links": [
      {
        "text": "compute/axon.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py",
        "original_deepwiki_href": "compute/axon.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/wandb/wandb.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py",
        "original_deepwiki_href": "compute/wandb/wandb.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "Database Operations",
        "href": "/validator-system/database-operations#2.3",
        "original_deepwiki_href": "#2.3",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Communication Protocols",
        "href": "/communication-protocols#5",
        "original_deepwiki_href": "#5",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"Validator Instances\"\n        VAL1[\"Validator Process<br/>neurons/validator.py\"]\n        VAL2[\"Validator Process<br/>neurons/validator.py\"]\n        VAL3[\"Validator Process<br/>neurons/validator.py\"]\n    end\n    \n    subgraph \"Miner Instances\"\n        MIN1[\"Miner Process<br/>neurons/miner.py\"]\n        MIN2[\"Miner Process<br/>neurons/miner.py\"]\n        MIN3[\"Miner Process<br/>neurons/miner.py\"]\n    end\n    \n    subgraph \"WandB Cloud Service\"\n        WANDB_PROJECT[\"opencompute Project<br/>neuralinternet/opencompute\"]\n        WANDB_RUNS[\"Individual Runs<br/>validator-{hotkey}<br/>miner-{hotkey}\"]\n        WANDB_CONFIG[\"Run Configurations<br/>Hardware Specs<br/>Allocation State<br/>Performance Stats\"]\n    end\n    \n    subgraph \"Prometheus Infrastructure\"\n        PROM_AXON[\"ComputeSubnetAxon<br/>Prometheus Endpoints\"]\n        PROM_METRICS[\"Network Metrics<br/>Performance Data\"]\n        PROM_EXTRINSIC[\"prometheus_extrinsic<br/>Blockchain Integration\"]\n    end\n    \n    subgraph \"Local State\"\n        COMPUTE_DB[\"ComputeDb<br/>SQLite Database\"]\n        WANDB_RUNS_TABLE[\"wandb_runs table<br/>hotkey -> run_id mapping\"]\n    end\n    \n    VAL1 --> WANDB_PROJECT\n    VAL2 --> WANDB_PROJECT\n    VAL3 --> WANDB_PROJECT\n    MIN1 --> WANDB_PROJECT\n    MIN2 --> WANDB_PROJECT\n    MIN3 --> WANDB_PROJECT\n    \n    WANDB_PROJECT --> WANDB_RUNS\n    WANDB_RUNS --> WANDB_CONFIG\n    \n    VAL1 --> PROM_AXON\n    MIN1 --> PROM_AXON\n    PROM_AXON --> PROM_METRICS\n    PROM_AXON --> PROM_EXTRINSIC\n    \n    VAL1 --> COMPUTE_DB\n    MIN1 --> COMPUTE_DB\n    COMPUTE_DB --> WANDB_RUNS_TABLE",
      "graph TD\n    subgraph \"ComputeWandb Class\"\n        INIT[\"__init__<br/>Run Initialization\"]\n        CONFIG[\"update_config<br/>Configuration Management\"]\n        SIGN[\"sign_run<br/>Cryptographic Signing\"]\n        VERIFY[\"verify_run<br/>Signature Verification\"]\n    end\n    \n    subgraph \"Validator Operations\"\n        UPDATE_STATS[\"update_stats<br/>Challenge Results\"]\n        UPDATE_ALLOC[\"update_allocated_hotkeys<br/>Resource Allocation State\"]\n        UPDATE_PEN[\"update_penalized_hotkeys<br/>Blacklist Management\"]\n        GET_STATS[\"get_stats_allocated<br/>Cross-Validator Aggregation\"]\n    end\n    \n    subgraph \"Miner Operations\"\n        UPDATE_SPECS[\"update_specs<br/>Hardware Specifications\"]\n        UPDATE_MINER_ALLOC[\"update_allocated<br/>Allocation Status\"]\n        UPDATE_PORT[\"update_miner_port_open<br/>Network Accessibility\"]\n        SYNC_ALLOC[\"sync_allocated<br/>State Synchronization\"]\n    end\n    \n    subgraph \"Data Retrieval\"\n        GET_ALLOC[\"get_allocated_hotkeys<br/>Active Allocations\"]\n        GET_PEN[\"get_penalized_hotkeys<br/>Blacklisted Keys\"]\n        GET_SPECS[\"get_miner_specs<br/>Hardware Information\"]\n    end\n    \n    subgraph \"WandB API Layer\"\n        API_INSTANCE[\"wandb.Api<br/>API Client\"]\n        PROJECT_REF[\"api.project<br/>opencompute\"]\n        RUNS_QUERY[\"api.runs<br/>Query Interface\"]\n    end\n    \n    INIT --> CONFIG\n    CONFIG --> SIGN\n    UPDATE_STATS --> SIGN\n    UPDATE_ALLOC --> SIGN\n    UPDATE_PEN --> SIGN\n    UPDATE_SPECS --> SIGN\n    UPDATE_MINER_ALLOC --> SIGN\n    UPDATE_PORT --> SIGN\n    \n    GET_STATS --> VERIFY\n    GET_ALLOC --> VERIFY\n    GET_PEN --> VERIFY\n    GET_SPECS --> VERIFY\n    \n    API_INSTANCE --> PROJECT_REF\n    PROJECT_REF --> RUNS_QUERY\n    RUNS_QUERY --> GET_STATS\n    RUNS_QUERY --> GET_ALLOC\n    RUNS_QUERY --> GET_PEN\n    RUNS_QUERY --> GET_SPECS",
      "graph LR\n    subgraph \"Hardware Detection\"\n        PERF_INFO[\"get_perf_info<br/>neurons/Validator/script.py\"]\n        GPU_SPECS[\"GPU Specifications<br/>Name, Count, Memory\"]\n        CPU_SPECS[\"CPU Specifications<br/>Cores, Architecture\"]\n    end\n    \n    subgraph \"WandB Upload\"\n        UPDATE_SPECS[\"update_specs<br/>compute/wandb/wandb.py\"]\n        RUN_CONFIG[\"run.config<br/>specs field\"]\n        SIGNATURE[\"sign_run<br/>Cryptographic Verification\"]\n    end\n    \n    subgraph \"Validator Consumption\"\n        GET_MINER_SPECS[\"get_miner_specs<br/>Query Interface\"]\n        QUERYABLE_UIDS[\"queryable_uids<br/>Network Participants\"]\n        ALLOCATION_LOGIC[\"Resource Allocation<br/>Decision Making\"]\n    end\n    \n    PERF_INFO --> GPU_SPECS\n    PERF_INFO --> CPU_SPECS\n    GPU_SPECS --> UPDATE_SPECS\n    CPU_SPECS --> UPDATE_SPECS\n    UPDATE_SPECS --> RUN_CONFIG\n    RUN_CONFIG --> SIGNATURE\n    \n    GET_MINER_SPECS --> QUERYABLE_UIDS\n    QUERYABLE_UIDS --> ALLOCATION_LOGIC",
      "graph TB\n    subgraph \"Validator State Updates\"\n        ALLOC_UPDATE[\"update_allocated_hotkeys<br/>List of Allocated Keys\"]\n        STATS_UPDATE[\"update_stats<br/>Performance Statistics\"]\n        PEN_UPDATE[\"update_penalized_hotkeys<br/>Blacklist Management\"]\n    end\n    \n    subgraph \"Cross-Validator Aggregation\"\n        GET_ALLOC_HOTKEYS[\"get_allocated_hotkeys<br/>Aggregate Across Validators\"]\n        GET_STATS_ALLOC[\"get_stats_allocated<br/>Statistics Aggregation\"]\n        CONFLICT_RESOLUTION[\"pick_dominant_dict<br/>Score-Based Resolution\"]\n    end\n    \n    subgraph \"Database Synchronization\"\n        RETRIEVE_STATS[\"retrieve_stats<br/>neurons/Validator/database/pog.py\"]\n        WRITE_STATS[\"write_stats<br/>neurons/Validator/database/pog.py\"]\n        COMPUTE_DB[\"ComputeDb<br/>Local State Storage\"]\n    end\n    \n    subgraph \"Verification Layer\"\n        VERIFY_RUN[\"verify_run<br/>Signature Verification\"]\n        VALID_VALIDATORS[\"valid_validator_hotkeys<br/>Authorized Validators\"]\n        SIGNATURE_CHECK[\"Cryptographic Validation\"]\n    end\n    \n    ALLOC_UPDATE --> GET_ALLOC_HOTKEYS\n    STATS_UPDATE --> GET_STATS_ALLOC\n    PEN_UPDATE --> GET_ALLOC_HOTKEYS\n    \n    GET_STATS_ALLOC --> CONFLICT_RESOLUTION\n    CONFLICT_RESOLUTION --> RETRIEVE_STATS\n    RETRIEVE_STATS --> WRITE_STATS\n    WRITE_STATS --> COMPUTE_DB\n    \n    GET_ALLOC_HOTKEYS --> VERIFY_RUN\n    GET_STATS_ALLOC --> VERIFY_RUN\n    VERIFY_RUN --> VALID_VALIDATORS\n    VALID_VALIDATORS --> SIGNATURE_CHECK",
      "graph TD\n    subgraph \"Signing Process\"\n        RUN_ID[\"run.id<br/>WandB Run Identifier\"]\n        DATA_HASH[\"SHA-256 Hash<br/>Computed from run_id\"]\n        WALLET_SIGN[\"wallet.hotkey.sign<br/>Cryptographic Signature\"]\n        CONFIG_UPDATE[\"run.config.update<br/>Store Signature\"]\n    end\n    \n    subgraph \"Verification Process\"\n        EXTRACT_SIG[\"Extract Signature<br/>From run.config\"]\n        RECREATE_HASH[\"Recreate Hash<br/>From run_id\"]\n        KEYPAIR_VERIFY[\"bt.Keypair.verify<br/>Signature Validation\"]\n        RESULT[\"Verification Result<br/>Boolean\"]\n    end\n    \n    subgraph \"Security Enforcement\"\n        VALID_HOTKEYS[\"valid_validator_hotkeys<br/>Authorized Keys\"]\n        ACCESS_CONTROL[\"Data Access Control<br/>Based on Verification\"]\n        FLAG_PARAM[\"flag Parameter<br/>Enforcement Toggle\"]\n    end\n    \n    RUN_ID --> DATA_HASH\n    DATA_HASH --> WALLET_SIGN\n    WALLET_SIGN --> CONFIG_UPDATE\n    \n    EXTRACT_SIG --> RECREATE_HASH\n    RECREATE_HASH --> KEYPAIR_VERIFY\n    KEYPAIR_VERIFY --> RESULT\n    \n    RESULT --> ACCESS_CONTROL\n    VALID_HOTKEYS --> ACCESS_CONTROL\n    FLAG_PARAM --> ACCESS_CONTROL",
      "graph LR\n    subgraph \"Custom Implementations\"\n        COMPUTE_SUBTENSOR[\"ComputeSubnetSubtensor<br/>compute/axon.py\"]\n        SERVE_PROMETHEUS[\"serve_prometheus<br/>Method\"]\n        DO_SERVE[\"do_serve_prometheus<br/>Extrinsic Handler\"]\n    end\n    \n    subgraph \"Blockchain Integration\"\n        PROMETHEUS_EXTRINSIC[\"prometheus_extrinsic<br/>compute/prometheus.py\"]\n        SUBSTRATE_CALL[\"substrate.compose_call<br/>SubtensorModule\"]\n        SIGNED_EXTRINSIC[\"create_signed_extrinsic<br/>Blockchain Transaction\"]\n    end\n    \n    subgraph \"Network Deployment\"\n        AXON_SERVE[\"Custom Axon<br/>ComputeSubnetAxon\"]\n        METRICS_ENDPOINT[\"Prometheus Endpoint<br/>Port Configuration\"]\n        NETWORK_REGISTRATION[\"Blockchain Registration<br/>Network Visibility\"]\n    end\n    \n    COMPUTE_SUBTENSOR --> SERVE_PROMETHEUS\n    SERVE_PROMETHEUS --> DO_SERVE\n    DO_SERVE --> PROMETHEUS_EXTRINSIC\n    \n    PROMETHEUS_EXTRINSIC --> SUBSTRATE_CALL\n    SUBSTRATE_CALL --> SIGNED_EXTRINSIC\n    \n    AXON_SERVE --> METRICS_ENDPOINT\n    METRICS_ENDPOINT --> NETWORK_REGISTRATION",
      "graph TD\n    subgraph \"Validation Process\"\n        POG_VALIDATION[\"Proof-of-GPU<br/>Validation\"]\n        CHALLENGE_RESULTS[\"Challenge Results<br/>Success/Failure\"]\n        SCORING[\"Score Calculation<br/>Performance Metrics\"]\n    end\n    \n    subgraph \"Local Storage\"\n        COMPUTE_DB_WRITE[\"ComputeDb Write<br/>Local Statistics\"]\n        POG_STATS[\"pog_stats table<br/>GPU Performance\"]\n        STATS_TABLE[\"stats table<br/>Miner Scores\"]\n    end\n    \n    subgraph \"WandB Logging\"\n        UPDATE_STATS_CALL[\"update_stats<br/>Aggregate Results\"]\n        WANDB_LOG[\"wandb.log<br/>Time Series Data\"]\n        CONFIG_UPDATE_STATS[\"run.config.update<br/>Latest State\"]\n    end\n    \n    subgraph \"Cross-Network Sync\"\n        ALLOCATED_UPDATE[\"update_allocated_hotkeys<br/>Resource State\"]\n        PENALIZED_UPDATE[\"update_penalized_hotkeys<br/>Blacklist State\"]\n        SIGNATURE_APPLY[\"sign_run<br/>Cryptographic Proof\"]\n    end\n    \n    POG_VALIDATION --> CHALLENGE_RESULTS\n    CHALLENGE_RESULTS --> SCORING\n    SCORING --> COMPUTE_DB_WRITE\n    \n    COMPUTE_DB_WRITE --> POG_STATS\n    COMPUTE_DB_WRITE --> STATS_TABLE\n    \n    SCORING --> UPDATE_STATS_CALL\n    UPDATE_STATS_CALL --> WANDB_LOG\n    WANDB_LOG --> CONFIG_UPDATE_STATS\n    \n    CONFIG_UPDATE_STATS --> ALLOCATED_UPDATE\n    ALLOCATED_UPDATE --> PENALIZED_UPDATE\n    PENALIZED_UPDATE --> SIGNATURE_APPLY"
    ],
    "potential_frontmatter": {
      "title": "Monitoring and Metrics"
    }
  },
  "/neuralinternet/ni-compute/6.1-wandb-integration": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/6.1-wandb-integration",
    "title": "WandB Integration",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/6.1-wandb-integration",
    "level": 1,
    "target_astro_path": "/monitoring-and-metrics/wandb-integration",
    "main_markdown_content": "# WandB Integration\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/wandb/wandb.py](compute/wandb/wandb.py)\n\n</details>\n\n\n\nThe WandB Integration system provides distributed state management and experiment tracking across the NI Compute Subnet using Weights & Biases (WandB) as a centralized data store. This system enables validators and miners to share critical network state including hardware specifications, allocation status, performance metrics, and penalty information in a verifiable and tamper-resistant manner.\n\nThis document covers the technical implementation of WandB integration for network-wide data synchronization. For Prometheus-based local metrics collection, see [Prometheus Metrics](#6.2).\n\n## Architecture Overview\n\nThe WandB integration operates as a distributed state management layer that sits between local database storage and network-wide coordination. Each validator and miner maintains its own WandB run that serves as both a data publication mechanism and a verification layer through cryptographic signatures.\n\n```mermaid\ngraph TB\n    subgraph \"Local Systems\"\n        V1[\"Validator Instance\"]\n        V2[\"Validator Instance\"]\n        M1[\"Miner Instance\"]\n        M2[\"Miner Instance\"]\n        DB1[\"ComputeDb (Local)\"]\n        DB2[\"ComputeDb (Local)\"]\n    end\n    \n    subgraph \"WandB Cloud Platform\"\n        PROJECT[\"opencompute Project\"]\n        VRUN1[\"validator-{hotkey} Run\"]\n        VRUN2[\"validator-{hotkey} Run\"]\n        MRUN1[\"miner-{hotkey} Run\"]\n        MRUN2[\"miner-{hotkey} Run\"]\n    end\n    \n    subgraph \"Shared Network State\"\n        ALLOCATED[\"allocated_hotkeys\"]\n        PENALIZED[\"penalized_hotkeys\"]\n        SPECS[\"miner_specs\"]\n        STATS[\"validator_stats\"]\n    end\n    \n    V1 -->|\"ComputeWandb.update_allocated_hotkeys()\"| VRUN1\n    V2 -->|\"ComputeWandb.update_stats()\"| VRUN2\n    M1 -->|\"ComputeWandb.update_specs()\"| MRUN1\n    M2 -->|\"ComputeWandb.update_allocated()\"| MRUN2\n    \n    V1 -->|\"write_stats()\"| DB1\n    M1 -->|\"save_run_id()\"| DB2\n    \n    VRUN1 --> ALLOCATED\n    VRUN2 --> STATS\n    MRUN1 --> SPECS\n    MRUN2 --> ALLOCATED\n    \n    V1 -.->|\"get_allocated_hotkeys()\"| ALLOCATED\n    V1 -.->|\"get_miner_specs()\"| SPECS\n    V2 -.->|\"get_stats_allocated()\"| STATS\n```\n\n**Sources:** [compute/wandb/wandb.py:1-648]()\n\n## Core Components\n\n### ComputeWandb Class\n\nThe `ComputeWandb` class serves as the primary interface for all WandB operations within the compute subnet. It manages authentication, run lifecycle, data synchronization, and cryptographic verification.\n\n```mermaid\nclassDiagram\n    class ComputeWandb {\n        +run: wandb.Run\n        +config: bt.config\n        +wallet: bt.wallet\n        +hotkey: str\n        +role: str\n        +db: ComputeDb\n        +api: wandb.Api\n        +run_id: str\n        \n        +__init__(config, wallet, role)\n        +update_config()\n        +save_run_id(hotkey, run_id)\n        +get_run_id(hotkey)\n        +update_specs()\n        +log_chain_data(data)\n        +update_allocated(allocated)\n        +update_stats(stats)\n        +update_allocated_hotkeys(hotkey_list)\n        +update_penalized_hotkeys(hotkey_list)\n        +get_allocated_hotkeys(valid_validators, flag)\n        +get_stats_allocated(valid_validators, flag)\n        +get_miner_specs(queryable_uids)\n        +sign_run()\n        +verify_run(run)\n        +sync_allocated(hotkey)\n    }\n    \n    class ComputeDb {\n        +get_cursor()\n        +conn: Connection\n    }\n    \n    class wandb_Api {\n        +runs()\n        +project()\n        +flush()\n    }\n    \n    ComputeWandb --> ComputeDb\n    ComputeWandb --> wandb_Api\n```\n\n**Sources:** [compute/wandb/wandb.py:19-648]()\n\n### Authentication and Run Management\n\nThe system manages WandB authentication through API keys and run persistence through local database storage. Each hotkey maintains a single persistent run across restarts.\n\n| Configuration Parameter | Value | Purpose |\n|------------------------|-------|---------|\n| `PUBLIC_WANDB_NAME` | `\"opencompute\"` | Project name |\n| `PUBLIC_WANDB_ENTITY` | `\"neuralinternet\"` | Organization entity |\n| Run naming pattern | `\"{role}-{hotkey}\"` | Unique run identification |\n\nThe authentication flow handles multiple scenarios:\n\n```mermaid\nflowchart TD\n    START[\"ComputeWandb.__init__()\"]\n    CHECK_KEY{\"WANDB_API_KEY exists?\"}\n    CHECK_NETRC{\"~/.netrc exists?\"}\n    ERROR[\"Raise ValueError\"]\n    \n    GET_RUN_ID[\"get_run_id(hotkey)\"]\n    RUN_EXISTS{\"run_id found?\"}\n    \n    QUERY_WANDB[\"Query WandB for existing runs\"]\n    RUNS_FOUND{\"runs.length >= 1?\"}\n    \n    CREATE_RUN[\"wandb.init() new run\"]\n    SAVE_RUN[\"save_run_id()\"]\n    \n    RESUME_RUN[\"wandb.init(id=run_id, resume='allow')\"]\n    UPDATE_CONFIG[\"update_config()\"]\n    SIGN[\"sign_run()\"]\n    \n    START --> CHECK_KEY\n    CHECK_KEY -->|No| CHECK_NETRC\n    CHECK_KEY -->|Yes| GET_RUN_ID\n    CHECK_NETRC -->|No| ERROR\n    CHECK_NETRC -->|Yes| GET_RUN_ID\n    \n    GET_RUN_ID --> RUN_EXISTS\n    RUN_EXISTS -->|No| QUERY_WANDB\n    RUN_EXISTS -->|Yes| RESUME_RUN\n    \n    QUERY_WANDB --> RUNS_FOUND\n    RUNS_FOUND -->|No| CREATE_RUN\n    RUNS_FOUND -->|Yes| SAVE_RUN\n    \n    CREATE_RUN --> SAVE_RUN\n    SAVE_RUN --> RESUME_RUN\n    RESUME_RUN --> UPDATE_CONFIG\n    UPDATE_CONFIG --> SIGN\n```\n\n**Sources:** [compute/wandb/wandb.py:22-88](), [compute/wandb/wandb.py:109-138]()\n\n## Data Synchronization Patterns\n\n### Validator Data Flow\n\nValidators publish aggregated network statistics and maintain lists of allocated and penalized hotkeys. The synchronization ensures consistency between local database state and distributed WandB state.\n\n```mermaid\nsequenceDiagram\n    participant VDB as \"ComputeDb (Validator)\"\n    participant CW as \"ComputeWandb\"\n    participant WB as \"WandB API\"\n    participant NET as \"Network State\"\n    \n    Note over VDB,NET: Stats Update Cycle\n    VDB->>CW: retrieve_stats(db)\n    CW->>CW: update_allocated_hotkeys(hotkey_list)\n    CW->>VDB: write_stats(db, updated_stats)\n    CW->>WB: run.config.update(allocated_hotkeys, stats)\n    CW->>WB: run.log(allocated_hotkeys)\n    CW->>CW: sign_run()\n    \n    Note over VDB,NET: Network Query Cycle\n    CW->>WB: api.runs(filters=validator_runs)\n    WB-->>CW: validator_run_configs\n    CW->>CW: verify_run(run) for each\n    CW-->>NET: aggregated_allocated_hotkeys\n    CW-->>NET: aggregated_stats\n```\n\n**Sources:** [compute/wandb/wandb.py:198-230](), [compute/wandb/wandb.py:291-332](), [compute/wandb/wandb.py:334-450]()\n\n### Miner Data Flow\n\nMiners publish hardware specifications and allocation status, enabling validators to discover available resources and verify capabilities.\n\n```mermaid\nsequenceDiagram\n    participant M as \"Miner Process\"\n    participant CW as \"ComputeWandb\"\n    participant WB as \"WandB API\"\n    participant V as \"Validator Query\"\n    \n    Note over M,V: Specs Publication\n    M->>CW: update_specs()\n    CW->>CW: get_perf_info(encrypted=False)\n    CW->>WB: run.config.update(specs)\n    CW->>CW: sign_run()\n    \n    Note over M,V: Allocation Status Update\n    M->>CW: update_allocated(validator_hotkey)\n    CW->>WB: run.config.update(allocated)\n    CW->>CW: sign_run()\n    \n    Note over M,V: Validator Discovery\n    V->>WB: api.runs(filters=miner_runs)\n    WB-->>V: miner_run_configs\n    V->>V: verify_run(run) for each\n    V-->>V: hotkey_to_specs_mapping\n```\n\n**Sources:** [compute/wandb/wandb.py:140-159](), [compute/wandb/wandb.py:168-184](), [compute/wandb/wandb.py:540-574]()\n\n## Security and Verification\n\n### Cryptographic Signature System\n\nAll WandB runs are signed using the participant's hotkey to prevent data tampering and ensure authenticity. The signature covers the run ID to prevent replay attacks.\n\n```mermaid\nflowchart LR\n    subgraph \"Signing Process\"\n        RUN_ID[\"run.id\"]\n        HASH[\"SHA-256 Hash\"]\n        SIGN[\"wallet.hotkey.sign()\"]\n        STORE[\"run.config.signature\"]\n    end\n    \n    subgraph \"Verification Process\"\n        RETRIEVE[\"run.config.signature\"]\n        RECREATE[\"SHA-256(run.id)\"]\n        VERIFY[\"bt.Keypair.verify()\"]\n        RESULT[\"True/False\"]\n    end\n    \n    RUN_ID --> HASH\n    HASH --> SIGN\n    SIGN --> STORE\n    \n    RETRIEVE --> VERIFY\n    RECREATE --> VERIFY\n    VERIFY --> RESULT\n```\n\nThe verification process validates both signature authenticity and validator authorization:\n\n| Verification Check | Implementation | Purpose |\n|-------------------|----------------|---------|\n| Signature validity | `bt.Keypair(ss58_address=hotkey).verify()` | Prevents data tampering |\n| Validator authorization | `hotkey in valid_validator_hotkeys` | Prevents unauthorized updates |\n| Data existence | Config field presence checks | Ensures required data |\n\n**Sources:** [compute/wandb/wandb.py:576-616]()\n\n## Configuration and Setup\n\n### Environment Requirements\n\n| Requirement | Configuration Method | Purpose |\n|-------------|---------------------|---------|\n| WandB API Key | `WANDB_API_KEY` environment variable | Authentication |\n| WandB Login | `wandb login` command | Alternative authentication |\n| Network file | `~/.netrc` | Credential storage |\n\n### Run Configuration Schema\n\nThe system maintains a standardized configuration schema across all runs:\n\n```json\n{\n  \"hotkey\": \"ss58_address\",\n  \"role\": \"validator|miner\", \n  \"config\": \"bt.config_object\",\n  \"version\": \"version_integer\",\n  \"specs\": \"hardware_specifications\",\n  \"allocated\": \"boolean_or_hotkey\",\n  \"allocated_hotkeys\": [\"hotkey_list\"],\n  \"penalized_hotkeys\": [\"hotkey_list\"],\n  \"stats\": \"uid_to_stats_mapping\",\n  \"signature\": \"hex_signature\"\n}\n```\n\n### Database Integration\n\nThe system maintains local state persistence through the `wandb_runs` table in `ComputeDb`:\n\n| Column | Type | Purpose |\n|--------|------|---------|\n| `hotkey` | TEXT | Participant identifier |\n| `run_id` | TEXT | WandB run identifier |\n\n**Sources:** [compute/wandb/wandb.py:15-52](), [compute/wandb/wandb.py:90-108](), [compute/wandb/wandb.py:109-138]()",
    "resolved_links": [
      {
        "text": "compute/wandb/wandb.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py",
        "original_deepwiki_href": "compute/wandb/wandb.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "Prometheus Metrics",
        "href": "/monitoring-and-metrics/prometheus-metrics#6.2",
        "original_deepwiki_href": "#6.2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"Local Systems\"\n        V1[\"Validator Instance\"]\n        V2[\"Validator Instance\"]\n        M1[\"Miner Instance\"]\n        M2[\"Miner Instance\"]\n        DB1[\"ComputeDb (Local)\"]\n        DB2[\"ComputeDb (Local)\"]\n    end\n    \n    subgraph \"WandB Cloud Platform\"\n        PROJECT[\"opencompute Project\"]\n        VRUN1[\"validator-{hotkey} Run\"]\n        VRUN2[\"validator-{hotkey} Run\"]\n        MRUN1[\"miner-{hotkey} Run\"]\n        MRUN2[\"miner-{hotkey} Run\"]\n    end\n    \n    subgraph \"Shared Network State\"\n        ALLOCATED[\"allocated_hotkeys\"]\n        PENALIZED[\"penalized_hotkeys\"]\n        SPECS[\"miner_specs\"]\n        STATS[\"validator_stats\"]\n    end\n    \n    V1 -->|\"ComputeWandb.update_allocated_hotkeys()\"| VRUN1\n    V2 -->|\"ComputeWandb.update_stats()\"| VRUN2\n    M1 -->|\"ComputeWandb.update_specs()\"| MRUN1\n    M2 -->|\"ComputeWandb.update_allocated()\"| MRUN2\n    \n    V1 -->|\"write_stats()\"| DB1\n    M1 -->|\"save_run_id()\"| DB2\n    \n    VRUN1 --> ALLOCATED\n    VRUN2 --> STATS\n    MRUN1 --> SPECS\n    MRUN2 --> ALLOCATED\n    \n    V1 -.->|\"get_allocated_hotkeys()\"| ALLOCATED\n    V1 -.->|\"get_miner_specs()\"| SPECS\n    V2 -.->|\"get_stats_allocated()\"| STATS",
      "classDiagram\n    class ComputeWandb {\n        +run: wandb.Run\n        +config: bt.config\n        +wallet: bt.wallet\n        +hotkey: str\n        +role: str\n        +db: ComputeDb\n        +api: wandb.Api\n        +run_id: str\n        \n        +__init__(config, wallet, role)\n        +update_config()\n        +save_run_id(hotkey, run_id)\n        +get_run_id(hotkey)\n        +update_specs()\n        +log_chain_data(data)\n        +update_allocated(allocated)\n        +update_stats(stats)\n        +update_allocated_hotkeys(hotkey_list)\n        +update_penalized_hotkeys(hotkey_list)\n        +get_allocated_hotkeys(valid_validators, flag)\n        +get_stats_allocated(valid_validators, flag)\n        +get_miner_specs(queryable_uids)\n        +sign_run()\n        +verify_run(run)\n        +sync_allocated(hotkey)\n    }\n    \n    class ComputeDb {\n        +get_cursor()\n        +conn: Connection\n    }\n    \n    class wandb_Api {\n        +runs()\n        +project()\n        +flush()\n    }\n    \n    ComputeWandb --> ComputeDb\n    ComputeWandb --> wandb_Api",
      "flowchart TD\n    START[\"ComputeWandb.__init__()\"]\n    CHECK_KEY{\"WANDB_API_KEY exists?\"}\n    CHECK_NETRC{\"~/.netrc exists?\"}\n    ERROR[\"Raise ValueError\"]\n    \n    GET_RUN_ID[\"get_run_id(hotkey)\"]\n    RUN_EXISTS{\"run_id found?\"}\n    \n    QUERY_WANDB[\"Query WandB for existing runs\"]\n    RUNS_FOUND{\"runs.length >= 1?\"}\n    \n    CREATE_RUN[\"wandb.init() new run\"]\n    SAVE_RUN[\"save_run_id()\"]\n    \n    RESUME_RUN[\"wandb.init(id=run_id, resume='allow')\"]\n    UPDATE_CONFIG[\"update_config()\"]\n    SIGN[\"sign_run()\"]\n    \n    START --> CHECK_KEY\n    CHECK_KEY -->|No| CHECK_NETRC\n    CHECK_KEY -->|Yes| GET_RUN_ID\n    CHECK_NETRC -->|No| ERROR\n    CHECK_NETRC -->|Yes| GET_RUN_ID\n    \n    GET_RUN_ID --> RUN_EXISTS\n    RUN_EXISTS -->|No| QUERY_WANDB\n    RUN_EXISTS -->|Yes| RESUME_RUN\n    \n    QUERY_WANDB --> RUNS_FOUND\n    RUNS_FOUND -->|No| CREATE_RUN\n    RUNS_FOUND -->|Yes| SAVE_RUN\n    \n    CREATE_RUN --> SAVE_RUN\n    SAVE_RUN --> RESUME_RUN\n    RESUME_RUN --> UPDATE_CONFIG\n    UPDATE_CONFIG --> SIGN",
      "sequenceDiagram\n    participant VDB as \"ComputeDb (Validator)\"\n    participant CW as \"ComputeWandb\"\n    participant WB as \"WandB API\"\n    participant NET as \"Network State\"\n    \n    Note over VDB,NET: Stats Update Cycle\n    VDB->>CW: retrieve_stats(db)\n    CW->>CW: update_allocated_hotkeys(hotkey_list)\n    CW->>VDB: write_stats(db, updated_stats)\n    CW->>WB: run.config.update(allocated_hotkeys, stats)\n    CW->>WB: run.log(allocated_hotkeys)\n    CW->>CW: sign_run()\n    \n    Note over VDB,NET: Network Query Cycle\n    CW->>WB: api.runs(filters=validator_runs)\n    WB-->>CW: validator_run_configs\n    CW->>CW: verify_run(run) for each\n    CW-->>NET: aggregated_allocated_hotkeys\n    CW-->>NET: aggregated_stats",
      "sequenceDiagram\n    participant M as \"Miner Process\"\n    participant CW as \"ComputeWandb\"\n    participant WB as \"WandB API\"\n    participant V as \"Validator Query\"\n    \n    Note over M,V: Specs Publication\n    M->>CW: update_specs()\n    CW->>CW: get_perf_info(encrypted=False)\n    CW->>WB: run.config.update(specs)\n    CW->>CW: sign_run()\n    \n    Note over M,V: Allocation Status Update\n    M->>CW: update_allocated(validator_hotkey)\n    CW->>WB: run.config.update(allocated)\n    CW->>CW: sign_run()\n    \n    Note over M,V: Validator Discovery\n    V->>WB: api.runs(filters=miner_runs)\n    WB-->>V: miner_run_configs\n    V->>V: verify_run(run) for each\n    V-->>V: hotkey_to_specs_mapping",
      "flowchart LR\n    subgraph \"Signing Process\"\n        RUN_ID[\"run.id\"]\n        HASH[\"SHA-256 Hash\"]\n        SIGN[\"wallet.hotkey.sign()\"]\n        STORE[\"run.config.signature\"]\n    end\n    \n    subgraph \"Verification Process\"\n        RETRIEVE[\"run.config.signature\"]\n        RECREATE[\"SHA-256(run.id)\"]\n        VERIFY[\"bt.Keypair.verify()\"]\n        RESULT[\"True/False\"]\n    end\n    \n    RUN_ID --> HASH\n    HASH --> SIGN\n    SIGN --> STORE\n    \n    RETRIEVE --> VERIFY\n    RECREATE --> VERIFY\n    VERIFY --> RESULT"
    ],
    "potential_frontmatter": {
      "title": "WandB Integration"
    }
  },
  "/neuralinternet/ni-compute/6.2-prometheus-metrics": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/6.2-prometheus-metrics",
    "title": "Prometheus Metrics",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/6.2-prometheus-metrics",
    "level": 1,
    "target_astro_path": "/monitoring-and-metrics/prometheus-metrics",
    "main_markdown_content": "# Prometheus Metrics\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/axon.py](compute/axon.py)\n\n</details>\n\n\n\nThis document explains how the NI Compute system implements and utilizes Prometheus metrics for monitoring and observability. Prometheus is a popular open-source monitoring and alerting toolkit used to collect and query time-series metrics from various systems. In NI Compute, Prometheus metrics provide crucial insights into the performance and behavior of validators and miners in the decentralized GPU compute marketplace.\n\nFor information about general monitoring and integration with Weights & Biases, see [WandB Integration](#6.1).\n\n## 1. Prometheus Metrics Architecture\n\nPrometheus metrics in NI Compute are implemented through a specialized registration system that integrates with the Bittensor blockchain. This integration ensures that monitoring endpoints are discoverable by other network participants.\n\n```mermaid\nflowchart TD\n    subgraph \"Prometheus Metrics System\"\n        PM[\"PrometheusMetrics\"]\n        PE[\"prometheus_extrinsic()\"]\n        DPS[\"do_serve_prometheus()\"]\n    end\n    \n    subgraph \"Validator\"\n        VI[\"init_prometheus()\"]\n        SS[\"sync_status()\"]\n        VI --> PE\n        SS --> VI\n    end\n    \n    subgraph \"Subtensor\"\n        CSS[\"ComputeSubnetSubtensor\"]\n        SP[\"serve_prometheus()\"]\n        CSS --> SP\n        SP --> PE\n    end\n    \n    subgraph \"Blockchain\"\n        BT[\"Bittensor Chain\"]\n        PI[\"PrometheusInfo\"]\n    end\n    \n    VI --> SP\n    PE --> DPS\n    DPS --> BT\n    BT --> PI\n    \n    style PM fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style PE fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style DPS fill:#f9f9f9,stroke:#333,stroke-width:1px\n```\n\nSources: [compute/axon.py:166-201](), [compute/axon.py:49]()\n\n## 2. Prometheus Registration Process\n\nThe registration process involves the validator registering its Prometheus metrics endpoint with the Bittensor blockchain, making it discoverable to other nodes and monitoring systems.\n\n```mermaid\nsequenceDiagram\n    participant Validator\n    participant Subtensor as ComputeSubnetSubtensor\n    participant Chain as Bittensor Blockchain\n    \n    Validator->>Validator: init_prometheus()\n    Validator->>Subtensor: serve_prometheus(wallet, port, netuid)\n    Subtensor->>Subtensor: prometheus_extrinsic()\n    \n    Note over Subtensor: Create PrometheusServeCallParams\n    \n    Subtensor->>Subtensor: Check if neuron needs update\n    \n    alt Needs Update\n        Subtensor->>Subtensor: do_serve_prometheus()\n        Subtensor->>Chain: Submit extrinsic\n        Chain->>Chain: Update PrometheusInfo\n        Chain-->>Subtensor: Confirm update\n        Subtensor-->>Validator: Return success\n    else Already Updated\n        Subtensor-->>Validator: Return success (already served)\n    end\n    \n    Validator->>Validator: Log success or failure\n```\n\nSources: [compute/axon.py:166-201](), [compute/axon.py:203-283]()\n\n## 3. Prometheus Metrics Implementation\n\n### 3.1 Registration Functions\n\nThe NI Compute system implements several key functions to handle Prometheus metrics registration:\n\n#### ComputeSubnetSubtensor.serve_prometheus()\n\nThe `serve_prometheus()` method in the `ComputeSubnetSubtensor` class coordinates Prometheus metrics registration on the blockchain. This method:\n\n- Accepts wallet, port, netuid, and wait parameters\n- Calls the `prometheus_extrinsic()` function to handle the registration process\n- Returns a boolean indicating success or failure\n- Includes comprehensive error handling and logging\n\n#### ComputeSubnetSubtensor.do_serve_prometheus()\n\nThe `do_serve_prometheus()` method handles the low-level blockchain interaction:\n\n- Composes a substrate call to `SubtensorModule.serve_prometheus`\n- Creates a signed extrinsic using the wallet hotkey\n- Implements retry logic with exponential backoff (3 tries, 2x backoff, max 4s delay)\n- Submits the extrinsic and processes the response\n- Returns a tuple of (success: bool, error: Optional[dict])\n\n#### prometheus_extrinsic()\n\nThe `prometheus_extrinsic()` function (imported from `compute.prometheus`) prepares the registration parameters and delegates to `do_serve_prometheus()`.\n\nSources: [compute/axon.py:166-201](), [compute/axon.py:203-283](), [compute/axon.py:258-283]()\n\n### 3.2 Versioning and Auto-Update\n\nThe system uses version information from `__version_as_int__` for Prometheus metrics registration. The version is embedded in the axon info that gets registered on the blockchain.\n\n```mermaid\nflowchart TD\n    subgraph \"Version Integration\"\n        VI[\"__version_as_int__\"]\n        AI[\"AxonInfo.version\"]\n        CSA[\"ComputeSubnetAxon.info()\"]\n        \n        VI --> AI\n        CSA --> AI\n    end\n    \n    subgraph \"Registration Process\"\n        SP[\"serve_prometheus()\"]\n        PE[\"prometheus_extrinsic()\"]\n        DSP[\"do_serve_prometheus()\"]\n        \n        SP --> PE\n        PE --> DSP\n        AI --> PE\n    end\n    \n    subgraph \"Blockchain\"\n        BC[\"Bittensor Chain\"]\n        PI[\"PrometheusInfo\"]\n        \n        DSP --> BC\n        BC --> PI\n    end\n```\n\nSources: [compute/axon.py:47](), [compute/axon.py:376-388]()\n\n## 4. Substrate Call Structure\n\nThe Prometheus registration uses a substrate call to the `SubtensorModule.serve_prometheus` function. The call parameters structure is defined by the Bittensor substrate interface and includes:\n\n| Parameter | Description | Implementation |\n|-----------|-------------|----------------|\n| `call_module` | Target module name | `\"SubtensorModule\"` |\n| `call_function` | Target function name | `\"serve_prometheus\"` |\n| `call_params` | Registration parameters | `PrometheusServeCallParams` |\n\nThe method creates a signed extrinsic using the wallet hotkey and submits it to the substrate interface with configurable wait options for inclusion and finalization.\n\nSources: [compute/axon.py:260-267](), [compute/axon.py:206]()\n\n## 5. Blockchain Integration\n\n### 5.1 ComputeSubnetSubtensor Extension\n\nThe `ComputeSubnetSubtensor` class extends Bittensor's base `Subtensor` class to add compute subnet-specific functionality:\n\n```mermaid\nclassDiagram\n    class Subtensor {\n        <<Bittensor Core>>\n        +substrate: SubstrateInterface\n        +compose_call()\n        +create_signed_extrinsic()\n    }\n    \n    class ComputeSubnetSubtensor {\n        +serve_prometheus(wallet, port, netuid)\n        +do_serve_prometheus(wallet, call_params)\n        -make_substrate_call_with_retry()\n    }\n    \n    class prometheus_extrinsic {\n        <<Function>>\n        (wallet, port, netuid)\n    }\n    \n    Subtensor <|-- ComputeSubnetSubtensor\n    ComputeSubnetSubtensor --> prometheus_extrinsic : calls\n```\n\n### 5.2 Extrinsic Submission Flow\n\nThe registration process uses Bittensor's substrate interface for blockchain interaction:\n\n1. **Call Composition**: Uses `substrate.compose_call()` to create the blockchain call\n2. **Extrinsic Creation**: Creates a signed extrinsic with `substrate.create_signed_extrinsic()`\n3. **Submission**: Submits via `substrate.submit_extrinsic()` with retry logic\n4. **Response Processing**: Processes events and checks for success/failure\n\nSources: [compute/axon.py:152-165](), [compute/axon.py:260-283]()\n\n## 6. Using Prometheus Metrics\n\n### 6.1 Accessing Metrics\n\nOnce registered, Prometheus metrics are available at:\n\n```\nhttp://<node_ip>:<prometheus_port>/metrics\n```\n\nThe exact port is determined by the validator's configuration, typically using Bittensor's default axon port.\n\n### 6.2 Common Metrics\n\nWhile the specific metrics exposed aren't explicitly documented in the code provided, Prometheus in Bittensor networks typically provides metrics such as:\n\n- Node operational status\n- Request counts and latencies\n- Resource utilization (CPU, memory, GPU)\n- Network activity\n- Validation and scoring information\n\n### 6.3 Integration with Monitoring Systems\n\nTo monitor NI Compute nodes using Prometheus:\n\n1. Configure a Prometheus server to scrape the registered endpoints\n2. Set up appropriate recording rules and alerts\n3. Use visualization tools like Grafana to create dashboards\n\n## 7. Monitoring Lifecycle\n\nThe NI Compute system ensures continuous monitoring availability through its lifecycle management:\n\n```mermaid\nflowchart LR\n    subgraph \"Node Startup\"\n        NS[\"Node Start\"]\n        IR[\"Initialize Registration\"]\n    end\n    \n    subgraph \"Periodic Checks\"\n        SS[\"sync_status()\"]\n        VC[\"Version Check\"]\n    end\n    \n    subgraph \"Updates\"\n        UP[\"Update Prometheus\"]\n        RE[\"Re-register if needed\"]\n    end\n    \n    NS --> IR\n    IR --> SS\n    SS -- \"Every sync cycle\" --> VC\n    VC -- \"Version mismatch\" --> UP\n    VC -- \"Needs refresh\" --> RE\n    \n    style NS fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style IR fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style SS fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style VC fill:#f9f9f9,stroke:#333,stroke-width:1px\n```\n\nSources: [neurons/validator.py:428-446](), [compute/prometheus.py:80-95]()\n\n## 8. Error Handling and Reliability\n\n### 8.1 Retry Logic Implementation\n\nThe `do_serve_prometheus()` method implements comprehensive retry logic for reliable blockchain communication:\n\n```mermaid\nflowchart LR\n    subgraph \"Retry Configuration\"\n        RC[\"@retry decorator\"]\n        D[\"delay=1\"]\n        T[\"tries=3\"] \n        B[\"backoff=2\"]\n        MD[\"max_delay=4\"]\n    end\n    \n    subgraph \"Submission Process\"\n        MSC[\"make_substrate_call_with_retry()\"]\n        CC[\"compose_call()\"]\n        CSE[\"create_signed_extrinsic()\"]\n        SE[\"submit_extrinsic()\"]\n    end\n    \n    subgraph \"Error Handling\"\n        SRE[\"SubstrateRequestException\"]\n        GE[\"General Exception\"]\n        EL[\"Error Logging\"]\n    end\n    \n    RC --> MSC\n    MSC --> CC\n    CC --> CSE\n    CSE --> SE\n    SE --> SRE\n    SE --> GE\n    SRE --> EL\n    GE --> EL\n```\n\n### 8.2 Exception Management\n\nThe system handles multiple types of errors:\n\n- **SubstrateRequestException**: Caught and logged with formatted error messages\n- **General Exceptions**: Unexpected errors are logged with full stack traces\n- **Response Processing**: Success/failure determined by response event processing\n\n### 8.3 Comprehensive Logging\n\nError logging includes:\n- Detailed exception information with `exc_info=True`\n- Formatted error messages using `format_error_message()`\n- Debug-level logging for successful operations\n\nSources: [compute/axon.py:258-283](), [compute/axon.py:244-256](), [compute/axon.py:197-201]()\n\n## 9. Summary\n\nPrometheus metrics in NI Compute provide essential observability into the decentralized GPU marketplace. The registration system ensures that metrics endpoints are discoverable through the Bittensor blockchain, allowing for comprehensive monitoring of validators and miners. Through version tracking and periodic updates, the system maintains monitoring capabilities even as the software evolves.",
    "resolved_links": [
      {
        "text": "compute/axon.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py",
        "original_deepwiki_href": "compute/axon.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/axon.py:166-201",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:49",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:203-283",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:258-283",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:47",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:376-388",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:260-267",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:206",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:152-165",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:260-283",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/validator.py:428-446",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/prometheus.py:80-95",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:244-256",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/axon.py:197-201",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "WandB Integration",
        "href": "/monitoring-and-metrics/wandb-integration#6.1",
        "original_deepwiki_href": "#6.1",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "flowchart TD\n    subgraph \"Prometheus Metrics System\"\n        PM[\"PrometheusMetrics\"]\n        PE[\"prometheus_extrinsic()\"]\n        DPS[\"do_serve_prometheus()\"]\n    end\n    \n    subgraph \"Validator\"\n        VI[\"init_prometheus()\"]\n        SS[\"sync_status()\"]\n        VI --> PE\n        SS --> VI\n    end\n    \n    subgraph \"Subtensor\"\n        CSS[\"ComputeSubnetSubtensor\"]\n        SP[\"serve_prometheus()\"]\n        CSS --> SP\n        SP --> PE\n    end\n    \n    subgraph \"Blockchain\"\n        BT[\"Bittensor Chain\"]\n        PI[\"PrometheusInfo\"]\n    end\n    \n    VI --> SP\n    PE --> DPS\n    DPS --> BT\n    BT --> PI\n    \n    style PM fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style PE fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style DPS fill:#f9f9f9,stroke:#333,stroke-width:1px",
      "sequenceDiagram\n    participant Validator\n    participant Subtensor as ComputeSubnetSubtensor\n    participant Chain as Bittensor Blockchain\n    \n    Validator->>Validator: init_prometheus()\n    Validator->>Subtensor: serve_prometheus(wallet, port, netuid)\n    Subtensor->>Subtensor: prometheus_extrinsic()\n    \n    Note over Subtensor: Create PrometheusServeCallParams\n    \n    Subtensor->>Subtensor: Check if neuron needs update\n    \n    alt Needs Update\n        Subtensor->>Subtensor: do_serve_prometheus()\n        Subtensor->>Chain: Submit extrinsic\n        Chain->>Chain: Update PrometheusInfo\n        Chain-->>Subtensor: Confirm update\n        Subtensor-->>Validator: Return success\n    else Already Updated\n        Subtensor-->>Validator: Return success (already served)\n    end\n    \n    Validator->>Validator: Log success or failure",
      "flowchart TD\n    subgraph \"Version Integration\"\n        VI[\"__version_as_int__\"]\n        AI[\"AxonInfo.version\"]\n        CSA[\"ComputeSubnetAxon.info()\"]\n        \n        VI --> AI\n        CSA --> AI\n    end\n    \n    subgraph \"Registration Process\"\n        SP[\"serve_prometheus()\"]\n        PE[\"prometheus_extrinsic()\"]\n        DSP[\"do_serve_prometheus()\"]\n        \n        SP --> PE\n        PE --> DSP\n        AI --> PE\n    end\n    \n    subgraph \"Blockchain\"\n        BC[\"Bittensor Chain\"]\n        PI[\"PrometheusInfo\"]\n        \n        DSP --> BC\n        BC --> PI\n    end",
      "classDiagram\n    class Subtensor {\n        <<Bittensor Core>>\n        +substrate: SubstrateInterface\n        +compose_call()\n        +create_signed_extrinsic()\n    }\n    \n    class ComputeSubnetSubtensor {\n        +serve_prometheus(wallet, port, netuid)\n        +do_serve_prometheus(wallet, call_params)\n        -make_substrate_call_with_retry()\n    }\n    \n    class prometheus_extrinsic {\n        <<Function>>\n        (wallet, port, netuid)\n    }\n    \n    Subtensor <|-- ComputeSubnetSubtensor\n    ComputeSubnetSubtensor --> prometheus_extrinsic : calls",
      "flowchart LR\n    subgraph \"Node Startup\"\n        NS[\"Node Start\"]\n        IR[\"Initialize Registration\"]\n    end\n    \n    subgraph \"Periodic Checks\"\n        SS[\"sync_status()\"]\n        VC[\"Version Check\"]\n    end\n    \n    subgraph \"Updates\"\n        UP[\"Update Prometheus\"]\n        RE[\"Re-register if needed\"]\n    end\n    \n    NS --> IR\n    IR --> SS\n    SS -- \"Every sync cycle\" --> VC\n    VC -- \"Version mismatch\" --> UP\n    VC -- \"Needs refresh\" --> RE\n    \n    style NS fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style IR fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style SS fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style VC fill:#f9f9f9,stroke:#333,stroke-width:1px",
      "flowchart LR\n    subgraph \"Retry Configuration\"\n        RC[\"@retry decorator\"]\n        D[\"delay=1\"]\n        T[\"tries=3\"] \n        B[\"backoff=2\"]\n        MD[\"max_delay=4\"]\n    end\n    \n    subgraph \"Submission Process\"\n        MSC[\"make_substrate_call_with_retry()\"]\n        CC[\"compose_call()\"]\n        CSE[\"create_signed_extrinsic()\"]\n        SE[\"submit_extrinsic()\"]\n    end\n    \n    subgraph \"Error Handling\"\n        SRE[\"SubstrateRequestException\"]\n        GE[\"General Exception\"]\n        EL[\"Error Logging\"]\n    end\n    \n    RC --> MSC\n    MSC --> CC\n    CC --> CSE\n    CSE --> SE\n    SE --> SRE\n    SE --> GE\n    SRE --> EL\n    GE --> EL"
    ],
    "potential_frontmatter": {
      "title": "Prometheus Metrics"
    }
  },
  "/neuralinternet/ni-compute/7-configuration": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/7-configuration",
    "title": "Configuration",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/7-configuration",
    "level": 0,
    "target_astro_path": "/configuration",
    "main_markdown_content": "# Configuration\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [compute/__init__.py](compute/__init__.py)\n- [compute/utils/math.py](compute/utils/math.py)\n- [compute/utils/parser.py](compute/utils/parser.py)\n- [config.yaml](config.yaml)\n- [neurons/Validator/database/miner.py](neurons/Validator/database/miner.py)\n- [neurons/Validator/database/pog.py](neurons/Validator/database/pog.py)\n- [neurons/Validator/miner_script_m_merkletree.py](neurons/Validator/miner_script_m_merkletree.py)\n- [neurons/Validator/pog.py](neurons/Validator/pog.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document covers the configuration system for the NI Compute Subnet, including static configuration constants, command-line argument parsing, GPU performance parameters, and runtime configuration options. The configuration system manages settings for validators, miners, and the resource allocation API across the entire compute subnet infrastructure.\n\nFor information about command-line argument usage in specific components, see [Command-line Arguments](#7.1). For GPU performance benchmarks and hardware-specific settings, see [GPU Performance Configuration](#7.2).\n\n## Configuration Architecture\n\nThe configuration system operates through multiple layers and sources that provide settings for different aspects of the compute subnet:\n\n```mermaid\ngraph TB\n    subgraph \"Configuration Sources\"\n        CLI[\"Command Line Arguments<br/>ComputeArgPaser\"]\n        YAML[\"config.yaml<br/>GPU Performance Data\"]\n        STATIC[\"compute/__init__.py<br/>Static Constants\"]\n        ENV[\"Environment Variables<br/>.env file\"]\n    end\n    \n    subgraph \"Configuration Categories\"\n        NETWORK[\"Network Configuration<br/>netuid, timeout values\"]\n        GPU_CONFIG[\"GPU Configuration<br/>performance data, tolerance\"]\n        MINER_CONFIG[\"Miner Configuration<br/>hashcat, whitelist settings\"]\n        VALIDATOR_CONFIG[\"Validator Configuration<br/>batch sizes, thresholds\"]\n        POG_CONFIG[\"Proof-of-GPU Configuration<br/>merkle proof settings\"]\n    end\n    \n    subgraph \"Target Components\"\n        MINER[\"Miner Process<br/>neurons/miner.py\"]\n        VALIDATOR[\"Validator Process<br/>neurons/validator.py\"]\n        API[\"RegisterAPI<br/>FastAPI Service\"]\n    end\n    \n    CLI --> NETWORK\n    CLI --> MINER_CONFIG\n    CLI --> VALIDATOR_CONFIG\n    \n    YAML --> GPU_CONFIG\n    YAML --> POG_CONFIG\n    \n    STATIC --> NETWORK\n    STATIC --> MINER_CONFIG\n    STATIC --> POG_CONFIG\n    \n    ENV --> API\n    \n    NETWORK --> MINER\n    NETWORK --> VALIDATOR\n    NETWORK --> API\n    \n    GPU_CONFIG --> VALIDATOR\n    MINER_CONFIG --> MINER\n    VALIDATOR_CONFIG --> VALIDATOR\n    POG_CONFIG --> VALIDATOR\n```\n\n**Sources:** [compute/__init__.py:1-93](), [config.yaml:1-104](), [compute/utils/parser.py:1-170]()\n\n## Static Configuration Constants\n\nThe compute subnet defines core system parameters as static constants in the main module. These constants control network behavior, validation timing, and system limits:\n\n### Network and Validation Constants\n\n```mermaid\ngraph LR\n    CONSTANTS[\"compute/__init__.py\"] --> NETWORK_PARAMS[\"`Network Parameters\n    validator_permit_stake: 1.0e4\n    weights_rate_limit: 100\n    specs_timeout: 60`\"]\n    \n    CONSTANTS --> POG_PARAMS[\"`Proof-of-GPU Parameters\n    pog_retry_limit: 30\n    pog_retry_interval: 80\n    pow_timeout: 30`\"]\n    \n    CONSTANTS --> MINER_PARAMS[\"`Miner Parameters\n    miner_priority_specs: 1\n    miner_priority_challenge: 2\n    miner_priority_allocate: 3`\"]\n    \n    CONSTANTS --> SECURITY[\"`Security Lists\n    SUSPECTED_EXPLOITERS_HOTKEYS\n    TRUSTED_VALIDATORS_HOTKEYS`\"]\n```\n\nKey configuration constants include:\n\n| Category | Constant | Default Value | Purpose |\n|----------|----------|---------------|---------|\n| Network | `validator_permit_stake` | `1.0e4` | Minimum stake required for validator |\n| Network | `weights_rate_limit` | `100` | Rate limit for weight updates |\n| Validation | `specs_timeout` | `60` | Timeout for hardware specs queries |\n| Proof-of-GPU | `pog_retry_limit` | `30` | Maximum PoG retry attempts |\n| Proof-of-GPU | `pog_retry_interval` | `80` | Seconds between PoG retries |\n| Proof-of-Work | `pow_min_difficulty` | `7` | Minimum PoW difficulty |\n| Proof-of-Work | `pow_max_difficulty` | `12` | Maximum PoW difficulty |\n| Miner | `miner_hashcat_location` | `\"hashcat\"` | Default hashcat binary path |\n\n**Sources:** [compute/__init__.py:29-58]()\n\n### Security Configuration\n\nThe system maintains predefined lists of trusted validators and suspected exploiters:\n\n```mermaid\ngraph TB\n    SECURITY_CONFIG[\"Security Configuration\"] --> TRUSTED[\"TRUSTED_VALIDATORS_HOTKEYS<br/>List of verified validator hotkeys\"]\n    SECURITY_CONFIG --> EXPLOITERS[\"SUSPECTED_EXPLOITERS_HOTKEYS<br/>List of blacklisted hotkeys\"]\n    \n    TRUSTED --> VALIDATOR_PROCESS[\"Validator Whitelist Logic\"]\n    EXPLOITERS --> BLACKLIST_FILTER[\"Automatic Blacklisting\"]\n```\n\n**Sources:** [compute/__init__.py:59-92]()\n\n## Command-Line Argument System\n\nThe `ComputeArgPaser` class extends Python's `ArgumentParser` to provide comprehensive command-line configuration for both miners and validators:\n\n```mermaid\ngraph TB\n    PARSER[\"ComputeArgPaser\"] --> BASE_ARGS[\"`Base Arguments\n    --netuid: Subnet ID\n    --auto_update: Auto-update flag\n    --blacklist.exploiters: Use exploiter list`\"]\n    \n    PARSER --> BLACKLIST_ARGS[\"`Blacklist/Whitelist\n    --blacklist.hotkeys\n    --blacklist.coldkeys\n    --whitelist.hotkeys\n    --whitelist.coldkeys`\"]\n    \n    PARSER --> VALIDATOR_ARGS[\"`Validator Arguments\n    --validator.whitelist.unrecognized\n    --validator.perform.hardware.query\n    --validator.challenge.batch.size\n    --validator.specs.batch.size`\"]\n    \n    PARSER --> MINER_ARGS[\"`Miner Arguments\n    --miner.hashcat.path\n    --miner.hashcat.workload.profile\n    --miner.whitelist.not.enough.stake\n    --ssh.port`\"]\n    \n    PARSER --> BITTENSOR_ARGS[\"`Bittensor Integration\n    bt.subtensor.add_args()\n    bt.logging.add_args()\n    bt.wallet.add_args()\n    bt.axon.add_args()`\"]\n```\n\n### Argument Categories\n\nThe parser organizes arguments into logical categories:\n\n| Category | Method | Arguments |\n|----------|--------|-----------|\n| Base | `__init__` | `netuid`, `auto_update`, blacklist/whitelist options |\n| Validator | `add_validator_argument()` | batch sizes, hardware query settings, thresholds |\n| Miner | `add_miner_argument()` | hashcat configuration, SSH port, whitelist settings |\n| Bittensor | Built-in | subtensor, logging, wallet, axon arguments |\n\n**Sources:** [compute/utils/parser.py:8-71](), [compute/utils/parser.py:72-115](), [compute/utils/parser.py:116-166]()\n\n## GPU Performance Configuration\n\nThe `config.yaml` file contains comprehensive GPU performance data used for validation and scoring:\n\n```mermaid\ngraph TB\n    GPU_CONFIG[\"config.yaml\"] --> PERF_DATA[\"`GPU Performance Data\n    GPU_TFLOPS_FP16: FP16 performance\n    GPU_TFLOPS_FP32: FP32 performance\n    GPU_AVRAM: Available VRAM`\"]\n    \n    GPU_CONFIG --> TOLERANCE[\"`Tolerance Pairs\n    gpu_tolerance_pairs:\n    Similar GPU mappings`\"]\n    \n    GPU_CONFIG --> SCORES[\"`GPU Scores\n    gpu_scores:\n    Scoring weights by model`\"]\n    \n    PERF_DATA --> IDENTIFICATION[\"GPU Identification Logic<br/>identify_gpu()\"]\n    TOLERANCE --> VALIDATION[\"Performance Validation<br/>Tolerance Handling\"]\n    SCORES --> SCORING[\"Miner Scoring System\"]\n```\n\n### Performance Data Structure\n\nThe GPU configuration includes three main performance categories:\n\n1. **FP16 Performance** (`GPU_TFLOPS_FP16`): Theoretical FP16 TFLOPS for each GPU model\n2. **FP32 Performance** (`GPU_TFLOPS_FP32`): Theoretical FP32 TFLOPS for each GPU model  \n3. **Available VRAM** (`GPU_AVRAM`): Effective VRAM capacity in GB\n\nExample configuration structure:\n```yaml\ngpu_performance:\n  GPU_TFLOPS_FP16:\n    NVIDIA H100: 330\n    NVIDIA A100-SXM4-80GB: 238.8\n  GPU_TFLOPS_FP32:\n    NVIDIA H100: 37.2\n    NVIDIA A100-SXM4-80GB: 18.2\n  GPU_AVRAM:\n    NVIDIA H100: 34.36\n    NVIDIA A100-SXM4-80GB: 34.36\n```\n\n**Sources:** [config.yaml:1-94]()\n\n### GPU Tolerance and Scoring\n\nThe configuration defines tolerance pairs for GPUs with similar performance characteristics and assigns scoring weights:\n\n```mermaid\ngraph LR\n    TOLERANCE_PAIRS[\"gpu_tolerance_pairs\"] --> SIMILAR_GPUS[\"`Similar GPU Pairs\n    NVIDIA L40 ↔ NVIDIA RTX 6000 Ada\n    NVIDIA A100 variants ↔ similar models`\"]\n    \n    GPU_SCORES[\"gpu_scores\"] --> SCORE_WEIGHTS[\"`Scoring Weights\n    NVIDIA H200: 4.0\n    NVIDIA H100: 2.80\n    NVIDIA A100-SXM4-80GB: 1.90`\"]\n    \n    SIMILAR_GPUS --> VALIDATION_TOLERANCE[\"Validation Tolerance Logic\"]\n    SCORE_WEIGHTS --> MINER_SCORING[\"Miner Performance Scoring\"]\n```\n\n**Sources:** [config.yaml:63-94]()\n\n## Merkle Proof Configuration\n\nThe system includes specific configuration for Proof-of-GPU Merkle tree operations:\n\n| Parameter | Default Value | Purpose |\n|-----------|---------------|---------|\n| `miner_script_path` | `\"neurons/Validator/miner_script_m_merkletree.py\"` | Path to Merkle proof script |\n| `time_tolerance` | `5` | Time tolerance for proof verification |\n| `submatrix_size` | `512` | Size of submatrices for proof generation |\n| `hash_algorithm` | `'sha256'` | Hash algorithm for Merkle trees |\n| `pog_retry_limit` | `22` | Maximum Proof-of-GPU retry attempts |\n| `pog_retry_interval` | `60` | Seconds between PoG retries |\n| `max_workers` | `64` | Maximum concurrent workers |\n| `max_random_delay` | `900` | Maximum random delay in seconds |\n\n**Sources:** [config.yaml:95-104]()\n\n## Configuration Loading and Usage\n\nThe configuration system loads and applies settings through multiple mechanisms:\n\n```mermaid\ngraph TB\n    LOAD_CONFIG[\"`Configuration Loading Process`\"] --> YAML_LOAD[\"load_yaml_config()<br/>YAML file parsing\"]\n    LOAD_CONFIG --> PARSER_CONFIG[\"ComputeArgPaser.config<br/>bt.config() integration\"]\n    LOAD_CONFIG --> STATIC_IMPORT[\"Static constant imports<br/>from compute import *\"]\n    \n    YAML_LOAD --> GPU_IDENTIFY[\"identify_gpu()<br/>Performance-based identification\"]\n    PARSER_CONFIG --> COMPONENT_CONFIG[\"Component Configuration<br/>Miners, Validators, API\"]\n    STATIC_IMPORT --> SYSTEM_LIMITS[\"System Limits<br/>Timeouts, thresholds\"]\n    \n    GPU_IDENTIFY --> POG_VALIDATION[\"Proof-of-GPU Validation\"]\n    COMPONENT_CONFIG --> RUNTIME_BEHAVIOR[\"Runtime Behavior Control\"]\n    SYSTEM_LIMITS --> NETWORK_OPERATIONS[\"Network Operations\"]\n```\n\n### Configuration Access Patterns\n\nComponents access configuration through several patterns:\n\n1. **Static Import**: Direct import of constants from `compute` module\n2. **YAML Loading**: Dynamic loading of GPU performance data via `load_yaml_config()`\n3. **Argument Parsing**: Runtime configuration through `ComputeArgPaser.config`\n4. **Environment Variables**: API keys and paths from `.env` files\n\n**Sources:** [neurons/Validator/pog.py:14-26](), [compute/utils/parser.py:70-71](), [compute/__init__.py:1-93]()\n\n## Configuration Validation and Error Handling\n\nThe system includes validation mechanisms for configuration parameters:\n\n```mermaid\ngraph TB\n    VALIDATION[\"`Configuration Validation`\"] --> YAML_VALIDATION[\"YAML Validation<br/>FileNotFoundError, YAMLError\"]\n    VALIDATION --> RANGE_VALIDATION[\"Range Validation<br/>pow_min_difficulty ≤ pow_max_difficulty\"]\n    VALIDATION --> TYPE_VALIDATION[\"Type Validation<br/>Numeric parameters, string paths\"]\n    \n    YAML_VALIDATION --> ERROR_HANDLING[\"Error Handling<br/>Graceful degradation\"]\n    RANGE_VALIDATION --> BOUNDS_CHECKING[\"Bounds Checking<br/>Prevent invalid configurations\"]\n    TYPE_VALIDATION --> CONVERSION[\"Type Conversion<br/>String to numeric conversion\"]\n```\n\nThe configuration system provides robust error handling for common issues:\n\n- Missing configuration files (YAML not found)\n- Invalid YAML syntax or structure\n- Out-of-range numeric parameters\n- Invalid GPU model specifications\n- Missing required command-line arguments\n\n**Sources:** [neurons/Validator/pog.py:22-25](), [compute/utils/math.py:16-21]()",
    "resolved_links": [
      {
        "text": "README.md",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/README.md",
        "original_deepwiki_href": "README.md",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/__init__.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py",
        "original_deepwiki_href": "compute/__init__.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/utils/math.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/math.py",
        "original_deepwiki_href": "compute/utils/math.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/utils/parser.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py",
        "original_deepwiki_href": "compute/utils/parser.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "config.yaml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml",
        "original_deepwiki_href": "config.yaml",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/database/miner.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/miner.py",
        "original_deepwiki_href": "neurons/Validator/database/miner.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/database/pog.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py",
        "original_deepwiki_href": "neurons/Validator/database/pog.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/miner_script_m_merkletree.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/miner_script_m_merkletree.py",
        "original_deepwiki_href": "neurons/Validator/miner_script_m_merkletree.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/pog.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py",
        "original_deepwiki_href": "neurons/Validator/pog.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "Command-line Arguments",
        "href": "/configuration/command-line-arguments#7.1",
        "original_deepwiki_href": "#7.1",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "GPU Performance Configuration",
        "href": "/configuration/gpu-performance-configuration#7.2",
        "original_deepwiki_href": "#7.2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"Configuration Sources\"\n        CLI[\"Command Line Arguments<br/>ComputeArgPaser\"]\n        YAML[\"config.yaml<br/>GPU Performance Data\"]\n        STATIC[\"compute/__init__.py<br/>Static Constants\"]\n        ENV[\"Environment Variables<br/>.env file\"]\n    end\n    \n    subgraph \"Configuration Categories\"\n        NETWORK[\"Network Configuration<br/>netuid, timeout values\"]\n        GPU_CONFIG[\"GPU Configuration<br/>performance data, tolerance\"]\n        MINER_CONFIG[\"Miner Configuration<br/>hashcat, whitelist settings\"]\n        VALIDATOR_CONFIG[\"Validator Configuration<br/>batch sizes, thresholds\"]\n        POG_CONFIG[\"Proof-of-GPU Configuration<br/>merkle proof settings\"]\n    end\n    \n    subgraph \"Target Components\"\n        MINER[\"Miner Process<br/>neurons/miner.py\"]\n        VALIDATOR[\"Validator Process<br/>neurons/validator.py\"]\n        API[\"RegisterAPI<br/>FastAPI Service\"]\n    end\n    \n    CLI --> NETWORK\n    CLI --> MINER_CONFIG\n    CLI --> VALIDATOR_CONFIG\n    \n    YAML --> GPU_CONFIG\n    YAML --> POG_CONFIG\n    \n    STATIC --> NETWORK\n    STATIC --> MINER_CONFIG\n    STATIC --> POG_CONFIG\n    \n    ENV --> API\n    \n    NETWORK --> MINER\n    NETWORK --> VALIDATOR\n    NETWORK --> API\n    \n    GPU_CONFIG --> VALIDATOR\n    MINER_CONFIG --> MINER\n    VALIDATOR_CONFIG --> VALIDATOR\n    POG_CONFIG --> VALIDATOR",
      "graph LR\n    CONSTANTS[\"compute/__init__.py\"] --> NETWORK_PARAMS[\"`Network Parameters\n    validator_permit_stake: 1.0e4\n    weights_rate_limit: 100\n    specs_timeout: 60`\"]\n    \n    CONSTANTS --> POG_PARAMS[\"`Proof-of-GPU Parameters\n    pog_retry_limit: 30\n    pog_retry_interval: 80\n    pow_timeout: 30`\"]\n    \n    CONSTANTS --> MINER_PARAMS[\"`Miner Parameters\n    miner_priority_specs: 1\n    miner_priority_challenge: 2\n    miner_priority_allocate: 3`\"]\n    \n    CONSTANTS --> SECURITY[\"`Security Lists\n    SUSPECTED_EXPLOITERS_HOTKEYS\n    TRUSTED_VALIDATORS_HOTKEYS`\"]",
      "graph TB\n    SECURITY_CONFIG[\"Security Configuration\"] --> TRUSTED[\"TRUSTED_VALIDATORS_HOTKEYS<br/>List of verified validator hotkeys\"]\n    SECURITY_CONFIG --> EXPLOITERS[\"SUSPECTED_EXPLOITERS_HOTKEYS<br/>List of blacklisted hotkeys\"]\n    \n    TRUSTED --> VALIDATOR_PROCESS[\"Validator Whitelist Logic\"]\n    EXPLOITERS --> BLACKLIST_FILTER[\"Automatic Blacklisting\"]",
      "graph TB\n    PARSER[\"ComputeArgPaser\"] --> BASE_ARGS[\"`Base Arguments\n    --netuid: Subnet ID\n    --auto_update: Auto-update flag\n    --blacklist.exploiters: Use exploiter list`\"]\n    \n    PARSER --> BLACKLIST_ARGS[\"`Blacklist/Whitelist\n    --blacklist.hotkeys\n    --blacklist.coldkeys\n    --whitelist.hotkeys\n    --whitelist.coldkeys`\"]\n    \n    PARSER --> VALIDATOR_ARGS[\"`Validator Arguments\n    --validator.whitelist.unrecognized\n    --validator.perform.hardware.query\n    --validator.challenge.batch.size\n    --validator.specs.batch.size`\"]\n    \n    PARSER --> MINER_ARGS[\"`Miner Arguments\n    --miner.hashcat.path\n    --miner.hashcat.workload.profile\n    --miner.whitelist.not.enough.stake\n    --ssh.port`\"]\n    \n    PARSER --> BITTENSOR_ARGS[\"`Bittensor Integration\n    bt.subtensor.add_args()\n    bt.logging.add_args()\n    bt.wallet.add_args()\n    bt.axon.add_args()`\"]",
      "graph TB\n    GPU_CONFIG[\"config.yaml\"] --> PERF_DATA[\"`GPU Performance Data\n    GPU_TFLOPS_FP16: FP16 performance\n    GPU_TFLOPS_FP32: FP32 performance\n    GPU_AVRAM: Available VRAM`\"]\n    \n    GPU_CONFIG --> TOLERANCE[\"`Tolerance Pairs\n    gpu_tolerance_pairs:\n    Similar GPU mappings`\"]\n    \n    GPU_CONFIG --> SCORES[\"`GPU Scores\n    gpu_scores:\n    Scoring weights by model`\"]\n    \n    PERF_DATA --> IDENTIFICATION[\"GPU Identification Logic<br/>identify_gpu()\"]\n    TOLERANCE --> VALIDATION[\"Performance Validation<br/>Tolerance Handling\"]\n    SCORES --> SCORING[\"Miner Scoring System\"]",
      "graph LR\n    TOLERANCE_PAIRS[\"gpu_tolerance_pairs\"] --> SIMILAR_GPUS[\"`Similar GPU Pairs\n    NVIDIA L40 ↔ NVIDIA RTX 6000 Ada\n    NVIDIA A100 variants ↔ similar models`\"]\n    \n    GPU_SCORES[\"gpu_scores\"] --> SCORE_WEIGHTS[\"`Scoring Weights\n    NVIDIA H200: 4.0\n    NVIDIA H100: 2.80\n    NVIDIA A100-SXM4-80GB: 1.90`\"]\n    \n    SIMILAR_GPUS --> VALIDATION_TOLERANCE[\"Validation Tolerance Logic\"]\n    SCORE_WEIGHTS --> MINER_SCORING[\"Miner Performance Scoring\"]",
      "graph TB\n    LOAD_CONFIG[\"`Configuration Loading Process`\"] --> YAML_LOAD[\"load_yaml_config()<br/>YAML file parsing\"]\n    LOAD_CONFIG --> PARSER_CONFIG[\"ComputeArgPaser.config<br/>bt.config() integration\"]\n    LOAD_CONFIG --> STATIC_IMPORT[\"Static constant imports<br/>from compute import *\"]\n    \n    YAML_LOAD --> GPU_IDENTIFY[\"identify_gpu()<br/>Performance-based identification\"]\n    PARSER_CONFIG --> COMPONENT_CONFIG[\"Component Configuration<br/>Miners, Validators, API\"]\n    STATIC_IMPORT --> SYSTEM_LIMITS[\"System Limits<br/>Timeouts, thresholds\"]\n    \n    GPU_IDENTIFY --> POG_VALIDATION[\"Proof-of-GPU Validation\"]\n    COMPONENT_CONFIG --> RUNTIME_BEHAVIOR[\"Runtime Behavior Control\"]\n    SYSTEM_LIMITS --> NETWORK_OPERATIONS[\"Network Operations\"]",
      "graph TB\n    VALIDATION[\"`Configuration Validation`\"] --> YAML_VALIDATION[\"YAML Validation<br/>FileNotFoundError, YAMLError\"]\n    VALIDATION --> RANGE_VALIDATION[\"Range Validation<br/>pow_min_difficulty ≤ pow_max_difficulty\"]\n    VALIDATION --> TYPE_VALIDATION[\"Type Validation<br/>Numeric parameters, string paths\"]\n    \n    YAML_VALIDATION --> ERROR_HANDLING[\"Error Handling<br/>Graceful degradation\"]\n    RANGE_VALIDATION --> BOUNDS_CHECKING[\"Bounds Checking<br/>Prevent invalid configurations\"]\n    TYPE_VALIDATION --> CONVERSION[\"Type Conversion<br/>String to numeric conversion\"]"
    ],
    "potential_frontmatter": {
      "title": "Configuration"
    }
  },
  "/neuralinternet/ni-compute/7.1-command-line-arguments": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/7.1-command-line-arguments",
    "title": "Command-line Arguments",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/7.1-command-line-arguments",
    "level": 1,
    "target_astro_path": "/configuration/command-line-arguments",
    "main_markdown_content": "# Command-line Arguments\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [compute/__init__.py](compute/__init__.py)\n- [compute/utils/parser.py](compute/utils/parser.py)\n\n</details>\n\n\n\nThis document provides a comprehensive reference for all command-line arguments available in the NI Compute Subnet. The arguments control the behavior of miners and validators, including network configuration, validation parameters, resource allocation settings, and security controls.\n\nFor information about GPU performance configuration parameters, see [GPU Performance Configuration](#7.2). For details about using the registration CLI tools, see [Registration CLI](#8.1).\n\n## Argument Parser Architecture\n\nThe command-line argument system is built using the `ComputeArgPaser` class, which extends Python's `argparse.ArgumentParser` to provide both compute subnet-specific arguments and inherited Bittensor framework arguments.\n\n### Argument Parser Structure\n\n```mermaid\ngraph TD\n    subgraph \"ComputeArgPaser Class\"\n        PARSER[\"ComputeArgPaser\"]\n        INIT[\"__init__()\"]\n        VALIDATOR_ARGS[\"add_validator_argument()\"]\n        MINER_ARGS[\"add_miner_argument()\"]\n        PARSE_LIST[\"parse_list()\"]\n    end\n    \n    subgraph \"Bittensor Framework Args\"\n        BT_SUBTENSOR[\"bt.subtensor.add_args()\"]\n        BT_LOGGING[\"bt.logging.add_args()\"]\n        BT_WALLET[\"bt.wallet.add_args()\"]\n        BT_AXON[\"bt.axon.add_args()\"]\n    end\n    \n    subgraph \"Configuration Objects\"\n        CONFIG[\"bt.config\"]\n        NETUID[\"--netuid\"]\n        AUTO_UPDATE[\"--auto_update\"]\n        BLACKLIST[\"--blacklist.*\"]\n        WHITELIST[\"--whitelist.*\"]\n    end\n    \n    INIT --> VALIDATOR_ARGS\n    INIT --> MINER_ARGS\n    INIT --> BT_SUBTENSOR\n    INIT --> BT_LOGGING\n    INIT --> BT_WALLET\n    INIT --> BT_AXON\n    \n    PARSER --> CONFIG\n    \n    VALIDATOR_ARGS --> CONFIG\n    MINER_ARGS --> CONFIG\n    BT_SUBTENSOR --> CONFIG\n    BT_LOGGING --> CONFIG\n    BT_WALLET --> CONFIG\n    BT_AXON --> CONFIG\n```\n\nSources: [compute/utils/parser.py:8-71]()\n\n## Core Arguments\n\n### Network Configuration\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--netuid` | int | 27 | The chain subnet UID (27 for mainnet, 15 for testnet) |\n| `--auto_update` | flag | True | Automatically update the git repository |\n\nThe `--netuid` argument determines which Bittensor subnet the process connects to. The main production network uses netuid 27, while the test network uses netuid 15.\n\nSources: [compute/utils/parser.py:12-22]()\n\n### Security and Access Control\n\nThe subnet provides comprehensive blacklisting and whitelisting capabilities for both hotkeys and coldkeys:\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--blacklist.exploiters` | flag | True | Automatically blacklist known exploiter hotkeys |\n| `--blacklist.hotkeys` | list | [] | Comma-separated list of hotkeys to blacklist |\n| `--blacklist.coldkeys` | list | [] | Comma-separated list of coldkeys to blacklist |\n| `--whitelist.hotkeys` | list | [] | Comma-separated list of hotkeys to whitelist |\n| `--whitelist.coldkeys` | list | [] | Comma-separated list of coldkeys to whitelist |\n\nThe `--blacklist.exploiters` flag automatically applies the hardcoded list of suspected exploiter hotkeys defined in `SUSPECTED_EXPLOITERS_HOTKEYS`.\n\nSources: [compute/utils/parser.py:24-57](), [compute/__init__.py:60-77]()\n\n## Validator Arguments\n\nValidators use specialized arguments to control their validation behavior, hardware querying, and performance parameters.\n\n### Validation Control Arguments\n\n```mermaid\ngraph LR\n    subgraph \"Validator Configuration\"\n        WHITELIST_UNREC[\"--validator.whitelist.unrecognized\"]\n        HARDWARE_QUERY[\"--validator.perform.hardware.query\"]\n        PROMETHEUS_UPDATE[\"--validator.force.update.prometheus\"]\n        WHITELIST_THRESHOLD[\"--validator.whitelist.updated.threshold\"]\n    end\n    \n    subgraph \"Batch Processing\"\n        CHALLENGE_BATCH[\"--validator.challenge.batch.size\"]\n        SPECS_BATCH[\"--validator.specs.batch.size\"]\n    end\n    \n    subgraph \"Validator Process\"\n        VALIDATION_LOGIC[\"Validation Logic\"]\n        HARDWARE_SPECS[\"Hardware Specs Collection\"]\n        CHALLENGE_SYSTEM[\"Challenge System\"]\n    end\n    \n    WHITELIST_UNREC --> VALIDATION_LOGIC\n    HARDWARE_QUERY --> HARDWARE_SPECS\n    CHALLENGE_BATCH --> CHALLENGE_SYSTEM\n    SPECS_BATCH --> HARDWARE_SPECS\n    PROMETHEUS_UPDATE --> VALIDATION_LOGIC\n    WHITELIST_THRESHOLD --> VALIDATION_LOGIC\n```\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--validator.whitelist.unrecognized` | flag | False | Whitelist unrecognized miners |\n| `--validator.perform.hardware.query` | bool | True | Enable hardware specs collection from miners |\n| `--validator.challenge.batch.size` | int | 256 | Batch size for challenge processing |\n| `--validator.specs.batch.size` | int | 64 | Batch size for hardware specs queries |\n| `--validator.force.update.prometheus` | flag | False | Force Prometheus version update |\n| `--validator.whitelist.updated.threshold` | int | 60 | Quorum threshold percentage for whitelisting |\n\nThe batch size arguments allow validators with lower hardware specifications to reduce memory and processing load by processing smaller batches of miners at a time.\n\nSources: [compute/utils/parser.py:72-114]()\n\n## Miner Arguments\n\nMiners have specialized arguments for hashcat configuration, resource allocation, and validator interaction policies.\n\n### Hashcat Configuration\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--miner.hashcat.path` | str | \"hashcat\" | Path to the hashcat binary executable |\n| `--miner.hashcat.workload.profile` | str | \"3\" | Performance profile (1=Low, 2=Economic, 3=High, 4=Insane) |\n| `--miner.hashcat.extended.options` | str | \"\" | Additional hashcat command-line options |\n\nThe hashcat configuration controls how miners respond to Proof-of-Work challenges. The workload profile directly impacts the computational intensity and power consumption.\n\nSources: [compute/utils/parser.py:117-137](), [compute/__init__.py:55-57]()\n\n### Miner Security and Validation Policies\n\n```mermaid\ngraph TD\n    subgraph \"Miner Whitelist Policy\"\n        NOT_ENOUGH_STAKE[\"--miner.whitelist.not.enough.stake\"]\n        NOT_UPDATED[\"--miner.whitelist.not.updated\"]\n        UPDATED_THRESHOLD[\"--miner.whitelist.updated.threshold\"]\n    end\n    \n    subgraph \"Resource Allocation\"\n        SSH_PORT[\"--ssh.port\"]\n        ALLOCATION_SERVICE[\"Allocation Service\"]\n    end\n    \n    subgraph \"Validator Interaction\"\n        STAKE_CHECK[\"Validator Stake Verification\"]\n        VERSION_CHECK[\"Validator Version Check\"]\n        THRESHOLD_CHECK[\"Quorum Threshold Check\"]\n    end\n    \n    NOT_ENOUGH_STAKE --> STAKE_CHECK\n    NOT_UPDATED --> VERSION_CHECK\n    UPDATED_THRESHOLD --> THRESHOLD_CHECK\n    SSH_PORT --> ALLOCATION_SERVICE\n    \n    STAKE_CHECK --> ALLOCATION_SERVICE\n    VERSION_CHECK --> ALLOCATION_SERVICE\n    THRESHOLD_CHECK --> ALLOCATION_SERVICE\n```\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--miner.whitelist.not.enough.stake` | flag | False | Accept validators without sufficient stake |\n| `--miner.whitelist.not.updated` | flag | False | Accept validators not using latest code version |\n| `--miner.whitelist.updated.threshold` | int | 60 | Quorum threshold percentage before applying whitelist |\n| `--ssh.port` | int | 4444 | SSH port for resource allocation service |\n\nThese security policies allow miners to control which validators they accept requests from based on stake requirements and code version compliance.\n\nSources: [compute/utils/parser.py:138-165]()\n\n## Inherited Bittensor Arguments\n\nThe argument parser incorporates standard Bittensor framework arguments through method calls that extend the parser with additional argument groups.\n\n### Bittensor Framework Integration\n\n```mermaid\ngraph LR\n    subgraph \"Bittensor Components\"\n        SUBTENSOR[\"bt.subtensor\"]\n        LOGGING[\"bt.logging\"]\n        WALLET[\"bt.wallet\"]\n        AXON[\"bt.axon\"]\n    end\n    \n    subgraph \"Added Arguments\"\n        SUBTENSOR_ARGS[\"--subtensor.network<br/>--subtensor.chain_endpoint\"]\n        LOGGING_ARGS[\"--logging.debug<br/>--logging.trace<br/>--logging.logging_dir\"]\n        WALLET_ARGS[\"--wallet.name<br/>--wallet.hotkey<br/>--wallet.path\"]\n        AXON_ARGS[\"--axon.port\"]\n    end\n    \n    subgraph \"Configuration Object\"\n        BT_CONFIG[\"bt.config\"]\n    end\n    \n    SUBTENSOR --> SUBTENSOR_ARGS\n    LOGGING --> LOGGING_ARGS\n    WALLET --> WALLET_ARGS\n    AXON --> AXON_ARGS\n    \n    SUBTENSOR_ARGS --> BT_CONFIG\n    LOGGING_ARGS --> BT_CONFIG\n    WALLET_ARGS --> BT_CONFIG\n    AXON_ARGS --> BT_CONFIG\n```\n\n### Subtensor Arguments\n- `--subtensor.network`: Network endpoint (finney, test, or custom)\n- `--subtensor.chain_endpoint`: Custom blockchain endpoint URL\n\n### Logging Arguments\n- `--logging.debug`: Enable debug-level logging\n- `--logging.trace`: Enable trace-level logging\n- `--logging.logging_dir`: Directory for log files\n\n### Wallet Arguments\n- `--wallet.name`: Coldkey wallet name\n- `--wallet.hotkey`: Hotkey name\n- `--wallet.path`: Custom wallet directory path\n\n### Axon Arguments\n- `--axon.port`: Port for serving the axon (default: 8091)\n\nSources: [compute/utils/parser.py:61-70]()\n\n## Usage Examples\n\n### Miner Command Line\n\n```bash\npm2 start ./neurons/miner.py --name MINER_NAME --interpreter python3 -- \\\n  --netuid 27 \\\n  --subtensor.network finney \\\n  --wallet.name COLDKEY_NAME \\\n  --wallet.hotkey HOTKEY_NAME \\\n  --axon.port 8091 \\\n  --ssh.port 4444 \\\n  --logging.debug \\\n  --auto_update\n```\n\n### Validator Command Line\n\n```bash\npm2 start ./neurons/validator.py --name VALIDATOR_NAME --interpreter python3 -- \\\n  --netuid 27 \\\n  --subtensor.network finney \\\n  --wallet.name COLDKEY_NAME \\\n  --wallet.hotkey HOTKEY_NAME \\\n  --validator.specs.batch.size 64 \\\n  --validator.challenge.batch.size 256 \\\n  --logging.debug \\\n  --auto_update\n```\n\n### Advanced Configuration Examples\n\nFor miners with custom hashcat optimization:\n```bash\n--miner.hashcat.workload.profile 4 \\\n--miner.hashcat.extended.options \"-O --force\"\n```\n\nFor validators with restrictive whitelisting:\n```bash\n--validator.whitelist.updated.threshold 80 \\\n--blacklist.exploiters \\\n--blacklist.hotkeys \"5HZ1ATsziEMDm1iUqNWQatfEDb1JSNf37AiG8s3X4pZzoP3A\"\n```\n\nSources: [README.md:362-425]()",
    "resolved_links": [
      {
        "text": "README.md",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/README.md",
        "original_deepwiki_href": "README.md",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/__init__.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py",
        "original_deepwiki_href": "compute/__init__.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/utils/parser.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py",
        "original_deepwiki_href": "compute/utils/parser.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/utils/parser.py:8-71",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/parser.py:12-22",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/parser.py:24-57",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/__init__.py:60-77",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/parser.py:72-114",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/parser.py:117-137",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/__init__.py:55-57",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/parser.py:138-165",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/utils/parser.py:61-70",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "README.md:362-425",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "GPU Performance Configuration",
        "href": "/configuration/gpu-performance-configuration#7.2",
        "original_deepwiki_href": "#7.2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Registration CLI",
        "href": "/cli-tools/registration-cli#8.1",
        "original_deepwiki_href": "#8.1",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TD\n    subgraph \"ComputeArgPaser Class\"\n        PARSER[\"ComputeArgPaser\"]\n        INIT[\"__init__()\"]\n        VALIDATOR_ARGS[\"add_validator_argument()\"]\n        MINER_ARGS[\"add_miner_argument()\"]\n        PARSE_LIST[\"parse_list()\"]\n    end\n    \n    subgraph \"Bittensor Framework Args\"\n        BT_SUBTENSOR[\"bt.subtensor.add_args()\"]\n        BT_LOGGING[\"bt.logging.add_args()\"]\n        BT_WALLET[\"bt.wallet.add_args()\"]\n        BT_AXON[\"bt.axon.add_args()\"]\n    end\n    \n    subgraph \"Configuration Objects\"\n        CONFIG[\"bt.config\"]\n        NETUID[\"--netuid\"]\n        AUTO_UPDATE[\"--auto_update\"]\n        BLACKLIST[\"--blacklist.*\"]\n        WHITELIST[\"--whitelist.*\"]\n    end\n    \n    INIT --> VALIDATOR_ARGS\n    INIT --> MINER_ARGS\n    INIT --> BT_SUBTENSOR\n    INIT --> BT_LOGGING\n    INIT --> BT_WALLET\n    INIT --> BT_AXON\n    \n    PARSER --> CONFIG\n    \n    VALIDATOR_ARGS --> CONFIG\n    MINER_ARGS --> CONFIG\n    BT_SUBTENSOR --> CONFIG\n    BT_LOGGING --> CONFIG\n    BT_WALLET --> CONFIG\n    BT_AXON --> CONFIG",
      "graph LR\n    subgraph \"Validator Configuration\"\n        WHITELIST_UNREC[\"--validator.whitelist.unrecognized\"]\n        HARDWARE_QUERY[\"--validator.perform.hardware.query\"]\n        PROMETHEUS_UPDATE[\"--validator.force.update.prometheus\"]\n        WHITELIST_THRESHOLD[\"--validator.whitelist.updated.threshold\"]\n    end\n    \n    subgraph \"Batch Processing\"\n        CHALLENGE_BATCH[\"--validator.challenge.batch.size\"]\n        SPECS_BATCH[\"--validator.specs.batch.size\"]\n    end\n    \n    subgraph \"Validator Process\"\n        VALIDATION_LOGIC[\"Validation Logic\"]\n        HARDWARE_SPECS[\"Hardware Specs Collection\"]\n        CHALLENGE_SYSTEM[\"Challenge System\"]\n    end\n    \n    WHITELIST_UNREC --> VALIDATION_LOGIC\n    HARDWARE_QUERY --> HARDWARE_SPECS\n    CHALLENGE_BATCH --> CHALLENGE_SYSTEM\n    SPECS_BATCH --> HARDWARE_SPECS\n    PROMETHEUS_UPDATE --> VALIDATION_LOGIC\n    WHITELIST_THRESHOLD --> VALIDATION_LOGIC",
      "graph TD\n    subgraph \"Miner Whitelist Policy\"\n        NOT_ENOUGH_STAKE[\"--miner.whitelist.not.enough.stake\"]\n        NOT_UPDATED[\"--miner.whitelist.not.updated\"]\n        UPDATED_THRESHOLD[\"--miner.whitelist.updated.threshold\"]\n    end\n    \n    subgraph \"Resource Allocation\"\n        SSH_PORT[\"--ssh.port\"]\n        ALLOCATION_SERVICE[\"Allocation Service\"]\n    end\n    \n    subgraph \"Validator Interaction\"\n        STAKE_CHECK[\"Validator Stake Verification\"]\n        VERSION_CHECK[\"Validator Version Check\"]\n        THRESHOLD_CHECK[\"Quorum Threshold Check\"]\n    end\n    \n    NOT_ENOUGH_STAKE --> STAKE_CHECK\n    NOT_UPDATED --> VERSION_CHECK\n    UPDATED_THRESHOLD --> THRESHOLD_CHECK\n    SSH_PORT --> ALLOCATION_SERVICE\n    \n    STAKE_CHECK --> ALLOCATION_SERVICE\n    VERSION_CHECK --> ALLOCATION_SERVICE\n    THRESHOLD_CHECK --> ALLOCATION_SERVICE",
      "graph LR\n    subgraph \"Bittensor Components\"\n        SUBTENSOR[\"bt.subtensor\"]\n        LOGGING[\"bt.logging\"]\n        WALLET[\"bt.wallet\"]\n        AXON[\"bt.axon\"]\n    end\n    \n    subgraph \"Added Arguments\"\n        SUBTENSOR_ARGS[\"--subtensor.network<br/>--subtensor.chain_endpoint\"]\n        LOGGING_ARGS[\"--logging.debug<br/>--logging.trace<br/>--logging.logging_dir\"]\n        WALLET_ARGS[\"--wallet.name<br/>--wallet.hotkey<br/>--wallet.path\"]\n        AXON_ARGS[\"--axon.port\"]\n    end\n    \n    subgraph \"Configuration Object\"\n        BT_CONFIG[\"bt.config\"]\n    end\n    \n    SUBTENSOR --> SUBTENSOR_ARGS\n    LOGGING --> LOGGING_ARGS\n    WALLET --> WALLET_ARGS\n    AXON --> AXON_ARGS\n    \n    SUBTENSOR_ARGS --> BT_CONFIG\n    LOGGING_ARGS --> BT_CONFIG\n    WALLET_ARGS --> BT_CONFIG\n    AXON_ARGS --> BT_CONFIG"
    ],
    "potential_frontmatter": {
      "title": "Command-line Arguments"
    }
  },
  "/neuralinternet/ni-compute/7.2-gpu-performance-configuration": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/7.2-gpu-performance-configuration",
    "title": "GPU Performance Configuration",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/7.2-gpu-performance-configuration",
    "level": 1,
    "target_astro_path": "/configuration/gpu-performance-configuration",
    "main_markdown_content": "# GPU Performance Configuration\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [compute/__init__.py](compute/__init__.py)\n- [compute/utils/math.py](compute/utils/math.py)\n- [config.yaml](config.yaml)\n- [neurons/Validator/database/miner.py](neurons/Validator/database/miner.py)\n- [neurons/Validator/database/pog.py](neurons/Validator/database/pog.py)\n- [neurons/Validator/miner_script_m_merkletree.py](neurons/Validator/miner_script_m_merkletree.py)\n- [neurons/Validator/pog.py](neurons/Validator/pog.py)\n\n</details>\n\n\n\n## Overview\n\nThis document details the GPU performance configuration system used for Proof-of-GPU validation in the NI Compute subnet. It covers the performance benchmarks, tolerance settings, identification logic, and Merkle proof parameters that enable validators to verify miner GPU capabilities. For overall system configuration options, see [Command-line Arguments](#7.1).\n\nThe GPU performance configuration consists of several key components:\n- Performance benchmark data (TFLOPS, AVRAM) for GPU identification\n- Tolerance pairs for handling equivalent GPU models  \n- Merkle proof parameters for cryptographic verification\n- Benchmarking timeouts and retry limits\n\nSources: [config.yaml:1-104](), [compute/__init__.py:37-48]()\n\n## GPU Performance Benchmarks\n\nThe system maintains comprehensive performance data for GPU models in `config.yaml` under the `gpu_performance` section. This data enables accurate GPU identification and performance verification through three key metrics:\n\n### Performance Data Structure\n\n**FP16 TFLOPS Configuration**\n```yaml\nGPU_TFLOPS_FP16:\n  NVIDIA B200: 1205\n  NVIDIA H200: 610\n  NVIDIA H100 80GB HBM3: 570\n  NVIDIA A100-SXM4-80GB: 238.8\n```\n\n**FP32 TFLOPS Configuration**  \n```yaml\nGPU_TFLOPS_FP32:\n  NVIDIA B200: 67.2\n  NVIDIA H200: 49.6\n  NVIDIA H100 80GB HBM3: 49.0\n  NVIDIA A100-SXM4-80GB: 18.2\n```\n\n**VRAM Configuration**\n```yaml\nGPU_AVRAM:\n  NVIDIA B200: 68.72\n  NVIDIA H200: 68.72\n  NVIDIA H100 80GB HBM3: 34.36\n  NVIDIA A100-SXM4-80GB: 34.36\n```\n\n### GPU Performance Ranking\n\nThe `gpu_scores` section assigns relative performance values used by the scoring system:\n\n| GPU Model | Performance Score |\n|-----------|------------------|\n| NVIDIA B200 | 5.00 |\n| NVIDIA H200 | 4.0 |\n| NVIDIA H100 80GB HBM3 | 3.30 |\n| NVIDIA H100 | 2.80 |\n| NVIDIA A100-SXM4-80GB | 1.90 |\n| NVIDIA L40s | 0.90 |\n| NVIDIA RTX 6000 Ada Generation | 0.83 |\n| NVIDIA RTX 4090 | 0.68 |\n\nSources: [config.yaml:1-94]()\n\n## GPU Tolerance Configuration\n\nThe system handles functionally equivalent GPU models through tolerance pairs that prevent false negatives during GPU identification. This mechanism accounts for naming variations and similar performance characteristics.\n\n### Tolerance Pairs Configuration\n\n```mermaid\ngraph LR\n    subgraph \"gpu_tolerance_pairs Configuration\"\n        L40[\"NVIDIA L40\"] <--> RTX6000[\"NVIDIA RTX 6000 Ada Generation\"]\n        A100PCIe[\"NVIDIA A100 80GB PCIe\"] <--> A100SXM[\"NVIDIA A100-SXM4-80GB\"] \n        H100_80GB[\"NVIDIA H100 80GB HBM3\"] <--> H100[\"NVIDIA H100\"]\n        A40[\"NVIDIA A40\"] <--> RTXA6000[\"NVIDIA RTX A6000\"]\n        RTXA5000[\"NVIDIA RTX A5000\"] <--> RTX4000[\"NVIDIA RTX 4000 Ada Generation\"]\n    end\n```\n\n### Tolerance Implementation\n\nThe `identify_gpu` function in `neurons/Validator/pog.py` applies tolerance logic during GPU identification:\n\n```python\n# Check if identified GPU matches the tolerance pair\nif identified_gpu in tolerance_pairs and reported_name == tolerance_pairs.get(identified_gpu):\n    identified_gpu = reported_name\n# Check reverse mapping\nelif reported_name in tolerance_pairs and identified_gpu == tolerance_pairs.get(reported_name):\n    identified_gpu = reported_name\n```\n\nThis allows miners with equivalent hardware to receive consistent identification regardless of minor naming differences.\n\nSources: [config.yaml:63-73](), [neurons/Validator/pog.py:27-73]()\n\n## Merkle Proof Configuration\n\nThe Proof-of-GPU system uses Merkle tree verification to cryptographically validate GPU computations. The merkle proof configuration parameters control this verification process.\n\n### Merkle Proof Parameters\n\n```yaml\nmerkle_proof:\n  miner_script_path: \"neurons/Validator/miner_script_m_merkletree.py\"\n  time_tolerance: 5\n  submatrix_size: 512\n  hash_algorithm: 'sha256'\n  pog_retry_limit: 22\n  pog_retry_interval: 60  # seconds\n  max_workers: 64\n  max_random_delay: 900  # 900 seconds\n```\n\n### Merkle Tree Process Flow\n\n```mermaid\nflowchart TD\n    subgraph \"Merkle Proof Verification Process\"\n        validator[\"Validator\"] -->|\"send_script_and_request_hash\"| ssh[\"SSH Connection\"]\n        ssh -->|\"execute_script_on_miner\"| script[\"miner_script_m_merkletree.py\"]\n        script -->|\"generate_matrix_torch\"| matrices[\"Matrix Generation\"]\n        matrices -->|\"build_merkle_tree_rows\"| tree[\"Merkle Tree\"]\n        tree -->|\"get_merkle_proof_row\"| proof[\"Merkle Proofs\"]\n        proof -->|\"verify_merkle_proof_row\"| validator\n        validator -->|\"verify_responses\"| result[\"Verification Result\"]\n    end\n```\n\n### Implementation Components\n\nThe Merkle proof system involves several key functions:\n- `send_script_and_request_hash()`: Transfers and verifies the benchmark script\n- `execute_script_on_miner()`: Runs computation modes (benchmark/compute/proof)\n- `build_merkle_tree_rows()`: Constructs Merkle trees from computation results\n- `verify_merkle_proof_row()`: Validates individual proof elements\n- `verify_responses()`: Performs overall verification with failure tolerance\n\nSources: [config.yaml:95-104](), [neurons/Validator/pog.py:75-340]()\n\n## Benchmarking Parameters\n\nThe system uses several timeout and retry parameters to ensure reliable GPU performance validation while handling network and hardware variations.\n\n### Core PoG Parameters\n\nFrom `compute/__init__.py`:\n```python\n# Proof of GPU settings\npog_retry_limit = 30\npog_retry_interval = 80  # seconds\nspecs_timeout = 60  # Time before specs requests timeout\n```\n\n### Benchmark Execution Flow\n\n```mermaid\nflowchart TD\n    subgraph \"GPU Benchmarking Process\"\n        start[\"Validator initiates PoG\"] --> send[\"send_script_and_request_hash()\"]\n        send --> verify[\"Verify script hash\"]\n        verify --> benchmark[\"execute_script_on_miner(mode='benchmark')\"]\n        benchmark --> parse[\"parse_benchmark_output()\"]\n        parse --> compute[\"execute_script_on_miner(mode='compute')\"]\n        compute --> merkle[\"parse_merkle_output()\"]\n        merkle --> proof[\"execute_script_on_miner(mode='proof')\"]\n        proof --> validate[\"verify_responses()\"]\n        validate --> result[\"GPU identification & scoring\"]\n    end\n```\n\n### Benchmark Output Parsing\n\nThe `parse_benchmark_output` function processes miner responses:\n```python\nnum_gpus, vram, size_fp16, time_fp16, size_fp32, time_fp32 = parse_benchmark_output(output)\n```\n\nThis extracts:\n- GPU count\n- Available VRAM \n- FP16 matrix size and execution time\n- FP32 matrix size and execution time\n\nThese values are then used by `identify_gpu()` to match against the performance database.\n\nSources: [compute/__init__.py:37-48](), [neurons/Validator/pog.py:101-146]()\n\n## GPU Identification Logic\n\nThe core GPU identification process combines performance benchmarking with tolerance-aware matching to accurately identify miner hardware capabilities.\n\n### Identification Algorithm\n\n```mermaid\nflowchart TD\n    subgraph \"identify_gpu Function Flow\"\n        input[\"Input: fp16_tflops, fp32_tflops, estimated_avram, reported_name\"] \n        input --> calculate[\"Calculate combined_scores for all GPU models\"]\n        calculate --> deviation[\"fp16_deviation + fp32_deviation + avram_deviation / 3\"]\n        deviation --> sort[\"Sort by lowest deviation score\"]\n        sort --> identify[\"identified_gpu = best_match\"]\n        identify --> tolerance{\"Check tolerance_pairs\"}\n        tolerance -->|\"Match found\"| adjust[\"Apply tolerance adjustment\"]\n        tolerance -->|\"No match\"| return[\"Return identified_gpu\"]\n        adjust --> return\n    end\n```\n\n### Performance Deviation Calculation\n\nThe `identify_gpu` function calculates deviation scores for each GPU model:\n\n```python\nfp16_deviation = abs(fp16_tflops - fp16_theoretical) / fp16_theoretical\nfp32_deviation = abs(fp32_tflops - fp32_theoretical) / fp32_theoretical  \navram_deviation = abs(estimated_avram - avram_theoretical) / avram_theoretical\ncombined_score = (fp16_deviation + fp32_deviation + avram_deviation) / 3\n```\n\nThe GPU with the lowest combined deviation score is selected as the identified model.\n\n### Benchmark Script Integration\n\nThe `miner_script_m_merkletree.py` script provides multiple execution modes:\n- `benchmark`: Matrix multiplication performance testing\n- `compute`: Merkle tree computation with PRNG matrices\n- `proof`: Generate cryptographic proofs for verification\n- `gpu_info`: Basic GPU detection and enumeration\n\nSources: [neurons/Validator/pog.py:27-73](), [neurons/Validator/miner_script_m_merkletree.py:21-388]()\n\n## Configuration Loading and Validation\n\nThe GPU performance configuration is loaded and validated through the YAML configuration system with error handling for missing or malformed data.\n\n### Configuration Loading Process\n\n```mermaid\nflowchart TD\n    subgraph \"load_yaml_config Function\"\n        start[\"load_yaml_config(file_path)\"] --> open[\"Open config.yaml\"]\n        open --> parse[\"yaml.safe_load(data)\"]\n        parse --> validate[\"Validate gpu_performance section\"]\n        validate --> return[\"Return configuration dict\"]\n        \n        parse -->|\"FileNotFoundError\"| error1[\"Raise FileNotFoundError\"]\n        parse -->|\"YAMLError\"| error2[\"Raise ValueError\"]\n    end\n```\n\n### Configuration Structure Access\n\nThe loaded configuration provides access to all GPU performance data:\n\n```python\ngpu_data = load_yaml_config(\"config.yaml\")\nGPU_TFLOPS_FP16 = gpu_data[\"gpu_performance\"][\"GPU_TFLOPS_FP16\"]\nGPU_TFLOPS_FP32 = gpu_data[\"gpu_performance\"][\"GPU_TFLOPS_FP32\"] \nGPU_AVRAM = gpu_data[\"gpu_performance\"][\"GPU_AVRAM\"]\ntolerance_pairs = gpu_data[\"gpu_performance\"][\"gpu_tolerance_pairs\"]\n```\n\n### Database Integration\n\nGPU configuration data is persisted using database functions:\n- `update_pog_stats()`: Stores GPU name and count for miners\n- `get_pog_specs()`: Retrieves most recent GPU specifications\n- `write_stats()`: Stores comprehensive performance data with JSON serialization\n\nSources: [neurons/Validator/pog.py:14-26](), [neurons/Validator/database/pog.py:24-98]()\n\n## Implementation Details\n\n### Key Components\n\n1. **GPU Performance Configuration**: Defined in `config.yaml`\n2. **Score Calculation Logic**: Implemented in `neurons/Validator/calculate_pow_score.py`\n3. **GPU Data Storage**: Managed by functions in `neurons/Validator/database/pog.py`\n4. **Mathematical Utilities**: Provided in `compute/utils/math.py`\n\n### Database Interaction\n\nGPU specifications are stored in the database using JSON serialization:\n\n```python\n# Convert dict to JSON string for storage\nif isinstance(raw_specs, dict):\n    gpu_specs = json.dumps(raw_specs)\nelse:\n    gpu_specs = raw_specs\n\n# When retrieving\nraw_gpu_specs = row[2]\nif raw_gpu_specs:\n    try:\n        gpu_specs = json.loads(raw_gpu_specs)  # Convert from JSON -> dict\n    except Exception as e:\n        gpu_specs = None\n```\n\nThis allows flexible storage of different GPU configurations while maintaining a structured database schema.\n\nSources: [neurons/Validator/database/pog.py:100-186]()",
    "resolved_links": [
      {
        "text": "compute/__init__.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py",
        "original_deepwiki_href": "compute/__init__.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "compute/utils/math.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/math.py",
        "original_deepwiki_href": "compute/utils/math.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "config.yaml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml",
        "original_deepwiki_href": "config.yaml",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/database/miner.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/miner.py",
        "original_deepwiki_href": "neurons/Validator/database/miner.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/database/pog.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py",
        "original_deepwiki_href": "neurons/Validator/database/pog.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/miner_script_m_merkletree.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/miner_script_m_merkletree.py",
        "original_deepwiki_href": "neurons/Validator/miner_script_m_merkletree.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/Validator/pog.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py",
        "original_deepwiki_href": "neurons/Validator/pog.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "config.yaml:1-104",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "compute/__init__.py:37-48",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "config.yaml:1-94",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "config.yaml:63-73",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/pog.py:27-73",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "config.yaml:95-104",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/pog.py:75-340",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/pog.py:101-146",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/miner_script_m_merkletree.py:21-388",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/pog.py:14-26",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/pog.py:24-98",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/Validator/database/pog.py:100-186",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Command-line Arguments",
        "href": "/configuration/command-line-arguments#7.1",
        "original_deepwiki_href": "#7.1",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph LR\n    subgraph \"gpu_tolerance_pairs Configuration\"\n        L40[\"NVIDIA L40\"] <--> RTX6000[\"NVIDIA RTX 6000 Ada Generation\"]\n        A100PCIe[\"NVIDIA A100 80GB PCIe\"] <--> A100SXM[\"NVIDIA A100-SXM4-80GB\"] \n        H100_80GB[\"NVIDIA H100 80GB HBM3\"] <--> H100[\"NVIDIA H100\"]\n        A40[\"NVIDIA A40\"] <--> RTXA6000[\"NVIDIA RTX A6000\"]\n        RTXA5000[\"NVIDIA RTX A5000\"] <--> RTX4000[\"NVIDIA RTX 4000 Ada Generation\"]\n    end",
      "flowchart TD\n    subgraph \"Merkle Proof Verification Process\"\n        validator[\"Validator\"] -->|\"send_script_and_request_hash\"| ssh[\"SSH Connection\"]\n        ssh -->|\"execute_script_on_miner\"| script[\"miner_script_m_merkletree.py\"]\n        script -->|\"generate_matrix_torch\"| matrices[\"Matrix Generation\"]\n        matrices -->|\"build_merkle_tree_rows\"| tree[\"Merkle Tree\"]\n        tree -->|\"get_merkle_proof_row\"| proof[\"Merkle Proofs\"]\n        proof -->|\"verify_merkle_proof_row\"| validator\n        validator -->|\"verify_responses\"| result[\"Verification Result\"]\n    end",
      "flowchart TD\n    subgraph \"GPU Benchmarking Process\"\n        start[\"Validator initiates PoG\"] --> send[\"send_script_and_request_hash()\"]\n        send --> verify[\"Verify script hash\"]\n        verify --> benchmark[\"execute_script_on_miner(mode='benchmark')\"]\n        benchmark --> parse[\"parse_benchmark_output()\"]\n        parse --> compute[\"execute_script_on_miner(mode='compute')\"]\n        compute --> merkle[\"parse_merkle_output()\"]\n        merkle --> proof[\"execute_script_on_miner(mode='proof')\"]\n        proof --> validate[\"verify_responses()\"]\n        validate --> result[\"GPU identification & scoring\"]\n    end",
      "flowchart TD\n    subgraph \"identify_gpu Function Flow\"\n        input[\"Input: fp16_tflops, fp32_tflops, estimated_avram, reported_name\"] \n        input --> calculate[\"Calculate combined_scores for all GPU models\"]\n        calculate --> deviation[\"fp16_deviation + fp32_deviation + avram_deviation / 3\"]\n        deviation --> sort[\"Sort by lowest deviation score\"]\n        sort --> identify[\"identified_gpu = best_match\"]\n        identify --> tolerance{\"Check tolerance_pairs\"}\n        tolerance -->|\"Match found\"| adjust[\"Apply tolerance adjustment\"]\n        tolerance -->|\"No match\"| return[\"Return identified_gpu\"]\n        adjust --> return\n    end",
      "flowchart TD\n    subgraph \"load_yaml_config Function\"\n        start[\"load_yaml_config(file_path)\"] --> open[\"Open config.yaml\"]\n        open --> parse[\"yaml.safe_load(data)\"]\n        parse --> validate[\"Validate gpu_performance section\"]\n        validate --> return[\"Return configuration dict\"]\n        \n        parse -->|\"FileNotFoundError\"| error1[\"Raise FileNotFoundError\"]\n        parse -->|\"YAMLError\"| error2[\"Raise ValueError\"]\n    end"
    ],
    "potential_frontmatter": {
      "title": "GPU Performance Configuration"
    }
  },
  "/neuralinternet/ni-compute/8-cli-tools": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/8-cli-tools",
    "title": "CLI Tools",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/8-cli-tools",
    "level": 0,
    "target_astro_path": "/cli-tools",
    "main_markdown_content": "# CLI Tools\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/register.py](neurons/register.py)\n\n</details>\n\n\n\nThis document covers the command-line utilities provided by the NI Compute Subnet for interacting with the compute resource marketplace. The primary CLI tool is the Registration CLI, which enables users to allocate, deallocate, and manage compute resources on the network.\n\nFor information about the underlying Resource Allocation API that these tools interact with, see [Resource Allocation API](#4). For details about the communication protocols used, see [Communication Protocols](#5).\n\n## Registration CLI Tool\n\nThe main CLI tool is implemented in `neurons/register.py` and provides a comprehensive interface for managing compute resources on the subnet. This tool serves as the primary user interface for validators and resource consumers to interact with the decentralized compute marketplace.\n\n### CLI Command Architecture\n\nThe Registration CLI follows a command-driven architecture with the following components:\n\n```mermaid\ngraph TB\n    subgraph \"CLI Interface Layer\"\n        MAIN[\"main()\"]\n        PARSER[\"argparse.ArgumentParser\"]\n        SUBPARSERS[\"command subparsers\"]\n    end\n    \n    subgraph \"Core Commands\"\n        ALLOCATE[\"allocate()\"]\n        ALLOCATE_HOTKEY[\"allocate_hotkey()\"]\n        DEALLOCATE[\"deallocate()\"]\n        LIST_ALLOC[\"list_allocations()\"]\n        LIST_HOTKEYS[\"list_allocations_hotkeys()\"]\n        LIST_RESOURCES[\"list_resources()\"]\n        PENALIZE[\"penalize_hotkey()\"]\n        DEPENALIZE[\"depenalize_hotkey()\"]\n        LIST_PENALTIES[\"list_penalizations()\"]\n    end\n    \n    subgraph \"Configuration Layer\"\n        GET_CONFIG[\"get_config()\"]\n        GET_CONFIG_CLI[\"get_config_cli()\"]\n    end\n    \n    subgraph \"Backend Integration\"\n        ALLOC_CONTAINER[\"allocate_container()\"]\n        ALLOC_CONTAINER_HK[\"allocate_container_hotkey()\"]\n        UPDATE_ALLOC_DB[\"update_allocation_db()\"]\n        UPDATE_ALLOC_WANDB[\"update_allocation_wandb()\"]\n    end\n    \n    MAIN --> PARSER\n    PARSER --> SUBPARSERS\n    SUBPARSERS --> ALLOCATE\n    SUBPARSERS --> ALLOCATE_HOTKEY\n    SUBPARSERS --> DEALLOCATE\n    SUBPARSERS --> LIST_ALLOC\n    SUBPARSERS --> LIST_HOTKEYS\n    SUBPARSERS --> LIST_RESOURCES\n    SUBPARSERS --> PENALIZE\n    SUBPARSERS --> DEPENALIZE\n    SUBPARSERS --> LIST_PENALTIES\n    \n    ALLOCATE --> GET_CONFIG_CLI\n    ALLOCATE --> ALLOC_CONTAINER\n    ALLOCATE_HOTKEY --> GET_CONFIG\n    ALLOCATE_HOTKEY --> ALLOC_CONTAINER_HK\n    DEALLOCATE --> UPDATE_ALLOC_DB\n    DEALLOCATE --> UPDATE_ALLOC_WANDB\n```\n\nSources: [neurons/register.py:781-854]()\n\n### Available Commands\n\nThe CLI provides the following commands for resource management:\n\n| Command | Function | Description |\n|---------|----------|-------------|\n| `a` | `allocate()` | Allocate resource via device requirements (GPU) |\n| `a_hotkey` | `allocate_hotkey()` | Allocate resource via specific hotkey |\n| `d` | `deallocate()` | De-allocate resource(s) |\n| `list_a` | `list_allocations()` | List allocated resources |\n| `list_ah` | `list_allocations_hotkeys()` | List allocated resource hotkeys |\n| `list_r` | `list_resources()` | List available resources |\n| `p_hotkey` | `penalize_hotkey()` | Penalize resource via hotkey |\n| `dp_hotkey` | `depenalize_hotkey()` | De-penalize resource via hotkey |\n| `list_p` | `list_penalizations()` | List penalized hotkeys |\n\nSources: [neurons/register.py:790-827]()\n\n## Resource Allocation Flow\n\nThe allocation process involves several steps from CLI input to resource provisioning:\n\n```mermaid\nsequenceDiagram\n    participant USER as \"User\"\n    participant CLI as \"register.py\"\n    participant CONFIG as \"get_config_cli()\"\n    participant ALLOC as \"allocate_container()\"\n    participant BT as \"bt.wallet/bt.subtensor\"\n    participant DB as \"ComputeDb\"\n    participant DENDRITE as \"bt.dendrite\"\n    participant MINER as \"Miner Axon\"\n    participant WANDB as \"ComputeWandb\"\n    \n    USER->>CLI: \"a\" command\n    CLI->>CONFIG: \"get_config_cli()\"\n    CONFIG->>USER: \"prompt for GPU specs\"\n    USER->>CONFIG: \"gpu_type, gpu_size\"\n    CLI->>ALLOC: \"allocate_container(device_requirement)\"\n    \n    ALLOC->>BT: \"create wallet/subtensor/dendrite\"\n    ALLOC->>BT: \"subtensor.metagraph(netuid)\"\n    ALLOC->>DB: \"select_allocate_miners_hotkey()\"\n    ALLOC->>DENDRITE: \"query(axons, Allocate, checking=True)\"\n    MINER->>ALLOC: \"availability response\"\n    \n    ALLOC->>DENDRITE: \"query(best_axon, Allocate, checking=False)\"\n    MINER->>ALLOC: \"allocation response with SSH details\"\n    ALLOC->>CLI: \"encrypted connection info\"\n    CLI->>CLI: \"rsa.decrypt_data()\"\n    CLI->>DB: \"update_allocation_db()\"\n    CLI->>WANDB: \"update_allocation_wandb()\"\n    CLI->>USER: \"display SSH connection details\"\n```\n\nSources: [neurons/register.py:230-288](), [neurons/register.py:117-180]()\n\n## System Integration\n\nThe CLI tool integrates with multiple system components to provide comprehensive resource management:\n\n```mermaid\ngraph TB\n    subgraph \"CLI Layer\"\n        REGISTER[\"neurons/register.py\"]\n        COMMANDS[\"CLI Commands\"]\n    end\n    \n    subgraph \"Bittensor Network\"\n        WALLET[\"bt.wallet\"]\n        SUBTENSOR[\"bt.subtensor\"]\n        DENDRITE[\"bt.dendrite\"]\n        METAGRAPH[\"metagraph\"]\n    end\n    \n    subgraph \"Protocols\"\n        ALLOCATE_PROTO[\"Allocate Protocol\"]\n    end\n    \n    subgraph \"Data Layer\"\n        COMPUTEDB[\"ComputeDb\"]\n        ALLOCATION_TABLE[\"allocation table\"]\n        BLACKLIST_TABLE[\"blacklist table\"]\n    end\n    \n    subgraph \"Monitoring\"\n        COMPUTEWANDB[\"ComputeWandb\"]\n        ALLOCATED_HOTKEYS[\"allocated_hotkeys\"]\n        PENALIZED_HOTKEYS[\"penalized_hotkeys\"]\n    end\n    \n    subgraph \"Security\"\n        RSA[\"RSAEncryption\"]\n        KEYPAIR[\"generate_key_pair()\"]\n        DECRYPT[\"decrypt_data()\"]\n    end\n    \n    REGISTER --> COMMANDS\n    COMMANDS --> WALLET\n    COMMANDS --> SUBTENSOR\n    COMMANDS --> DENDRITE\n    SUBTENSOR --> METAGRAPH\n    \n    COMMANDS --> ALLOCATE_PROTO\n    DENDRITE --> ALLOCATE_PROTO\n    \n    COMMANDS --> COMPUTEDB\n    COMPUTEDB --> ALLOCATION_TABLE\n    COMPUTEDB --> BLACKLIST_TABLE\n    \n    COMMANDS --> COMPUTEWANDB\n    COMPUTEWANDB --> ALLOCATED_HOTKEYS\n    COMPUTEWANDB --> PENALIZED_HOTKEYS\n    \n    COMMANDS --> RSA\n    RSA --> KEYPAIR\n    RSA --> DECRYPT\n```\n\nSources: [neurons/register.py:27-39](), [neurons/register.py:118-135]()\n\n## Configuration and Setup\n\nThe CLI tool supports two configuration modes:\n\n### Command-line Configuration\nThe `get_config()` function provides standard configuration parsing for non-interactive use:\n\n```python\n# Key configuration parameters\n--netuid         # Subnet UID (default: 1)\n--gpu_type       # Required GPU type\n--gpu_size       # Required GPU memory in MB\n--wallet.*       # Wallet configuration\n--subtensor.*    # Subtensor configuration\n--logging.*      # Logging configuration\n```\n\n### Interactive Configuration\nThe `get_config_cli()` function provides interactive prompts for user-friendly operation:\n\n- Prompts for GPU type if not provided\n- Prompts for GPU memory in GB (converted to MB)\n- Sets up logging directory structure\n\nSources: [neurons/register.py:43-75](), [neurons/register.py:79-113]()\n\n## Resource Management Operations\n\n### Allocation Operations\nThe CLI provides two allocation methods:\n\n1. **Device-based allocation** (`allocate()`): Searches for resources matching specific hardware requirements\n2. **Hotkey-based allocation** (`allocate_hotkey()`): Allocates resources from a specific miner hotkey\n\nBoth methods return SSH connection details for accessing allocated resources.\n\n### Deallocation Operations\nThe `deallocate()` function supports:\n- Single or multiple hotkey deallocation\n- Database state updates before network communication\n- Graceful error handling for offline miners\n\n### Resource Listing Operations\nThe CLI provides multiple listing commands:\n- `list_allocations()`: Detailed view of allocated resources\n- `list_allocations_hotkeys()`: Simplified hotkey-only view\n- `list_resources()`: Comprehensive network resource overview with availability status\n\nSources: [neurons/register.py:230-288](), [neurons/register.py:350-445](), [neurons/register.py:446-643]()",
    "resolved_links": [
      {
        "text": "neurons/register.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py",
        "original_deepwiki_href": "neurons/register.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/register.py:781-854",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:790-827",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:230-288",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:117-180",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:27-39",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:118-135",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:43-75",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:79-113",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:350-445",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:446-643",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Resource Allocation API",
        "href": "/resource-allocation-api#4",
        "original_deepwiki_href": "#4",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Communication Protocols",
        "href": "/communication-protocols#5",
        "original_deepwiki_href": "#5",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    subgraph \"CLI Interface Layer\"\n        MAIN[\"main()\"]\n        PARSER[\"argparse.ArgumentParser\"]\n        SUBPARSERS[\"command subparsers\"]\n    end\n    \n    subgraph \"Core Commands\"\n        ALLOCATE[\"allocate()\"]\n        ALLOCATE_HOTKEY[\"allocate_hotkey()\"]\n        DEALLOCATE[\"deallocate()\"]\n        LIST_ALLOC[\"list_allocations()\"]\n        LIST_HOTKEYS[\"list_allocations_hotkeys()\"]\n        LIST_RESOURCES[\"list_resources()\"]\n        PENALIZE[\"penalize_hotkey()\"]\n        DEPENALIZE[\"depenalize_hotkey()\"]\n        LIST_PENALTIES[\"list_penalizations()\"]\n    end\n    \n    subgraph \"Configuration Layer\"\n        GET_CONFIG[\"get_config()\"]\n        GET_CONFIG_CLI[\"get_config_cli()\"]\n    end\n    \n    subgraph \"Backend Integration\"\n        ALLOC_CONTAINER[\"allocate_container()\"]\n        ALLOC_CONTAINER_HK[\"allocate_container_hotkey()\"]\n        UPDATE_ALLOC_DB[\"update_allocation_db()\"]\n        UPDATE_ALLOC_WANDB[\"update_allocation_wandb()\"]\n    end\n    \n    MAIN --> PARSER\n    PARSER --> SUBPARSERS\n    SUBPARSERS --> ALLOCATE\n    SUBPARSERS --> ALLOCATE_HOTKEY\n    SUBPARSERS --> DEALLOCATE\n    SUBPARSERS --> LIST_ALLOC\n    SUBPARSERS --> LIST_HOTKEYS\n    SUBPARSERS --> LIST_RESOURCES\n    SUBPARSERS --> PENALIZE\n    SUBPARSERS --> DEPENALIZE\n    SUBPARSERS --> LIST_PENALTIES\n    \n    ALLOCATE --> GET_CONFIG_CLI\n    ALLOCATE --> ALLOC_CONTAINER\n    ALLOCATE_HOTKEY --> GET_CONFIG\n    ALLOCATE_HOTKEY --> ALLOC_CONTAINER_HK\n    DEALLOCATE --> UPDATE_ALLOC_DB\n    DEALLOCATE --> UPDATE_ALLOC_WANDB",
      "sequenceDiagram\n    participant USER as \"User\"\n    participant CLI as \"register.py\"\n    participant CONFIG as \"get_config_cli()\"\n    participant ALLOC as \"allocate_container()\"\n    participant BT as \"bt.wallet/bt.subtensor\"\n    participant DB as \"ComputeDb\"\n    participant DENDRITE as \"bt.dendrite\"\n    participant MINER as \"Miner Axon\"\n    participant WANDB as \"ComputeWandb\"\n    \n    USER->>CLI: \"a\" command\n    CLI->>CONFIG: \"get_config_cli()\"\n    CONFIG->>USER: \"prompt for GPU specs\"\n    USER->>CONFIG: \"gpu_type, gpu_size\"\n    CLI->>ALLOC: \"allocate_container(device_requirement)\"\n    \n    ALLOC->>BT: \"create wallet/subtensor/dendrite\"\n    ALLOC->>BT: \"subtensor.metagraph(netuid)\"\n    ALLOC->>DB: \"select_allocate_miners_hotkey()\"\n    ALLOC->>DENDRITE: \"query(axons, Allocate, checking=True)\"\n    MINER->>ALLOC: \"availability response\"\n    \n    ALLOC->>DENDRITE: \"query(best_axon, Allocate, checking=False)\"\n    MINER->>ALLOC: \"allocation response with SSH details\"\n    ALLOC->>CLI: \"encrypted connection info\"\n    CLI->>CLI: \"rsa.decrypt_data()\"\n    CLI->>DB: \"update_allocation_db()\"\n    CLI->>WANDB: \"update_allocation_wandb()\"\n    CLI->>USER: \"display SSH connection details\"",
      "graph TB\n    subgraph \"CLI Layer\"\n        REGISTER[\"neurons/register.py\"]\n        COMMANDS[\"CLI Commands\"]\n    end\n    \n    subgraph \"Bittensor Network\"\n        WALLET[\"bt.wallet\"]\n        SUBTENSOR[\"bt.subtensor\"]\n        DENDRITE[\"bt.dendrite\"]\n        METAGRAPH[\"metagraph\"]\n    end\n    \n    subgraph \"Protocols\"\n        ALLOCATE_PROTO[\"Allocate Protocol\"]\n    end\n    \n    subgraph \"Data Layer\"\n        COMPUTEDB[\"ComputeDb\"]\n        ALLOCATION_TABLE[\"allocation table\"]\n        BLACKLIST_TABLE[\"blacklist table\"]\n    end\n    \n    subgraph \"Monitoring\"\n        COMPUTEWANDB[\"ComputeWandb\"]\n        ALLOCATED_HOTKEYS[\"allocated_hotkeys\"]\n        PENALIZED_HOTKEYS[\"penalized_hotkeys\"]\n    end\n    \n    subgraph \"Security\"\n        RSA[\"RSAEncryption\"]\n        KEYPAIR[\"generate_key_pair()\"]\n        DECRYPT[\"decrypt_data()\"]\n    end\n    \n    REGISTER --> COMMANDS\n    COMMANDS --> WALLET\n    COMMANDS --> SUBTENSOR\n    COMMANDS --> DENDRITE\n    SUBTENSOR --> METAGRAPH\n    \n    COMMANDS --> ALLOCATE_PROTO\n    DENDRITE --> ALLOCATE_PROTO\n    \n    COMMANDS --> COMPUTEDB\n    COMPUTEDB --> ALLOCATION_TABLE\n    COMPUTEDB --> BLACKLIST_TABLE\n    \n    COMMANDS --> COMPUTEWANDB\n    COMPUTEWANDB --> ALLOCATED_HOTKEYS\n    COMPUTEWANDB --> PENALIZED_HOTKEYS\n    \n    COMMANDS --> RSA\n    RSA --> KEYPAIR\n    RSA --> DECRYPT"
    ],
    "potential_frontmatter": {
      "title": "CLI Tools"
    }
  },
  "/neuralinternet/ni-compute/8.1-registration-cli": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/8.1-registration-cli",
    "title": "Registration CLI",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/8.1-registration-cli",
    "level": 1,
    "target_astro_path": "/cli-tools/registration-cli",
    "main_markdown_content": "# Registration CLI\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [neurons/register.py](neurons/register.py)\n\n</details>\n\n\n\nThe Registration CLI is a command-line interface tool for interacting with the NI Compute Subnet resource allocation system. It provides commands for allocating compute resources from miners, deallocating them when finished, and managing the penalty system for network validators.\n\nThis tool is primarily used by network participants who need to allocate GPU compute resources from miners on the subnet. For information about the underlying Resource Allocation API that this CLI interacts with, see [Resource Allocation API](#4). For details about the validator system that manages resource scoring, see [Validator System](#2).\n\n## Overview\n\nThe Registration CLI (`neurons/register.py`) serves as the primary user interface for resource allocation operations on the NI Compute Subnet. It connects to the Bittensor network to discover available miners, negotiate resource allocation, and manage the lifecycle of compute allocations.\n\n### CLI Command Structure\n\nThe CLI operates in an interactive loop, accepting single-letter commands for different operations:\n\n| Command | Function | Description |\n|---------|----------|-------------|\n| `a` | `allocate()` | Allocate resources by GPU requirements |\n| `a_hotkey` | `allocate_hotkey()` | Allocate resources by specific miner hotkey |\n| `d` | `deallocate()` | Deallocate resources |\n| `list_a` | `list_allocations()` | List currently allocated resources |\n| `list_ah` | `list_allocations_hotkeys()` | List allocated resource hotkeys |\n| `list_r` | `list_resources()` | List all available resources on network |\n| `p_hotkey` | `penalize_hotkey()` | Add miner to penalty blacklist |\n| `dp_hotkey` | `depenalize_hotkey()` | Remove miner from penalty blacklist |\n| `list_p` | `list_penalizations()` | List penalized miners |\n\nSources: [neurons/register.py:790-830]()\n\n## System Architecture\n\n### CLI Integration with Network Components\n\n```mermaid\ngraph TD\n    CLI[\"`register.py`<br/>CLI Interface\"]\n    CONFIG[\"`get_config()`<br/>Configuration Parser\"]\n    WALLET[\"`bt.wallet`<br/>Bittensor Wallet\"]\n    SUBTENSOR[\"`bt.subtensor`<br/>Blockchain Connection\"]\n    DENDRITE[\"`bt.dendrite`<br/>RPC Client\"]\n    METAGRAPH[\"`metagraph`<br/>Network State\"]\n    \n    subgraph \"Local State Management\"\n        COMPUTEDB[\"`ComputeDb`<br/>SQLite Database\"]\n        WANDB[\"`ComputeWandb`<br/>Distributed State\"]\n    end\n    \n    subgraph \"Network Communication\" \n        ALLOCATE_PROTOCOL[\"`Allocate`<br/>Protocol Messages\"]\n        RSA[\"`RSAEncryption`<br/>Secure Communication\"]\n    end\n    \n    subgraph \"Miner Network\"\n        MINERS[\"`Miners`<br/>Resource Providers\"]\n        CONTAINERS[\"`Docker Containers`<br/>Allocated Resources\"]\n    end\n    \n    CLI --> CONFIG\n    CLI --> WALLET\n    CLI --> SUBTENSOR\n    CLI --> DENDRITE\n    CLI --> COMPUTEDB\n    CLI --> WANDB\n    \n    SUBTENSOR --> METAGRAPH\n    DENDRITE --> ALLOCATE_PROTOCOL\n    ALLOCATE_PROTOCOL --> RSA\n    \n    DENDRITE --> MINERS\n    MINERS --> CONTAINERS\n    \n    COMPUTEDB --> WANDB\n```\n\nSources: [neurons/register.py:43-75](), [neurons/register.py:117-180](), [neurons/register.py:784-787]()\n\n### Resource Allocation Flow\n\n```mermaid\nsequenceDiagram\n    participant CLI as \"register.py CLI\"\n    participant DB as \"ComputeDb\"\n    participant WANDB as \"ComputeWandb\" \n    participant METAGRAPH as \"metagraph\"\n    participant DENDRITE as \"dendrite\"\n    participant MINER as \"Miner Axon\"\n    participant RSA as \"RSAEncryption\"\n    \n    Note over CLI,MINER: Resource Allocation Process\n    \n    CLI->>DB: \"select_allocate_miners_hotkey()\"\n    DB-->>CLI: \"candidate_hotkeys[]\"\n    \n    CLI->>METAGRAPH: \"Query network state\"\n    METAGRAPH-->>CLI: \"axon_candidates[]\"\n    \n    CLI->>DENDRITE: \"Query candidates (checking=True)\"\n    DENDRITE->>MINER: \"Allocate(checking=True)\"\n    MINER-->>DENDRITE: \"{'status': True/False}\"\n    DENDRITE-->>CLI: \"responses[]\"\n    \n    CLI->>CLI: \"Filter available candidates\"\n    CLI->>CLI: \"Sort by metagraph scores\"\n    \n    loop \"For each sorted candidate\"\n        CLI->>RSA: \"generate_key_pair()\"\n        RSA-->>CLI: \"private_key, public_key\"\n        \n        CLI->>DENDRITE: \"Query allocation (checking=False)\"\n        DENDRITE->>MINER: \"Allocate(public_key, timeline)\"\n        MINER-->>DENDRITE: \"{'status': True, 'info': encrypted_data}\"\n        DENDRITE-->>CLI: \"allocation_response\"\n        \n        alt \"Allocation successful\"\n            CLI->>RSA: \"decrypt_data()\"\n            RSA-->>CLI: \"ssh_credentials\"\n            CLI->>DB: \"update_allocation_db()\"\n            CLI->>WANDB: \"update_allocated_hotkeys()\"\n            CLI->>CLI: \"Display SSH details\"\n        end\n    end\n```\n\nSources: [neurons/register.py:117-180](), [neurons/register.py:230-288]()\n\n## Core Functions\n\n### Resource Allocation Functions\n\n#### `allocate_container()`\nPrimary allocation function that finds and allocates resources based on device requirements.\n\n**Key Operations:**\n- Queries `ComputeDb` using `select_allocate_miners_hotkey()` to find candidate miners\n- Filters candidates through availability check (`checking=True`)\n- Sorts candidates by metagraph scores\n- Attempts allocation with highest-scored available miner\n- Returns encrypted SSH connection details\n\n**Parameters:**\n- `device_requirement`: Hardware specifications (GPU type, memory, CPU, RAM, disk)\n- `timeline`: Allocation duration in minutes\n- `public_key`: RSA public key for secure communication\n\nSources: [neurons/register.py:117-180]()\n\n#### `allocate_container_hotkey()`\nDirect allocation to a specific miner by hotkey, bypassing candidate selection.\n\n**Key Differences:**\n- Targets specific miner hotkey directly\n- Uses fixed device requirements structure\n- Includes `docker_requirement` for container specifications\n\nSources: [neurons/register.py:184-227]()\n\n### Deallocation Functions\n\n#### `deallocate()`\nHandles resource deallocation for multiple hotkeys simultaneously.\n\n**Process Flow:**\n1. Query local database for allocation details by hotkey\n2. Update database and WandB to mark as deallocated\n3. Send deallocation request to miner via `Allocate` protocol\n4. Handle batch processing for multiple hotkeys\n\n**Key Features:**\n- Supports comma-separated multiple hotkeys\n- Immediate database update before network communication\n- Error handling for missing hotkeys or network failures\n\nSources: [neurons/register.py:350-445]()\n\n### Information Display Functions\n\n#### `list_resources()`\nDisplays comprehensive network resource overview with GPU specifications and availability status.\n\n**Display Components:**\n- Tabular format with hotkey, GPU details, CPU, RAM, storage\n- Resource availability status (Available/Reserved)\n- Summary statistics for GPU instances and total counts\n- Integration with WandB allocated hotkeys for status updates\n\nSources: [neurons/register.py:531-643]()\n\n#### `list_allocations()`\nShows detailed information about currently allocated resources including SSH connection details.\n\n**Information Displayed:**\n- Allocation ID, hotkey, resource type\n- SSH credentials (username, password, port, IP)\n- Ready-to-use SSH commands\n\nSources: [neurons/register.py:446-488]()\n\n### Penalty Management Functions\n\n#### `penalize_hotkey()` and `depenalize_hotkey()`\nAdministrative functions for managing miner penalties through blacklist system.\n\n**Penalty Process:**\n- Validates hotkeys against network miner details\n- Updates local `blacklist` table via `update_blacklist_db()`\n- Synchronizes penalty status with WandB distributed state\n- Supports batch operations for multiple hotkeys\n\nSources: [neurons/register.py:671-762]()\n\n## Configuration and Setup\n\n### Configuration Parsers\n\nThe CLI provides two configuration modes:\n\n#### `get_config()`\nStandard argument parsing for programmatic usage with command-line parameters.\n\n#### `get_config_cli()`\nInteractive configuration that prompts for missing GPU requirements when not provided via command line.\n\n**Interactive Prompts:**\n- GPU type selection\n- GPU memory specification (converted from GB to MB)\n- Automatic logging directory setup\n\nSources: [neurons/register.py:43-113]()\n\n### Database Integration\n\nThe CLI integrates with multiple data persistence layers:\n\n#### Local Database Operations\n- **ComputeDb**: SQLite database for allocation tracking\n- **Tables Used**: `allocation`, `blacklist`, `miner`, `stats`\n- **Key Operations**: Resource queries, allocation updates, penalty management\n\n#### Distributed State Management\n- **ComputeWandb**: Synchronization with distributed validator state\n- **Functions**: `update_allocated_hotkeys()`, `update_penalized_hotkeys()`\n- **Purpose**: Cross-validator consistency for resource allocation status\n\nSources: [neurons/register.py:35](), [neurons/register.py:644-670]()\n\n## Security and Communication\n\n### RSA Encryption Integration\nAll resource allocation communications use RSA encryption for secure credential exchange:\n\n```mermaid\ngraph LR\n    CLI[\"`CLI`<br/>register.py\"]\n    RSA_GEN[\"`generate_key_pair()`<br/>RSA Key Generation\"]\n    MINER[\"`Miner`<br/>Resource Provider\"]\n    RSA_DECRYPT[\"`decrypt_data()`<br/>RSA Decryption\"]\n    \n    CLI --> RSA_GEN\n    RSA_GEN --> CLI\n    CLI -->|\"public_key\"| MINER\n    MINER -->|\"encrypted credentials\"| CLI\n    CLI --> RSA_DECRYPT\n    RSA_DECRYPT --> CLI\n```\n\n**Security Features:**\n- Unique key pair generation for each allocation\n- Encrypted SSH credential transmission\n- Base64 encoding for network transport\n\nSources: [neurons/register.py:31](), [neurons/register.py:237](), [neurons/register.py:244]()\n\n### Protocol Communication\nUses Bittensor's `Allocate` protocol for standardized miner communication:\n\n**Protocol Parameters:**\n- `timeline`: Allocation duration\n- `device_requirement`: Hardware specifications\n- `checking`: Boolean flag for availability check vs actual allocation\n- `public_key`: RSA public key for encryption\n- `docker_requirement`: Container specifications (optional)\n\nSources: [neurons/register.py:32](), [neurons/register.py:144](), [neurons/register.py:168-170]()",
    "resolved_links": [
      {
        "text": "neurons/register.py",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py",
        "original_deepwiki_href": "neurons/register.py",
        "context": "collapsible_aside_link"
      },
      {
        "text": "neurons/register.py:790-830",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:43-75",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:117-180",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:784-787",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:230-288",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:184-227",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:350-445",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:531-643",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:446-488",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:671-762",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:43-113",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:35",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:644-670",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:31",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:237",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:244",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:32",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:144",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "neurons/register.py:168-170",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "Resource Allocation API",
        "href": "/resource-allocation-api#4",
        "original_deepwiki_href": "#4",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Validator System",
        "href": "/validator-system#2",
        "original_deepwiki_href": "#2",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TD\n    CLI[\"`register.py`<br/>CLI Interface\"]\n    CONFIG[\"`get_config()`<br/>Configuration Parser\"]\n    WALLET[\"`bt.wallet`<br/>Bittensor Wallet\"]\n    SUBTENSOR[\"`bt.subtensor`<br/>Blockchain Connection\"]\n    DENDRITE[\"`bt.dendrite`<br/>RPC Client\"]\n    METAGRAPH[\"`metagraph`<br/>Network State\"]\n    \n    subgraph \"Local State Management\"\n        COMPUTEDB[\"`ComputeDb`<br/>SQLite Database\"]\n        WANDB[\"`ComputeWandb`<br/>Distributed State\"]\n    end\n    \n    subgraph \"Network Communication\" \n        ALLOCATE_PROTOCOL[\"`Allocate`<br/>Protocol Messages\"]\n        RSA[\"`RSAEncryption`<br/>Secure Communication\"]\n    end\n    \n    subgraph \"Miner Network\"\n        MINERS[\"`Miners`<br/>Resource Providers\"]\n        CONTAINERS[\"`Docker Containers`<br/>Allocated Resources\"]\n    end\n    \n    CLI --> CONFIG\n    CLI --> WALLET\n    CLI --> SUBTENSOR\n    CLI --> DENDRITE\n    CLI --> COMPUTEDB\n    CLI --> WANDB\n    \n    SUBTENSOR --> METAGRAPH\n    DENDRITE --> ALLOCATE_PROTOCOL\n    ALLOCATE_PROTOCOL --> RSA\n    \n    DENDRITE --> MINERS\n    MINERS --> CONTAINERS\n    \n    COMPUTEDB --> WANDB",
      "sequenceDiagram\n    participant CLI as \"register.py CLI\"\n    participant DB as \"ComputeDb\"\n    participant WANDB as \"ComputeWandb\" \n    participant METAGRAPH as \"metagraph\"\n    participant DENDRITE as \"dendrite\"\n    participant MINER as \"Miner Axon\"\n    participant RSA as \"RSAEncryption\"\n    \n    Note over CLI,MINER: Resource Allocation Process\n    \n    CLI->>DB: \"select_allocate_miners_hotkey()\"\n    DB-->>CLI: \"candidate_hotkeys[]\"\n    \n    CLI->>METAGRAPH: \"Query network state\"\n    METAGRAPH-->>CLI: \"axon_candidates[]\"\n    \n    CLI->>DENDRITE: \"Query candidates (checking=True)\"\n    DENDRITE->>MINER: \"Allocate(checking=True)\"\n    MINER-->>DENDRITE: \"{'status': True/False}\"\n    DENDRITE-->>CLI: \"responses[]\"\n    \n    CLI->>CLI: \"Filter available candidates\"\n    CLI->>CLI: \"Sort by metagraph scores\"\n    \n    loop \"For each sorted candidate\"\n        CLI->>RSA: \"generate_key_pair()\"\n        RSA-->>CLI: \"private_key, public_key\"\n        \n        CLI->>DENDRITE: \"Query allocation (checking=False)\"\n        DENDRITE->>MINER: \"Allocate(public_key, timeline)\"\n        MINER-->>DENDRITE: \"{'status': True, 'info': encrypted_data}\"\n        DENDRITE-->>CLI: \"allocation_response\"\n        \n        alt \"Allocation successful\"\n            CLI->>RSA: \"decrypt_data()\"\n            RSA-->>CLI: \"ssh_credentials\"\n            CLI->>DB: \"update_allocation_db()\"\n            CLI->>WANDB: \"update_allocated_hotkeys()\"\n            CLI->>CLI: \"Display SSH details\"\n        end\n    end",
      "graph LR\n    CLI[\"`CLI`<br/>register.py\"]\n    RSA_GEN[\"`generate_key_pair()`<br/>RSA Key Generation\"]\n    MINER[\"`Miner`<br/>Resource Provider\"]\n    RSA_DECRYPT[\"`decrypt_data()`<br/>RSA Decryption\"]\n    \n    CLI --> RSA_GEN\n    RSA_GEN --> CLI\n    CLI -->|\"public_key\"| MINER\n    MINER -->|\"encrypted credentials\"| CLI\n    CLI --> RSA_DECRYPT\n    RSA_DECRYPT --> CLI"
    ],
    "potential_frontmatter": {
      "title": "Registration CLI"
    }
  },
  "/neuralinternet/ni-compute/9-development": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/9-development",
    "title": "Development",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/9-development",
    "level": 0,
    "target_astro_path": "/development",
    "main_markdown_content": "# Development\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/pre-commit.yml](.github/workflows/pre-commit.yml)\n- [.gitignore](.gitignore)\n- [.pre-commit-config.yaml](.pre-commit-config.yaml)\n- [pyproject.toml](pyproject.toml)\n- [requirements-dev.txt](requirements-dev.txt)\n- [scripts/check-branch-name.sh](scripts/check-branch-name.sh)\n- [scripts/check-current-branch.sh](scripts/check-current-branch.sh)\n\n</details>\n\n\n\nThis document covers the development workflows, code quality tools, and project structure for the NI Compute Subnet codebase. It includes pre-commit hooks, branch naming conventions, dependency management, and testing infrastructure that ensures code quality and maintainability.\n\nFor information about specific CLI tools used in development, see [CLI Tools](#8). For details about system configuration, see [Configuration](#7).\n\n## Development Workflow Overview\n\nThe development process enforces code quality through automated checks and standardized workflows. The system uses pre-commit hooks, branch naming validation, and continuous integration to maintain code standards.\n\n### Pre-commit Hook System\n\n```mermaid\ngraph TB\n    DEV[\"Developer\"]\n    COMMIT[\"git commit\"]\n    PRECOMMIT[\"pre-commit hooks\"]\n    \n    subgraph \"Code Quality Checks\"\n        TRAILING[\"trailing-whitespace\"]\n        EOF[\"end-of-file-fixer\"]\n        COMMITLINT[\"commitlint\"]\n        BRANCH[\"check-branch-name\"]\n        PIPREQS[\"pip-compile requirements\"]\n        PIPDEV[\"pip-compile dev requirements\"]\n        PYTEST[\"run-pytest\"]\n    end\n    \n    PUSH[\"git push\"]\n    GHACTIONS[\"GitHub Actions\"]\n    \n    DEV --> COMMIT\n    COMMIT --> PRECOMMIT\n    PRECOMMIT --> TRAILING\n    PRECOMMIT --> EOF\n    PRECOMMIT --> COMMITLINT\n    PRECOMMIT --> BRANCH\n    PRECOMMIT --> PIPREQS\n    PRECOMMIT --> PIPDEV\n    PRECOMMIT --> PYTEST\n    \n    TRAILING --> PUSH\n    EOF --> PUSH\n    COMMITLINT --> PUSH\n    BRANCH --> PUSH\n    PIPREQS --> PUSH\n    PIPDEV --> PUSH\n    PYTEST --> PUSH\n    \n    PUSH --> GHACTIONS\n```\n\nSources: [.pre-commit-config.yaml:1-64](), [.github/workflows/pre-commit.yml:1-22]()\n\n### Branch Naming Convention\n\nThe codebase enforces strict branch naming conventions through the `check-branch-name.sh` script:\n\n| Branch Type | Format | Example |\n|-------------|--------|---------|\n| Feature | `feat/CSN-XXXX-description` | `feat/CSN-1234-add-gpu-monitoring` |\n| Bugfix | `fix/CSN-XXXX-description` | `fix/CSN-5678-memory-leak` |\n| Hotfix | `hotfix/vX.Y.Z-CSN-XXXX-description` | `hotfix/v1.2.3-CSN-1234-critical-fix` |\n| Release | `release/vX.Y.Z` | `release/v1.2.3` |\n| Protected | `main`, `dev` | No validation required |\n\n**Branch Validation Rules:**\n- Must include JIRA ticket format: `CSN-XXXX` (where XXXX is any number of digits)\n- Description must be lowercase, kebab-case (hyphens only)\n- No underscores, spaces, or special characters in description\n- Must start with allowed prefix: `feat`, `fix`, `hotfix`, `chore`, `refactor`, `test`, `spike`, `prototype`, `release`, `docs`\n\nSources: [scripts/check-branch-name.sh:1-114](), [scripts/check-current-branch.sh:1-7]()\n\n## Code Quality Enforcement\n\n### Pre-commit Hook Configuration\n\nThe pre-commit system runs multiple quality checks before allowing commits:\n\n```mermaid\ngraph LR\n    subgraph \"File Quality\"\n        TRAIL[\"trailing-whitespace\"]\n        EOF[\"end-of-file-fixer\"]\n    end\n    \n    subgraph \"Commit Standards\"\n        COMMITLINT[\"commitlint<br/>@commitlint/config-conventional\"]\n        BRANCH[\"check-branch-name.sh\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        PIPMAIN[\"pip-compile<br/>requirements.txt\"]\n        PIPDEV[\"pip-compile<br/>requirements-dev.txt\"]\n    end\n    \n    subgraph \"Testing\"\n        PYTEST[\"pytest<br/>--alluredir allure-results\"]\n    end\n    \n    TRAIL --> COMMITLINT\n    EOF --> BRANCH\n    COMMITLINT --> PIPMAIN\n    BRANCH --> PIPDEV\n    PIPMAIN --> PYTEST\n    PIPDEV --> PYTEST\n```\n\nSources: [.pre-commit-config.yaml:1-64]()\n\n### Continuous Integration\n\nGitHub Actions runs the same pre-commit checks on pull requests and pushes to `dev` and `main` branches:\n\n```yaml\n# Workflow triggers\non:\n  pull_request:\n  push:\n    branches: [\"dev\", \"main\"]\n```\n\nThe CI pipeline installs dependencies and runs all pre-commit hooks to ensure code quality standards are maintained across the repository.\n\nSources: [.github/workflows/pre-commit.yml:1-22]()\n\n## Project Structure and Dependencies\n\n### Project Configuration\n\nThe project uses `pyproject.toml` for modern Python packaging and dependency management:\n\n```mermaid\ngraph TB\n    subgraph \"Build System\"\n        HATCH[\"hatchling>=1.24.2\"]\n        BUILD[\"hatchling.build\"]\n    end\n    \n    subgraph \"Project Metadata\"\n        NAME[\"NI-Compute\"]\n        DESC[\"NI Compute Subnet\"]\n        AUTHOR[\"neuralinternet.ai\"]\n        LICENSE[\"MIT\"]\n        PYTHON[\">=3.10\"]\n    end\n    \n    subgraph \"Dependencies\"\n        CORE[\"Core Dependencies<br/>bittensor, torch, docker\"]\n        DEV[\"Development Dependencies<br/>pytest, pre-commit, pip-tools\"]\n    end\n    \n    subgraph \"Package Structure\"\n        COMPUTE[\"compute/\"]\n        NEURONS[\"neurons/\"]\n    end\n    \n    HATCH --> BUILD\n    BUILD --> NAME\n    NAME --> CORE\n    CORE --> DEV\n    DEV --> COMPUTE\n    COMPUTE --> NEURONS\n```\n\nSources: [pyproject.toml:1-97]()\n\n### Dependency Management\n\nThe project uses `pip-tools` for dependency resolution and pinning:\n\n| File | Purpose | Generation Command |\n|------|---------|-------------------|\n| `requirements.txt` | Production dependencies | `pip-compile pyproject.toml` |\n| `requirements-dev.txt` | Development dependencies | `pip-compile --extra dev pyproject.toml` |\n\n**Key Dependencies:**\n- **Core**: `bittensor==9.0.0`, `torch==2.5.1`, `docker==7.0.0`\n- **GPU/Compute**: `GPUtil==1.4.0`, `igpu==0.1.2`, `numpy==2.0.2`\n- **Security**: `cryptography==43.0.1`, `paramiko==3.4.1`\n- **Monitoring**: `wandb==0.19.0`, `psutil==5.9.8`\n- **Development**: `pytest`, `pre-commit`, `allure-pytest`\n\nSources: [pyproject.toml:36-69](), [requirements-dev.txt:1-418]()\n\n### Testing Configuration\n\nThe project uses `pytest` with coverage reporting and Allure for test reporting:\n\n```toml\n[tool.pytest.ini_options]\naddopts = \"-v --cov=. --cov-report=term-missing\"\ntestpaths = [\"tests\"]\n```\n\nTests are automatically run during pre-commit with Allure results generation:\n```bash\npython -m pytest --alluredir allure-results\n```\n\nSources: [pyproject.toml:92-97](), [.pre-commit-config.yaml:56-63]()\n\n## File Organization and Exclusions\n\n### Git Ignore Patterns\n\nThe `.gitignore` file excludes development artifacts and sensitive data:\n\n```mermaid\ngraph TB\n    subgraph \"Python Artifacts\"\n        PYCACHE[\"__pycache__/\"]\n        PYCDLL[\"*.py[cod]\"]\n        DIST[\"dist/, build/, *.egg-info/\"]\n    end\n    \n    subgraph \"Development Tools\"\n        WANDB[\"wandb/\"]\n        PYTEST[\"allure-results, .pytest_cache/\"]\n        COVERAGE[\".coverage, htmlcov/\"]\n    end\n    \n    subgraph \"IDE and Editors\"\n        IDEA[\".idea/\"]\n        VSCODE[\".vscode/\"]\n        DSSTORE[\".DS_Store\"]\n    end\n    \n    subgraph \"Project Specific\"\n        DATABASE[\"database.db\"]\n        CERTS[\"cert/\"]\n        MINERAPP[\"neurons/Miner/app\"]\n        REGAPI[\"neurons/register-api/\"]\n    end\n    \n    PYCACHE --> WANDB\n    PYCDLL --> PYTEST\n    DIST --> COVERAGE\n    WANDB --> IDEA\n    PYTEST --> VSCODE\n    COVERAGE --> DSSTORE\n    IDEA --> DATABASE\n    VSCODE --> CERTS\n    DSSTORE --> MINERAPP\n    DATABASE --> REGAPI\n```\n\nSources: [.gitignore:1-263]()\n\n### Build Configuration\n\nThe project packages include:\n- `compute/` - Core compute subnet logic\n- `neurons/` - Validator and miner implementations\n\nBuild artifacts are excluded from the source distribution:\n```toml\n[tool.hatch.build.targets.sdist]\nexclude = [\"/.github\"]\n```\n\nVersion information is managed through `compute/__init__.py`:\n```toml\n[tool.hatch.version]\npath = \"compute/__init__.py\"\n```\n\nSources: [pyproject.toml:78-91]()",
    "resolved_links": [
      {
        "text": ".github/workflows/pre-commit.yml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml",
        "original_deepwiki_href": ".github/workflows/pre-commit.yml",
        "context": "collapsible_aside_link"
      },
      {
        "text": ".gitignore",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/.gitignore",
        "original_deepwiki_href": ".gitignore",
        "context": "collapsible_aside_link"
      },
      {
        "text": ".pre-commit-config.yaml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml",
        "original_deepwiki_href": ".pre-commit-config.yaml",
        "context": "collapsible_aside_link"
      },
      {
        "text": "pyproject.toml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml",
        "original_deepwiki_href": "pyproject.toml",
        "context": "collapsible_aside_link"
      },
      {
        "text": "requirements-dev.txt",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/requirements-dev.txt",
        "original_deepwiki_href": "requirements-dev.txt",
        "context": "collapsible_aside_link"
      },
      {
        "text": "scripts/check-branch-name.sh",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh",
        "original_deepwiki_href": "scripts/check-branch-name.sh",
        "context": "collapsible_aside_link"
      },
      {
        "text": "scripts/check-current-branch.sh",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-current-branch.sh",
        "original_deepwiki_href": "scripts/check-current-branch.sh",
        "context": "collapsible_aside_link"
      },
      {
        "text": ".pre-commit-config.yaml:1-64",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": ".github/workflows/pre-commit.yml:1-22",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/check-branch-name.sh:1-114",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "scripts/check-current-branch.sh:1-7",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "pyproject.toml:1-97",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "pyproject.toml:36-69",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "requirements-dev.txt:1-418",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "pyproject.toml:92-97",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": ".pre-commit-config.yaml:56-63",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": ".gitignore:1-263",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "pyproject.toml:78-91",
        "href": "",
        "original_deepwiki_href": "",
        "context": "inline_source_link"
      },
      {
        "text": "CLI Tools",
        "href": "/cli-tools#8",
        "original_deepwiki_href": "#8",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Configuration",
        "href": "/configuration#7",
        "original_deepwiki_href": "#7",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TB\n    DEV[\"Developer\"]\n    COMMIT[\"git commit\"]\n    PRECOMMIT[\"pre-commit hooks\"]\n    \n    subgraph \"Code Quality Checks\"\n        TRAILING[\"trailing-whitespace\"]\n        EOF[\"end-of-file-fixer\"]\n        COMMITLINT[\"commitlint\"]\n        BRANCH[\"check-branch-name\"]\n        PIPREQS[\"pip-compile requirements\"]\n        PIPDEV[\"pip-compile dev requirements\"]\n        PYTEST[\"run-pytest\"]\n    end\n    \n    PUSH[\"git push\"]\n    GHACTIONS[\"GitHub Actions\"]\n    \n    DEV --> COMMIT\n    COMMIT --> PRECOMMIT\n    PRECOMMIT --> TRAILING\n    PRECOMMIT --> EOF\n    PRECOMMIT --> COMMITLINT\n    PRECOMMIT --> BRANCH\n    PRECOMMIT --> PIPREQS\n    PRECOMMIT --> PIPDEV\n    PRECOMMIT --> PYTEST\n    \n    TRAILING --> PUSH\n    EOF --> PUSH\n    COMMITLINT --> PUSH\n    BRANCH --> PUSH\n    PIPREQS --> PUSH\n    PIPDEV --> PUSH\n    PYTEST --> PUSH\n    \n    PUSH --> GHACTIONS",
      "graph LR\n    subgraph \"File Quality\"\n        TRAIL[\"trailing-whitespace\"]\n        EOF[\"end-of-file-fixer\"]\n    end\n    \n    subgraph \"Commit Standards\"\n        COMMITLINT[\"commitlint<br/>@commitlint/config-conventional\"]\n        BRANCH[\"check-branch-name.sh\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        PIPMAIN[\"pip-compile<br/>requirements.txt\"]\n        PIPDEV[\"pip-compile<br/>requirements-dev.txt\"]\n    end\n    \n    subgraph \"Testing\"\n        PYTEST[\"pytest<br/>--alluredir allure-results\"]\n    end\n    \n    TRAIL --> COMMITLINT\n    EOF --> BRANCH\n    COMMITLINT --> PIPMAIN\n    BRANCH --> PIPDEV\n    PIPMAIN --> PYTEST\n    PIPDEV --> PYTEST",
      "graph TB\n    subgraph \"Build System\"\n        HATCH[\"hatchling>=1.24.2\"]\n        BUILD[\"hatchling.build\"]\n    end\n    \n    subgraph \"Project Metadata\"\n        NAME[\"NI-Compute\"]\n        DESC[\"NI Compute Subnet\"]\n        AUTHOR[\"neuralinternet.ai\"]\n        LICENSE[\"MIT\"]\n        PYTHON[\">=3.10\"]\n    end\n    \n    subgraph \"Dependencies\"\n        CORE[\"Core Dependencies<br/>bittensor, torch, docker\"]\n        DEV[\"Development Dependencies<br/>pytest, pre-commit, pip-tools\"]\n    end\n    \n    subgraph \"Package Structure\"\n        COMPUTE[\"compute/\"]\n        NEURONS[\"neurons/\"]\n    end\n    \n    HATCH --> BUILD\n    BUILD --> NAME\n    NAME --> CORE\n    CORE --> DEV\n    DEV --> COMPUTE\n    COMPUTE --> NEURONS",
      "graph TB\n    subgraph \"Python Artifacts\"\n        PYCACHE[\"__pycache__/\"]\n        PYCDLL[\"*.py[cod]\"]\n        DIST[\"dist/, build/, *.egg-info/\"]\n    end\n    \n    subgraph \"Development Tools\"\n        WANDB[\"wandb/\"]\n        PYTEST[\"allure-results, .pytest_cache/\"]\n        COVERAGE[\".coverage, htmlcov/\"]\n    end\n    \n    subgraph \"IDE and Editors\"\n        IDEA[\".idea/\"]\n        VSCODE[\".vscode/\"]\n        DSSTORE[\".DS_Store\"]\n    end\n    \n    subgraph \"Project Specific\"\n        DATABASE[\"database.db\"]\n        CERTS[\"cert/\"]\n        MINERAPP[\"neurons/Miner/app\"]\n        REGAPI[\"neurons/register-api/\"]\n    end\n    \n    PYCACHE --> WANDB\n    PYCDLL --> PYTEST\n    DIST --> COVERAGE\n    WANDB --> IDEA\n    PYTEST --> VSCODE\n    COVERAGE --> DSSTORE\n    IDEA --> DATABASE\n    VSCODE --> CERTS\n    DSSTORE --> MINERAPP\n    DATABASE --> REGAPI"
    ],
    "potential_frontmatter": {
      "title": "Development"
    }
  },
  "/neuralinternet/ni-compute/9.1-development-workflow": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/9.1-development-workflow",
    "title": "Development Workflow",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/9.1-development-workflow",
    "level": 1,
    "target_astro_path": "/development/development-workflow",
    "main_markdown_content": "# Development Workflow\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/pre-commit.yml](.github/workflows/pre-commit.yml)\n- [.pre-commit-config.yaml](.pre-commit-config.yaml)\n- [scripts/check-branch-name.sh](scripts/check-branch-name.sh)\n- [scripts/check-current-branch.sh](scripts/check-current-branch.sh)\n\n</details>\n\n\n\nThis document covers the development workflow, code quality enforcement, and automated testing infrastructure for the NI Compute Subnet project. It details the pre-commit hooks system, branch naming conventions, continuous integration pipeline, and dependency management processes that ensure code quality and consistency across the project.\n\nFor information about project structure and organization, see [Project Structure](#9.2). For details about CLI tools used in development, see [CLI Tools](#8).\n\n## Pre-commit Hooks System\n\nThe project uses a comprehensive pre-commit hooks system to enforce code quality standards before commits are made. The configuration is defined in [.pre-commit-config.yaml:1-64]() and includes multiple stages of validation.\n\n### Hook Configuration\n\nThe pre-commit system includes the following hook categories:\n\n| Hook Category | Purpose | Stage |\n|---------------|---------|-------|\n| Code Formatting | Trailing whitespace and end-of-file fixes | `pre-commit`, `manual` |\n| Commit Validation | Conventional commit message format | `commit-msg` |\n| Branch Validation | Branch naming convention enforcement | `pre-commit`, `pre-push`, `manual` |\n| Dependency Management | Requirements file synchronization | `pre-commit`, `manual` |\n| Testing | Automated test execution | `pre-commit`, `manual` |\n\n### Pre-commit Hook Flow\n\n```mermaid\nflowchart TD\n    DEV[\"Developer makes commit\"] --> PRECOMMIT[\"pre-commit hooks triggered\"]\n    \n    PRECOMMIT --> WHITESPACE[\"trailing-whitespace hook\"]\n    PRECOMMIT --> ENDFILE[\"end-of-file-fixer hook\"]\n    PRECOMMIT --> BRANCH[\"check-branch-name hook\"]\n    PRECOMMIT --> PIPCOMPILE[\"pip-compile hooks\"]\n    PRECOMMIT --> PYTEST[\"run-pytest hook\"]\n    \n    WHITESPACE --> PASS1{Passes?}\n    ENDFILE --> PASS2{Passes?}\n    BRANCH --> BRANCHSCRIPT[\"./scripts/check-current-branch.sh\"]\n    BRANCHSCRIPT --> PASS3{Passes?}\n    PIPCOMPILE --> REQSYNC[\"Requirements sync\"]\n    REQSYNC --> PASS4{Passes?}\n    PYTEST --> TESTRUN[\"pytest --alluredir allure-results\"]\n    TESTRUN --> PASS5{Passes?}\n    \n    PASS1 -->|No| FAIL[\"Commit blocked\"]\n    PASS2 -->|No| FAIL\n    PASS3 -->|No| FAIL\n    PASS4 -->|No| FAIL\n    PASS5 -->|No| FAIL\n    \n    PASS1 -->|Yes| SUCCESS[\"All hooks pass\"]\n    PASS2 -->|Yes| SUCCESS\n    PASS3 -->|Yes| SUCCESS\n    PASS4 -->|Yes| SUCCESS\n    PASS5 -->|Yes| SUCCESS\n    \n    SUCCESS --> COMMIT[\"Commit allowed\"]\n    FAIL --> ABORT[\"Fix issues and retry\"]\n```\n\n**Sources:** [.pre-commit-config.yaml:1-64](), [scripts/check-current-branch.sh:1-7]()\n\n## Branch Naming Conventions\n\nThe project enforces strict branch naming conventions through the `check-branch-name.sh` script, which validates branch names against predefined patterns.\n\n### Allowed Branch Patterns\n\nThe branch validation system supports the following patterns:\n\n| Branch Type | Pattern | Example |\n|-------------|---------|---------|\n| Feature | `feat/CSN-XXXX-description` | `feat/CSN-1234-add-gpu-validation` |  \n| Bug Fix | `fix/CSN-XXXX-description` | `fix/CSN-5678-memory-leak-issue` |\n| Hotfix | `hotfix/vX.Y.Z-CSN-XXXX-description` | `hotfix/v1.2.3-CSN-9999-critical-fix` |\n| Release | `release/vX.Y.Z` | `release/v2.1.0` |\n| Chore | `chore/CSN-XXXX-description` | `chore/CSN-1111-update-dependencies` |\n\n### Branch Validation Logic\n\n```mermaid\nflowchart TD\n    BRANCH_INPUT[\"Branch name input\"] --> EMPTY_CHECK{Empty or HEAD?}\n    EMPTY_CHECK -->|Yes| ERROR1[\"❌ Unable to determine branch name\"]\n    \n    EMPTY_CHECK -->|No| SPECIAL_CHECK{dev or main branch?}\n    SPECIAL_CHECK -->|Yes| ALLOW1[\"✅ Branch allowed without validation\"]\n    \n    SPECIAL_CHECK -->|No| RELEASE_CHECK{Starts with release/?}\n    RELEASE_CHECK -->|Yes| RELEASE_PATTERN[\"Check release/vX.Y.Z pattern\"]\n    RELEASE_PATTERN --> RELEASE_VALID{Valid?}\n    RELEASE_VALID -->|Yes| ALLOW2[\"✅ Valid release branch\"]\n    RELEASE_VALID -->|No| ERROR2[\"❌ Invalid release format\"]\n    \n    RELEASE_CHECK -->|No| HOTFIX_CHECK{Starts with hotfix/?}\n    HOTFIX_CHECK -->|Yes| HOTFIX_PATTERN[\"Check hotfix/vX.Y.Z-CSN-XXXX-desc\"]\n    HOTFIX_PATTERN --> HOTFIX_VALID{Valid?}\n    HOTFIX_VALID -->|Yes| ALLOW3[\"✅ Valid hotfix branch\"]\n    HOTFIX_VALID -->|No| ERROR3[\"❌ Invalid hotfix format\"]\n    \n    HOTFIX_CHECK -->|No| PREFIX_CHECK[\"Check allowed prefixes\"]\n    PREFIX_CHECK --> PREFIX_VALID{Valid prefix?}\n    PREFIX_VALID -->|No| ERROR4[\"❌ Invalid prefix\"]\n    \n    PREFIX_VALID -->|Yes| JIRA_CHECK[\"Extract JIRA ticket CSN-XXXX\"]\n    JIRA_CHECK --> JIRA_VALID{JIRA found?}\n    JIRA_VALID -->|No| ERROR5[\"❌ Missing JIRA ticket\"]\n    \n    JIRA_VALID -->|Yes| DESC_CHECK[\"Check description format\"]\n    DESC_CHECK --> KEBAB_CASE{Lowercase kebab-case?}\n    KEBAB_CASE -->|No| ERROR6[\"❌ Invalid description format\"]\n    KEBAB_CASE -->|Yes| ALLOW4[\"✅ Valid branch name\"]\n    \n    ERROR1 --> EXIT1[\"exit 1\"]\n    ERROR2 --> EXIT1\n    ERROR3 --> EXIT1\n    ERROR4 --> EXIT1\n    ERROR5 --> EXIT1\n    ERROR6 --> EXIT1\n    \n    ALLOW1 --> EXIT0[\"exit 0\"]\n    ALLOW2 --> EXIT0\n    ALLOW3 --> EXIT0\n    ALLOW4 --> EXIT0\n```\n\n**Sources:** [scripts/check-branch-name.sh:1-114](), [.pre-commit-config.yaml:19-26]()\n\n### Branch Name Validation Implementation\n\nThe validation logic is implemented in [scripts/check-branch-name.sh:1-114]() with the following key components:\n\n- **Allowed Prefixes**: [scripts/check-branch-name.sh:4]() defines `feat|fix|hotfix|chore|refactor|test|spike|prototype|release|docs`\n- **JIRA Pattern**: [scripts/check-branch-name.sh:7]() requires `CSN-[0-9]+` format\n- **Release Pattern**: [scripts/check-branch-name.sh:10]() validates `release/v[0-9]+\\.[0-9]+\\.[0-9]+(-[a-z0-9]+)?`\n- **Hotfix Pattern**: [scripts/check-branch-name.sh:16]() validates `hotfix/v[0-9]+\\.[0-9]+\\.[0-9]+`\n\n**Sources:** [scripts/check-branch-name.sh:4-16]()\n\n## Code Quality Enforcement\n\nThe development workflow enforces code quality through multiple automated checks that run at different stages of the development process.\n\n### Quality Check Pipeline\n\n```mermaid\ngraph LR\n    subgraph \"Pre-commit Stage\"\n        PC_WHITESPACE[\"trailing-whitespace\"]\n        PC_EOF[\"end-of-file-fixer\"]\n        PC_BRANCH[\"check-branch-name\"]\n        PC_DEPS[\"pip-compile\"]\n        PC_TEST[\"run-pytest\"]\n    end\n    \n    subgraph \"Commit Message Stage\"\n        CM_LINT[\"commitlint\"]\n        CM_CONV[\"@commitlint/config-conventional\"]\n    end\n    \n    subgraph \"Post-checkout Stage\"\n        PCO_BRANCH[\"post-checkout-check\"]\n    end\n    \n    subgraph \"CI/CD Stage\"\n        CI_PRECOMMIT[\"pre-commit/action@v3.0.1\"]\n        CI_PYTHON[\"Python 3.12 setup\"]\n        CI_DEPS[\"Install dependencies\"]\n    end\n    \n    PC_WHITESPACE --> COMMIT_READY\n    PC_EOF --> COMMIT_READY\n    PC_BRANCH --> COMMIT_READY\n    PC_DEPS --> COMMIT_READY\n    PC_TEST --> COMMIT_READY\n    \n    COMMIT_READY --> CM_LINT\n    CM_LINT --> CM_CONV\n    CM_CONV --> COMMITTED\n    \n    COMMITTED --> PCO_BRANCH\n    PCO_BRANCH --> CHECKOUT_COMPLETE\n    \n    CHECKOUT_COMPLETE --> CI_PYTHON\n    CI_PYTHON --> CI_DEPS\n    CI_DEPS --> CI_PRECOMMIT\n    CI_PRECOMMIT --> PIPELINE_SUCCESS\n```\n\n**Sources:** [.pre-commit-config.yaml:1-64](), [.github/workflows/pre-commit.yml:1-22]()\n\n### Testing Integration\n\nThe pre-commit system includes automated test execution using `pytest` with Allure reporting:\n\n- **Test Command**: [.pre-commit-config.yaml:58]() executes `python -m pytest --alluredir allure-results`\n- **Test Stage**: [.pre-commit-config.yaml:60]() runs on `pre-commit` and `manual` stages\n- **Always Run**: [.pre-commit-config.yaml:61]() ensures tests run regardless of file changes\n- **No Filenames**: [.pre-commit-config.yaml:63]() prevents passing filenames to pytest\n\n**Sources:** [.pre-commit-config.yaml:56-63]()\n\n## Continuous Integration Pipeline\n\nThe project uses GitHub Actions for continuous integration, with the workflow defined in [.github/workflows/pre-commit.yml:1-22]().\n\n### CI/CD Workflow Configuration\n\n| Configuration | Value | Purpose |\n|---------------|-------|---------|\n| **Trigger Events** | `pull_request`, `push` to `dev`/`main` | Automated validation on key events |\n| **Runner** | `ubuntu-latest` | Consistent Linux environment |\n| **Python Version** | `3.12` | Latest stable Python version |\n| **Cache Strategy** | `pip` with `pyproject.toml`, `requirements*.txt` | Dependency caching for performance |\n\n### CI Pipeline Steps\n\n```mermaid\nsequenceDiagram\n    participant TRIGGER as \"PR/Push Event\"\n    participant RUNNER as \"ubuntu-latest Runner\"\n    participant PYTHON as \"Python 3.12 Setup\"\n    participant CACHE as \"Pip Cache\"\n    participant DEPS as \"Dependencies\"\n    participant PRECOMMIT as \"pre-commit/action\"\n    \n    TRIGGER->>RUNNER: \"Start CI job\"\n    RUNNER->>PYTHON: \"Setup Python 3.12\"\n    PYTHON->>CACHE: \"Check pip cache\"\n    CACHE->>DEPS: \"Install -e .[dev] -r requirements*.txt\"\n    DEPS->>PRECOMMIT: \"Run pre-commit hooks\"\n    PRECOMMIT->>RUNNER: \"Report results\"\n    RUNNER->>TRIGGER: \"Complete CI job\"\n```\n\n**Sources:** [.github/workflows/pre-commit.yml:8-22]()\n\n## Dependency Management\n\nThe project uses `pip-tools` for dependency management with automated synchronization through pre-commit hooks.\n\n### Requirements Compilation\n\nThe dependency management system includes two compilation targets:\n\n- **Production Requirements**: [.pre-commit-config.yaml:40]() compiles `pyproject.toml` to `requirements.txt`\n- **Development Requirements**: [.pre-commit-config.yaml:49]() compiles with `--extra dev` to `requirements-dev.txt`\n\n### Dependency Compilation Flow\n\n```mermaid\nflowchart TD\n    PYPROJECT[\"pyproject.toml changes\"] --> PRECOMMIT_TRIGGER[\"pre-commit triggered\"]\n    \n    PRECOMMIT_TRIGGER --> PROD_COMPILE[\"pip-compile hook\"]\n    PRECOMMIT_TRIGGER --> DEV_COMPILE[\"pip-compile-dev hook\"]\n    \n    PROD_COMPILE --> PROD_CMD[\"python -m piptools compile --quiet -o requirements.txt\"]\n    DEV_COMPILE --> DEV_CMD[\"python -m piptools compile --quiet --extra dev -o requirements-dev.txt\"]\n    \n    PROD_CMD --> REQ_FILE[\"requirements.txt updated\"]\n    DEV_CMD --> REQ_DEV_FILE[\"requirements-dev.txt updated\"]\n    \n    REQ_FILE --> COMMIT_READY[\"Dependencies synchronized\"]\n    REQ_DEV_FILE --> COMMIT_READY\n    \n    COMMIT_READY --> COMMIT_SUCCESS[\"Commit proceeds\"]\n```\n\n**Sources:** [.pre-commit-config.yaml:38-53]()\n\n### Installation Command Sequence\n\nThe CI/CD pipeline installs dependencies in the following order as defined in [.github/workflows/pre-commit.yml:20]():\n\n1. `pip install -e .[dev]` - Install package in development mode with dev extras\n2. `-r requirements.txt` - Install production dependencies\n3. `-r requirements-dev.txt` - Install development dependencies\n\n**Sources:** [.github/workflows/pre-commit.yml:20]()",
    "resolved_links": [
      {
        "text": ".github/workflows/pre-commit.yml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml",
        "original_deepwiki_href": ".github/workflows/pre-commit.yml",
        "context": "collapsible_aside_link"
      },
      {
        "text": ".pre-commit-config.yaml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml",
        "original_deepwiki_href": ".pre-commit-config.yaml",
        "context": "collapsible_aside_link"
      },
      {
        "text": "scripts/check-branch-name.sh",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh",
        "original_deepwiki_href": "scripts/check-branch-name.sh",
        "context": "collapsible_aside_link"
      },
      {
        "text": "scripts/check-current-branch.sh",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-current-branch.sh",
        "original_deepwiki_href": "scripts/check-current-branch.sh",
        "context": "collapsible_aside_link"
      },
      {
        "text": "Project Structure",
        "href": "/development/project-structure#9.2",
        "original_deepwiki_href": "#9.2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "CLI Tools",
        "href": "/cli-tools#8",
        "original_deepwiki_href": "#8",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "flowchart TD\n    DEV[\"Developer makes commit\"] --> PRECOMMIT[\"pre-commit hooks triggered\"]\n    \n    PRECOMMIT --> WHITESPACE[\"trailing-whitespace hook\"]\n    PRECOMMIT --> ENDFILE[\"end-of-file-fixer hook\"]\n    PRECOMMIT --> BRANCH[\"check-branch-name hook\"]\n    PRECOMMIT --> PIPCOMPILE[\"pip-compile hooks\"]\n    PRECOMMIT --> PYTEST[\"run-pytest hook\"]\n    \n    WHITESPACE --> PASS1{Passes?}\n    ENDFILE --> PASS2{Passes?}\n    BRANCH --> BRANCHSCRIPT[\"./scripts/check-current-branch.sh\"]\n    BRANCHSCRIPT --> PASS3{Passes?}\n    PIPCOMPILE --> REQSYNC[\"Requirements sync\"]\n    REQSYNC --> PASS4{Passes?}\n    PYTEST --> TESTRUN[\"pytest --alluredir allure-results\"]\n    TESTRUN --> PASS5{Passes?}\n    \n    PASS1 -->|No| FAIL[\"Commit blocked\"]\n    PASS2 -->|No| FAIL\n    PASS3 -->|No| FAIL\n    PASS4 -->|No| FAIL\n    PASS5 -->|No| FAIL\n    \n    PASS1 -->|Yes| SUCCESS[\"All hooks pass\"]\n    PASS2 -->|Yes| SUCCESS\n    PASS3 -->|Yes| SUCCESS\n    PASS4 -->|Yes| SUCCESS\n    PASS5 -->|Yes| SUCCESS\n    \n    SUCCESS --> COMMIT[\"Commit allowed\"]\n    FAIL --> ABORT[\"Fix issues and retry\"]",
      "flowchart TD\n    BRANCH_INPUT[\"Branch name input\"] --> EMPTY_CHECK{Empty or HEAD?}\n    EMPTY_CHECK -->|Yes| ERROR1[\"❌ Unable to determine branch name\"]\n    \n    EMPTY_CHECK -->|No| SPECIAL_CHECK{dev or main branch?}\n    SPECIAL_CHECK -->|Yes| ALLOW1[\"✅ Branch allowed without validation\"]\n    \n    SPECIAL_CHECK -->|No| RELEASE_CHECK{Starts with release/?}\n    RELEASE_CHECK -->|Yes| RELEASE_PATTERN[\"Check release/vX.Y.Z pattern\"]\n    RELEASE_PATTERN --> RELEASE_VALID{Valid?}\n    RELEASE_VALID -->|Yes| ALLOW2[\"✅ Valid release branch\"]\n    RELEASE_VALID -->|No| ERROR2[\"❌ Invalid release format\"]\n    \n    RELEASE_CHECK -->|No| HOTFIX_CHECK{Starts with hotfix/?}\n    HOTFIX_CHECK -->|Yes| HOTFIX_PATTERN[\"Check hotfix/vX.Y.Z-CSN-XXXX-desc\"]\n    HOTFIX_PATTERN --> HOTFIX_VALID{Valid?}\n    HOTFIX_VALID -->|Yes| ALLOW3[\"✅ Valid hotfix branch\"]\n    HOTFIX_VALID -->|No| ERROR3[\"❌ Invalid hotfix format\"]\n    \n    HOTFIX_CHECK -->|No| PREFIX_CHECK[\"Check allowed prefixes\"]\n    PREFIX_CHECK --> PREFIX_VALID{Valid prefix?}\n    PREFIX_VALID -->|No| ERROR4[\"❌ Invalid prefix\"]\n    \n    PREFIX_VALID -->|Yes| JIRA_CHECK[\"Extract JIRA ticket CSN-XXXX\"]\n    JIRA_CHECK --> JIRA_VALID{JIRA found?}\n    JIRA_VALID -->|No| ERROR5[\"❌ Missing JIRA ticket\"]\n    \n    JIRA_VALID -->|Yes| DESC_CHECK[\"Check description format\"]\n    DESC_CHECK --> KEBAB_CASE{Lowercase kebab-case?}\n    KEBAB_CASE -->|No| ERROR6[\"❌ Invalid description format\"]\n    KEBAB_CASE -->|Yes| ALLOW4[\"✅ Valid branch name\"]\n    \n    ERROR1 --> EXIT1[\"exit 1\"]\n    ERROR2 --> EXIT1\n    ERROR3 --> EXIT1\n    ERROR4 --> EXIT1\n    ERROR5 --> EXIT1\n    ERROR6 --> EXIT1\n    \n    ALLOW1 --> EXIT0[\"exit 0\"]\n    ALLOW2 --> EXIT0\n    ALLOW3 --> EXIT0\n    ALLOW4 --> EXIT0",
      "graph LR\n    subgraph \"Pre-commit Stage\"\n        PC_WHITESPACE[\"trailing-whitespace\"]\n        PC_EOF[\"end-of-file-fixer\"]\n        PC_BRANCH[\"check-branch-name\"]\n        PC_DEPS[\"pip-compile\"]\n        PC_TEST[\"run-pytest\"]\n    end\n    \n    subgraph \"Commit Message Stage\"\n        CM_LINT[\"commitlint\"]\n        CM_CONV[\"@commitlint/config-conventional\"]\n    end\n    \n    subgraph \"Post-checkout Stage\"\n        PCO_BRANCH[\"post-checkout-check\"]\n    end\n    \n    subgraph \"CI/CD Stage\"\n        CI_PRECOMMIT[\"pre-commit/action@v3.0.1\"]\n        CI_PYTHON[\"Python 3.12 setup\"]\n        CI_DEPS[\"Install dependencies\"]\n    end\n    \n    PC_WHITESPACE --> COMMIT_READY\n    PC_EOF --> COMMIT_READY\n    PC_BRANCH --> COMMIT_READY\n    PC_DEPS --> COMMIT_READY\n    PC_TEST --> COMMIT_READY\n    \n    COMMIT_READY --> CM_LINT\n    CM_LINT --> CM_CONV\n    CM_CONV --> COMMITTED\n    \n    COMMITTED --> PCO_BRANCH\n    PCO_BRANCH --> CHECKOUT_COMPLETE\n    \n    CHECKOUT_COMPLETE --> CI_PYTHON\n    CI_PYTHON --> CI_DEPS\n    CI_DEPS --> CI_PRECOMMIT\n    CI_PRECOMMIT --> PIPELINE_SUCCESS",
      "sequenceDiagram\n    participant TRIGGER as \"PR/Push Event\"\n    participant RUNNER as \"ubuntu-latest Runner\"\n    participant PYTHON as \"Python 3.12 Setup\"\n    participant CACHE as \"Pip Cache\"\n    participant DEPS as \"Dependencies\"\n    participant PRECOMMIT as \"pre-commit/action\"\n    \n    TRIGGER->>RUNNER: \"Start CI job\"\n    RUNNER->>PYTHON: \"Setup Python 3.12\"\n    PYTHON->>CACHE: \"Check pip cache\"\n    CACHE->>DEPS: \"Install -e .[dev] -r requirements*.txt\"\n    DEPS->>PRECOMMIT: \"Run pre-commit hooks\"\n    PRECOMMIT->>RUNNER: \"Report results\"\n    RUNNER->>TRIGGER: \"Complete CI job\"",
      "flowchart TD\n    PYPROJECT[\"pyproject.toml changes\"] --> PRECOMMIT_TRIGGER[\"pre-commit triggered\"]\n    \n    PRECOMMIT_TRIGGER --> PROD_COMPILE[\"pip-compile hook\"]\n    PRECOMMIT_TRIGGER --> DEV_COMPILE[\"pip-compile-dev hook\"]\n    \n    PROD_COMPILE --> PROD_CMD[\"python -m piptools compile --quiet -o requirements.txt\"]\n    DEV_COMPILE --> DEV_CMD[\"python -m piptools compile --quiet --extra dev -o requirements-dev.txt\"]\n    \n    PROD_CMD --> REQ_FILE[\"requirements.txt updated\"]\n    DEV_CMD --> REQ_DEV_FILE[\"requirements-dev.txt updated\"]\n    \n    REQ_FILE --> COMMIT_READY[\"Dependencies synchronized\"]\n    REQ_DEV_FILE --> COMMIT_READY\n    \n    COMMIT_READY --> COMMIT_SUCCESS[\"Commit proceeds\"]"
    ],
    "potential_frontmatter": {
      "title": "Development Workflow"
    }
  },
  "/neuralinternet/ni-compute/9.2-project-structure": {
    "original_deepwiki_href": "/neuralinternet/ni-compute/9.2-project-structure",
    "title": "Project Structure",
    "full_deepwiki_url": "https://deepwiki.com/neuralinternet/ni-compute/9.2-project-structure",
    "level": 1,
    "target_astro_path": "/development/project-structure",
    "main_markdown_content": "# Project Structure\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [pyproject.toml](pyproject.toml)\n- [requirements-dev.txt](requirements-dev.txt)\n- [requirements.txt](requirements.txt)\n\n</details>\n\n\n\nThis document describes the organizational structure, dependency management, and build configuration of the NI Compute Subnet codebase. It covers the package layout, key dependencies, build tooling, and development infrastructure that supports the distributed GPU compute marketplace.\n\nFor information about installation and setup procedures, see [Installation and Setup](#1.2). For details about development workflows and code quality tools, see [Development Workflow](#9.1).\n\n## Package Organization\n\nThe codebase is organized into two main Python packages with a clear separation between core compute logic and network protocol implementations.\n\n```mermaid\ngraph TD\n    subgraph \"Root Directory\"\n        ROOT[\"ni-compute/\"]\n    end\n    \n    subgraph \"Core Packages\"\n        COMPUTE[\"compute/\"]\n        NEURONS[\"neurons/\"]\n    end\n    \n    subgraph \"Configuration Files\"\n        PYPROJECT[\"pyproject.toml\"]\n        REQS[\"requirements.txt\"]\n        DEVREQS[\"requirements-dev.txt\"]\n        GITIGNORE[\".gitignore\"]\n    end\n    \n    subgraph \"compute/ Package\"\n        COMPINIT[\"compute/__init__.py\"]\n        COMPMODULES[\"Core compute logic<br/>Database, Protocols, Utils\"]\n    end\n    \n    subgraph \"neurons/ Package\"\n        VALIDATOR[\"neurons/validator.py\"]\n        MINER[\"neurons/miner.py\"]\n        REGISTER[\"neurons/register.py\"]\n        MINERDIR[\"neurons/Miner/\"]\n        REGISTERAPI[\"neurons/register-api/\"]\n    end\n    \n    ROOT --> COMPUTE\n    ROOT --> NEURONS\n    ROOT --> PYPROJECT\n    ROOT --> REQS\n    ROOT --> DEVREQS\n    ROOT --> GITIGNORE\n    \n    COMPUTE --> COMPINIT\n    COMPUTE --> COMPMODULES\n    \n    NEURONS --> VALIDATOR\n    NEURONS --> MINER\n    NEURONS --> REGISTER\n    NEURONS --> MINERDIR\n    NEURONS --> REGISTERAPI\n    \n    style COMPUTE fill:#f9f9f9\n    style NEURONS fill:#f9f9f9\n    style PYPROJECT fill:#e6f3ff\n```\n\n**Sources:** [pyproject.toml:87-91](), [.gitignore:248-262]()\n\n## Core Dependencies Architecture\n\nThe project relies on a carefully curated set of dependencies that enable blockchain integration, GPU computation, containerization, and distributed system monitoring.\n\n```mermaid\ngraph TB\n    subgraph \"Blockchain Layer\"\n        BITTENSOR[\"bittensor==9.0.0\"]\n        BTCLI[\"bittensor-cli==9.1.0\"]\n        BTWALLET[\"bittensor-wallet==3.0.4\"]\n        SUBSTRATE[\"async-substrate-interface==1.0.3\"]\n    end\n    \n    subgraph \"GPU Computing\"\n        TORCH[\"torch==2.5.1\"]\n        NVIDIA[\"nvidia-* packages<br/>CUDA Runtime\"]\n        GPUTIL[\"GPUtil==1.4.0\"]\n        IGPU[\"igpu==0.1.2\"]\n        PYNVML[\"pynvml==12.0.0\"]\n    end\n    \n    subgraph \"Web Services\"\n        FASTAPI[\"fastapi==0.110.3\"]\n        UVICORN[\"uvicorn==0.34.0\"]\n        STARLETTE[\"starlette==0.37.2\"]\n        AIOHTTP[\"aiohttp==3.10.11\"]\n    end\n    \n    subgraph \"Containerization\"\n        DOCKER[\"docker==7.0.0\"]\n        PARAMIKO[\"paramiko==3.4.1\"]\n    end\n    \n    subgraph \"Monitoring & Logging\"\n        WANDB[\"wandb==0.19.0\"]\n        PSUTIL[\"psutil==5.9.8\"]\n    end\n    \n    subgraph \"Cryptography\"\n        CRYPTO[\"cryptography==43.0.1\"]\n        BLAKE3[\"blake3==1.0.4\"]\n        PYCRYPTO[\"pycryptodome==3.21.0\"]\n    end\n    \n    subgraph \"Data Processing\"\n        NUMPY[\"numpy==2.0.2\"]\n        REQUESTS[\"requests==2.31.0\"]\n        MSGPACK[\"msgpack-numpy-opentensor==0.5.0\"]\n    end\n    \n    BITTENSOR --> SUBSTRATE\n    BITTENSOR --> BTWALLET\n    BITTENSOR --> BTCLI\n    \n    TORCH --> NVIDIA\n    GPUTIL --> NVIDIA\n    IGPU --> PYNVML\n    \n    FASTAPI --> STARLETTE\n    FASTAPI --> UVICORN\n    \n    style BITTENSOR fill:#ff9999\n    style TORCH fill:#99ff99\n    style FASTAPI fill:#99ccff\n    style DOCKER fill:#ffcc99\n```\n\n**Sources:** [requirements.txt:36-59](), [pyproject.toml:36-59]()\n\n## Build and Packaging Configuration\n\nThe project uses modern Python packaging standards with `hatchling` as the build backend and supports both source and wheel distributions.\n\n| Component | Configuration | Purpose |\n|-----------|---------------|---------|\n| **Build Backend** | `hatchling>=1.24.2` | Modern Python build system |\n| **Version Source** | `compute/__init__.py` | Dynamic versioning from package |\n| **Python Requirement** | `>=3.10` | Minimum Python version |\n| **Package Distribution** | `compute`, `neurons` | Core packages included in wheel |\n| **License** | MIT | Open source license |\n\n```mermaid\ngraph LR\n    subgraph \"Source Code\"\n        SRC_COMPUTE[\"compute/\"]\n        SRC_NEURONS[\"neurons/\"]\n        VERSION[\"compute/__init__.py<br/>(version)\"]\n    end\n    \n    subgraph \"Build Configuration\"\n        PYPROJECT_BUILD[\"pyproject.toml<br/>[build-system]\"]\n        HATCH_VERSION[\"[tool.hatch.version]\"]\n        HATCH_WHEEL[\"[tool.hatch.build.targets.wheel]\"]\n    end\n    \n    subgraph \"Build Artifacts\"\n        SDIST[\"Source Distribution<br/>.tar.gz\"]\n        WHEEL[\"Wheel Distribution<br/>.whl\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        REQS_MAIN[\"requirements.txt<br/>(pip-compile)\"]\n        REQS_DEV[\"requirements-dev.txt<br/>(pip-compile)\"]\n        PYPROJECT_DEPS[\"pyproject.toml<br/>[project.dependencies]\"]\n    end\n    \n    VERSION --> HATCH_VERSION\n    HATCH_VERSION --> PYPROJECT_BUILD\n    SRC_COMPUTE --> HATCH_WHEEL\n    SRC_NEURONS --> HATCH_WHEEL\n    \n    PYPROJECT_BUILD --> SDIST\n    PYPROJECT_BUILD --> WHEEL\n    \n    PYPROJECT_DEPS --> REQS_MAIN\n    PYPROJECT_DEPS --> REQS_DEV\n    \n    style PYPROJECT_BUILD fill:#e6f3ff\n    style WHEEL fill:#f9f9f9\n```\n\n**Sources:** [pyproject.toml:1-5](), [pyproject.toml:78-91](), [requirements.txt:1-6]()\n\n## Development Environment Structure\n\nThe development environment includes comprehensive tooling for code quality, testing, and dependency management with automated workflows.\n\n```mermaid\ngraph TD\n    subgraph \"Development Dependencies\"\n        PRECOMMIT[\"pre-commit==4.1.0\"]\n        PYTEST[\"pytest==8.3.5\"]\n        PYTESTCOV[\"pytest-cov==6.0.0\"]\n        ALLURE[\"allure-pytest==2.13.5\"]\n        PIPTOOLS[\"pip-tools==7.4.1\"]\n    end\n    \n    subgraph \"Code Quality\"\n        HOOKS[\"Pre-commit Hooks\"]\n        COVERAGE[\"Coverage Reports\"]\n        TESTING[\"Test Suite\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        COMPILE[\"pip-compile\"]\n        SYNC[\"pip-sync\"]\n        LOCK[\"requirements.txt\"]\n        DEVLOCK[\"requirements-dev.txt\"]\n    end\n    \n    subgraph \"Build Tools\"\n        HATCH[\"hatchling\"]\n        BUILD[\"build==1.2.2.post1\"]\n        WHEEL_BUILD[\"Wheel Building\"]\n    end\n    \n    PRECOMMIT --> HOOKS\n    PYTEST --> TESTING\n    PYTESTCOV --> COVERAGE\n    ALLURE --> TESTING\n    \n    PIPTOOLS --> COMPILE\n    COMPILE --> LOCK\n    COMPILE --> DEVLOCK\n    \n    HATCH --> BUILD\n    BUILD --> WHEEL_BUILD\n    \n    style PRECOMMIT fill:#ff9999\n    style PYTEST fill:#99ff99\n    style PIPTOOLS fill:#99ccff\n```\n\n**Sources:** [requirements-dev.txt:242-307](), [pyproject.toml:61-69](), [pyproject.toml:92-96]()\n\n## File Exclusions and Generated Content\n\nThe project maintains clean version control by excluding build artifacts, runtime data, and generated content from the repository.\n\n| Category | Files/Patterns | Purpose |\n|----------|----------------|---------|\n| **Python Artifacts** | `__pycache__/`, `*.pyc`, `build/`, `dist/` | Standard Python build artifacts |\n| **Runtime Data** | `database.db`, `wandb/` | Validator database and monitoring data |\n| **Generated Content** | `neurons/Miner/app`, `neurons/register-api/` | Dynamically generated applications |\n| **Security** | `cert/`, `.env` | SSL certificates and environment variables |\n| **Development** | `.idea/`, `.pytest_cache/`, `allure-results` | IDE and testing artifacts |\n\n**Sources:** [.gitignore:1-262]()\n\n## Dependency Resolution Strategy\n\nThe project uses a two-stage dependency management approach with `pip-tools` for deterministic builds and dependency conflict resolution.\n\n```mermaid\nflowchart TD\n    subgraph \"Source Dependencies\"\n        PYPROJECT_MAIN[\"pyproject.toml<br/>[project.dependencies]\"]\n        PYPROJECT_DEV[\"pyproject.toml<br/>[project.optional-dependencies.dev]\"]\n    end\n    \n    subgraph \"Compilation Process\"\n        PIPCOMPILE[\"pip-compile\"]\n        RESOLVER[\"Dependency Resolver\"]\n    end\n    \n    subgraph \"Lock Files\"\n        REQUIREMENTS[\"requirements.txt<br/>(production pins)\"]\n        REQUIREMENTS_DEV[\"requirements-dev.txt<br/>(dev + production pins)\"]\n    end\n    \n    subgraph \"Installation\"\n        PIPSYNC[\"pip-sync\"]\n        VENV[\"Virtual Environment\"]\n    end\n    \n    PYPROJECT_MAIN --> PIPCOMPILE\n    PYPROJECT_DEV --> PIPCOMPILE\n    PIPCOMPILE --> RESOLVER\n    RESOLVER --> REQUIREMENTS\n    RESOLVER --> REQUIREMENTS_DEV\n    \n    REQUIREMENTS --> PIPSYNC\n    REQUIREMENTS_DEV --> PIPSYNC\n    PIPSYNC --> VENV\n    \n    style PIPCOMPILE fill:#99ccff\n    style REQUIREMENTS fill:#f9f9f9\n    style REQUIREMENTS_DEV fill:#f9f9f9\n```\n\n**Sources:** [requirements.txt:1-6](), [requirements-dev.txt:1-6](), [pyproject.toml:36-69]()",
    "resolved_links": [
      {
        "text": ".gitignore",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/.gitignore",
        "original_deepwiki_href": ".gitignore",
        "context": "collapsible_aside_link"
      },
      {
        "text": "pyproject.toml",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml",
        "original_deepwiki_href": "pyproject.toml",
        "context": "collapsible_aside_link"
      },
      {
        "text": "requirements-dev.txt",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/requirements-dev.txt",
        "original_deepwiki_href": "requirements-dev.txt",
        "context": "collapsible_aside_link"
      },
      {
        "text": "requirements.txt",
        "href": "https://github.com/neuralinternet/SN27/blob/6261c454/requirements.txt",
        "original_deepwiki_href": "requirements.txt",
        "context": "collapsible_aside_link"
      },
      {
        "text": "Installation and Setup",
        "href": "/installation-and-setup#1.2",
        "original_deepwiki_href": "#1.2",
        "context": "internal_page_link_from_content_body"
      },
      {
        "text": "Development Workflow",
        "href": "/development/development-workflow#9.1",
        "original_deepwiki_href": "#9.1",
        "context": "internal_page_link_from_content_body"
      }
    ],
    "mermaid_diagrams": [
      "graph TD\n    subgraph \"Root Directory\"\n        ROOT[\"ni-compute/\"]\n    end\n    \n    subgraph \"Core Packages\"\n        COMPUTE[\"compute/\"]\n        NEURONS[\"neurons/\"]\n    end\n    \n    subgraph \"Configuration Files\"\n        PYPROJECT[\"pyproject.toml\"]\n        REQS[\"requirements.txt\"]\n        DEVREQS[\"requirements-dev.txt\"]\n        GITIGNORE[\".gitignore\"]\n    end\n    \n    subgraph \"compute/ Package\"\n        COMPINIT[\"compute/__init__.py\"]\n        COMPMODULES[\"Core compute logic<br/>Database, Protocols, Utils\"]\n    end\n    \n    subgraph \"neurons/ Package\"\n        VALIDATOR[\"neurons/validator.py\"]\n        MINER[\"neurons/miner.py\"]\n        REGISTER[\"neurons/register.py\"]\n        MINERDIR[\"neurons/Miner/\"]\n        REGISTERAPI[\"neurons/register-api/\"]\n    end\n    \n    ROOT --> COMPUTE\n    ROOT --> NEURONS\n    ROOT --> PYPROJECT\n    ROOT --> REQS\n    ROOT --> DEVREQS\n    ROOT --> GITIGNORE\n    \n    COMPUTE --> COMPINIT\n    COMPUTE --> COMPMODULES\n    \n    NEURONS --> VALIDATOR\n    NEURONS --> MINER\n    NEURONS --> REGISTER\n    NEURONS --> MINERDIR\n    NEURONS --> REGISTERAPI\n    \n    style COMPUTE fill:#f9f9f9\n    style NEURONS fill:#f9f9f9\n    style PYPROJECT fill:#e6f3ff",
      "graph TB\n    subgraph \"Blockchain Layer\"\n        BITTENSOR[\"bittensor==9.0.0\"]\n        BTCLI[\"bittensor-cli==9.1.0\"]\n        BTWALLET[\"bittensor-wallet==3.0.4\"]\n        SUBSTRATE[\"async-substrate-interface==1.0.3\"]\n    end\n    \n    subgraph \"GPU Computing\"\n        TORCH[\"torch==2.5.1\"]\n        NVIDIA[\"nvidia-* packages<br/>CUDA Runtime\"]\n        GPUTIL[\"GPUtil==1.4.0\"]\n        IGPU[\"igpu==0.1.2\"]\n        PYNVML[\"pynvml==12.0.0\"]\n    end\n    \n    subgraph \"Web Services\"\n        FASTAPI[\"fastapi==0.110.3\"]\n        UVICORN[\"uvicorn==0.34.0\"]\n        STARLETTE[\"starlette==0.37.2\"]\n        AIOHTTP[\"aiohttp==3.10.11\"]\n    end\n    \n    subgraph \"Containerization\"\n        DOCKER[\"docker==7.0.0\"]\n        PARAMIKO[\"paramiko==3.4.1\"]\n    end\n    \n    subgraph \"Monitoring & Logging\"\n        WANDB[\"wandb==0.19.0\"]\n        PSUTIL[\"psutil==5.9.8\"]\n    end\n    \n    subgraph \"Cryptography\"\n        CRYPTO[\"cryptography==43.0.1\"]\n        BLAKE3[\"blake3==1.0.4\"]\n        PYCRYPTO[\"pycryptodome==3.21.0\"]\n    end\n    \n    subgraph \"Data Processing\"\n        NUMPY[\"numpy==2.0.2\"]\n        REQUESTS[\"requests==2.31.0\"]\n        MSGPACK[\"msgpack-numpy-opentensor==0.5.0\"]\n    end\n    \n    BITTENSOR --> SUBSTRATE\n    BITTENSOR --> BTWALLET\n    BITTENSOR --> BTCLI\n    \n    TORCH --> NVIDIA\n    GPUTIL --> NVIDIA\n    IGPU --> PYNVML\n    \n    FASTAPI --> STARLETTE\n    FASTAPI --> UVICORN\n    \n    style BITTENSOR fill:#ff9999\n    style TORCH fill:#99ff99\n    style FASTAPI fill:#99ccff\n    style DOCKER fill:#ffcc99",
      "graph LR\n    subgraph \"Source Code\"\n        SRC_COMPUTE[\"compute/\"]\n        SRC_NEURONS[\"neurons/\"]\n        VERSION[\"compute/__init__.py<br/>(version)\"]\n    end\n    \n    subgraph \"Build Configuration\"\n        PYPROJECT_BUILD[\"pyproject.toml<br/>[build-system]\"]\n        HATCH_VERSION[\"[tool.hatch.version]\"]\n        HATCH_WHEEL[\"[tool.hatch.build.targets.wheel]\"]\n    end\n    \n    subgraph \"Build Artifacts\"\n        SDIST[\"Source Distribution<br/>.tar.gz\"]\n        WHEEL[\"Wheel Distribution<br/>.whl\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        REQS_MAIN[\"requirements.txt<br/>(pip-compile)\"]\n        REQS_DEV[\"requirements-dev.txt<br/>(pip-compile)\"]\n        PYPROJECT_DEPS[\"pyproject.toml<br/>[project.dependencies]\"]\n    end\n    \n    VERSION --> HATCH_VERSION\n    HATCH_VERSION --> PYPROJECT_BUILD\n    SRC_COMPUTE --> HATCH_WHEEL\n    SRC_NEURONS --> HATCH_WHEEL\n    \n    PYPROJECT_BUILD --> SDIST\n    PYPROJECT_BUILD --> WHEEL\n    \n    PYPROJECT_DEPS --> REQS_MAIN\n    PYPROJECT_DEPS --> REQS_DEV\n    \n    style PYPROJECT_BUILD fill:#e6f3ff\n    style WHEEL fill:#f9f9f9",
      "graph TD\n    subgraph \"Development Dependencies\"\n        PRECOMMIT[\"pre-commit==4.1.0\"]\n        PYTEST[\"pytest==8.3.5\"]\n        PYTESTCOV[\"pytest-cov==6.0.0\"]\n        ALLURE[\"allure-pytest==2.13.5\"]\n        PIPTOOLS[\"pip-tools==7.4.1\"]\n    end\n    \n    subgraph \"Code Quality\"\n        HOOKS[\"Pre-commit Hooks\"]\n        COVERAGE[\"Coverage Reports\"]\n        TESTING[\"Test Suite\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        COMPILE[\"pip-compile\"]\n        SYNC[\"pip-sync\"]\n        LOCK[\"requirements.txt\"]\n        DEVLOCK[\"requirements-dev.txt\"]\n    end\n    \n    subgraph \"Build Tools\"\n        HATCH[\"hatchling\"]\n        BUILD[\"build==1.2.2.post1\"]\n        WHEEL_BUILD[\"Wheel Building\"]\n    end\n    \n    PRECOMMIT --> HOOKS\n    PYTEST --> TESTING\n    PYTESTCOV --> COVERAGE\n    ALLURE --> TESTING\n    \n    PIPTOOLS --> COMPILE\n    COMPILE --> LOCK\n    COMPILE --> DEVLOCK\n    \n    HATCH --> BUILD\n    BUILD --> WHEEL_BUILD\n    \n    style PRECOMMIT fill:#ff9999\n    style PYTEST fill:#99ff99\n    style PIPTOOLS fill:#99ccff",
      "flowchart TD\n    subgraph \"Source Dependencies\"\n        PYPROJECT_MAIN[\"pyproject.toml<br/>[project.dependencies]\"]\n        PYPROJECT_DEV[\"pyproject.toml<br/>[project.optional-dependencies.dev]\"]\n    end\n    \n    subgraph \"Compilation Process\"\n        PIPCOMPILE[\"pip-compile\"]\n        RESOLVER[\"Dependency Resolver\"]\n    end\n    \n    subgraph \"Lock Files\"\n        REQUIREMENTS[\"requirements.txt<br/>(production pins)\"]\n        REQUIREMENTS_DEV[\"requirements-dev.txt<br/>(dev + production pins)\"]\n    end\n    \n    subgraph \"Installation\"\n        PIPSYNC[\"pip-sync\"]\n        VENV[\"Virtual Environment\"]\n    end\n    \n    PYPROJECT_MAIN --> PIPCOMPILE\n    PYPROJECT_DEV --> PIPCOMPILE\n    PIPCOMPILE --> RESOLVER\n    RESOLVER --> REQUIREMENTS\n    RESOLVER --> REQUIREMENTS_DEV\n    \n    REQUIREMENTS --> PIPSYNC\n    REQUIREMENTS_DEV --> PIPSYNC\n    PIPSYNC --> VENV\n    \n    style PIPCOMPILE fill:#99ccff\n    style REQUIREMENTS fill:#f9f9f9\n    style REQUIREMENTS_DEV fill:#f9f9f9"
    ],
    "potential_frontmatter": {
      "title": "Project Structure"
    }
  }
}