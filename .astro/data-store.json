[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.8.0","content-config-digest","9a95ec2e8398aaca","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://docs.neuralinternet.ai\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"where\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[[null,{\"strategy\":\"pre-mermaid\"}],null,[null,{\"experimentalHeadingIdCompat\":false}],null,[null,{\"themes\":[{\"name\":\"Night Owl No Italics\",\"type\":\"dark\",\"colors\":{\"focusBorder\":\"#122d42\",\"foreground\":\"#d6deeb\",\"disabledForeground\":\"#cccccc80\",\"descriptionForeground\":\"#d6deebb3\",\"errorForeground\":\"#ef5350\",\"icon.foreground\":\"#c5c5c5\",\"contrastActiveBorder\":null,\"contrastBorder\":\"#122d42\",\"textBlockQuote.background\":\"#7f7f7f1a\",\"textBlockQuote.border\":\"#007acc80\",\"textCodeBlock.background\":\"#4f4f4f\",\"textLink.activeForeground\":\"#3794ff\",\"textLink.foreground\":\"#3794ff\",\"textPreformat.foreground\":\"#d7ba7d\",\"textSeparator.foreground\":\"#ffffff2e\",\"editor.background\":\"#23262f\",\"editor.foreground\":\"#d6deeb\",\"editorLineNumber.foreground\":\"#4b6479\",\"editorLineNumber.activeForeground\":\"#c5e4fd\",\"editorActiveLineNumber.foreground\":\"#c6c6c6\",\"editor.selectionBackground\":\"#1d3b53\",\"editor.inactiveSelectionBackground\":\"#7e57c25a\",\"editor.selectionHighlightBackground\":\"#5f7e9779\",\"editorError.foreground\":\"#ef5350\",\"editorWarning.foreground\":\"#b39554\",\"editorInfo.foreground\":\"#3794ff\",\"editorHint.foreground\":\"#eeeeeeb2\",\"problemsErrorIcon.foreground\":\"#ef5350\",\"problemsWarningIcon.foreground\":\"#b39554\",\"problemsInfoIcon.foreground\":\"#3794ff\",\"editor.findMatchBackground\":\"#5f7e9779\",\"editor.findMatchHighlightBackground\":\"#1085bb5d\",\"editor.findRangeHighlightBackground\":\"#3a3d4166\",\"editorLink.activeForeground\":\"#4e94ce\",\"editorLightBulb.foreground\":\"#ffcc00\",\"editorLightBulbAutoFix.foreground\":\"#75beff\",\"diffEditor.insertedTextBackground\":\"#99b76d23\",\"diffEditor.insertedTextBorder\":\"#c5e47833\",\"diffEditor.removedTextBackground\":\"#ef535033\",\"diffEditor.removedTextBorder\":\"#ef53504d\",\"diffEditor.insertedLineBackground\":\"#9bb95533\",\"diffEditor.removedLineBackground\":\"#ff000033\",\"editorStickyScroll.background\":\"#011627\",\"editorStickyScrollHover.background\":\"#2a2d2e\",\"editorInlayHint.background\":\"#5f7e97cc\",\"editorInlayHint.foreground\":\"#ffffff\",\"editorInlayHint.typeBackground\":\"#5f7e97cc\",\"editorInlayHint.typeForeground\":\"#ffffff\",\"editorInlayHint.parameterBackground\":\"#5f7e97cc\",\"editorInlayHint.parameterForeground\":\"#ffffff\",\"editorPane.background\":\"#011627\",\"editorGroup.emptyBackground\":\"#011627\",\"editorGroup.focusedEmptyBorder\":null,\"editorGroupHeader.tabsBackground\":\"var(--sl-color-black)\",\"editorGroupHeader.tabsBorder\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"editorGroupHeader.noTabsBackground\":\"#011627\",\"editorGroupHeader.border\":null,\"editorGroup.border\":\"#011627\",\"editorGroup.dropBackground\":\"#7e57c273\",\"editorGroup.dropIntoPromptForeground\":\"#d6deeb\",\"editorGroup.dropIntoPromptBackground\":\"#021320\",\"editorGroup.dropIntoPromptBorder\":null,\"sideBySideEditor.horizontalBorder\":\"#011627\",\"sideBySideEditor.verticalBorder\":\"#011627\",\"scrollbar.shadow\":\"#010b14\",\"scrollbarSlider.background\":\"#ffffff17\",\"scrollbarSlider.hoverBackground\":\"#ffffff40\",\"scrollbarSlider.activeBackground\":\"#084d8180\",\"panel.background\":\"#011627\",\"panel.border\":\"#5f7e97\",\"panelTitle.activeBorder\":\"#5f7e97\",\"panelTitle.activeForeground\":\"#ffffffcc\",\"panelTitle.inactiveForeground\":\"#d6deeb80\",\"panelSectionHeader.background\":\"#80808051\",\"terminal.background\":\"#011627\",\"widget.shadow\":\"#011627\",\"editorWidget.background\":\"#021320\",\"editorWidget.foreground\":\"#d6deeb\",\"editorWidget.border\":\"#5f7e97\",\"quickInput.background\":\"#021320\",\"quickInput.foreground\":\"#d6deeb\",\"quickInputTitle.background\":\"#ffffff1a\",\"pickerGroup.foreground\":\"#d1aaff\",\"pickerGroup.border\":\"#011627\",\"editor.hoverHighlightBackground\":\"#7e57c25a\",\"editorHoverWidget.background\":\"#011627\",\"editorHoverWidget.foreground\":\"#d6deeb\",\"editorHoverWidget.border\":\"#5f7e97\",\"editorHoverWidget.statusBarBackground\":\"#011a2f\",\"titleBar.activeBackground\":\"var(--sl-color-black)\",\"titleBar.activeForeground\":\"var(--sl-color-text)\",\"titleBar.inactiveBackground\":\"#010e1a\",\"titleBar.inactiveForeground\":\"#eeefff99\",\"titleBar.border\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"toolbar.hoverBackground\":\"#5a5d5e50\",\"toolbar.activeBackground\":\"#63666750\",\"tab.activeBackground\":\"#0b2942\",\"tab.unfocusedActiveBackground\":\"#0b2942\",\"tab.inactiveBackground\":\"#01111d\",\"tab.unfocusedInactiveBackground\":\"#01111d\",\"tab.activeForeground\":\"var(--sl-color-text)\",\"tab.inactiveForeground\":\"#5f7e97\",\"tab.unfocusedActiveForeground\":\"#5f7e97\",\"tab.unfocusedInactiveForeground\":\"#5f7e97\",\"tab.hoverBackground\":null,\"tab.unfocusedHoverBackground\":null,\"tab.hoverForeground\":null,\"tab.unfocusedHoverForeground\":null,\"tab.border\":\"#272b3b\",\"tab.lastPinnedBorder\":\"#585858\",\"tab.activeBorder\":\"transparent\",\"tab.unfocusedActiveBorder\":\"#262a39\",\"tab.activeBorderTop\":\"var(--sl-color-accent-high)\",\"tab.unfocusedActiveBorderTop\":null,\"tab.hoverBorder\":null,\"tab.unfocusedHoverBorder\":null,\"tab.activeModifiedBorder\":\"#3399cc\",\"tab.inactiveModifiedBorder\":\"#3399cc80\",\"tab.unfocusedActiveModifiedBorder\":\"#3399cc80\",\"tab.unfocusedInactiveModifiedBorder\":\"#3399cc40\",\"badge.background\":\"#5f7e97\",\"badge.foreground\":\"#ffffff\",\"button.background\":\"#7e57c2cc\",\"button.foreground\":\"#ffffffcc\",\"button.border\":\"#122d42\",\"button.separator\":\"#ffffff52\",\"button.hoverBackground\":\"#7e57c2\",\"button.secondaryBackground\":\"#3a3d41\",\"button.secondaryForeground\":\"#ffffff\",\"button.secondaryHoverBackground\":\"#46494e\",\"dropdown.background\":\"#011627\",\"dropdown.foreground\":\"#ffffffcc\",\"dropdown.border\":\"#5f7e97\",\"list.activeSelectionBackground\":\"#234d708c\",\"list.activeSelectionForeground\":\"#ffffff\",\"tree.indentGuidesStroke\":\"#585858\",\"input.background\":\"#0b253a\",\"input.foreground\":\"#ffffffcc\",\"input.placeholderForeground\":\"#5f7e97\",\"inputOption.activeBorder\":\"#ffffffcc\",\"inputOption.hoverBackground\":\"#5a5d5e80\",\"inputOption.activeBackground\":\"#122d4266\",\"inputOption.activeForeground\":\"#ffffff\",\"inputValidation.infoBackground\":\"#00589ef2\",\"inputValidation.infoBorder\":\"#64b5f6\",\"inputValidation.warningBackground\":\"#675700f2\",\"inputValidation.warningBorder\":\"#ffca28\",\"inputValidation.errorBackground\":\"#ab0300f2\",\"inputValidation.errorBorder\":\"#ef5350\",\"keybindingLabel.background\":\"#8080802b\",\"keybindingLabel.foreground\":\"#cccccc\",\"keybindingLabel.border\":\"#33333399\",\"keybindingLabel.bottomBorder\":\"#44444499\",\"menu.foreground\":\"#ffffffcc\",\"menu.background\":\"#011627\",\"menu.selectionForeground\":\"#ffffff\",\"menu.selectionBackground\":\"#234d708c\",\"menu.separatorBackground\":\"#606060\",\"editor.snippetTabstopHighlightBackground\":\"#7c7c74c\",\"editor.snippetFinalTabstopHighlightBorder\":\"#525252\",\"terminal.ansiBlack\":\"#011627\",\"terminal.ansiRed\":\"#ef5350\",\"terminal.ansiGreen\":\"#22da6e\",\"terminal.ansiYellow\":\"#c5e478\",\"terminal.ansiBlue\":\"#82aaff\",\"terminal.ansiMagenta\":\"#c792ea\",\"terminal.ansiCyan\":\"#21c7a8\",\"terminal.ansiWhite\":\"#ffffff\",\"terminal.ansiBrightBlack\":\"#575656\",\"terminal.ansiBrightRed\":\"#ef5350\",\"terminal.ansiBrightGreen\":\"#22da6e\",\"terminal.ansiBrightYellow\":\"#ffeb95\",\"terminal.ansiBrightBlue\":\"#82aaff\",\"terminal.ansiBrightMagenta\":\"#c792ea\",\"terminal.ansiBrightCyan\":\"#7fdbca\",\"terminal.ansiBrightWhite\":\"#ffffff\",\"selection.background\":\"#4373c2\",\"input.border\":\"#5f7e97\",\"punctuation.definition.generic.begin.html\":\"#ef5350f2\",\"progress.background\":\"#7e57c2\",\"breadcrumb.foreground\":\"#a599e9\",\"breadcrumb.focusForeground\":\"#ffffff\",\"breadcrumb.activeSelectionForeground\":\"#ffffff\",\"breadcrumbPicker.background\":\"#001122\",\"list.invalidItemForeground\":\"#975f94\",\"list.dropBackground\":\"#011627\",\"list.focusBackground\":\"#010d18\",\"list.focusForeground\":\"#ffffff\",\"list.highlightForeground\":\"#ffffff\",\"list.hoverBackground\":\"#011627\",\"list.hoverForeground\":\"#ffffff\",\"list.inactiveSelectionBackground\":\"#0e293f\",\"list.inactiveSelectionForeground\":\"#5f7e97\",\"activityBar.background\":\"#011627\",\"activityBar.dropBackground\":\"#5f7e97\",\"activityBar.foreground\":\"#5f7e97\",\"activityBar.border\":\"#011627\",\"activityBarBadge.background\":\"#44596b\",\"activityBarBadge.foreground\":\"#ffffff\",\"sideBar.background\":\"#011627\",\"sideBar.foreground\":\"#89a4bb\",\"sideBar.border\":\"#011627\",\"sideBarTitle.foreground\":\"#5f7e97\",\"sideBarSectionHeader.background\":\"#011627\",\"sideBarSectionHeader.foreground\":\"#5f7e97\",\"editorCursor.foreground\":\"#80a4c2\",\"editor.wordHighlightBackground\":\"#f6bbe533\",\"editor.wordHighlightStrongBackground\":\"#e2a2f433\",\"editor.lineHighlightBackground\":\"#0003\",\"editor.rangeHighlightBackground\":\"#7e57c25a\",\"editorIndentGuide.background\":\"#5e81ce52\",\"editorIndentGuide.activeBackground\":\"#7e97ac\",\"editorRuler.foreground\":\"#5e81ce52\",\"editorCodeLens.foreground\":\"#5e82ceb4\",\"editorBracketMatch.background\":\"#5f7e974d\",\"editorOverviewRuler.currentContentForeground\":\"#7e57c2\",\"editorOverviewRuler.incomingContentForeground\":\"#7e57c2\",\"editorOverviewRuler.commonContentForeground\":\"#7e57c2\",\"editorGutter.background\":\"#011627\",\"editorGutter.modifiedBackground\":\"#e2b93d\",\"editorGutter.addedBackground\":\"#9ccc65\",\"editorGutter.deletedBackground\":\"#ef5350\",\"editorSuggestWidget.background\":\"#2c3043\",\"editorSuggestWidget.border\":\"#2b2f40\",\"editorSuggestWidget.foreground\":\"#d6deeb\",\"editorSuggestWidget.highlightForeground\":\"#ffffff\",\"editorSuggestWidget.selectedBackground\":\"#5f7e97\",\"debugExceptionWidget.background\":\"#011627\",\"debugExceptionWidget.border\":\"#5f7e97\",\"editorMarkerNavigation.background\":\"#0b2942\",\"editorMarkerNavigationError.background\":\"#ef5350\",\"editorMarkerNavigationWarning.background\":\"#ffca28\",\"peekView.border\":\"#5f7e97\",\"peekViewEditor.background\":\"#011627\",\"peekViewEditor.matchHighlightBackground\":\"#7e57c25a\",\"peekViewResult.background\":\"#011627\",\"peekViewResult.fileForeground\":\"#5f7e97\",\"peekViewResult.lineForeground\":\"#5f7e97\",\"peekViewResult.matchHighlightBackground\":\"#ffffffcc\",\"peekViewResult.selectionBackground\":\"#2e3250\",\"peekViewResult.selectionForeground\":\"#5f7e97\",\"peekViewTitle.background\":\"#011627\",\"peekViewTitleDescription.foreground\":\"#697098\",\"peekViewTitleLabel.foreground\":\"#5f7e97\",\"merge.currentHeaderBackground\":\"#5f7e97\",\"merge.incomingHeaderBackground\":\"#7e57c25a\",\"statusBar.background\":\"#011627\",\"statusBar.foreground\":\"#5f7e97\",\"statusBar.border\":\"#262a39\",\"statusBar.debuggingBackground\":\"#202431\",\"statusBar.debuggingBorder\":\"#1f2330\",\"statusBar.noFolderBackground\":\"#011627\",\"statusBar.noFolderBorder\":\"#25293a\",\"statusBarItem.activeBackground\":\"#202431\",\"statusBarItem.hoverBackground\":\"#202431\",\"statusBarItem.prominentBackground\":\"#202431\",\"statusBarItem.prominentHoverBackground\":\"#202431\",\"notifications.background\":\"#01111d\",\"notifications.border\":\"#262a39\",\"notificationCenter.border\":\"#262a39\",\"notificationToast.border\":\"#262a39\",\"notifications.foreground\":\"#ffffffcc\",\"notificationLink.foreground\":\"#80cbc4\",\"extensionButton.prominentForeground\":\"#ffffffcc\",\"extensionButton.prominentBackground\":\"#7e57c2cc\",\"extensionButton.prominentHoverBackground\":\"#7e57c2\",\"terminal.selectionBackground\":\"#1b90dd4d\",\"terminalCursor.background\":\"#234d70\",\"debugToolBar.background\":\"#011627\",\"welcomePage.buttonBackground\":\"#011627\",\"welcomePage.buttonHoverBackground\":\"#011627\",\"walkThrough.embeddedEditorBackground\":\"#011627\",\"gitDecoration.modifiedResourceForeground\":\"#a2bffc\",\"gitDecoration.deletedResourceForeground\":\"#ef535090\",\"gitDecoration.untrackedResourceForeground\":\"#c5e478ff\",\"gitDecoration.ignoredResourceForeground\":\"#395a75\",\"gitDecoration.conflictingResourceForeground\":\"#ffeb95cc\",\"source.elm\":\"#5f7e97\",\"string.quoted.single.js\":\"#ffffff\",\"meta.objectliteral.js\":\"#82aaff\"},\"fg\":\"#d6deeb\",\"bg\":\"#23262f\",\"semanticHighlighting\":false,\"settings\":[{\"name\":\"Changed\",\"scope\":[\"markup.changed\",\"meta.diff.header.git\",\"meta.diff.header.from-file\",\"meta.diff.header.to-file\"],\"settings\":{\"foreground\":\"#a2bffc\"}},{\"name\":\"Deleted\",\"scope\":[\"markup.deleted.diff\"],\"settings\":{\"foreground\":\"#f27775fe\"}},{\"name\":\"Inserted\",\"scope\":[\"markup.inserted.diff\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Global settings\",\"settings\":{\"background\":\"#011627\",\"foreground\":\"#d6deeb\"}},{\"name\":\"Comment\",\"scope\":[\"comment\"],\"settings\":{\"foreground\":\"#919f9f\",\"fontStyle\":\"\"}},{\"name\":\"String\",\"scope\":[\"string\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"String Quoted\",\"scope\":[\"string.quoted\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"Support Constant Math\",\"scope\":[\"support.constant.math\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Number\",\"scope\":[\"constant.numeric\",\"constant.character.numeric\"],\"settings\":{\"foreground\":\"#f78c6c\",\"fontStyle\":\"\"}},{\"name\":\"Built-in constant\",\"scope\":[\"constant.language\",\"punctuation.definition.constant\",\"variable.other.constant\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"User-defined constant\",\"scope\":[\"constant.character\",\"constant.other\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Constant Character Escape\",\"scope\":[\"constant.character.escape\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"RegExp String\",\"scope\":[\"string.regexp\",\"string.regexp keyword.other\"],\"settings\":{\"foreground\":\"#5ca7e4\"}},{\"name\":\"Comma in functions\",\"scope\":[\"meta.function punctuation.separator.comma\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"Variable\",\"scope\":[\"variable\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Keyword\",\"scope\":[\"punctuation.accessor\",\"keyword\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Storage\",\"scope\":[\"storage\",\"meta.var.expr\",\"meta.class meta.method.declaration meta.var.expr storage.type.js\",\"storage.type.property.js\",\"storage.type.property.ts\",\"storage.type.property.tsx\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type.function.arrow.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Class name\",\"scope\":[\"entity.name.class\",\"meta.class entity.name.type.class\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Inherited class\",\"scope\":[\"entity.other.inherited-class\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Function name\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Meta Tag\",\"scope\":[\"punctuation.definition.tag\",\"meta.tag\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"HTML Tag names\",\"scope\":[\"entity.name.tag\",\"meta.tag.other.html\",\"meta.tag.other.js\",\"meta.tag.other.tsx\",\"entity.name.tag.tsx\",\"entity.name.tag.js\",\"entity.name.tag\",\"meta.tag.js\",\"meta.tag.tsx\",\"meta.tag.html\"],\"settings\":{\"foreground\":\"#caece6\",\"fontStyle\":\"\"}},{\"name\":\"Tag attribute\",\"scope\":[\"entity.other.attribute-name\"],\"settings\":{\"fontStyle\":\"\",\"foreground\":\"#c5e478\"}},{\"name\":\"Entity Name Tag Custom\",\"scope\":[\"entity.name.tag.custom\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Library (function & constant)\",\"scope\":[\"support.function\",\"support.constant\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Support Constant Property Value meta\",\"scope\":[\"support.constant.meta.property-value\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Library class/type\",\"scope\":[\"support.type\",\"support.class\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Support Variable DOM\",\"scope\":[\"support.variable.dom\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Invalid\",\"scope\":[\"invalid\"],\"settings\":{\"background\":\"#ff2c83\",\"foreground\":\"#ffffff\"}},{\"name\":\"Invalid deprecated\",\"scope\":[\"invalid.deprecated\"],\"settings\":{\"foreground\":\"#ffffff\",\"background\":\"#d3423e\"}},{\"name\":\"Keyword Operator\",\"scope\":[\"keyword.operator\"],\"settings\":{\"foreground\":\"#7fdbca\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Relational\",\"scope\":[\"keyword.operator.relational\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Assignment\",\"scope\":[\"keyword.operator.assignment\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Arithmetic\",\"scope\":[\"keyword.operator.arithmetic\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Bitwise\",\"scope\":[\"keyword.operator.bitwise\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Increment\",\"scope\":[\"keyword.operator.increment\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Ternary\",\"scope\":[\"keyword.operator.ternary\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Double-Slashed Comment\",\"scope\":[\"comment.line.double-slash\"],\"settings\":{\"foreground\":\"#919f9f\"}},{\"name\":\"Object\",\"scope\":[\"object\"],\"settings\":{\"foreground\":\"#cdebf7\"}},{\"name\":\"Null\",\"scope\":[\"constant.language.null\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Meta Brace\",\"scope\":[\"meta.brace\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Meta Delimiter Period\",\"scope\":[\"meta.delimiter.period\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Punctuation Definition String\",\"scope\":[\"punctuation.definition.string\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Punctuation Definition String Markdown\",\"scope\":[\"punctuation.definition.string.begin.markdown\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Boolean\",\"scope\":[\"constant.language.boolean\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Object Comma\",\"scope\":[\"object.comma\"],\"settings\":{\"foreground\":\"#ffffff\"}},{\"name\":\"Variable Parameter Function\",\"scope\":[\"variable.parameter.function\"],\"settings\":{\"foreground\":\"#7fdbca\",\"fontStyle\":\"\"}},{\"name\":\"Support Type Property Name & entity name tags\",\"scope\":[\"support.type.vendor.property-name\",\"support.constant.vendor.property-value\",\"support.type.property-name\",\"meta.property-list entity.name.tag\"],\"settings\":{\"foreground\":\"#80cbc4\",\"fontStyle\":\"\"}},{\"name\":\"Entity Name tag reference in stylesheets\",\"scope\":[\"meta.property-list entity.name.tag.reference\"],\"settings\":{\"foreground\":\"#57eaf1\"}},{\"name\":\"Constant Other Color RGB Value Punctuation Definition Constant\",\"scope\":[\"constant.other.color.rgb-value punctuation.definition.constant\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Constant Other Color\",\"scope\":[\"constant.other.color\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Keyword Other Unit\",\"scope\":[\"keyword.other.unit\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Meta Selector\",\"scope\":[\"meta.selector\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Entity Other Attribute Name Id\",\"scope\":[\"entity.other.attribute-name.id\"],\"settings\":{\"foreground\":\"#fad430\"}},{\"name\":\"Meta Property Name\",\"scope\":[\"meta.property-name\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Doctypes\",\"scope\":[\"entity.name.tag.doctype\",\"meta.tag.sgml.doctype\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Punctuation Definition Parameters\",\"scope\":[\"punctuation.definition.parameters\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Keyword Control Operator\",\"scope\":[\"keyword.control.operator\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Keyword Operator Logical\",\"scope\":[\"keyword.operator.logical\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Variable Instances\",\"scope\":[\"variable.instance\",\"variable.other.instance\",\"variable.readwrite.instance\",\"variable.other.readwrite.instance\",\"variable.other.property\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Variable Property Other object property\",\"scope\":[\"variable.other.object.property\"],\"settings\":{\"foreground\":\"#faf39f\",\"fontStyle\":\"\"}},{\"name\":\"Variable Property Other object\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Entity Name Function\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#82aaff\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Comparison, returns, imports, and Keyword Operator Ruby\",\"scope\":[\"keyword.control.conditional.js\",\"keyword.operator.comparison\",\"keyword.control.flow.js\",\"keyword.control.flow.ts\",\"keyword.control.flow.tsx\",\"keyword.control.ruby\",\"keyword.control.def.ruby\",\"keyword.control.loop.js\",\"keyword.control.loop.ts\",\"keyword.control.import.js\",\"keyword.control.import.ts\",\"keyword.control.import.tsx\",\"keyword.control.from.js\",\"keyword.control.from.ts\",\"keyword.control.from.tsx\",\"keyword.control.conditional.js\",\"keyword.control.conditional.ts\",\"keyword.control.switch.js\",\"keyword.control.switch.ts\",\"keyword.operator.instanceof.js\",\"keyword.operator.expression.instanceof.ts\",\"keyword.operator.expression.instanceof.tsx\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Support Constant, `new` keyword, Special Method Keyword, `debugger`, other keywords\",\"scope\":[\"support.constant\",\"keyword.other.special-method\",\"keyword.other.new\",\"keyword.other.debugger\",\"keyword.control\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Support Function\",\"scope\":[\"support.function\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Invalid Broken\",\"scope\":[\"invalid.broken\"],\"settings\":{\"foreground\":\"#989da0\",\"background\":\"#F78C6C\"}},{\"name\":\"Invalid Unimplemented\",\"scope\":[\"invalid.unimplemented\"],\"settings\":{\"background\":\"#8BD649\",\"foreground\":\"#ffffff\"}},{\"name\":\"Invalid Illegal\",\"scope\":[\"invalid.illegal\"],\"settings\":{\"foreground\":\"#ffffff\",\"background\":\"#ec5f67\"}},{\"name\":\"Language Variable\",\"scope\":[\"variable.language\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Support Variable Property\",\"scope\":[\"support.variable.property\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Variable Function\",\"scope\":[\"variable.function\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Variable Interpolation\",\"scope\":[\"variable.interpolation\"],\"settings\":{\"foreground\":\"#ef787f\"}},{\"name\":\"Meta Function Call\",\"scope\":[\"meta.function-call\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Punctuation Section Embedded\",\"scope\":[\"punctuation.section.embedded\"],\"settings\":{\"foreground\":\"#e2817f\"}},{\"name\":\"Punctuation Tweaks\",\"scope\":[\"punctuation.terminator.expression\",\"punctuation.definition.arguments\",\"punctuation.definition.array\",\"punctuation.section.array\",\"meta.array\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"More Punctuation Tweaks\",\"scope\":[\"punctuation.definition.list.begin\",\"punctuation.definition.list.end\",\"punctuation.separator.arguments\",\"punctuation.definition.list\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Template Strings\",\"scope\":[\"string.template meta.template.expression\"],\"settings\":{\"foreground\":\"#e2817f\"}},{\"name\":\"Backtics(``) in Template Strings\",\"scope\":[\"string.template punctuation.definition.string\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Italics\",\"scope\":[\"italic\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"italic\"}},{\"name\":\"Bold\",\"scope\":[\"bold\"],\"settings\":{\"foreground\":\"#c5e478\",\"fontStyle\":\"bold\"}},{\"name\":\"Quote\",\"scope\":[\"quote\"],\"settings\":{\"foreground\":\"#969bb7\",\"fontStyle\":\"\"}},{\"name\":\"Raw Code\",\"scope\":[\"raw\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"CoffeScript Variable Assignment\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#31e1eb\"}},{\"name\":\"CoffeScript Parameter Function\",\"scope\":[\"variable.parameter.function.coffee\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"CoffeeScript Assignments\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"C# Readwrite Variables\",\"scope\":[\"variable.other.readwrite.cs\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C# Classes & Storage types\",\"scope\":[\"entity.name.type.class.cs\",\"storage.type.cs\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"C# Namespaces\",\"scope\":[\"entity.name.type.namespace.cs\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"C# Unquoted String Zone\",\"scope\":[\"string.unquoted.preprocessor.message.cs\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C# Region\",\"scope\":[\"punctuation.separator.hash.cs\",\"keyword.preprocessor.region.cs\",\"keyword.preprocessor.endregion.cs\"],\"settings\":{\"foreground\":\"#ffcb8b\",\"fontStyle\":\"bold\"}},{\"name\":\"C# Other Variables\",\"scope\":[\"variable.other.object.cs\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"C# Enum\",\"scope\":[\"entity.name.type.enum.cs\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Dart String\",\"scope\":[\"string.interpolated.single.dart\",\"string.interpolated.double.dart\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Dart Class\",\"scope\":[\"support.class.dart\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Tag names in Stylesheets\",\"scope\":[\"entity.name.tag.css\",\"entity.name.tag.less\",\"entity.name.tag.custom.css\",\"support.constant.property-value.css\"],\"settings\":{\"foreground\":\"#ff6d6d\",\"fontStyle\":\"\"}},{\"name\":\"Wildcard(*) selector in Stylesheets\",\"scope\":[\"entity.name.tag.wildcard.css\",\"entity.name.tag.wildcard.less\",\"entity.name.tag.wildcard.scss\",\"entity.name.tag.wildcard.sass\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"CSS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Attribute Name for CSS\",\"scope\":[\"meta.attribute-selector.css entity.other.attribute-name.attribute\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Elixir Classes\",\"scope\":[\"source.elixir support.type.elixir\",\"source.elixir meta.module.elixir entity.name.class.elixir\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Elixir Functions\",\"scope\":[\"source.elixir entity.name.function\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir Constants\",\"scope\":[\"source.elixir constant.other.symbol.elixir\",\"source.elixir constant.other.keywords.elixir\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Elixir String Punctuations\",\"scope\":[\"source.elixir punctuation.definition.string\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir\",\"scope\":[\"source.elixir variable.other.readwrite.module.elixir\",\"source.elixir variable.other.readwrite.module.elixir punctuation.definition.variable.elixir\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir Binary Punctuations\",\"scope\":[\"source.elixir .punctuation.binary.elixir\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Closure Constant Keyword\",\"scope\":[\"constant.keyword.clojure\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Go Function Calls\",\"scope\":[\"source.go meta.function-call.go\"],\"settings\":{\"foreground\":\"#dddddd\"}},{\"name\":\"Go Keywords\",\"scope\":[\"source.go keyword.package.go\",\"source.go keyword.import.go\",\"source.go keyword.function.go\",\"source.go keyword.type.go\",\"source.go keyword.struct.go\",\"source.go keyword.interface.go\",\"source.go keyword.const.go\",\"source.go keyword.var.go\",\"source.go keyword.map.go\",\"source.go keyword.channel.go\",\"source.go keyword.control.go\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Go Constants e.g. nil, string format (%s, %d, etc.)\",\"scope\":[\"source.go constant.language.go\",\"source.go constant.other.placeholder.go\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"C++ Functions\",\"scope\":[\"entity.name.function.preprocessor.cpp\",\"entity.scope.name.cpp\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"C++ Meta Namespace\",\"scope\":[\"meta.namespace-block.cpp\"],\"settings\":{\"foreground\":\"#e0dec6\"}},{\"name\":\"C++ Language Primitive Storage\",\"scope\":[\"storage.type.language.primitive.cpp\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"C++ Preprocessor Macro\",\"scope\":[\"meta.preprocessor.macro.cpp\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C++ Variable Parameter\",\"scope\":[\"variable.parameter\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Powershell Variables\",\"scope\":[\"variable.other.readwrite.powershell\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Powershell Function\",\"scope\":[\"support.function.powershell\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"ID Attribute Name in HTML\",\"scope\":[\"entity.other.attribute-name.id.html\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"HTML Punctuation Definition Tag\",\"scope\":[\"punctuation.definition.tag.html\"],\"settings\":{\"foreground\":\"#6ae9f0\"}},{\"name\":\"HTML Doctype\",\"scope\":[\"meta.tag.sgml.doctype.html\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Classes\",\"scope\":[\"meta.class entity.name.type.class.js\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"JavaScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.js\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"JavaScript Terminator\",\"scope\":[\"terminator.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Meta Punctuation Definition\",\"scope\":[\"meta.js punctuation.definition.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Entity Names in Code Documentations\",\"scope\":[\"entity.name.type.instance.jsdoc\",\"entity.name.type.instance.phpdoc\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"Other Variables in Code Documentations\",\"scope\":[\"variable.other.jsdoc\",\"variable.other.phpdoc\"],\"settings\":{\"foreground\":\"#78ccf0\"}},{\"name\":\"JavaScript module imports and exports\",\"scope\":[\"variable.other.meta.import.js\",\"meta.import.js variable.other\",\"variable.other.meta.export.js\",\"meta.export.js variable.other\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Variable Parameter Function\",\"scope\":[\"variable.parameter.function.js\"],\"settings\":{\"foreground\":\"#8b96ea\"}},{\"name\":\"JavaScript[React] Variable Other Object\",\"scope\":[\"variable.other.object.js\",\"variable.other.object.jsx\",\"variable.object.property.js\",\"variable.object.property.jsx\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Variables\",\"scope\":[\"variable.js\",\"variable.other.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Entity Name Type\",\"scope\":[\"entity.name.type.js\",\"entity.name.type.module.js\"],\"settings\":{\"foreground\":\"#ffcb8b\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Support Classes\",\"scope\":[\"support.class.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JSON Property Names\",\"scope\":[\"support.type.property-name.json\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"JSON Support Constants\",\"scope\":[\"support.constant.json\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"JSON Property values (string)\",\"scope\":[\"meta.structure.dictionary.value.json string.quoted.double\"],\"settings\":{\"foreground\":\"#c789d6\"}},{\"name\":\"Strings in JSON values\",\"scope\":[\"string.quoted.double.json punctuation.definition.string.json\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Specific JSON Property values like null\",\"scope\":[\"meta.structure.dictionary.json meta.structure.dictionary.value constant.language\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"JavaScript Other Variable\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Ruby Variables\",\"scope\":[\"variable.other.ruby\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Ruby Class\",\"scope\":[\"entity.name.type.class.ruby\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"Ruby Hashkeys\",\"scope\":[\"constant.language.symbol.hashkey.ruby\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"LESS Tag names\",\"scope\":[\"entity.name.tag.less\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"LESS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Attribute Name for LESS\",\"scope\":[\"meta.attribute-selector.less entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Markdown Headings\",\"scope\":[\"markup.heading.markdown\",\"markup.heading.setext.1.markdown\",\"markup.heading.setext.2.markdown\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown Italics\",\"scope\":[\"markup.italic.markdown\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"italic\"}},{\"name\":\"Markdown Bold\",\"scope\":[\"markup.bold.markdown\"],\"settings\":{\"foreground\":\"#c5e478\",\"fontStyle\":\"bold\"}},{\"name\":\"Markdown Quote + others\",\"scope\":[\"markup.quote.markdown\"],\"settings\":{\"foreground\":\"#969bb7\",\"fontStyle\":\"\"}},{\"name\":\"Markdown Raw Code + others\",\"scope\":[\"markup.inline.raw.markdown\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Markdown Links\",\"scope\":[\"markup.underline.link.markdown\",\"markup.underline.link.image.markdown\"],\"settings\":{\"foreground\":\"#ff869a\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Link Title and Description\",\"scope\":[\"string.other.link.title.markdown\",\"string.other.link.description.markdown\"],\"settings\":{\"foreground\":\"#d6deeb\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Punctuation\",\"scope\":[\"punctuation.definition.string.markdown\",\"punctuation.definition.string.begin.markdown\",\"punctuation.definition.string.end.markdown\",\"meta.link.inline.markdown punctuation.definition.string\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown MetaData Punctuation\",\"scope\":[\"punctuation.definition.metadata.markdown\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Markdown List Punctuation\",\"scope\":[\"beginning.punctuation.definition.list.markdown\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown Inline Raw String\",\"scope\":[\"markup.inline.raw.string.markdown\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"PHP Variables\",\"scope\":[\"variable.other.php\"],\"settings\":{\"foreground\":\"#bec5d4\"}},{\"name\":\"Support Classes in PHP\",\"scope\":[\"support.class.php\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Punctuations in PHP function calls\",\"scope\":[\"meta.function-call.php punctuation\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"PHP Global Variables\",\"scope\":[\"variable.other.global.php\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Declaration Punctuation in PHP Global Variables\",\"scope\":[\"variable.other.global.php punctuation.definition.variable\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Language Constants in Python\",\"scope\":[\"constant.language.python\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Python Function Parameter and Arguments\",\"scope\":[\"variable.parameter.function.python\",\"meta.function-call.arguments.python\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Python Function Call\",\"scope\":[\"meta.function-call.python\",\"meta.function-call.generic.python\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"Punctuations in Python\",\"scope\":[\"punctuation.python\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Decorator Functions in Python\",\"scope\":[\"entity.name.function.decorator.python\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Python Language Variable\",\"scope\":[\"source.python variable.language.special\"],\"settings\":{\"foreground\":\"#8eace3\"}},{\"name\":\"Python import control keyword\",\"scope\":[\"keyword.control\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"SCSS Variable\",\"scope\":[\"variable.scss\",\"variable.sass\",\"variable.parameter.url.scss\",\"variable.parameter.url.sass\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#bec5d4\"}},{\"name\":\"Attribute Name for SASS\",\"scope\":[\"meta.attribute-selector.scss entity.other.attribute-name.attribute\",\"meta.attribute-selector.sass entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Tag names in SASS\",\"scope\":[\"entity.name.tag.scss\",\"entity.name.tag.sass\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"SASS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.scss\",\"keyword.other.unit.sass\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"TypeScript[React] Variables and Object Properties\",\"scope\":[\"variable.other.readwrite.alias.ts\",\"variable.other.readwrite.alias.tsx\",\"variable.other.readwrite.ts\",\"variable.other.readwrite.tsx\",\"variable.other.object.ts\",\"variable.other.object.tsx\",\"variable.object.property.ts\",\"variable.object.property.tsx\",\"variable.other.ts\",\"variable.other.tsx\",\"variable.tsx\",\"variable.ts\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript[React] Entity Name Types\",\"scope\":[\"entity.name.type.ts\",\"entity.name.type.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript[React] Node Classes\",\"scope\":[\"support.class.node.ts\",\"support.class.node.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"TypeScript[React] Entity Name Types as Parameters\",\"scope\":[\"meta.type.parameters.ts entity.name.type\",\"meta.type.parameters.tsx entity.name.type\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"TypeScript[React] Import/Export Punctuations\",\"scope\":[\"meta.import.ts punctuation.definition.block\",\"meta.import.tsx punctuation.definition.block\",\"meta.export.ts punctuation.definition.block\",\"meta.export.tsx punctuation.definition.block\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.decorator punctuation.decorator.ts\",\"meta.decorator punctuation.decorator.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.tag.js meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"YAML Entity Name Tags\",\"scope\":[\"entity.name.tag.yaml\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"JavaScript Variable Other ReadWrite\",\"scope\":[\"variable.other.readwrite.js\",\"variable.parameter\"],\"settings\":{\"foreground\":\"#d7dbe0\"}},{\"name\":\"Support Class Component\",\"scope\":[\"support.class.component.js\",\"support.class.component.tsx\"],\"settings\":{\"foreground\":\"#f78c6c\",\"fontStyle\":\"\"}},{\"name\":\"Text nested in React tags\",\"scope\":[\"meta.jsx.children\",\"meta.jsx.children.js\",\"meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript Classes\",\"scope\":[\"meta.class entity.name.type.class.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript Entity Name Type\",\"scope\":[\"entity.name.type.tsx\",\"entity.name.type.module.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript Class Variable Keyword\",\"scope\":[\"meta.class.ts meta.var.expr.ts storage.type.ts\",\"meta.class.tsx meta.var.expr.tsx storage.type.tsx\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"TypeScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.ts\",\"meta.method.declaration storage.type.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"normalize font style of certain components\",\"scope\":[\"meta.property-list.css meta.property-value.css variable.other.less\",\"meta.property-list.scss variable.scss\",\"meta.property-list.sass variable.sass\",\"meta.brace\",\"keyword.operator.operator\",\"keyword.operator.or.regexp\",\"keyword.operator.expression.in\",\"keyword.operator.relational\",\"keyword.operator.assignment\",\"keyword.operator.comparison\",\"keyword.operator.type\",\"keyword.operator\",\"keyword\",\"punctuation.definintion.string\",\"punctuation\",\"variable.other.readwrite.js\",\"storage.type\",\"source.css\",\"string.quoted\"],\"settings\":{\"fontStyle\":\"\"}}],\"styleOverrides\":{\"frames\":{\"editorBackground\":\"var(--sl-color-gray-6)\",\"terminalBackground\":\"var(--sl-color-gray-6)\",\"editorActiveTabBackground\":\"var(--sl-color-gray-6)\",\"terminalTitlebarDotsForeground\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"terminalTitlebarDotsOpacity\":\"0.75\",\"inlineButtonForeground\":\"var(--sl-color-text)\",\"frameBoxShadowCssValue\":\"none\"},\"textMarkers\":{\"markBackground\":\"#ffffff17\",\"markBorderColor\":\"#ffffff40\"}}},{\"name\":\"Night Owl Light\",\"type\":\"light\",\"colors\":{\"focusBorder\":\"#93a1a1\",\"foreground\":\"#403f53\",\"disabledForeground\":\"#61616180\",\"descriptionForeground\":\"#403f53\",\"errorForeground\":\"#403f53\",\"icon.foreground\":\"#424242\",\"contrastActiveBorder\":null,\"contrastBorder\":null,\"textBlockQuote.background\":\"#7f7f7f1a\",\"textBlockQuote.border\":\"#007acc80\",\"textCodeBlock.background\":\"#dcdcdc66\",\"textLink.activeForeground\":\"#006ab1\",\"textLink.foreground\":\"#006ab1\",\"textPreformat.foreground\":\"#a31515\",\"textSeparator.foreground\":\"#0000002e\",\"editor.background\":\"#f6f7f9\",\"editor.foreground\":\"#403f53\",\"editorLineNumber.foreground\":\"#90a7b2\",\"editorLineNumber.activeForeground\":\"#403f53\",\"editorActiveLineNumber.foreground\":\"#0b216f\",\"editor.selectionBackground\":\"#e0e0e0\",\"editor.inactiveSelectionBackground\":\"#e0e0e080\",\"editor.selectionHighlightBackground\":\"#339cec33\",\"editorError.foreground\":\"#e64d49\",\"editorWarning.foreground\":\"#daaa01\",\"editorInfo.foreground\":\"#1a85ff\",\"editorHint.foreground\":\"#6c6c6c\",\"problemsErrorIcon.foreground\":\"#e64d49\",\"problemsWarningIcon.foreground\":\"#daaa01\",\"problemsInfoIcon.foreground\":\"#1a85ff\",\"editor.findMatchBackground\":\"#93a1a16c\",\"editor.findMatchHighlightBackground\":\"#93a1a16c\",\"editor.findRangeHighlightBackground\":\"#7497a633\",\"editorLink.activeForeground\":\"#0000ff\",\"editorLightBulb.foreground\":\"#ddb100\",\"editorLightBulbAutoFix.foreground\":\"#007acc\",\"diffEditor.insertedTextBackground\":\"#9ccc2c40\",\"diffEditor.insertedTextBorder\":null,\"diffEditor.removedTextBackground\":\"#ff000033\",\"diffEditor.removedTextBorder\":null,\"diffEditor.insertedLineBackground\":\"#9bb95533\",\"diffEditor.removedLineBackground\":\"#ff000033\",\"editorStickyScroll.background\":\"#fbfbfb\",\"editorStickyScrollHover.background\":\"#f0f0f0\",\"editorInlayHint.background\":\"#2aa29899\",\"editorInlayHint.foreground\":\"#f0f0f0\",\"editorInlayHint.typeBackground\":\"#2aa29899\",\"editorInlayHint.typeForeground\":\"#f0f0f0\",\"editorInlayHint.parameterBackground\":\"#2aa29899\",\"editorInlayHint.parameterForeground\":\"#f0f0f0\",\"editorPane.background\":\"#fbfbfb\",\"editorGroup.emptyBackground\":null,\"editorGroup.focusedEmptyBorder\":null,\"editorGroupHeader.tabsBackground\":\"var(--sl-color-gray-6)\",\"editorGroupHeader.tabsBorder\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"editorGroupHeader.noTabsBackground\":\"#f0f0f0\",\"editorGroupHeader.border\":null,\"editorGroup.border\":\"#f0f0f0\",\"editorGroup.dropBackground\":\"#2677cb2d\",\"editorGroup.dropIntoPromptForeground\":\"#403f53\",\"editorGroup.dropIntoPromptBackground\":\"#f0f0f0\",\"editorGroup.dropIntoPromptBorder\":null,\"sideBySideEditor.horizontalBorder\":\"#f0f0f0\",\"sideBySideEditor.verticalBorder\":\"#f0f0f0\",\"scrollbar.shadow\":\"#cccccc\",\"scrollbarSlider.background\":\"#0000001a\",\"scrollbarSlider.hoverBackground\":\"#00000055\",\"scrollbarSlider.activeBackground\":\"#00000099\",\"panel.background\":\"#f0f0f0\",\"panel.border\":\"#d9d9d9\",\"panelTitle.activeBorder\":\"#424242\",\"panelTitle.activeForeground\":\"#424242\",\"panelTitle.inactiveForeground\":\"#424242bf\",\"panelSectionHeader.background\":\"#80808051\",\"terminal.background\":\"#f6f6f6\",\"widget.shadow\":\"#d9d9d9\",\"editorWidget.background\":\"#f0f0f0\",\"editorWidget.foreground\":\"#403f53\",\"editorWidget.border\":\"#d9d9d9\",\"quickInput.background\":\"#f0f0f0\",\"quickInput.foreground\":\"#403f53\",\"quickInputTitle.background\":\"#0000000f\",\"pickerGroup.foreground\":\"#403f53\",\"pickerGroup.border\":\"#d9d9d9\",\"editor.hoverHighlightBackground\":\"#339cec33\",\"editorHoverWidget.background\":\"#f0f0f0\",\"editorHoverWidget.foreground\":\"#403f53\",\"editorHoverWidget.border\":\"#d9d9d9\",\"editorHoverWidget.statusBarBackground\":\"#e4e4e4\",\"titleBar.activeBackground\":\"var(--sl-color-gray-6)\",\"titleBar.activeForeground\":\"var(--sl-color-text)\",\"titleBar.inactiveBackground\":\"#f0f0f099\",\"titleBar.inactiveForeground\":\"#33333399\",\"titleBar.border\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"toolbar.hoverBackground\":\"#b8b8b850\",\"toolbar.activeBackground\":\"#a6a6a650\",\"tab.activeBackground\":\"#f6f6f6\",\"tab.unfocusedActiveBackground\":\"#f6f6f6\",\"tab.inactiveBackground\":\"#f0f0f0\",\"tab.unfocusedInactiveBackground\":\"#f0f0f0\",\"tab.activeForeground\":\"var(--sl-color-text)\",\"tab.inactiveForeground\":\"#403f53\",\"tab.unfocusedActiveForeground\":\"#403f53b3\",\"tab.unfocusedInactiveForeground\":\"#403f5380\",\"tab.hoverBackground\":null,\"tab.unfocusedHoverBackground\":null,\"tab.hoverForeground\":null,\"tab.unfocusedHoverForeground\":null,\"tab.border\":\"#f0f0f0\",\"tab.lastPinnedBorder\":\"#a9a9a9\",\"tab.activeBorder\":\"transparent\",\"tab.unfocusedActiveBorder\":null,\"tab.activeBorderTop\":\"var(--sl-color-accent)\",\"tab.unfocusedActiveBorderTop\":null,\"tab.hoverBorder\":null,\"tab.unfocusedHoverBorder\":null,\"tab.activeModifiedBorder\":\"#2aa298\",\"tab.inactiveModifiedBorder\":\"#93a1a1\",\"tab.unfocusedActiveModifiedBorder\":\"#93a1a1\",\"tab.unfocusedInactiveModifiedBorder\":\"#93a1a1\",\"badge.background\":\"#2aa298\",\"badge.foreground\":\"#f0f0f0\",\"button.background\":\"#2aa298\",\"button.foreground\":\"#f0f0f0\",\"button.border\":null,\"button.separator\":\"#f0f0f066\",\"button.hoverBackground\":\"#22827a\",\"button.secondaryBackground\":\"#5f6a79\",\"button.secondaryForeground\":\"#ffffff\",\"button.secondaryHoverBackground\":\"#4c5561\",\"dropdown.background\":\"#f0f0f0\",\"dropdown.foreground\":\"#403f53\",\"dropdown.border\":\"#d9d9d9\",\"list.activeSelectionBackground\":\"#d3e8f8\",\"list.activeSelectionForeground\":\"#403f53\",\"tree.indentGuidesStroke\":\"#a9a9a9\",\"input.background\":\"#f0f0f0\",\"input.foreground\":\"#403f53\",\"input.placeholderForeground\":\"#93a1a1\",\"inputOption.activeBorder\":\"#2aa298\",\"inputOption.hoverBackground\":\"#b8b8b850\",\"inputOption.activeBackground\":\"#93a1a133\",\"inputOption.activeForeground\":\"#000000\",\"inputValidation.infoBackground\":\"#f0f0f0\",\"inputValidation.infoBorder\":\"#d0d0d0\",\"inputValidation.warningBackground\":\"#daaa01\",\"inputValidation.warningBorder\":\"#e0af02\",\"inputValidation.errorBackground\":\"#f76e6e\",\"inputValidation.errorBorder\":\"#de3d3b\",\"keybindingLabel.background\":\"#dddddd66\",\"keybindingLabel.foreground\":\"#555555\",\"keybindingLabel.border\":\"#cccccc66\",\"keybindingLabel.bottomBorder\":\"#bbbbbb66\",\"menu.foreground\":\"#403f53\",\"menu.background\":\"#f0f0f0\",\"menu.selectionForeground\":\"#403f53\",\"menu.selectionBackground\":\"#d3e8f8\",\"menu.separatorBackground\":\"#d4d4d4\",\"editor.snippetTabstopHighlightBackground\":\"#0a326433\",\"editor.snippetFinalTabstopHighlightBorder\":\"#0a326480\",\"terminal.ansiBlack\":\"#403f53\",\"terminal.ansiRed\":\"#de3d3b\",\"terminal.ansiGreen\":\"#08916a\",\"terminal.ansiYellow\":\"#e0af02\",\"terminal.ansiBlue\":\"#288ed7\",\"terminal.ansiMagenta\":\"#d6438a\",\"terminal.ansiCyan\":\"#2aa298\",\"terminal.ansiWhite\":\"#f0f0f0\",\"terminal.ansiBrightBlack\":\"#403f53\",\"terminal.ansiBrightRed\":\"#de3d3b\",\"terminal.ansiBrightGreen\":\"#08916a\",\"terminal.ansiBrightYellow\":\"#daaa01\",\"terminal.ansiBrightBlue\":\"#288ed7\",\"terminal.ansiBrightMagenta\":\"#d6438a\",\"terminal.ansiBrightCyan\":\"#2aa298\",\"terminal.ansiBrightWhite\":\"#f0f0f0\",\"selection.background\":\"#7a8181ad\",\"notifications.background\":\"#f0f0f0\",\"notifications.foreground\":\"#403f53\",\"notificationLink.foreground\":\"#994cc3\",\"notifications.border\":\"#cccccc\",\"notificationCenter.border\":\"#cccccc\",\"notificationToast.border\":\"#cccccc\",\"notificationCenterHeader.foreground\":\"#403f53\",\"notificationCenterHeader.background\":\"#f0f0f0\",\"input.border\":\"#d9d9d9\",\"progressBar.background\":\"#2aa298\",\"list.inactiveSelectionBackground\":\"#e0e7ea\",\"list.inactiveSelectionForeground\":\"#403f53\",\"list.focusBackground\":\"#d3e8f8\",\"list.hoverBackground\":\"#d3e8f8\",\"list.focusForeground\":\"#403f53\",\"list.hoverForeground\":\"#403f53\",\"list.highlightForeground\":\"#403f53\",\"list.errorForeground\":\"#e64d49\",\"list.warningForeground\":\"#daaa01\",\"activityBar.background\":\"#f0f0f0\",\"activityBar.foreground\":\"#403f53\",\"activityBar.dropBackground\":\"#d0d0d0\",\"activityBarBadge.background\":\"#403f53\",\"activityBarBadge.foreground\":\"#f0f0f0\",\"activityBar.border\":\"#f0f0f0\",\"sideBar.background\":\"#f0f0f0\",\"sideBar.foreground\":\"#403f53\",\"sideBarTitle.foreground\":\"#403f53\",\"sideBar.border\":\"#f0f0f0\",\"editorGroup.background\":\"#f6f6f6\",\"editorCursor.foreground\":\"#90a7b2\",\"editor.wordHighlightBackground\":\"#339cec33\",\"editor.wordHighlightStrongBackground\":\"#007dd659\",\"editor.lineHighlightBackground\":\"#f0f0f0\",\"editor.rangeHighlightBackground\":\"#7497a633\",\"editorWhitespace.foreground\":\"#d9d9d9\",\"editorIndentGuide.background\":\"#d9d9d9\",\"editorCodeLens.foreground\":\"#403f53\",\"editorBracketMatch.background\":\"#d3e8f8\",\"editorBracketMatch.border\":\"#2aa298\",\"editorError.border\":\"#fbfbfb\",\"editorWarning.border\":\"#daaa01\",\"editorGutter.addedBackground\":\"#49d0c5\",\"editorGutter.modifiedBackground\":\"#6fbef6\",\"editorGutter.deletedBackground\":\"#f76e6e\",\"editorRuler.foreground\":\"#d9d9d9\",\"editorOverviewRuler.errorForeground\":\"#e64d49\",\"editorOverviewRuler.warningForeground\":\"#daaa01\",\"editorSuggestWidget.background\":\"#f0f0f0\",\"editorSuggestWidget.foreground\":\"#403f53\",\"editorSuggestWidget.highlightForeground\":\"#403f53\",\"editorSuggestWidget.selectedBackground\":\"#d3e8f8\",\"editorSuggestWidget.border\":\"#d9d9d9\",\"debugExceptionWidget.background\":\"#f0f0f0\",\"debugExceptionWidget.border\":\"#d9d9d9\",\"editorMarkerNavigation.background\":\"#d0d0d0\",\"editorMarkerNavigationError.background\":\"#f76e6e\",\"editorMarkerNavigationWarning.background\":\"#daaa01\",\"debugToolBar.background\":\"#f0f0f0\",\"extensionButton.prominentBackground\":\"#2aa298\",\"extensionButton.prominentForeground\":\"#f0f0f0\",\"statusBar.background\":\"#f0f0f0\",\"statusBar.border\":\"#f0f0f0\",\"statusBar.debuggingBackground\":\"#f0f0f0\",\"statusBar.debuggingForeground\":\"#403f53\",\"statusBar.foreground\":\"#403f53\",\"statusBar.noFolderBackground\":\"#f0f0f0\",\"statusBar.noFolderForeground\":\"#403f53\",\"peekView.border\":\"#d9d9d9\",\"peekViewEditor.background\":\"#f6f6f6\",\"peekViewEditorGutter.background\":\"#f6f6f6\",\"peekViewEditor.matchHighlightBackground\":\"#49d0c5\",\"peekViewResult.background\":\"#f0f0f0\",\"peekViewResult.fileForeground\":\"#403f53\",\"peekViewResult.lineForeground\":\"#403f53\",\"peekViewResult.matchHighlightBackground\":\"#49d0c5\",\"peekViewResult.selectionBackground\":\"#e0e7ea\",\"peekViewResult.selectionForeground\":\"#403f53\",\"peekViewTitle.background\":\"#f0f0f0\",\"peekViewTitleLabel.foreground\":\"#403f53\",\"peekViewTitleDescription.foreground\":\"#403f53\",\"terminal.foreground\":\"#403f53\"},\"fg\":\"#403f53\",\"bg\":\"#f6f7f9\",\"semanticHighlighting\":false,\"settings\":[{\"name\":\"Changed\",\"scope\":[\"markup.changed\",\"meta.diff.header.git\",\"meta.diff.header.from-file\",\"meta.diff.header.to-file\"],\"settings\":{\"foreground\":\"#556484\"}},{\"name\":\"Deleted\",\"scope\":[\"markup.deleted.diff\"],\"settings\":{\"foreground\":\"#ae3c3afd\"}},{\"name\":\"Inserted\",\"scope\":[\"markup.inserted.diff\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Global settings\",\"settings\":{\"background\":\"#011627\",\"foreground\":\"#403f53\"}},{\"name\":\"Comment\",\"scope\":[\"comment\"],\"settings\":{\"foreground\":\"#5f636f\"}},{\"name\":\"String\",\"scope\":[\"string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"String Quoted\",\"scope\":[\"string.quoted\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Support Constant Math\",\"scope\":[\"support.constant.math\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Number\",\"scope\":[\"constant.numeric\",\"constant.character.numeric\"],\"settings\":{\"foreground\":\"#aa0982\",\"fontStyle\":\"\"}},{\"name\":\"Built-in constant\",\"scope\":[\"constant.language\",\"punctuation.definition.constant\",\"variable.other.constant\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"User-defined constant\",\"scope\":[\"constant.character\",\"constant.other\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Constant Character Escape\",\"scope\":[\"constant.character.escape\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"RegExp String\",\"scope\":[\"string.regexp\",\"string.regexp keyword.other\"],\"settings\":{\"foreground\":\"#3a688f\"}},{\"name\":\"Comma in functions\",\"scope\":[\"meta.function punctuation.separator.comma\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"Variable\",\"scope\":[\"variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Keyword\",\"scope\":[\"punctuation.accessor\",\"keyword\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage\",\"scope\":[\"storage\",\"meta.var.expr\",\"meta.class meta.method.declaration meta.var.expr storage.type.js\",\"storage.type.property.js\",\"storage.type.property.ts\",\"storage.type.property.tsx\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type.function.arrow.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Class name\",\"scope\":[\"entity.name.class\",\"meta.class entity.name.type.class\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Inherited class\",\"scope\":[\"entity.other.inherited-class\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Function name\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Meta Tag\",\"scope\":[\"punctuation.definition.tag\",\"meta.tag\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"HTML Tag names\",\"scope\":[\"entity.name.tag\",\"meta.tag.other.html\",\"meta.tag.other.js\",\"meta.tag.other.tsx\",\"entity.name.tag.tsx\",\"entity.name.tag.js\",\"entity.name.tag\",\"meta.tag.js\",\"meta.tag.tsx\",\"meta.tag.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Tag attribute\",\"scope\":[\"entity.other.attribute-name\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Entity Name Tag Custom\",\"scope\":[\"entity.name.tag.custom\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Library (function & constant)\",\"scope\":[\"support.function\",\"support.constant\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Support Constant Property Value meta\",\"scope\":[\"support.constant.meta.property-value\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Library class/type\",\"scope\":[\"support.type\",\"support.class\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Support Variable DOM\",\"scope\":[\"support.variable.dom\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Invalid\",\"scope\":[\"invalid\"],\"settings\":{\"foreground\":\"#bb2060\"}},{\"name\":\"Invalid deprecated\",\"scope\":[\"invalid.deprecated\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Keyword Operator\",\"scope\":[\"keyword.operator\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Relational\",\"scope\":[\"keyword.operator.relational\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Assignment\",\"scope\":[\"keyword.operator.assignment\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Arithmetic\",\"scope\":[\"keyword.operator.arithmetic\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Bitwise\",\"scope\":[\"keyword.operator.bitwise\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Increment\",\"scope\":[\"keyword.operator.increment\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Ternary\",\"scope\":[\"keyword.operator.ternary\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Double-Slashed Comment\",\"scope\":[\"comment.line.double-slash\"],\"settings\":{\"foreground\":\"#5d6376\"}},{\"name\":\"Object\",\"scope\":[\"object\"],\"settings\":{\"foreground\":\"#58656a\"}},{\"name\":\"Null\",\"scope\":[\"constant.language.null\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Meta Brace\",\"scope\":[\"meta.brace\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Meta Delimiter Period\",\"scope\":[\"meta.delimiter.period\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Punctuation Definition String\",\"scope\":[\"punctuation.definition.string\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Punctuation Definition String Markdown\",\"scope\":[\"punctuation.definition.string.begin.markdown\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Boolean\",\"scope\":[\"constant.language.boolean\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Object Comma\",\"scope\":[\"object.comma\"],\"settings\":{\"foreground\":\"#646464\"}},{\"name\":\"Variable Parameter Function\",\"scope\":[\"variable.parameter.function\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Support Type Property Name & entity name tags\",\"scope\":[\"support.type.vendor.property-name\",\"support.constant.vendor.property-value\",\"support.type.property-name\",\"meta.property-list entity.name.tag\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Entity Name tag reference in stylesheets\",\"scope\":[\"meta.property-list entity.name.tag.reference\"],\"settings\":{\"foreground\":\"#286d70\"}},{\"name\":\"Constant Other Color RGB Value Punctuation Definition Constant\",\"scope\":[\"constant.other.color.rgb-value punctuation.definition.constant\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Constant Other Color\",\"scope\":[\"constant.other.color\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Keyword Other Unit\",\"scope\":[\"keyword.other.unit\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Meta Selector\",\"scope\":[\"meta.selector\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Entity Other Attribute Name Id\",\"scope\":[\"entity.other.attribute-name.id\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Meta Property Name\",\"scope\":[\"meta.property-name\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Doctypes\",\"scope\":[\"entity.name.tag.doctype\",\"meta.tag.sgml.doctype\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Punctuation Definition Parameters\",\"scope\":[\"punctuation.definition.parameters\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Keyword Control Operator\",\"scope\":[\"keyword.control.operator\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Keyword Operator Logical\",\"scope\":[\"keyword.operator.logical\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"\"}},{\"name\":\"Variable Instances\",\"scope\":[\"variable.instance\",\"variable.other.instance\",\"variable.readwrite.instance\",\"variable.other.readwrite.instance\",\"variable.other.property\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Variable Property Other object property\",\"scope\":[\"variable.other.object.property\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Variable Property Other object\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Entity Name Function\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Keyword Operator Comparison, imports, returns and Keyword Operator Ruby\",\"scope\":[\"keyword.operator.comparison\",\"keyword.control.flow.js\",\"keyword.control.flow.ts\",\"keyword.control.flow.tsx\",\"keyword.control.ruby\",\"keyword.control.module.ruby\",\"keyword.control.class.ruby\",\"keyword.control.def.ruby\",\"keyword.control.loop.js\",\"keyword.control.loop.ts\",\"keyword.control.import.js\",\"keyword.control.import.ts\",\"keyword.control.import.tsx\",\"keyword.control.from.js\",\"keyword.control.from.ts\",\"keyword.control.from.tsx\",\"keyword.operator.instanceof.js\",\"keyword.operator.expression.instanceof.ts\",\"keyword.operator.expression.instanceof.tsx\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Control Conditional\",\"scope\":[\"keyword.control.conditional.js\",\"keyword.control.conditional.ts\",\"keyword.control.switch.js\",\"keyword.control.switch.ts\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"\"}},{\"name\":\"Support Constant, `new` keyword, Special Method Keyword, `debugger`, other keywords\",\"scope\":[\"support.constant\",\"keyword.other.special-method\",\"keyword.other.new\",\"keyword.other.debugger\",\"keyword.control\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Support Function\",\"scope\":[\"support.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Invalid Broken\",\"scope\":[\"invalid.broken\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Invalid Unimplemented\",\"scope\":[\"invalid.unimplemented\"],\"settings\":{\"foreground\":\"#486e26\"}},{\"name\":\"Invalid Illegal\",\"scope\":[\"invalid.illegal\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Language Variable\",\"scope\":[\"variable.language\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Support Variable Property\",\"scope\":[\"support.variable.property\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Variable Function\",\"scope\":[\"variable.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variable Interpolation\",\"scope\":[\"variable.interpolation\"],\"settings\":{\"foreground\":\"#a64348\"}},{\"name\":\"Meta Function Call\",\"scope\":[\"meta.function-call\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Punctuation Section Embedded\",\"scope\":[\"punctuation.section.embedded\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Punctuation Tweaks\",\"scope\":[\"punctuation.terminator.expression\",\"punctuation.definition.arguments\",\"punctuation.definition.array\",\"punctuation.section.array\",\"meta.array\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"More Punctuation Tweaks\",\"scope\":[\"punctuation.definition.list.begin\",\"punctuation.definition.list.end\",\"punctuation.separator.arguments\",\"punctuation.definition.list\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Template Strings\",\"scope\":[\"string.template meta.template.expression\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Backtics(``) in Template Strings\",\"scope\":[\"string.template punctuation.definition.string\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Italics\",\"scope\":[\"italic\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"italic\"}},{\"name\":\"Bold\",\"scope\":[\"bold\"],\"settings\":{\"foreground\":\"#3b61b0\",\"fontStyle\":\"bold\"}},{\"name\":\"Quote\",\"scope\":[\"quote\"],\"settings\":{\"foreground\":\"#5c6285\"}},{\"name\":\"Raw Code\",\"scope\":[\"raw\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"CoffeScript Variable Assignment\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#186e73\"}},{\"name\":\"CoffeScript Parameter Function\",\"scope\":[\"variable.parameter.function.coffee\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"CoffeeScript Assignments\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"C# Readwrite Variables\",\"scope\":[\"variable.other.readwrite.cs\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"C# Classes & Storage types\",\"scope\":[\"entity.name.type.class.cs\",\"storage.type.cs\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"C# Namespaces\",\"scope\":[\"entity.name.type.namespace.cs\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Tag names in Stylesheets\",\"scope\":[\"entity.name.tag.css\",\"entity.name.tag.less\",\"entity.name.tag.custom.css\",\"support.constant.property-value.css\"],\"settings\":{\"foreground\":\"#984e4d\",\"fontStyle\":\"\"}},{\"name\":\"Wildcard(*) selector in Stylesheets\",\"scope\":[\"entity.name.tag.wildcard.css\",\"entity.name.tag.wildcard.less\",\"entity.name.tag.wildcard.scss\",\"entity.name.tag.wildcard.sass\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"CSS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Attribute Name for CSS\",\"scope\":[\"meta.attribute-selector.css entity.other.attribute-name.attribute\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Elixir Classes\",\"scope\":[\"source.elixir support.type.elixir\",\"source.elixir meta.module.elixir entity.name.class.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Functions\",\"scope\":[\"source.elixir entity.name.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Constants\",\"scope\":[\"source.elixir constant.other.symbol.elixir\",\"source.elixir constant.other.keywords.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir String Punctuations\",\"scope\":[\"source.elixir punctuation.definition.string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir\",\"scope\":[\"source.elixir variable.other.readwrite.module.elixir\",\"source.elixir variable.other.readwrite.module.elixir punctuation.definition.variable.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Binary Punctuations\",\"scope\":[\"source.elixir .punctuation.binary.elixir\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Closure Constant Keyword\",\"scope\":[\"constant.keyword.clojure\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Go Function Calls\",\"scope\":[\"source.go meta.function-call.go\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Go Keywords\",\"scope\":[\"source.go keyword.package.go\",\"source.go keyword.import.go\",\"source.go keyword.function.go\",\"source.go keyword.type.go\",\"source.go keyword.struct.go\",\"source.go keyword.interface.go\",\"source.go keyword.const.go\",\"source.go keyword.var.go\",\"source.go keyword.map.go\",\"source.go keyword.channel.go\",\"source.go keyword.control.go\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Go Constants e.g. nil, string format (%s, %d, etc.)\",\"scope\":[\"source.go constant.language.go\",\"source.go constant.other.placeholder.go\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"C++ Functions\",\"scope\":[\"entity.name.function.preprocessor.cpp\",\"entity.scope.name.cpp\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"C++ Meta Namespace\",\"scope\":[\"meta.namespace-block.cpp\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"C++ Language Primitive Storage\",\"scope\":[\"storage.type.language.primitive.cpp\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"C++ Preprocessor Macro\",\"scope\":[\"meta.preprocessor.macro.cpp\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"C++ Variable Parameter\",\"scope\":[\"variable.parameter\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Powershell Variables\",\"scope\":[\"variable.other.readwrite.powershell\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Powershell Function\",\"scope\":[\"support.function.powershell\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"ID Attribute Name in HTML\",\"scope\":[\"entity.other.attribute-name.id.html\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"HTML Punctuation Definition Tag\",\"scope\":[\"punctuation.definition.tag.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"HTML Doctype\",\"scope\":[\"meta.tag.sgml.doctype.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"JavaScript Classes\",\"scope\":[\"meta.class entity.name.type.class.js\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"JavaScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.js\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"JavaScript Terminator\",\"scope\":[\"terminator.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Meta Punctuation Definition\",\"scope\":[\"meta.js punctuation.definition.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Entity Names in Code Documentations\",\"scope\":[\"entity.name.type.instance.jsdoc\",\"entity.name.type.instance.phpdoc\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"Other Variables in Code Documentations\",\"scope\":[\"variable.other.jsdoc\",\"variable.other.phpdoc\"],\"settings\":{\"foreground\":\"#3e697c\"}},{\"name\":\"JavaScript module imports and exports\",\"scope\":[\"variable.other.meta.import.js\",\"meta.import.js variable.other\",\"variable.other.meta.export.js\",\"meta.export.js variable.other\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Variable Parameter Function\",\"scope\":[\"variable.parameter.function.js\"],\"settings\":{\"foreground\":\"#555ea2\"}},{\"name\":\"JavaScript[React] Variable Other Object\",\"scope\":[\"variable.other.object.js\",\"variable.other.object.jsx\",\"variable.object.property.js\",\"variable.object.property.jsx\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Variables\",\"scope\":[\"variable.js\",\"variable.other.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Entity Name Type\",\"scope\":[\"entity.name.type.js\",\"entity.name.type.module.js\"],\"settings\":{\"foreground\":\"#111111\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Support Classes\",\"scope\":[\"support.class.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JSON Property Names\",\"scope\":[\"support.type.property-name.json\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"JSON Support Constants\",\"scope\":[\"support.constant.json\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"JSON Property values (string)\",\"scope\":[\"meta.structure.dictionary.value.json string.quoted.double\"],\"settings\":{\"foreground\":\"#7c5686\"}},{\"name\":\"Strings in JSON values\",\"scope\":[\"string.quoted.double.json punctuation.definition.string.json\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Specific JSON Property values like null\",\"scope\":[\"meta.structure.dictionary.json meta.structure.dictionary.value constant.language\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"JavaScript Other Variable\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Ruby Variables\",\"scope\":[\"variable.other.ruby\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Ruby Class\",\"scope\":[\"entity.name.type.class.ruby\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Ruby Hashkeys\",\"scope\":[\"constant.language.symbol.hashkey.ruby\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Ruby Symbols\",\"scope\":[\"constant.language.symbol.ruby\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"LESS Tag names\",\"scope\":[\"entity.name.tag.less\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"LESS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Attribute Name for LESS\",\"scope\":[\"meta.attribute-selector.less entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Markdown Headings\",\"scope\":[\"markup.heading.markdown\",\"markup.heading.setext.1.markdown\",\"markup.heading.setext.2.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown Italics\",\"scope\":[\"markup.italic.markdown\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"italic\"}},{\"name\":\"Markdown Bold\",\"scope\":[\"markup.bold.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\",\"fontStyle\":\"bold\"}},{\"name\":\"Markdown Quote + others\",\"scope\":[\"markup.quote.markdown\"],\"settings\":{\"foreground\":\"#5c6285\"}},{\"name\":\"Markdown Raw Code + others\",\"scope\":[\"markup.inline.raw.markdown\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Markdown Links\",\"scope\":[\"markup.underline.link.markdown\",\"markup.underline.link.image.markdown\"],\"settings\":{\"foreground\":\"#954f5a\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Link Title and Description\",\"scope\":[\"string.other.link.title.markdown\",\"string.other.link.description.markdown\"],\"settings\":{\"foreground\":\"#403f53\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Punctuation\",\"scope\":[\"punctuation.definition.string.markdown\",\"punctuation.definition.string.begin.markdown\",\"punctuation.definition.string.end.markdown\",\"meta.link.inline.markdown punctuation.definition.string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown MetaData Punctuation\",\"scope\":[\"punctuation.definition.metadata.markdown\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Markdown List Punctuation\",\"scope\":[\"beginning.punctuation.definition.list.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown Inline Raw String\",\"scope\":[\"markup.inline.raw.string.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"PHP Variables\",\"scope\":[\"variable.other.php\",\"variable.other.property.php\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Support Classes in PHP\",\"scope\":[\"support.class.php\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Punctuations in PHP function calls\",\"scope\":[\"meta.function-call.php punctuation\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"PHP Global Variables\",\"scope\":[\"variable.other.global.php\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Declaration Punctuation in PHP Global Variables\",\"scope\":[\"variable.other.global.php punctuation.definition.variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Language Constants in Python\",\"scope\":[\"constant.language.python\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Python Function Parameter and Arguments\",\"scope\":[\"variable.parameter.function.python\",\"meta.function-call.arguments.python\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Python Function Call\",\"scope\":[\"meta.function-call.python\",\"meta.function-call.generic.python\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Punctuations in Python\",\"scope\":[\"punctuation.python\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Decorator Functions in Python\",\"scope\":[\"entity.name.function.decorator.python\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Python Language Variable\",\"scope\":[\"source.python variable.language.special\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Python import control keyword\",\"scope\":[\"keyword.control\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"SCSS Variable\",\"scope\":[\"variable.scss\",\"variable.sass\",\"variable.parameter.url.scss\",\"variable.parameter.url.sass\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Attribute Name for SASS\",\"scope\":[\"meta.attribute-selector.scss entity.other.attribute-name.attribute\",\"meta.attribute-selector.sass entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Tag names in SASS\",\"scope\":[\"entity.name.tag.scss\",\"entity.name.tag.sass\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"SASS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.scss\",\"keyword.other.unit.sass\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"TypeScript[React] Variables and Object Properties\",\"scope\":[\"variable.other.readwrite.alias.ts\",\"variable.other.readwrite.alias.tsx\",\"variable.other.readwrite.ts\",\"variable.other.readwrite.tsx\",\"variable.other.object.ts\",\"variable.other.object.tsx\",\"variable.object.property.ts\",\"variable.object.property.tsx\",\"variable.other.ts\",\"variable.other.tsx\",\"variable.tsx\",\"variable.ts\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript[React] Entity Name Types\",\"scope\":[\"entity.name.type.ts\",\"entity.name.type.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript[React] Node Classes\",\"scope\":[\"support.class.node.ts\",\"support.class.node.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"TypeScript[React] Entity Name Types as Parameters\",\"scope\":[\"meta.type.parameters.ts entity.name.type\",\"meta.type.parameters.tsx entity.name.type\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"TypeScript[React] Import/Export Punctuations\",\"scope\":[\"meta.import.ts punctuation.definition.block\",\"meta.import.tsx punctuation.definition.block\",\"meta.export.ts punctuation.definition.block\",\"meta.export.tsx punctuation.definition.block\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.decorator punctuation.decorator.ts\",\"meta.decorator punctuation.decorator.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.tag.js meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"YAML Entity Name Tags\",\"scope\":[\"entity.name.tag.yaml\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"JavaScript Variable Other ReadWrite\",\"scope\":[\"variable.other.readwrite.js\",\"variable.parameter\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Support Class Component\",\"scope\":[\"support.class.component.js\",\"support.class.component.tsx\"],\"settings\":{\"foreground\":\"#aa0982\",\"fontStyle\":\"\"}},{\"name\":\"Text nested in React tags\",\"scope\":[\"meta.jsx.children\",\"meta.jsx.children.js\",\"meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript Classes\",\"scope\":[\"meta.class entity.name.type.class.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript Entity Name Type\",\"scope\":[\"entity.name.type.tsx\",\"entity.name.type.module.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript Class Variable Keyword\",\"scope\":[\"meta.class.ts meta.var.expr.ts storage.type.ts\",\"meta.class.tsx meta.var.expr.tsx storage.type.tsx\"],\"settings\":{\"foreground\":\"#76578b\"}},{\"name\":\"TypeScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.ts\",\"meta.method.declaration storage.type.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"normalize font style of certain components\",\"scope\":[\"meta.property-list.css meta.property-value.css variable.other.less\",\"meta.property-list.scss variable.scss\",\"meta.property-list.sass variable.sass\",\"meta.brace\",\"keyword.operator.operator\",\"keyword.operator.or.regexp\",\"keyword.operator.expression.in\",\"keyword.operator.relational\",\"keyword.operator.assignment\",\"keyword.operator.comparison\",\"keyword.operator.type\",\"keyword.operator\",\"keyword\",\"punctuation.definintion.string\",\"punctuation\",\"variable.other.readwrite.js\",\"storage.type\",\"source.css\",\"string.quoted\"],\"settings\":{\"fontStyle\":\"\"}}],\"styleOverrides\":{\"frames\":{\"editorBackground\":\"var(--sl-color-gray-7)\",\"terminalBackground\":\"var(--sl-color-gray-7)\",\"editorActiveTabBackground\":\"var(--sl-color-gray-7)\",\"terminalTitlebarDotsForeground\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"terminalTitlebarDotsOpacity\":\"0.75\",\"inlineButtonForeground\":\"var(--sl-color-text)\",\"frameBoxShadowCssValue\":\"none\"},\"textMarkers\":{\"markBackground\":\"#0000001a\",\"markBorderColor\":\"#00000055\"}}}],\"defaultLocale\":\"en\",\"cascadeLayer\":\"starlight.components\",\"styleOverrides\":{\"borderRadius\":\"0px\",\"borderWidth\":\"1px\",\"codePaddingBlock\":\"0.75rem\",\"codePaddingInline\":\"1rem\",\"codeFontFamily\":\"var(--__sl-font-mono)\",\"codeFontSize\":\"var(--sl-text-code)\",\"codeLineHeight\":\"var(--sl-line-height)\",\"uiFontFamily\":\"var(--__sl-font)\",\"textMarkers\":{\"lineDiffIndicatorMarginLeft\":\"0.25rem\",\"defaultChroma\":\"45\",\"backgroundOpacity\":\"60%\"}},\"plugins\":[{\"name\":\"Starlight Plugin\",\"hooks\":{}},{\"name\":\"astro-expressive-code\",\"hooks\":{}}]}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false},\"prefetch\":{\"prefetchAll\":true},\"i18n\":{\"defaultLocale\":\"en\",\"locales\":[\"en\"],\"routing\":{\"prefixDefaultLocale\":false,\"redirectToDefaultLocale\":false,\"fallbackType\":\"redirect\"}}}","docs",["Map",11,12,24,25,34,35,44,45,54,55,64,65,74,75,84,85,94,95,104,105,114,115,124,125,134,135,144,145,154,155,164,165,174,175,184,185,194,195,204,205,214,215,224,225,234,235,244,245,300,301,400,401,418,419,457,458,484,485,529,530,547,548,586,587,645,646,680,681,690,691,711,712,753,754,832,833,842,843,852,853,862,863],"development",{"id":11,"data":13,"body":21,"filePath":22,"digest":23,"deferredRender":15},{"title":14,"editUrl":15,"head":16,"template":17,"sidebar":18,"pagefind":15,"draft":19},"Development",true,[],"doc",{"hidden":19,"attrs":20},false,{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\".github/workflows/pre-commit.yml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml\" />\n\n  \u003CSourceLink text=\".gitignore\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.gitignore\" />\n\n  \u003CSourceLink text=\".pre-commit-config.yaml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml\" />\n\n  \u003CSourceLink text=\"pyproject.toml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml\" />\n\n  \u003CSourceLink text=\"requirements-dev.txt\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements-dev.txt\" />\n\n  \u003CSourceLink text=\"scripts/check-branch-name.sh\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh\" />\n\n  \u003CSourceLink text=\"scripts/check-current-branch.sh\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-current-branch.sh\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document covers the development workflows, code quality tools, and project structure for the NI Compute Subnet codebase. It includes pre-commit hooks, branch naming conventions, dependency management, and testing infrastructure that ensures code quality and maintainability.\n\nFor information about specific CLI tools used in development, see [CLI Tools](/cli-tools#8). For details about system configuration, see [Configuration](/configuration#7).\n\n## Development Workflow Overview\n\nThe development process enforces code quality through automated checks and standardized workflows. The system uses pre-commit hooks, branch naming validation, and continuous integration to maintain code standards.\n\n### Pre-commit Hook System\n\n```mermaid\ngraph TB\n    DEV[\"Developer\"]\n    COMMIT[\"git commit\"]\n    PRECOMMIT[\"pre-commit hooks\"]\n    \n    subgraph \"Code Quality Checks\"\n        TRAILING[\"trailing-whitespace\"]\n        EOF[\"end-of-file-fixer\"]\n        COMMITLINT[\"commitlint\"]\n        BRANCH[\"check-branch-name\"]\n        PIPREQS[\"pip-compile requirements\"]\n        PIPDEV[\"pip-compile dev requirements\"]\n        PYTEST[\"run-pytest\"]\n    end\n    \n    PUSH[\"git push\"]\n    GHACTIONS[\"GitHub Actions\"]\n    \n    DEV --> COMMIT\n    COMMIT --> PRECOMMIT\n    PRECOMMIT --> TRAILING\n    PRECOMMIT --> EOF\n    PRECOMMIT --> COMMITLINT\n    PRECOMMIT --> BRANCH\n    PRECOMMIT --> PIPREQS\n    PRECOMMIT --> PIPDEV\n    PRECOMMIT --> PYTEST\n    \n    TRAILING --> PUSH\n    EOF --> PUSH\n    COMMITLINT --> PUSH\n    BRANCH --> PUSH\n    PIPREQS --> PUSH\n    PIPDEV --> PUSH\n    PYTEST --> PUSH\n    \n    PUSH --> GHACTIONS\n```\n\nSources: \u003CSourceLink text=\".pre-commit-config.yaml:1-64\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L1-L64\" />, \u003CSourceLink text=\".github/workflows/pre-commit.yml:1-22\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml#L1-L22\" />\n\n### Branch Naming Convention\n\nThe codebase enforces strict branch naming conventions through the `check-branch-name.sh` script:\n\n| Branch Type | Format | Example |\n|-------------|--------|---------|\n| Feature | `feat/CSN-XXXX-description` | `feat/CSN-1234-add-gpu-monitoring` |\n| Bugfix | `fix/CSN-XXXX-description` | `fix/CSN-5678-memory-leak` |\n| Hotfix | `hotfix/vX.Y.Z-CSN-XXXX-description` | `hotfix/v1.2.3-CSN-1234-critical-fix` |\n| Release | `release/vX.Y.Z` | `release/v1.2.3` |\n| Protected | `main`, `dev` | No validation required |\n\n**Branch Validation Rules:**\n- Must include JIRA ticket format: `CSN-XXXX` (where XXXX is any number of digits)\n- Description must be lowercase, kebab-case (hyphens only)\n- No underscores, spaces, or special characters in description\n- Must start with allowed prefix: `feat`, `fix`, `hotfix`, `chore`, `refactor`, `test`, `spike`, `prototype`, `release`, `docs`\n\nSources: \u003CSourceLink text=\"scripts/check-branch-name.sh:1-114\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh#L1-L114\" />, \u003CSourceLink text=\"scripts/check-current-branch.sh:1-7\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-current-branch.sh#L1-L7\" />\n\n## Code Quality Enforcement\n\n### Pre-commit Hook Configuration\n\nThe pre-commit system runs multiple quality checks before allowing commits:\n\n```mermaid\ngraph LR\n    subgraph \"File Quality\"\n        TRAIL[\"trailing-whitespace\"]\n        EOF[\"end-of-file-fixer\"]\n    end\n    \n    subgraph \"Commit Standards\"\n        COMMITLINT[\"commitlint\u003Cbr/>@commitlint/config-conventional\"]\n        BRANCH[\"check-branch-name.sh\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        PIPMAIN[\"pip-compile\u003Cbr/>requirements.txt\"]\n        PIPDEV[\"pip-compile\u003Cbr/>requirements-dev.txt\"]\n    end\n    \n    subgraph \"Testing\"\n        PYTEST[\"pytest\u003Cbr/>--alluredir allure-results\"]\n    end\n    \n    TRAIL --> COMMITLINT\n    EOF --> BRANCH\n    COMMITLINT --> PIPMAIN\n    BRANCH --> PIPDEV\n    PIPMAIN --> PYTEST\n    PIPDEV --> PYTEST\n```\n\nSources: \u003CSourceLink text=\".pre-commit-config.yaml:1-64\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L1-L64\" />\n\n### Continuous Integration\n\nGitHub Actions runs the same pre-commit checks on pull requests and pushes to `dev` and `main` branches:\n\n```yaml\n# Workflow triggers\non:\n  pull_request:\n  push:\n    branches: [\"dev\", \"main\"]\n```\n\nThe CI pipeline installs dependencies and runs all pre-commit hooks to ensure code quality standards are maintained across the repository.\n\nSources: \u003CSourceLink text=\".github/workflows/pre-commit.yml:1-22\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml#L1-L22\" />\n\n## Project Structure and Dependencies\n\n### Project Configuration\n\nThe project uses `pyproject.toml` for modern Python packaging and dependency management:\n\n```mermaid\ngraph TB\n    subgraph \"Build System\"\n        HATCH[\"hatchling>:1.24.2\"]\n        BUILD[\"hatchling.build\"]\n    end\n    \n    subgraph \"Project Metadata\"\n        NAME[\"NI-Compute\"]\n        DESC[\"NI Compute Subnet\"]\n        AUTHOR[\"neuralinternet.ai\"]\n        LICENSE[\"MIT\"]\n        PYTHON[\">:3.10\"]\n    end\n    \n    subgraph \"Dependencies\"\n        CORE[\"Core Dependencies\u003Cbr/>bittensor, torch, docker\"]\n        DEV[\"Development Dependencies\u003Cbr/>pytest, pre-commit, pip-tools\"]\n    end\n    \n    subgraph \"Package Structure\"\n        COMPUTE[\"compute/\"]\n        NEURONS[\"neurons/\"]\n    end\n    \n    HATCH --> BUILD\n    BUILD --> NAME\n    NAME --> CORE\n    CORE --> DEV\n    DEV --> COMPUTE\n    COMPUTE --> NEURONS\n```\n\nSources: \u003CSourceLink text=\"pyproject.toml:1-97\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L1-L97\" />\n\n### Dependency Management\n\nThe project uses `pip-tools` for dependency resolution and pinning:\n\n| File | Purpose | Generation Command |\n|------|---------|-------------------|\n| `requirements.txt` | Production dependencies | `pip-compile pyproject.toml` |\n| `requirements-dev.txt` | Development dependencies | `pip-compile --extra dev pyproject.toml` |\n\n**Key Dependencies:**\n- **Core**: `bittensor==9.0.0`, `torch==2.5.1`, `docker==7.0.0`\n- **GPU/Compute**: `GPUtil==1.4.0`, `igpu==0.1.2`, `numpy==2.0.2`\n- **Security**: `cryptography==43.0.1`, `paramiko==3.4.1`\n- **Monitoring**: `wandb==0.19.0`, `psutil==5.9.8`\n- **Development**: `pytest`, `pre-commit`, `allure-pytest`\n\nSources: \u003CSourceLink text=\"pyproject.toml:36-69\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L36-L69\" />, \u003CSourceLink text=\"requirements-dev.txt:1-418\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements-dev.txt#L1-L418\" />\n\n### Testing Configuration\n\nThe project uses `pytest` with coverage reporting and Allure for test reporting:\n\n```toml\n[tool.pytest.ini_options]\naddopts = \"-v --cov=. --cov-report=term-missing\"\ntestpaths = [\"tests\"]\n```\n\nTests are automatically run during pre-commit with Allure results generation:\n```bash\npython -m pytest --alluredir allure-results\n```\n\nSources: \u003CSourceLink text=\"pyproject.toml:92-97\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L92-L97\" />, \u003CSourceLink text=\".pre-commit-config.yaml:56-63\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L56-L63\" />\n\n## File Organization and Exclusions\n\n### Git Ignore Patterns\n\nThe `.gitignore` file excludes development artifacts and sensitive data:\n\n```mermaid\ngraph TB\n    subgraph \"Python Artifacts\"\n        PYCACHE[\"__pycache__/\"]\n        PYCDLL[\"*.py[cod]\"]\n        DIST[\"dist/, build/, *.egg-info/\"]\n    end\n    \n    subgraph \"Development Tools\"\n        WANDB[\"wandb/\"]\n        PYTEST[\"allure-results, .pytest_cache/\"]\n        COVERAGE[\".coverage, htmlcov/\"]\n    end\n    \n    subgraph \"IDE and Editors\"\n        IDEA[\".idea/\"]\n        VSCODE[\".vscode/\"]\n        DSSTORE[\".DS_Store\"]\n    end\n    \n    subgraph \"Project Specific\"\n        DATABASE[\"database.db\"]\n        CERTS[\"cert/\"]\n        MINERAPP[\"neurons/Miner/app\"]\n        REGAPI[\"neurons/register-api/\"]\n    end\n    \n    PYCACHE --> WANDB\n    PYCDLL --> PYTEST\n    DIST --> COVERAGE\n    WANDB --> IDEA\n    PYTEST --> VSCODE\n    COVERAGE --> DSSTORE\n    IDEA --> DATABASE\n    VSCODE --> CERTS\n    DSSTORE --> MINERAPP\n    DATABASE --> REGAPI\n```\n\nSources: \u003CSourceLink text=\".gitignore:1-263\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.gitignore#L1-L263\" />\n\n### Build Configuration\n\nThe project packages include:\n- `compute/` - Core compute subnet logic\n- `neurons/` - Validator and miner implementations\n\nBuild artifacts are excluded from the source distribution:\n```toml\n[tool.hatch.build.targets.sdist]\nexclude = [\"/.github\"]\n```\n\nVersion information is managed through `compute/__init__.py`:\n```toml\n[tool.hatch.version]\npath = \"compute/__init__.py\"\n```\n\nSources: \u003CSourceLink text=\"pyproject.toml:78-91\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L78-L91\" />","src/content/docs/development.mdx","e98c4a65d7f87ecc","cli-tools",{"id":24,"data":26,"body":31,"filePath":32,"digest":33,"deferredRender":15},{"title":27,"editUrl":15,"head":28,"template":17,"sidebar":29,"pagefind":15,"draft":19},"CLI Tools",[],{"hidden":19,"attrs":30},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/register.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document covers the command-line utilities provided by the NI Compute Subnet for interacting with the compute resource marketplace. The primary CLI tool is the Registration CLI, which enables users to allocate, deallocate, and manage compute resources on the network.\n\nFor information about the underlying Resource Allocation API that these tools interact with, see [Resource Allocation API](/resource-allocation-api#4). For details about the communication protocols used, see [Communication Protocols](/communication-protocols#5).\n\n## Registration CLI Tool\n\nThe main CLI tool is implemented in `neurons/register.py` and provides a comprehensive interface for managing compute resources on the subnet. This tool serves as the primary user interface for validators and resource consumers to interact with the decentralized compute marketplace.\n\n### CLI Command Architecture\n\nThe Registration CLI follows a command-driven architecture with the following components:\n\n```mermaid\ngraph TB\n    subgraph \"CLI Interface Layer\"\n        MAIN[\"main()\"]\n        PARSER[\"argparse.ArgumentParser\"]\n        SUBPARSERS[\"command subparsers\"]\n    end\n    \n    subgraph \"Core Commands\"\n        ALLOCATE[\"allocate()\"]\n        ALLOCATE_HOTKEY[\"allocate_hotkey()\"]\n        DEALLOCATE[\"deallocate()\"]\n        LIST_ALLOC[\"list_allocations()\"]\n        LIST_HOTKEYS[\"list_allocations_hotkeys()\"]\n        LIST_RESOURCES[\"list_resources()\"]\n        PENALIZE[\"penalize_hotkey()\"]\n        DEPENALIZE[\"depenalize_hotkey()\"]\n        LIST_PENALTIES[\"list_penalizations()\"]\n    end\n    \n    subgraph \"Configuration Layer\"\n        GET_CONFIG[\"get_config()\"]\n        GET_CONFIG_CLI[\"get_config_cli()\"]\n    end\n    \n    subgraph \"Backend Integration\"\n        ALLOC_CONTAINER[\"allocate_container()\"]\n        ALLOC_CONTAINER_HK[\"allocate_container_hotkey()\"]\n        UPDATE_ALLOC_DB[\"update_allocation_db()\"]\n        UPDATE_ALLOC_WANDB[\"update_allocation_wandb()\"]\n    end\n    \n    MAIN --> PARSER\n    PARSER --> SUBPARSERS\n    SUBPARSERS --> ALLOCATE\n    SUBPARSERS --> ALLOCATE_HOTKEY\n    SUBPARSERS --> DEALLOCATE\n    SUBPARSERS --> LIST_ALLOC\n    SUBPARSERS --> LIST_HOTKEYS\n    SUBPARSERS --> LIST_RESOURCES\n    SUBPARSERS --> PENALIZE\n    SUBPARSERS --> DEPENALIZE\n    SUBPARSERS --> LIST_PENALTIES\n    \n    ALLOCATE --> GET_CONFIG_CLI\n    ALLOCATE --> ALLOC_CONTAINER\n    ALLOCATE_HOTKEY --> GET_CONFIG\n    ALLOCATE_HOTKEY --> ALLOC_CONTAINER_HK\n    DEALLOCATE --> UPDATE_ALLOC_DB\n    DEALLOCATE --> UPDATE_ALLOC_WANDB\n```\n\nSources: \u003CSourceLink text=\"neurons/register.py:781-854\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L781-L854\" />\n\n### Available Commands\n\nThe CLI provides the following commands for resource management:\n\n| Command | Function | Description |\n|---------|----------|-------------|\n| `a` | `allocate()` | Allocate resource via device requirements (GPU) |\n| `a_hotkey` | `allocate_hotkey()` | Allocate resource via specific hotkey |\n| `d` | `deallocate()` | De-allocate resource(s) |\n| `list_a` | `list_allocations()` | List allocated resources |\n| `list_ah` | `list_allocations_hotkeys()` | List allocated resource hotkeys |\n| `list_r` | `list_resources()` | List available resources |\n| `p_hotkey` | `penalize_hotkey()` | Penalize resource via hotkey |\n| `dp_hotkey` | `depenalize_hotkey()` | De-penalize resource via hotkey |\n| `list_p` | `list_penalizations()` | List penalized hotkeys |\n\nSources: \u003CSourceLink text=\"neurons/register.py:790-827\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L790-L827\" />\n\n## Resource Allocation Flow\n\nThe allocation process involves several steps from CLI input to resource provisioning:\n\n```mermaid\nsequenceDiagram\n    participant USER as \"User\"\n    participant CLI as \"register.py\"\n    participant CONFIG as \"get_config_cli()\"\n    participant ALLOC as \"allocate_container()\"\n    participant BT as \"bt.wallet/bt.subtensor\"\n    participant DB as \"ComputeDb\"\n    participant DENDRITE as \"bt.dendrite\"\n    participant MINER as \"Miner Axon\"\n    participant WANDB as \"ComputeWandb\"\n    \n    USER->>CLI: \"a\" command\n    CLI->>CONFIG: \"get_config_cli()\"\n    CONFIG->>USER: \"prompt for GPU specs\"\n    USER->>CONFIG: \"gpu_type, gpu_size\"\n    CLI->>ALLOC: \"allocate_container(device_requirement)\"\n    \n    ALLOC->>BT: \"create wallet/subtensor/dendrite\"\n    ALLOC->>BT: \"subtensor.metagraph(netuid)\"\n    ALLOC->>DB: \"select_allocate_miners_hotkey()\"\n    ALLOC->>DENDRITE: \"query(axons, Allocate, checking=True)\"\n    MINER->>ALLOC: \"availability response\"\n    \n    ALLOC->>DENDRITE: \"query(best_axon, Allocate, checking=False)\"\n    MINER->>ALLOC: \"allocation response with SSH details\"\n    ALLOC->>CLI: \"encrypted connection info\"\n    CLI->>CLI: \"rsa.decrypt_data()\"\n    CLI->>DB: \"update_allocation_db()\"\n    CLI->>WANDB: \"update_allocation_wandb()\"\n    CLI->>USER: \"display SSH connection details\"\n```\n\nSources: \u003CSourceLink text=\"neurons/register.py:230-288\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L230-L288\" />, \u003CSourceLink text=\"neurons/register.py:117-180\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L117-L180\" />\n\n## System Integration\n\nThe CLI tool integrates with multiple system components to provide comprehensive resource management:\n\n```mermaid\ngraph TB\n    subgraph \"CLI Layer\"\n        REGISTER[\"neurons/register.py\"]\n        COMMANDS[\"CLI Commands\"]\n    end\n    \n    subgraph \"Bittensor Network\"\n        WALLET[\"bt.wallet\"]\n        SUBTENSOR[\"bt.subtensor\"]\n        DENDRITE[\"bt.dendrite\"]\n        METAGRAPH[\"metagraph\"]\n    end\n    \n    subgraph \"Protocols\"\n        ALLOCATE_PROTO[\"Allocate Protocol\"]\n    end\n    \n    subgraph \"Data Layer\"\n        COMPUTEDB[\"ComputeDb\"]\n        ALLOCATION_TABLE[\"allocation table\"]\n        BLACKLIST_TABLE[\"blacklist table\"]\n    end\n    \n    subgraph \"Monitoring\"\n        COMPUTEWANDB[\"ComputeWandb\"]\n        ALLOCATED_HOTKEYS[\"allocated_hotkeys\"]\n        PENALIZED_HOTKEYS[\"penalized_hotkeys\"]\n    end\n    \n    subgraph \"Security\"\n        RSA[\"RSAEncryption\"]\n        KEYPAIR[\"generate_key_pair()\"]\n        DECRYPT[\"decrypt_data()\"]\n    end\n    \n    REGISTER --> COMMANDS\n    COMMANDS --> WALLET\n    COMMANDS --> SUBTENSOR\n    COMMANDS --> DENDRITE\n    SUBTENSOR --> METAGRAPH\n    \n    COMMANDS --> ALLOCATE_PROTO\n    DENDRITE --> ALLOCATE_PROTO\n    \n    COMMANDS --> COMPUTEDB\n    COMPUTEDB --> ALLOCATION_TABLE\n    COMPUTEDB --> BLACKLIST_TABLE\n    \n    COMMANDS --> COMPUTEWANDB\n    COMPUTEWANDB --> ALLOCATED_HOTKEYS\n    COMPUTEWANDB --> PENALIZED_HOTKEYS\n    \n    COMMANDS --> RSA\n    RSA --> KEYPAIR\n    RSA --> DECRYPT\n```\n\nSources: \u003CSourceLink text=\"neurons/register.py:27-39\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L27-L39\" />, \u003CSourceLink text=\"neurons/register.py:118-135\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L118-L135\" />\n\n## Configuration and Setup\n\nThe CLI tool supports two configuration modes:\n\n### Command-line Configuration\nThe `get_config()` function provides standard configuration parsing for non-interactive use:\n\n```python\n# Key configuration parameters\n--netuid         # Subnet UID (default: 1)\n--gpu_type       # Required GPU type\n--gpu_size       # Required GPU memory in MB\n--wallet.*       # Wallet configuration\n--subtensor.*    # Subtensor configuration\n--logging.*      # Logging configuration\n```\n\n### Interactive Configuration\nThe `get_config_cli()` function provides interactive prompts for user-friendly operation:\n\n- Prompts for GPU type if not provided\n- Prompts for GPU memory in GB (converted to MB)\n- Sets up logging directory structure\n\nSources: \u003CSourceLink text=\"neurons/register.py:43-75\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L43-L75\" />, \u003CSourceLink text=\"neurons/register.py:79-113\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L79-L113\" />\n\n## Resource Management Operations\n\n### Allocation Operations\nThe CLI provides two allocation methods:\n\n1. **Device-based allocation** (`allocate()`): Searches for resources matching specific hardware requirements\n2. **Hotkey-based allocation** (`allocate_hotkey()`): Allocates resources from a specific miner hotkey\n\nBoth methods return SSH connection details for accessing allocated resources.\n\n### Deallocation Operations\nThe `deallocate()` function supports:\n- Single or multiple hotkey deallocation\n- Database state updates before network communication\n- Graceful error handling for offline miners\n\n### Resource Listing Operations\nThe CLI provides multiple listing commands:\n- `list_allocations()`: Detailed view of allocated resources\n- `list_allocations_hotkeys()`: Simplified hotkey-only view\n- `list_resources()`: Comprehensive network resource overview with availability status\n\nSources: \u003CSourceLink text=\"neurons/register.py:230-288\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L230-L288\" />, \u003CSourceLink text=\"neurons/register.py:350-445\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L350-L445\" />, \u003CSourceLink text=\"neurons/register.py:446-643\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L446-L643\" />","src/content/docs/cli-tools.mdx","ade532791b8d1498","architecture",{"id":34,"data":36,"body":41,"filePath":42,"digest":43,"deferredRender":15},{"title":37,"editUrl":15,"head":38,"template":17,"sidebar":39,"pagefind":15,"draft":19},"Architecture",[],{"hidden":19,"attrs":40},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"README.md\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md\" />\n\n  \u003CSourceLink text=\"compute/utils/parser.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py\" />\n\n  \u003CSourceLink text=\"neurons/miner.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py\" />\n\n  \u003CSourceLink text=\"neurons/register_api.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py\" />\n\n  \u003CSourceLink text=\"neurons/validator.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\n## Purpose and Scope\n\nThis document describes the high-level system architecture of the NI Compute Subnet, a decentralized GPU compute marketplace built on the Bittensor network. It covers the core system components, their interactions, communication protocols, and data flow patterns that enable validators to evaluate miner capabilities and allocate GPU resources to clients.\n\nFor detailed protocol specifications, see [Communication Protocols](/communication-protocols#5). For database schema and operations, see [Database Operations](/validator-system/database-operations#2.3). For installation and deployment procedures, see [Installation and Setup](/installation-and-setup#1.2).\n\n## System Overview\n\nThe NI Compute Subnet implements a three-tier architecture consisting of validators that assess miner performance, miners that provide GPU resources, and a resource allocation API that manages client requests. The system operates on Bittensor's peer-to-peer network while providing traditional REST API access for external clients.\n\n```mermaid\ngraph TB\n    subgraph \"External Clients\"\n        WEB[\"Web Applications\"]\n        CLI[\"CLI Tools\"]\n        API_CLIENTS[\"API Clients\"]\n    end\n    \n    subgraph \"NI Compute Subnet Core\"\n        subgraph \"Validator Layer\"\n            VALIDATOR[\"Validator\u003Cbr/>neurons/validator.py\"]\n            POG[\"ProofOfGPU\u003Cbr/>Benchmarking Engine\"]\n            SCORING[\"Scoring System\u003Cbr/>calc_score_pog()\"]\n        end\n        \n        subgraph \"Resource Allocation Layer\"\n            REGISTER_API[\"RegisterAPI\u003Cbr/>neurons/register_api.py\"]\n            ALLOCATION_LOGIC[\"Resource Management\u003Cbr/>_allocate_container()\"]\n            HEALTH_CHECK[\"Health Monitoring\u003Cbr/>_check_allocation()\"]\n        end\n        \n        subgraph \"Miner Layer\"\n            MINER[\"Miner\u003Cbr/>neurons/miner.py\"]\n            CONTAINER_MGR[\"Container Management\u003Cbr/>neurons/Miner/container.py\"]\n            ALLOCATION_HANDLER[\"Allocation Handler\u003Cbr/>register_allocation()\"]\n        end\n    end\n    \n    subgraph \"Bittensor Network\"\n        SUBTENSOR[\"ComputeSubnetSubtensor\u003Cbr/>Blockchain Interface\"]\n        METAGRAPH[\"Metagraph\u003Cbr/>Network State\"]\n        AXON_DENDRITE[\"Axon/Dendrite\u003Cbr/>P2P Communication\"]\n    end\n    \n    subgraph \"Data Layer\"\n        COMPUTE_DB[(\"ComputeDb\u003Cbr/>SQLite Database\")]\n        WANDB_STATE[(\"WandB\u003Cbr/>Distributed State\")]\n        CONFIG_FILES[(\"Configuration\u003Cbr/>config.yaml\")]\n    end\n    \n    %% External client interactions\n    WEB --> REGISTER_API\n    CLI --> REGISTER_API\n    API_CLIENTS --> REGISTER_API\n    \n    %% Core system interactions\n    VALIDATOR --> MINER\n    VALIDATOR --> SUBTENSOR\n    VALIDATOR --> POG\n    POG --> SCORING\n    \n    REGISTER_API --> ALLOCATION_LOGIC\n    ALLOCATION_LOGIC --> MINER\n    REGISTER_API --> HEALTH_CHECK\n    \n    MINER --> CONTAINER_MGR\n    MINER --> ALLOCATION_HANDLER\n    MINER --> AXON_DENDRITE\n    \n    %% Bittensor network interactions\n    VALIDATOR --> AXON_DENDRITE\n    MINER --> AXON_DENDRITE\n    AXON_DENDRITE --> SUBTENSOR\n    SUBTENSOR --> METAGRAPH\n    \n    %% Data layer interactions\n    VALIDATOR --> COMPUTE_DB\n    VALIDATOR --> WANDB_STATE\n    REGISTER_API --> COMPUTE_DB\n    MINER --> WANDB_STATE\n    VALIDATOR --> CONFIG_FILES\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:70-89\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L70-L89\" />, \u003CSourceLink text=\"neurons/miner.py:79-94\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L79-L94\" />, \u003CSourceLink text=\"neurons/register_api.py:229-303\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L229-L303\" />, \u003CSourceLink text=\"compute/axon.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py\" />, \u003CSourceLink text=\"compute/protocol.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py\" />\n\n## Core Components\n\n### Validator System\n\nThe `Validator` class implements the core validation logic that maintains network integrity by evaluating miner performance and setting network weights.\n\n```mermaid\ngraph TB\n    subgraph \"Validator Core\"\n        VALIDATOR_MAIN[\"Validator.__init__()\u003Cbr/>neurons/validator.py:130\"]\n        CONFIG_INIT[\"init_config()\u003Cbr/>neurons/validator.py:211\"]\n        SCORE_SYNC[\"sync_scores()\u003Cbr/>neurons/validator.py:312\"]\n    end\n    \n    subgraph \"Proof of GPU System\"\n        POG_MAIN[\"proof_of_gpu()\u003Cbr/>neurons/validator.py:663\"]\n        TEST_MINER[\"test_miner_gpu()\u003Cbr/>neurons/validator.py:799\"]\n        GPU_BENCHMARKS[\"GPU Benchmarking\u003Cbr/>Merkle Proof Verification\"]\n        SCRIPT_EXECUTION[\"execute_script_on_miner()\u003Cbr/>neurons/Validator/pog.py\"]\n    end\n    \n    subgraph \"Scoring Engine\"\n        CALC_SCORE[\"calc_score_pog()\u003Cbr/>neurons/Validator/calculate_pow_score.py\"]\n        RELIABILITY_SCORE[\"Reliability Scoring\u003Cbr/>Challenge Success Rate\"]\n        WEIGHT_SETTING[\"Network Weight Updates\u003Cbr/>Bittensor Integration\"]\n    end\n    \n    subgraph \"Data Management\"\n        COMPUTE_DB_OPS[\"ComputeDb Operations\u003Cbr/>SQLite Transactions\"]\n        WANDB_INTEGRATION[\"ComputeWandb\u003Cbr/>Distributed Metrics\"]\n        MINER_STATS[\"Miner Statistics\u003Cbr/>retrieve_stats()\"]\n    end\n    \n    VALIDATOR_MAIN --> CONFIG_INIT\n    VALIDATOR_MAIN --> POG_MAIN\n    VALIDATOR_MAIN --> SCORE_SYNC\n    \n    POG_MAIN --> TEST_MINER\n    TEST_MINER --> GPU_BENCHMARKS\n    TEST_MINER --> SCRIPT_EXECUTION\n    \n    SCORE_SYNC --> CALC_SCORE\n    CALC_SCORE --> RELIABILITY_SCORE\n    RELIABILITY_SCORE --> WEIGHT_SETTING\n    \n    VALIDATOR_MAIN --> COMPUTE_DB_OPS\n    VALIDATOR_MAIN --> WANDB_INTEGRATION\n    SCORE_SYNC --> MINER_STATS\n```\n\nThe validator operates on a continuous cycle, performing hardware verification every 360 blocks and updating scores based on GPU performance benchmarks and challenge response reliability.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:70-200\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L70-L200\" />, \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />, \u003CSourceLink text=\"neurons/Validator/calculate_pow_score.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py\" />, \u003CSourceLink text=\"neurons/Validator/database/\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/\" />\n\n### Miner System\n\nThe `Miner` class provides GPU compute resources to the network and handles allocation requests from validators and clients.\n\n```mermaid\ngraph TB\n    subgraph \"Miner Core\"\n        MINER_MAIN[\"Miner.__init__()\u003Cbr/>neurons/miner.py:117\"]\n        AXON_INIT[\"init_axon()\u003Cbr/>neurons/miner.py:222\"]\n        SYNC_STATUS[\"sync_status()\u003Cbr/>neurons/miner.py:304\"]\n    end\n    \n    subgraph \"Request Handlers\"\n        ALLOCATE_HANDLER[\"allocate()\u003Cbr/>neurons/miner.py:419\"]\n        CHALLENGE_HANDLER[\"challenge()\u003Cbr/>neurons/miner.py:491\"]\n        BLACKLIST_LOGIC[\"base_blacklist()\u003Cbr/>neurons/miner.py:330\"]\n    end\n    \n    subgraph \"Container Management\"\n        REGISTER_ALLOC[\"register_allocation()\u003Cbr/>neurons/Miner/allocate.py\"]\n        CONTAINER_OPS[\"Container Operations\u003Cbr/>neurons/Miner/container.py\"]\n        DOCKER_LIFECYCLE[\"Docker Lifecycle\u003Cbr/>build_sample_container()\"]\n    end\n    \n    subgraph \"Resource Monitoring\"\n        ALLOCATION_STATUS[\"Allocation Status Tracking\u003Cbr/>self.allocation_status\"]\n        WANDB_UPDATES[\"WandB State Updates\u003Cbr/>update_allocated()\"]\n        HEALTH_CHECKS[\"Health Check Responses\u003Cbr/>check_allocation()\"]\n    end\n    \n    MINER_MAIN --> AXON_INIT\n    MINER_MAIN --> SYNC_STATUS\n    \n    AXON_INIT --> ALLOCATE_HANDLER\n    AXON_INIT --> CHALLENGE_HANDLER\n    ALLOCATE_HANDLER --> BLACKLIST_LOGIC\n    \n    ALLOCATE_HANDLER --> REGISTER_ALLOC\n    REGISTER_ALLOC --> CONTAINER_OPS\n    CONTAINER_OPS --> DOCKER_LIFECYCLE\n    \n    ALLOCATE_HANDLER --> ALLOCATION_STATUS\n    ALLOCATION_STATUS --> WANDB_UPDATES\n    ALLOCATE_HANDLER --> HEALTH_CHECKS\n```\n\nThe miner continuously monitors for allocation opportunities while maintaining containerized environments for client workloads.\n\nSources: \u003CSourceLink text=\"neurons/miner.py:79-200\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L79-L200\" />, \u003CSourceLink text=\"neurons/Miner/allocate.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py\" />, \u003CSourceLink text=\"neurons/Miner/container.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py\" />, \u003CSourceLink text=\"compute/wandb/wandb.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py\" />\n\n### Resource Allocation API\n\nThe `RegisterAPI` class exposes REST endpoints for external clients to allocate and manage GPU resources.\n\n```mermaid\ngraph TB\n    subgraph \"API Layer\"\n        REGISTER_API[\"RegisterAPI.__init__()\u003Cbr/>neurons/register_api.py:230\"]\n        FASTAPI_APP[\"FastAPI Application\u003Cbr/>self.app\"]\n        ROUTE_SETUP[\"_setup_routes()\u003Cbr/>neurons/register_api.py:344\"]\n    end\n    \n    subgraph \"Allocation Endpoints\"\n        ALLOCATE_SPEC[\"allocate_spec()\u003Cbr/>/service/allocate_spec\"]\n        ALLOCATE_HOTKEY[\"allocate_hotkey()\u003Cbr/>/service/allocate_hotkey\"]\n        DEALLOCATE[\"deallocate()\u003Cbr/>/service/deallocate\"]\n        CHECK_STATUS[\"check_miner_status()\u003Cbr/>/service/check_miner_status\"]\n    end\n    \n    subgraph \"Resource Management\"\n        ALLOCATE_CONTAINER[\"_allocate_container()\u003Cbr/>Resource Discovery\"]\n        ALLOCATION_DB[\"update_allocation_db()\u003Cbr/>State Persistence\"]\n        HEALTH_MONITORING[\"_check_allocation()\u003Cbr/>Timeout Management\"]\n    end\n    \n    subgraph \"External Integrations\"\n        DENDRITE_CLIENT[\"bt.dendrite\u003Cbr/>Miner Communication\"]\n        WANDB_SYNC[\"_update_allocation_wandb()\u003Cbr/>Distributed State\"]\n        WEBHOOK_NOTIFY[\"_notify_allocation_status()\u003Cbr/>External Callbacks\"]\n    end\n    \n    REGISTER_API --> FASTAPI_APP\n    REGISTER_API --> ROUTE_SETUP\n    \n    ROUTE_SETUP --> ALLOCATE_SPEC\n    ROUTE_SETUP --> ALLOCATE_HOTKEY\n    ROUTE_SETUP --> DEALLOCATE\n    ROUTE_SETUP --> CHECK_STATUS\n    \n    ALLOCATE_SPEC --> ALLOCATE_CONTAINER\n    ALLOCATE_HOTKEY --> ALLOCATE_CONTAINER\n    ALLOCATE_CONTAINER --> ALLOCATION_DB\n    \n    REGISTER_API --> HEALTH_MONITORING\n    HEALTH_MONITORING --> ALLOCATION_DB\n    \n    ALLOCATE_CONTAINER --> DENDRITE_CLIENT\n    ALLOCATION_DB --> WANDB_SYNC\n    DEALLOCATE --> WEBHOOK_NOTIFY\n```\n\nThe API maintains allocation state in both local SQLite database and distributed WandB storage for cross-validator synchronization.\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:229-350\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L229-L350\" />, \u003CSourceLink text=\"neurons/register_api.py:407-850\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L407-L850\" />, \u003CSourceLink text=\"neurons/Validator/database/allocate.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/allocate.py\" />\n\n## Communication Architecture\n\nThe system implements a hybrid communication model combining Bittensor's peer-to-peer protocols with traditional REST APIs.\n\n```mermaid\ngraph TB\n    subgraph \"Protocol Layer\"\n        SPECS_PROTOCOL[\"Specs Protocol\u003Cbr/>compute/protocol.py\"]\n        ALLOCATE_PROTOCOL[\"Allocate Protocol\u003Cbr/>compute/protocol.py\"]\n        CHALLENGE_PROTOCOL[\"Challenge Protocol\u003Cbr/>compute/protocol.py\"]\n    end\n    \n    subgraph \"Bittensor Network Layer\"\n        COMPUTE_AXON[\"ComputeSubnetAxon\u003Cbr/>compute/axon.py\"]\n        COMPUTE_SUBTENSOR[\"ComputeSubnetSubtensor\u003Cbr/>compute/axon.py\"]\n        DENDRITE_CLIENT[\"bt.dendrite\u003Cbr/>RPC Client\"]\n    end\n    \n    subgraph \"REST API Layer\"\n        FASTAPI_ROUTES[\"FastAPI Routes\u003Cbr/>HTTP/HTTPS\"]\n        WEBSOCKET_CONN[\"WebSocket Connection\u003Cbr/>/connect\"]\n        API_MIDDLEWARE[\"IPWhitelistMiddleware\u003Cbr/>Security Layer\"]\n    end\n    \n    subgraph \"Communication Flows\"\n        V_TO_M[\"Validator → Miner\u003Cbr/>Specs/Challenge Queries\"]\n        API_TO_M[\"RegisterAPI → Miner\u003Cbr/>Allocation Requests\"]\n        CLIENT_TO_API[\"External Client → API\u003Cbr/>Resource Requests\"]\n    end\n    \n    SPECS_PROTOCOL --> COMPUTE_AXON\n    ALLOCATE_PROTOCOL --> COMPUTE_AXON\n    CHALLENGE_PROTOCOL --> COMPUTE_AXON\n    \n    COMPUTE_AXON --> COMPUTE_SUBTENSOR\n    COMPUTE_AXON --> DENDRITE_CLIENT\n    \n    FASTAPI_ROUTES --> API_MIDDLEWARE\n    WEBSOCKET_CONN --> FASTAPI_ROUTES\n    \n    V_TO_M --> SPECS_PROTOCOL\n    V_TO_M --> CHALLENGE_PROTOCOL\n    API_TO_M --> ALLOCATE_PROTOCOL\n    API_TO_M --> DENDRITE_CLIENT\n    CLIENT_TO_API --> FASTAPI_ROUTES\n```\n\n**Protocol Message Flow:**\n1. **Specs Query**: Validator requests hardware specifications from miners\n2. **Allocation Request**: RegisterAPI or Validator requests resource allocation \n3. **Challenge Response**: Validator sends proof-of-work challenges to miners\n4. **Health Check**: Periodic status verification of allocated resources\n\nSources: \u003CSourceLink text=\"compute/protocol.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py\" />, \u003CSourceLink text=\"compute/axon.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py\" />, \u003CSourceLink text=\"neurons/register_api.py:344-406\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L344-L406\" />, \u003CSourceLink text=\"neurons/validator.py:594-662\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L594-L662\" />\n\n## Data Architecture\n\nThe system uses a multi-tier data storage approach combining local SQLite databases with distributed state management.\n\n```mermaid\ngraph TB\n    subgraph \"Local Data Storage\"\n        COMPUTE_DB[(\"ComputeDb\u003Cbr/>SQLite Database\")]\n        MINER_TABLE[(\"miner table\u003Cbr/>uid, ss58_address\")]\n        POG_STATS[(\"pog_stats table\u003Cbr/>GPU performance data\")]\n        ALLOCATION_TABLE[(\"allocation table\u003Cbr/>active reservations\")]\n        CHALLENGE_DETAILS[(\"challenge_details table\u003Cbr/>success metrics\")]\n    end\n    \n    subgraph \"Distributed State\"\n        WANDB_RUNS[(\"WandB Validator Runs\u003Cbr/>Aggregated metrics\")]\n        WANDB_MINERS[(\"WandB Miner Runs\u003Cbr/>Hardware specifications\")]\n        WANDB_ALLOCATED[(\"Allocated Hotkeys\u003Cbr/>Resource status\")]\n        WANDB_PENALIZED[(\"Penalized Hotkeys\u003Cbr/>Blacklist data\")]\n    end\n    \n    subgraph \"Configuration Data\"\n        CONFIG_YAML[(\"config.yaml\u003Cbr/>GPU performance benchmarks\")]\n        ENV_CONFIG[(\"Environment Variables\u003Cbr/>API keys, endpoints\")]\n        PROTOCOL_SCHEMAS[(\"Protocol Definitions\u003Cbr/>Message validation\")]\n    end\n    \n    subgraph \"Data Access Layer\"\n        DB_OPERATIONS[\"Database Operations\u003Cbr/>compute/utils/db.py\"]\n        WANDB_CLIENT[\"ComputeWandb\u003Cbr/>compute/wandb/wandb.py\"]\n        CONFIG_LOADER[\"Configuration Loader\u003Cbr/>load_yaml_config()\"]\n    end\n    \n    COMPUTE_DB --> MINER_TABLE\n    COMPUTE_DB --> POG_STATS\n    COMPUTE_DB --> ALLOCATION_TABLE\n    COMPUTE_DB --> CHALLENGE_DETAILS\n    \n    WANDB_RUNS --> WANDB_MINERS\n    WANDB_RUNS --> WANDB_ALLOCATED\n    WANDB_RUNS --> WANDB_PENALIZED\n    \n    CONFIG_YAML --> ENV_CONFIG\n    CONFIG_YAML --> PROTOCOL_SCHEMAS\n    \n    DB_OPERATIONS --> COMPUTE_DB\n    WANDB_CLIENT --> WANDB_RUNS\n    CONFIG_LOADER --> CONFIG_YAML\n```\n\n**Data Synchronization Patterns:**\n- Local database stores operational state and query results\n- WandB provides cross-validator state synchronization\n- Configuration files define GPU performance baselines and system parameters\n\nSources: \u003CSourceLink text=\"compute/utils/db.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py\" />, \u003CSourceLink text=\"compute/wandb/wandb.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py\" />, \u003CSourceLink text=\"neurons/Validator/database/\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/\" />, \u003CSourceLink text=\"config.yaml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml\" />, \u003CSourceLink text=\"neurons/validator.py:178-181\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L178-L181\" />\n\n## Deployment Architecture\n\nThe system supports distributed deployment across multiple validator and miner nodes with centralized API services.\n\n```mermaid\ngraph TB\n    subgraph \"Validator Nodes\"\n        V1[\"Validator Instance 1\u003Cbr/>neurons/validator.py\"]\n        V2[\"Validator Instance 2\u003Cbr/>neurons/validator.py\"]\n        VN[\"Validator Instance N\u003Cbr/>neurons/validator.py\"]\n    end\n    \n    subgraph \"Miner Nodes\"\n        M1[\"Miner Instance 1\u003Cbr/>neurons/miner.py + Docker\"]\n        M2[\"Miner Instance 2\u003Cbr/>neurons/miner.py + Docker\"]\n        MN[\"Miner Instance N\u003Cbr/>neurons/miner.py + Docker\"]\n    end\n    \n    subgraph \"API Services\"\n        API1[\"RegisterAPI Instance 1\u003Cbr/>neurons/register_api.py\"]\n        API2[\"RegisterAPI Instance 2\u003Cbr/>neurons/register_api.py\"]\n        LB[\"Load Balancer\u003Cbr/>Optional\"]\n    end\n    \n    subgraph \"Infrastructure Services\"\n        BT_NETWORK[\"Bittensor Network\u003Cbr/>Subtensor/Metagraph\"]\n        WANDB_SERVICE[\"WandB Service\u003Cbr/>Distributed State\"]\n        MONITORING[\"System Monitoring\u003Cbr/>PM2/Prometheus\"]\n    end\n    \n    subgraph \"Network Configuration\"\n        FIREWALL[\"UFW Firewall\u003Cbr/>Ports 4444, 8091\"]\n        SSH_ACCESS[\"SSH Access\u003Cbr/>Container Management\"]\n        DOCKER_RUNTIME[\"Docker + NVIDIA Runtime\u003Cbr/>GPU Containers\"]\n    end\n    \n    V1 --> BT_NETWORK\n    V2 --> BT_NETWORK\n    VN --> BT_NETWORK\n    \n    M1 --> BT_NETWORK\n    M2 --> BT_NETWORK\n    MN --> BT_NETWORK\n    \n    API1 --> BT_NETWORK\n    API2 --> BT_NETWORK\n    LB --> API1\n    LB --> API2\n    \n    V1 --> WANDB_SERVICE\n    V2 --> WANDB_SERVICE\n    API1 --> WANDB_SERVICE\n    \n    M1 --> DOCKER_RUNTIME\n    M2 --> DOCKER_RUNTIME\n    MN --> DOCKER_RUNTIME\n    \n    FIREWALL --> SSH_ACCESS\n    SSH_ACCESS --> DOCKER_RUNTIME\n    \n    MONITORING --> V1\n    MONITORING --> M1\n    MONITORING --> API1\n```\n\n**Deployment Requirements:**\n- **Validators**: Require access to Subtensor endpoint and sufficient computational resources for GPU benchmarking\n- **Miners**: Need NVIDIA GPUs, Docker runtime, and open ports (4444 for SSH, 8091 for axon)\n- **RegisterAPI**: Can run on dedicated servers with database persistence and WandB integration\n\nSources: \u003CSourceLink text=\"README.md:110-340\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md#L110-L340\" />, \u003CSourceLink text=\"compute/utils/parser.py:159-165\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L159-L165\" />, \u003CSourceLink text=\"neurons/miner.py:154-167\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L154-L167\" />, \u003CSourceLink text=\"neurons/register_api.py:86-95\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L86-L95\" />","src/content/docs/architecture.mdx","3d71ed6c268dd3eb","communication-protocols",{"id":44,"data":46,"body":51,"filePath":52,"digest":53,"deferredRender":15},{"title":47,"editUrl":15,"head":48,"template":17,"sidebar":49,"pagefind":15,"draft":19},"Communication Protocols",[],{"hidden":19,"attrs":50},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/axon.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py\" />\n\n  \u003CSourceLink text=\"compute/protocol.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\n## Overview\n\nThis page documents the network communication protocols and message formats used between system components in the NI Compute subnet. The system implements three core Bittensor synapse protocols (`Specs`, `Allocate`, `Challenge`) and custom extensions to Bittensor's communication infrastructure (`ComputeSubnetAxon`, `ComputeSubnetSubtensor`, `ComputeSubnetAxonMiddleware`).\n\nThese protocols enable hardware specification discovery, resource allocation, and cryptographic validation challenges between validators and miners. For information about the HTTP API endpoints, see [Resource Allocation API](/resource-allocation-api#4).\n\nSources: \u003CSourceLink text=\"compute/protocol.py:1-136\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L1-L136\" />, \u003CSourceLink text=\"compute/axon.py:1-487\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L1-L487\" />\n\n## Protocol Stack\n\nNI Compute extends Bittensor's communication infrastructure with custom protocols specific to the GPU compute marketplace. The system implements three main synapse protocols and uses customized versions of Bittensor's Axon and Subtensor components.\n\n**Protocol Stack Architecture**\n```mermaid\ngraph TD\n    subgraph \"Bittensor Base Classes\"\n        bt_Synapse[\"bt.Synapse\"]\n        bittensor_Axon[\"bittensor.core.axon.Axon\"]\n        bittensor_Subtensor[\"bittensor.core.subtensor.Subtensor\"]\n        AxonMiddleware[\"bittensor.core.axon.AxonMiddleware\"]\n    end\n    \n    subgraph \"compute/protocol.py\"\n        Specs[\"class Specs(bt.Synapse)\"] --> bt_Synapse\n        Allocate[\"class Allocate(bt.Synapse)\"] --> bt_Synapse\n        Challenge[\"class Challenge(bt.Synapse)\"] --> bt_Synapse\n    end\n    \n    subgraph \"compute/axon.py\"\n        ComputeSubnetAxon[\"class ComputeSubnetAxon(axon)\"] --> bittensor_Axon\n        ComputeSubnetSubtensor[\"class ComputeSubnetSubtensor(subtensor)\"] --> bittensor_Subtensor\n        ComputeSubnetAxonMiddleware[\"class ComputeSubnetAxonMiddleware(AxonMiddleware)\"] --> AxonMiddleware\n        custom_serve_extrinsic[\"custom_serve_extrinsic()\"]\n    end\n    \n    subgraph \"Communication Methods\"\n        preprocess[\"preprocess()\"] --> ComputeSubnetAxonMiddleware\n        serve_prometheus[\"serve_prometheus()\"] --> ComputeSubnetSubtensor\n        info[\"info()\"] --> ComputeSubnetAxon\n    end\n    \n    ComputeSubnetAxon --> ComputeSubnetAxonMiddleware\n```\n\nSources: \u003CSourceLink text=\"compute/protocol.py:23-136\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L23-L136\" />, \u003CSourceLink text=\"compute/axon.py:152-487\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L152-L487\" />\n\n## Core Synapse Protocols\n\nNI Compute defines three primary protocols for communication between validators and miners, all extending Bittensor's Synapse base class.\n\n### Specs Protocol\n\nThe `Specs` class enables validators to query hardware specifications from miners, providing detailed information about CPU, GPU, RAM, and storage resources.\n\n**Specs Class Definition**\n```mermaid\nclassDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Specs {\n        +str specs_input\n        +str specs_output  \n        +deserialize() str\n    }\n    \n    bt_Synapse \u003C|-- Specs\n    \n    note for Specs \"compute/protocol.py:23-57\"\n```\n\n| Attribute | Type | Description |\n|-----------|------|-------------|\n| `specs_input` | `str` | Input parameter (typically empty string) |\n| `specs_output` | `str` | JSON response containing detailed hardware specifications |\n\nThe `deserialize()` method returns `self.specs_output` containing hardware details in the format:\n```\n{\"CPU\":{'count' : 4, 'vendor_id_raw' : 'AuthenticAMD', ...}}\n```\n\nSources: \u003CSourceLink text=\"compute/protocol.py:23-57\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L23-L57\" />\n\n### Allocate Protocol\n\nThe `Allocate` class facilitates resource allocation requests from validators to miners, including parameters for container configuration, access credentials, and resource requirements.\n\n**Allocate Class Definition**\n```mermaid\nclassDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Allocate {\n        +int timeline\n        +dict device_requirement\n        +bool checking\n        +dict output\n        +str public_key\n        +dict docker_requirement\n        +bool docker_change\n        +dict docker_action\n        +deserialize() dict\n    }\n    \n    bt_Synapse \u003C|-- Allocate\n    \n    note for Allocate \"compute/protocol.py:60-110\"\n```\n\n| Attribute | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `timeline` | `int` | `0` | Duration of allocation in seconds |\n| `device_requirement` | `dict` | `{}` | Hardware requirements specification |\n| `checking` | `bool` | `True` | Flag: `True` for availability check, `False` for actual allocation |\n| `public_key` | `str` | `\"\"` | Public key for secure communication |\n| `output` | `dict` | `{}` | Response containing allocation details from miner |\n| `docker_requirement` | `dict` | Default config | Container configuration with `base_image`, `ssh_key`, `ssh_port`, `volume_path`, `dockerfile` |\n| `docker_change` | `bool` | `False` | Flag indicating Docker configuration changes |\n| `docker_action` | `dict` | Action config | Docker actions with `action`, `ssh_key`, `key_type` |\n\nThe `deserialize()` method returns `self.output` containing the allocation response from the miner.\n\nSources: \u003CSourceLink text=\"compute/protocol.py:60-110\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L60-L110\" />\n\n### Challenge Protocol\n\nThe `Challenge` class enables validators to verify miner GPU capabilities through cryptographic proof-of-work challenges, ensuring that miners have the claimed GPU resources.\n\n**Challenge Class Definition**\n```mermaid\nclassDiagram\n    class bt_Synapse {\n        +deserialize()\n    }\n    \n    class Challenge {\n        +str challenge_hash\n        +str challenge_salt\n        +str challenge_mode\n        +str challenge_chars\n        +str challenge_mask\n        +int challenge_difficulty\n        +dict output\n        +deserialize() dict\n    }\n    \n    bt_Synapse \u003C|-- Challenge\n    \n    note for Challenge \"compute/protocol.py:112-136\"\n```\n\n| Attribute | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `challenge_hash` | `str` | `\"\"` | Cryptographic hash to be solved |\n| `challenge_salt` | `str` | `\"\"` | Salt value for the challenge |\n| `challenge_mode` | `str` | `\"\"` | Challenge type (hashcat mode) |\n| `challenge_chars` | `str` | `\"\"` | Character set for brute force |\n| `challenge_mask` | `str` | `\"\"` | Mask pattern for challenge |\n| `challenge_difficulty` | `int` | `compute.pow_min_difficulty` | Difficulty level of the challenge |\n| `output` | `dict` | `{}` | Response containing solution or error |\n\nThe `deserialize()` method returns `self.output` which contains either the password solution or error information:\n```python\n{\"password\": None, \"error\": f\"Hashcat execution failed with code {process.returncode}: {stderr}\"}\n```\n\nSources: \u003CSourceLink text=\"compute/protocol.py:112-136\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L112-L136\" />\n\n## Custom Axon and Subtensor\n\nNI Compute extends Bittensor's communication infrastructure with customized components to support the specific needs of the compute subnet.\n\n### ComputeSubnetAxon\n\nThe `ComputeSubnetAxon` class extends `bittensor.core.axon.Axon` to provide compute subnet-specific functionality, including customized version tracking and middleware processing.\n\n**ComputeSubnetAxon Request Processing Flow**\n```mermaid\nsequenceDiagram\n    participant Validator\n    participant ComputeSubnetAxon\n    participant ComputeSubnetAxonMiddleware\n    participant SynapseProtocol[\"Specs/Allocate/Challenge\"]\n    participant MinerLogic[\"Miner Handler Functions\"]\n    \n    Validator->>ComputeSubnetAxon: \"HTTP POST /{protocol_name}\"\n    ComputeSubnetAxon->>ComputeSubnetAxonMiddleware: \"process_request()\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"preprocess(request)\"\n    Note over ComputeSubnetAxonMiddleware: \"request.url.path.split('/')[1]\"\u003Cbr/>\"Extract protocol name\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"request_synapse.from_headers()\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxonMiddleware: \"Fill axon.nonce, axon.signature\"\n    ComputeSubnetAxonMiddleware->>SynapseProtocol: \"Forward to registered handler\"\n    SynapseProtocol->>MinerLogic: \"Execute miner logic\"\n    MinerLogic->>SynapseProtocol: \"Fill synapse response fields\"\n    SynapseProtocol->>ComputeSubnetAxonMiddleware: \"Return completed synapse\"\n    ComputeSubnetAxonMiddleware->>ComputeSubnetAxon: \"Return HTTP response\"\n    ComputeSubnetAxon->>Validator: \"Return synapse response\"\n```\n\n**Key Methods and Attributes**:\n\n| Method/Attribute | Description |\n|------------------|-------------|\n| `__init__()` | Initializes with custom middleware class `ComputeSubnetAxonMiddleware` |\n| `info()` | Returns `bittensor.AxonInfo` with `get_local_version()` and compute-specific placeholders |\n| `middleware_cls` | Set to `ComputeSubnetAxonMiddleware` |\n\nThe `info()` method returns customized axon information including:\n- `version`: From `get_local_version()`\n- `protocol`: `4`\n- `placeholder1`: `1`\n- `placeholder2`: `2`\n\nSources: \u003CSourceLink text=\"compute/axon.py:285-390\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L285-L390\" />\n\n### ComputeSubnetSubtensor\n\nThe `ComputeSubnetSubtensor` class extends `bittensor.core.subtensor.Subtensor` with additional functionality for Prometheus metrics extrinsic submission.\n\n**Key Methods**:\n\n| Method | Parameters | Description |\n|--------|------------|-------------|\n| `serve_prometheus()` | `wallet`, `port`, `netuid`, `wait_for_inclusion`, `wait_for_finalization` | Serves Prometheus metrics by submitting extrinsic to blockchain |\n| `do_serve_prometheus()` | `wallet`, `call_params`, `wait_for_inclusion`, `wait_for_finalization` | Core method for submitting Prometheus extrinsics with retry logic |\n\n**Prometheus Extrinsic Flow**:\n```mermaid\nsequenceDiagram\n    participant Miner\n    participant ComputeSubnetSubtensor\n    participant SubstrateInterface[\"self.substrate\"]\n    participant Blockchain[\"Bittensor Blockchain\"]\n    \n    Miner->>ComputeSubnetSubtensor: \"serve_prometheus(wallet, port, netuid)\"\n    ComputeSubnetSubtensor->>ComputeSubnetSubtensor: \"do_serve_prometheus()\"\n    ComputeSubnetSubtensor->>ComputeSubnetSubtensor: \"@retry(delay=1, tries=3, backoff=2)\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"compose_call('SubtensorModule', 'serve_prometheus')\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"create_signed_extrinsic(call, wallet.hotkey)\"\n    ComputeSubnetSubtensor->>SubstrateInterface: \"submit_extrinsic()\"\n    SubstrateInterface->>Blockchain: \"Submit prometheus extrinsic\"\n    Blockchain->>SubstrateInterface: \"Extrinsic response\"\n    SubstrateInterface->>ComputeSubnetSubtensor: \"response.process_events()\"\n    ComputeSubnetSubtensor->>Miner: \"return (success, error_message)\"\n```\n\nThe implementation includes retry logic with exponential backoff and comprehensive error handling for substrate communication failures.\n\nSources: \u003CSourceLink text=\"compute/axon.py:152-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L152-L283\" />\n\n### ComputeSubnetAxonMiddleware\n\nThe `ComputeSubnetAxonMiddleware` class extends `bittensor.core.axon.AxonMiddleware` and handles the preprocessing of requests, extracting the appropriate Synapse protocol based on the URL path and preparing it for execution.\n\n**Middleware Processing Implementation**:\n```mermaid\nflowchart TD\n    Request[\"HTTP Request\"] --> ExtractName[\"request.url.path.split('/')[1]\"]\n    ExtractName --> GetSynapse[\"self.axon.forward_class_types.get(request_name)\"]\n    GetSynapse --> CreateSynapse[\"request_synapse.from_headers(request.headers)\"]\n    CreateSynapse --> FillAxon[\"synapse.axon.__dict__.update()\"]\n    FillAxon --> FillDendrite[\"synapse.dendrite.__dict__.update()\"]\n    FillDendrite --> Sign[\"self.axon.wallet.hotkey.sign(message)\"]\n    Sign --> ReturnSynapse[\"return synapse\"]\n    \n    GetSynapse -->|\"None\"| UnknownSynapseError[\"raise UnknownSynapseError\"]\n    CreateSynapse -->|\"Exception\"| SynapseParsingError[\"raise SynapseParsingError\"]\n    ExtractName -->|\"Exception\"| InvalidRequestNameError[\"raise InvalidRequestNameError\"]\n```\n\n**Key `preprocess()` Method Logic**:\n\n1. **Request Name Extraction**: `request.url.path.split(\"/\")[1]`\n2. **Synapse Type Resolution**: `self.axon.forward_class_types.get(request_name)`\n3. **Synapse Creation**: `request_synapse.from_headers(request.headers)`\n4. **Axon Info Population**:\n   - `version`: `__version_as_int__`\n   - `uuid`: `str(self.axon.uuid)`\n   - `nonce`: `time.monotonic_ns()`\n   - `status_code`: `\"100\"`\n5. **Dendrite Info Population**: Client `port` and `ip`\n6. **Signature Generation**: \n   ```python\n   message = f\"{synapse.axon.nonce}.{synapse.dendrite.hotkey}.{synapse.axon.hotkey}.{synapse.axon.uuid}\"\n   synapse.axon.signature = f\"0x{self.axon.wallet.hotkey.sign(message).hex()}\"\n   ```\n\nSources: \u003CSourceLink text=\"compute/axon.py:391-487\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L391-L487\" />\n\n## Communication Flow\n\nThe following diagram illustrates the end-to-end communication flow between validators and miners using the three core protocols.\n\n**Complete Protocol Communication Flow**\n```mermaid\nsequenceDiagram\n    participant Validator[\"neurons/validator.py\"]\n    participant Dendrite[\"bt.dendrite\"]\n    participant MinerAxon[\"ComputeSubnetAxon\"]\n    participant Middleware[\"ComputeSubnetAxonMiddleware\"]\n    participant MinerLogic[\"neurons/miner.py\"]\n    \n    Note over Validator,MinerLogic: 1. Hardware Discovery (Specs Protocol)\n    Validator->>Dendrite: \"query(Specs(specs_input=''))\"\n    Dendrite->>MinerAxon: \"POST /Specs\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Specs)\"\n    MinerLogic->>MinerLogic: \"Get hardware specifications\"\n    MinerLogic->>Middleware: \"specs_output=hardware_json\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + specs_output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"\n    \n    Note over Validator,MinerLogic: 2. GPU Verification (Challenge Protocol)\n    Validator->>Dendrite: \"query(Challenge(challenge_hash, challenge_difficulty))\"\n    Dendrite->>MinerAxon: \"POST /Challenge\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Challenge)\"\n    MinerLogic->>MinerLogic: \"Execute hashcat proof-of-work\"\n    MinerLogic->>Middleware: \"output={'password': solution}\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"\n    \n    Note over Validator,MinerLogic: 3. Resource Allocation (Allocate Protocol)\n    Validator->>Dendrite: \"query(Allocate(timeline, device_requirement, docker_requirement))\"\n    Dendrite->>MinerAxon: \"POST /Allocate\"\n    MinerAxon->>Middleware: \"preprocess(request)\"\n    Middleware->>MinerLogic: \"forward_fn(Allocate)\"\n    MinerLogic->>MinerLogic: \"Create Docker container\"\n    MinerLogic->>Middleware: \"output={'ssh_address': address, 'ssh_port': port}\"\n    Middleware->>MinerAxon: \"return synapse\"\n    MinerAxon->>Dendrite: \"HTTP 200 + output\"\n    Dendrite->>Validator: \"synapse.deserialize()\"\n```\n\nSources: \u003CSourceLink text=\"compute/protocol.py:1-136\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L1-L136\" />, \u003CSourceLink text=\"compute/axon.py:391-487\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L391-L487\" />\n\n## Protocol Version Management\n\nNI Compute implements version tracking through the `__version_as_int__` global variable and `get_local_version()` function to ensure protocol compatibility between nodes.\n\n**Version Integration Points**\n```mermaid\nflowchart TD\n    version_as_int[\"__version_as_int__\"] --> axon_info[\"ComputeSubnetAxon.info()\"]\n    version_as_int --> serve_params[\"AxonServeCallParams.version\"]\n    version_as_int --> middleware[\"ComputeSubnetAxonMiddleware.preprocess()\"]\n    \n    get_local_version[\"get_local_version()\"] --> axon_info\n    \n    axon_info --> bittensor_AxonInfo[\"bittensor.AxonInfo\"]\n    serve_params --> custom_serve_extrinsic[\"custom_serve_extrinsic()\"]\n    middleware --> synapse_axon[\"synapse.axon.version\"]\n    \n    bittensor_AxonInfo --> blockchain[\"Blockchain Registration\"]\n    custom_serve_extrinsic --> blockchain\n```\n\n**Version Usage in Communication**:\n\n| Component | Usage | Code Reference |\n|-----------|-------|----------------|\n| `ComputeSubnetAxonMiddleware.preprocess()` | Sets `synapse.axon.version = __version_as_int__` | \u003CSourceLink text=\"compute/axon.py:470\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L470\" /> |\n| `ComputeSubnetAxon.info()` | Returns `bittensor.AxonInfo(version=get_local_version())` | \u003CSourceLink text=\"compute/axon.py:379\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L379\" /> |\n| `custom_serve_extrinsic()` | Uses `version=__version_as_int__` in `AxonServeCallParams` | \u003CSourceLink text=\"compute/axon.py:104\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L104\" /> |\n\nThe version is encoded as an integer for efficient blockchain storage and comparison, ensuring that all communication includes version information for compatibility checking.\n\nSources: \u003CSourceLink text=\"compute/axon.py:47-104\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L47-L104\" />, \u003CSourceLink text=\"compute/axon.py:376-388\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L376-L388\" />, \u003CSourceLink text=\"compute/axon.py:470\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L470\" />\n\n## Subtensor Integration\n\nNI Compute extends the standard Bittensor Subtensor to support custom extrinsics required for the compute subnet through the `custom_serve_extrinsic` function and `ComputeSubnetSubtensor` class.\n\n**Custom Serve Extrinsic Implementation**\n```mermaid\nflowchart TD\n    custom_serve_extrinsic[\"custom_serve_extrinsic()\"] --> unlock_key[\"unlock_key(wallet, 'hotkey')\"]\n    unlock_key --> create_params[\"AxonServeCallParams()\"]\n    create_params --> get_neuron[\"subtensor.get_neuron_for_pubkey_and_subnet()\"]\n    get_neuron --> check_updated{\"neuron_up_to_date?\"}\n    check_updated -->|\"True\"| return_true[\"return True\"]\n    check_updated -->|\"False\"| do_serve[\"do_serve_axon()\"]\n    do_serve --> wait_check{\"wait_for_inclusion or wait_for_finalization?\"}\n    wait_check -->|\"True\"| check_success{\"success?\"}\n    wait_check -->|\"False\"| return_true2[\"return True\"]\n    check_success -->|\"True\"| return_true3[\"return True\"]\n    check_success -->|\"False\"| return_false[\"return False\"]\n```\n\n**Key Customizations**:\n\n1. **Custom Version Integration**: Uses `__version_as_int__` from the compute subnet\n2. **Modified Parameters**: Includes compute-specific `placeholder1` and `placeholder2` values\n3. **Enhanced Logging**: Custom debug messages for axon serving status\n4. **Registration Override**: Replaces `bittensor.core.extrinsics.serving.serve_extrinsic` with custom implementation\n\n**AxonServeCallParams Structure**:\n```python\nparams = AxonServeCallParams(\n    version=__version_as_int__,           # Compute subnet version\n    ip=net.ip_to_int(ip),                # IP address as integer\n    port=port,                           # Port number\n    ip_type=net.ip_version(ip),          # IP version (4 or 6)\n    netuid=netuid,                       # Subnet ID\n    hotkey=wallet.hotkey.ss58_address,   # Hotkey address\n    coldkey=wallet.coldkeypub.ss58_address, # Coldkey address\n    protocol=protocol,                   # Protocol version\n    placeholder1=placeholder1,           # Reserved field\n    placeholder2=placeholder2,           # Reserved field\n    certificate=certificate,             # TLS certificate\n)\n```\n\nThe system patches the global Bittensor extrinsic function:\n```python\nbittensor.core.extrinsics.serving.serve_extrinsic = custom_serve_extrinsic\n```\n\nSources: \u003CSourceLink text=\"compute/axon.py:63-150\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L63-L150\" />\n\n## Summary\n\nThe communication protocols in NI Compute form a critical infrastructure layer that enables secure, efficient interaction between validators and miners in the decentralized GPU marketplace. By extending Bittensor's core communication components, NI Compute implements specialized protocols for hardware discovery, verification, and resource allocation.\n\nThis layered approach ensures:\n\n1. **Discoverability**: Validators can discover and verify miner hardware capabilities\n2. **Security**: Secure communication channels for sensitive operations\n3. **Resource Management**: Efficient allocation and management of GPU resources\n4. **Monitoring**: Integration with Prometheus for metrics collection\n\nUnderstanding these protocols is essential for developing and extending the NI Compute platform with new features and capabilities.","src/content/docs/communication-protocols.mdx","6aa1552be68f9822","index",{"id":54,"data":56,"body":61,"filePath":62,"digest":63,"deferredRender":15},{"title":57,"editUrl":15,"head":58,"template":17,"sidebar":59,"pagefind":15,"draft":19},"Overview",[],{"hidden":19,"attrs":60},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"README.md\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md\" />\n\n  \u003CSourceLink text=\"compute/__init__.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py\" />\n\n  \u003CSourceLink text=\"compute/utils/parser.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThe NI Compute Subnet is a decentralized GPU compute marketplace operating on the Bittensor network as subnet 27. It enables miners to contribute GPU resources and earn rewards based on their computational performance, while validators measure miner capabilities and allocate resources to clients through a trustless, permissionless system.\n\nThis document provides a high-level overview of the system architecture, core components, and their interactions. For detailed installation instructions, see [Installation and Setup](/installation-and-setup#1.2). For specific component documentation, refer to the [Validator System](/validator-system#2), [Miner System](/miner-system#3), and [Resource Allocation API](/resource-allocation-api#4) sections.\n\n## System Architecture\n\nThe NI Compute Subnet consists of three primary components that interact through the Bittensor network and custom communication protocols:\n\n```mermaid\ngraph TB\n    subgraph \"Bittensor Network\"\n        BT[\"Subtensor Blockchain\"]\n        META[\"Metagraph State\"]\n    end\n    \n    subgraph \"Validator System\"\n        VAL[\"Validator Process\u003Cbr/>neurons/validator.py\"]\n        POG[\"Proof-of-GPU Validation\"]\n        SCORE[\"Performance Scoring\"]\n        WEIGHTS[\"Weight Setting\"]\n    end\n    \n    subgraph \"Miner System\"\n        MIN[\"Miner Process\u003Cbr/>neurons/miner.py\"]\n        DOCKER[\"Docker Container Management\"]\n        SSH[\"SSH Resource Access\"]\n        HASHCAT[\"Hashcat PoW Challenges\"]\n    end\n    \n    subgraph \"Resource Allocation API\"\n        API[\"RegisterAPI\u003Cbr/>FastAPI Service\"]\n        ALLOC[\"Resource Discovery\"]\n        HEALTH[\"Health Monitoring\"]\n    end\n    \n    subgraph \"Data Layer\"\n        DB[\"ComputeDb\u003Cbr/>SQLite Database\"]\n        WANDB[\"WandB Metrics\"]\n        CONFIG[\"GPU Performance Config\"]\n    end\n    \n    VAL -->|\"Validates Performance\"| MIN\n    VAL -->|\"Sets Network Weights\"| BT\n    VAL -->|\"Queries Metagraph\"| META\n    \n    MIN -->|\"Registers Hotkey\"| BT\n    MIN -->|\"Manages Containers\"| DOCKER\n    MIN -->|\"Provides SSH Access\"| SSH\n    \n    API -->|\"Allocates Resources\"| MIN\n    API -->|\"Health Checks\"| HEALTH\n    \n    VAL -->|\"Stores Results\"| DB\n    VAL -->|\"Logs Metrics\"| WANDB\n    MIN -->|\"Updates Status\"| WANDB\n    \n    CONFIG -->|\"Configures Scoring\"| VAL\n```\n\n**Sources:** \u003CSourceLink text=\"README.md:1-535\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md#L1-L535\" />, \u003CSourceLink text=\"compute/__init__.py:1-93\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L1-L93\" />\n\n## Core Components\n\n### Validator System\nValidators are responsible for measuring miner performance and maintaining network integrity. They operate continuous validation cycles that include hardware specification queries, proof-of-GPU benchmarks, and cryptographic challenge verification.\n\nThe validator system implements a sophisticated scoring mechanism based on GPU performance metrics, with base scores assigned to different GPU models and scaling factors for multiple GPU configurations. Validators with sufficient stake (`validator_permit_stake = 1.0e4` TAO) can set network weights that determine miner rewards.\n\n**Key Classes and Constants:**\n- `neurons/validator.py` - Main validator process\n- `validator_permit_stake` - Minimum stake requirement for validators\n- `specs_timeout = 60` - Timeout for hardware specification requests\n- `pog_retry_limit = 30` - Maximum retries for proof-of-GPU validation\n\n### Miner System\nMiners contribute GPU computational resources to the network and respond to validator requests. They manage Docker containers for resource isolation, handle SSH-based resource allocation, and participate in proof-of-work challenges using Hashcat.\n\nThe miner system uses a priority-based request handling system where resource allocation requests (`miner_priority_allocate = 3`) take precedence over challenge responses (`miner_priority_challenge = 2`) and specification queries (`miner_priority_specs = 1`).\n\n**Key Classes and Constants:**\n- `neurons/miner.py` - Main miner process  \n- `miner_hashcat_location = \"hashcat\"` - Hashcat binary location\n- `miner_hashcat_workload_profile = \"3\"` - High performance profile\n- `pow_timeout = 30` - Proof-of-work challenge timeout\n\n### Resource Allocation API\nThe Resource Allocation API is a FastAPI-based web service that provides external access to the compute network. It handles resource discovery, allocation requests, and health monitoring of active allocations.\n\nThe API implements RSA encryption for secure communication and maintains state through both local database storage and distributed WandB synchronization.\n\n**Sources:** \u003CSourceLink text=\"compute/__init__.py:21-77\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L21-L77\" />, \u003CSourceLink text=\"README.md:87-108\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md#L87-L108\" />\n\n## Validation and Challenge System\n\nThe subnet implements multiple validation mechanisms to ensure miner integrity and performance:\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator\"\n    participant M as \"Miner\"\n    participant D as \"Docker Container\"\n    participant BT as \"Bittensor Network\"\n    \n    Note over V,M: \"Hardware Specification Phase\"\n    V->>M: \"Send Specs Request\"\n    M->>V: \"Return GPU/CPU Specs\"\n    \n    Note over V,M: \"Proof-of-GPU Phase\"\n    V->>M: \"Request PoG Allocation\"\n    M->>D: \"Create Container\"\n    D->>V: \"Provide SSH Access\"\n    V->>D: \"Run GPU Benchmarks\"\n    D->>V: \"Return Benchmark Results\"\n    \n    Note over V,M: \"Challenge Phase\"\n    V->>M: \"Send PoW Challenge\"\n    M->>M: \"Execute Hashcat\"\n    M->>V: \"Return Merkle Proof\"\n    \n    Note over V,BT: \"Weight Setting Phase\"\n    V->>V: \"Calculate Scores\"\n    V->>BT: \"Set Network Weights\"\n```\n\n**Proof-of-Work Configuration:**\n- `pow_min_difficulty = 7` - Minimum challenge difficulty\n- `pow_max_difficulty = 12` - Maximum challenge difficulty  \n- `pow_default_mode = \"610\"` - BLAKE2b-512 hash mode\n- `pow_default_chars` - Challenge character set including alphanumeric and special characters\n\n**Sources:** \u003CSourceLink text=\"compute/__init__.py:37-49\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L37-L49\" />, \u003CSourceLink text=\"README.md:437-450\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md#L437-L450\" />\n\n## Data Management and Monitoring\n\nThe system maintains state through multiple data persistence layers:\n\n| Component | Storage Type | Purpose |\n|-----------|--------------|---------|\n| ComputeDb | SQLite | Local miner stats, scores, allocations |\n| WandB | Distributed | Cross-validator metrics, distributed state |\n| Configuration | YAML/ENV | GPU performance benchmarks, API keys |\n\nThe monitoring system uses WandB for distributed state management and metrics collection, with separate runs for validators and miners to track performance and resource utilization.\n\n**Version Management:**\n- `__version__ = \"1.9.0\"` - Current subnet version\n- `__minimal_miner_version__ = \"1.8.5\"` - Minimum required miner version\n- `__minimal_validator_version__ = \"1.8.8\"` - Minimum required validator version\n\n**Sources:** \u003CSourceLink text=\"compute/__init__.py:20-26\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L20-L26\" />, \u003CSourceLink text=\"README.md:297-303\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md#L297-L303\" />\n\n## Network Communication\n\nThe subnet uses custom Bittensor synapse protocols for component communication:\n\n```mermaid\ngraph LR\n    subgraph \"Protocol Layer\"\n        SPECS[\"Specs Protocol\u003Cbr/>Hardware Queries\"]\n        ALLOC[\"Allocate Protocol\u003Cbr/>Resource Requests\"]  \n        CHALLENGE[\"Challenge Protocol\u003Cbr/>PoW Verification\"]\n    end\n    \n    subgraph \"Transport Layer\"\n        AXON[\"Custom Axon\u003Cbr/>Miner Endpoints\"]\n        SUBTENSOR[\"Custom Subtensor\u003Cbr/>Network Interface\"]\n    end\n    \n    SPECS --> AXON\n    ALLOC --> AXON\n    CHALLENGE --> AXON\n    \n    AXON --> SUBTENSOR\n```\n\nThe communication system includes blacklist management for suspected exploiters and trusted validator lists to maintain network security and integrity.\n\n**Sources:** \u003CSourceLink text=\"compute/__init__.py:59-92\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L59-L92\" />, \u003CSourceLink text=\"compute/utils/parser.py:8-170\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L8-L170\" />\n\nFor detailed information about specific components, see the [Validator System](/validator-system#2), [Miner System](/miner-system#3), [Resource Allocation API](/resource-allocation-api#4), and [Communication Protocols](/communication-protocols#5) sections.","src/content/docs/index.mdx","6d670b4e17d95940","monitoring-and-metrics",{"id":64,"data":66,"body":71,"filePath":72,"digest":73,"deferredRender":15},{"title":67,"editUrl":15,"head":68,"template":17,"sidebar":69,"pagefind":15,"draft":19},"Monitoring and Metrics",[],{"hidden":19,"attrs":70},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/axon.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py\" />\n\n  \u003CSourceLink text=\"compute/wandb/wandb.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document covers the monitoring and observability infrastructure used in the NI Compute Subnet. The system employs two primary monitoring solutions: Weights & Biases (WandB) for distributed state management and experiment tracking, and Prometheus for network observability metrics.\n\nFor information about the database operations used for local state persistence, see [Database Operations](/validator-system/database-operations#2.3). For details about the communication protocols that generate monitored events, see [Communication Protocols](/communication-protocols#5).\n\n## Architecture Overview\n\nThe monitoring system operates across three main components: validators, miners, and the resource allocation API. WandB serves as the primary distributed state store, while Prometheus provides real-time metrics collection.\n\n### Monitoring Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Validator Instances\"\n        VAL1[\"Validator Process\u003Cbr/>neurons/validator.py\"]\n        VAL2[\"Validator Process\u003Cbr/>neurons/validator.py\"]\n        VAL3[\"Validator Process\u003Cbr/>neurons/validator.py\"]\n    end\n    \n    subgraph \"Miner Instances\"\n        MIN1[\"Miner Process\u003Cbr/>neurons/miner.py\"]\n        MIN2[\"Miner Process\u003Cbr/>neurons/miner.py\"]\n        MIN3[\"Miner Process\u003Cbr/>neurons/miner.py\"]\n    end\n    \n    subgraph \"WandB Cloud Service\"\n        WANDB_PROJECT[\"opencompute Project\u003Cbr/>neuralinternet/opencompute\"]\n        WANDB_RUNS[\"Individual Runs\u003Cbr/>validator-{hotkey}\u003Cbr/>miner-{hotkey}\"]\n        WANDB_CONFIG[\"Run Configurations\u003Cbr/>Hardware Specs\u003Cbr/>Allocation State\u003Cbr/>Performance Stats\"]\n    end\n    \n    subgraph \"Prometheus Infrastructure\"\n        PROM_AXON[\"ComputeSubnetAxon\u003Cbr/>Prometheus Endpoints\"]\n        PROM_METRICS[\"Network Metrics\u003Cbr/>Performance Data\"]\n        PROM_EXTRINSIC[\"prometheus_extrinsic\u003Cbr/>Blockchain Integration\"]\n    end\n    \n    subgraph \"Local State\"\n        COMPUTE_DB[\"ComputeDb\u003Cbr/>SQLite Database\"]\n        WANDB_RUNS_TABLE[\"wandb_runs table\u003Cbr/>hotkey -> run_id mapping\"]\n    end\n    \n    VAL1 --> WANDB_PROJECT\n    VAL2 --> WANDB_PROJECT\n    VAL3 --> WANDB_PROJECT\n    MIN1 --> WANDB_PROJECT\n    MIN2 --> WANDB_PROJECT\n    MIN3 --> WANDB_PROJECT\n    \n    WANDB_PROJECT --> WANDB_RUNS\n    WANDB_RUNS --> WANDB_CONFIG\n    \n    VAL1 --> PROM_AXON\n    MIN1 --> PROM_AXON\n    PROM_AXON --> PROM_METRICS\n    PROM_AXON --> PROM_EXTRINSIC\n    \n    VAL1 --> COMPUTE_DB\n    MIN1 --> COMPUTE_DB\n    COMPUTE_DB --> WANDB_RUNS_TABLE\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:1-648\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L1-L648\" />, \u003CSourceLink text=\"compute/axon.py:152-284\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L152-L284\" />\n\n## WandB Integration\n\nThe `ComputeWandb` class provides the core monitoring infrastructure, managing distributed state across the network through individual WandB runs for each validator and miner.\n\n### WandB System Components\n\n```mermaid\ngraph TD\n    subgraph \"ComputeWandb Class\"\n        INIT[\"__init__\u003Cbr/>Run Initialization\"]\n        CONFIG[\"update_config\u003Cbr/>Configuration Management\"]\n        SIGN[\"sign_run\u003Cbr/>Cryptographic Signing\"]\n        VERIFY[\"verify_run\u003Cbr/>Signature Verification\"]\n    end\n    \n    subgraph \"Validator Operations\"\n        UPDATE_STATS[\"update_stats\u003Cbr/>Challenge Results\"]\n        UPDATE_ALLOC[\"update_allocated_hotkeys\u003Cbr/>Resource Allocation State\"]\n        UPDATE_PEN[\"update_penalized_hotkeys\u003Cbr/>Blacklist Management\"]\n        GET_STATS[\"get_stats_allocated\u003Cbr/>Cross-Validator Aggregation\"]\n    end\n    \n    subgraph \"Miner Operations\"\n        UPDATE_SPECS[\"update_specs\u003Cbr/>Hardware Specifications\"]\n        UPDATE_MINER_ALLOC[\"update_allocated\u003Cbr/>Allocation Status\"]\n        UPDATE_PORT[\"update_miner_port_open\u003Cbr/>Network Accessibility\"]\n        SYNC_ALLOC[\"sync_allocated\u003Cbr/>State Synchronization\"]\n    end\n    \n    subgraph \"Data Retrieval\"\n        GET_ALLOC[\"get_allocated_hotkeys\u003Cbr/>Active Allocations\"]\n        GET_PEN[\"get_penalized_hotkeys\u003Cbr/>Blacklisted Keys\"]\n        GET_SPECS[\"get_miner_specs\u003Cbr/>Hardware Information\"]\n    end\n    \n    subgraph \"WandB API Layer\"\n        API_INSTANCE[\"wandb.Api\u003Cbr/>API Client\"]\n        PROJECT_REF[\"api.project\u003Cbr/>opencompute\"]\n        RUNS_QUERY[\"api.runs\u003Cbr/>Query Interface\"]\n    end\n    \n    INIT --> CONFIG\n    CONFIG --> SIGN\n    UPDATE_STATS --> SIGN\n    UPDATE_ALLOC --> SIGN\n    UPDATE_PEN --> SIGN\n    UPDATE_SPECS --> SIGN\n    UPDATE_MINER_ALLOC --> SIGN\n    UPDATE_PORT --> SIGN\n    \n    GET_STATS --> VERIFY\n    GET_ALLOC --> VERIFY\n    GET_PEN --> VERIFY\n    GET_SPECS --> VERIFY\n    \n    API_INSTANCE --> PROJECT_REF\n    PROJECT_REF --> RUNS_QUERY\n    RUNS_QUERY --> GET_STATS\n    RUNS_QUERY --> GET_ALLOC\n    RUNS_QUERY --> GET_PEN\n    RUNS_QUERY --> GET_SPECS\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:19-648\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L19-L648\" />\n\n### Run Management and Initialization\n\nEach network participant creates a WandB run with a standardized naming convention: `{role}-{hotkey}`. The system handles run persistence through local database storage and automatic recovery.\n\n| Component | Description | Key Methods |\n|-----------|-------------|-------------|\n| Run Initialization | Creates or resumes WandB runs | `__init__`, `save_run_id`, `get_run_id` |\n| Configuration Management | Updates run configuration with network state | `update_config` |\n| State Persistence | Stores run IDs in local SQLite database | Database operations in `wandb_runs` table |\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:52-88\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L52-L88\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:109-138\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L109-L138\" />\n\n### Hardware Specifications Tracking\n\nMiners upload hardware specifications to enable validators to make informed allocation decisions. The `update_specs` method integrates with the performance measurement system to provide encrypted hardware details.\n\n```mermaid\ngraph LR\n    subgraph \"Hardware Detection\"\n        PERF_INFO[\"get_perf_info\u003Cbr/>neurons/Validator/script.py\"]\n        GPU_SPECS[\"GPU Specifications\u003Cbr/>Name, Count, Memory\"]\n        CPU_SPECS[\"CPU Specifications\u003Cbr/>Cores, Architecture\"]\n    end\n    \n    subgraph \"WandB Upload\"\n        UPDATE_SPECS[\"update_specs\u003Cbr/>compute/wandb/wandb.py\"]\n        RUN_CONFIG[\"run.config\u003Cbr/>specs field\"]\n        SIGNATURE[\"sign_run\u003Cbr/>Cryptographic Verification\"]\n    end\n    \n    subgraph \"Validator Consumption\"\n        GET_MINER_SPECS[\"get_miner_specs\u003Cbr/>Query Interface\"]\n        QUERYABLE_UIDS[\"queryable_uids\u003Cbr/>Network Participants\"]\n        ALLOCATION_LOGIC[\"Resource Allocation\u003Cbr/>Decision Making\"]\n    end\n    \n    PERF_INFO --> GPU_SPECS\n    PERF_INFO --> CPU_SPECS\n    GPU_SPECS --> UPDATE_SPECS\n    CPU_SPECS --> UPDATE_SPECS\n    UPDATE_SPECS --> RUN_CONFIG\n    RUN_CONFIG --> SIGNATURE\n    \n    GET_MINER_SPECS --> QUERYABLE_UIDS\n    QUERYABLE_UIDS --> ALLOCATION_LOGIC\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:140-159\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L140-L159\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:540-574\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L540-L574\" />\n\n## State Management and Synchronization\n\nThe WandB system maintains several critical state categories across the network, with built-in aggregation and conflict resolution mechanisms.\n\n### Allocation State Management\n\n```mermaid\ngraph TB\n    subgraph \"Validator State Updates\"\n        ALLOC_UPDATE[\"update_allocated_hotkeys\u003Cbr/>List of Allocated Keys\"]\n        STATS_UPDATE[\"update_stats\u003Cbr/>Performance Statistics\"]\n        PEN_UPDATE[\"update_penalized_hotkeys\u003Cbr/>Blacklist Management\"]\n    end\n    \n    subgraph \"Cross-Validator Aggregation\"\n        GET_ALLOC_HOTKEYS[\"get_allocated_hotkeys\u003Cbr/>Aggregate Across Validators\"]\n        GET_STATS_ALLOC[\"get_stats_allocated\u003Cbr/>Statistics Aggregation\"]\n        CONFLICT_RESOLUTION[\"pick_dominant_dict\u003Cbr/>Score-Based Resolution\"]\n    end\n    \n    subgraph \"Database Synchronization\"\n        RETRIEVE_STATS[\"retrieve_stats\u003Cbr/>neurons/Validator/database/pog.py\"]\n        WRITE_STATS[\"write_stats\u003Cbr/>neurons/Validator/database/pog.py\"]\n        COMPUTE_DB[\"ComputeDb\u003Cbr/>Local State Storage\"]\n    end\n    \n    subgraph \"Verification Layer\"\n        VERIFY_RUN[\"verify_run\u003Cbr/>Signature Verification\"]\n        VALID_VALIDATORS[\"valid_validator_hotkeys\u003Cbr/>Authorized Validators\"]\n        SIGNATURE_CHECK[\"Cryptographic Validation\"]\n    end\n    \n    ALLOC_UPDATE --> GET_ALLOC_HOTKEYS\n    STATS_UPDATE --> GET_STATS_ALLOC\n    PEN_UPDATE --> GET_ALLOC_HOTKEYS\n    \n    GET_STATS_ALLOC --> CONFLICT_RESOLUTION\n    CONFLICT_RESOLUTION --> RETRIEVE_STATS\n    RETRIEVE_STATS --> WRITE_STATS\n    WRITE_STATS --> COMPUTE_DB\n    \n    GET_ALLOC_HOTKEYS --> VERIFY_RUN\n    GET_STATS_ALLOC --> VERIFY_RUN\n    VERIFY_RUN --> VALID_VALIDATORS\n    VALID_VALIDATORS --> SIGNATURE_CHECK\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:198-250\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L198-L250\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:291-333\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L291-L333\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:334-450\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L334-L450\" />\n\n### Statistics Aggregation Algorithm\n\nThe `get_stats_allocated` method implements a sophisticated aggregation algorithm that resolves conflicts between multiple validator reports for the same miner UID.\n\n| Step | Process | Implementation |\n|------|---------|----------------|\n| 1. Collection | Query all validator runs with stats | WandB API filters |\n| 2. Verification | Validate cryptographic signatures | `verify_run` method |\n| 3. Filtering | Select entries with `own_score=True` and `allocated=True` | Boolean filtering |\n| 4. Aggregation | Group by UID, collect multiple reports | Dictionary aggregation |\n| 5. Resolution | Use `pick_dominant_dict` for conflict resolution | Counter-based selection |\n| 6. Scoring | Prefer highest score in case of ties | Score comparison |\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:334-450\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L334-L450\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:391-426\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L391-L426\" />\n\n## Security and Verification\n\nThe monitoring system implements cryptographic verification to ensure data integrity and prevent tampering.\n\n### Signature Verification Process\n\n```mermaid\ngraph TD\n    subgraph \"Signing Process\"\n        RUN_ID[\"run.id\u003Cbr/>WandB Run Identifier\"]\n        DATA_HASH[\"SHA-256 Hash\u003Cbr/>Computed from run_id\"]\n        WALLET_SIGN[\"wallet.hotkey.sign\u003Cbr/>Cryptographic Signature\"]\n        CONFIG_UPDATE[\"run.config.update\u003Cbr/>Store Signature\"]\n    end\n    \n    subgraph \"Verification Process\"\n        EXTRACT_SIG[\"Extract Signature\u003Cbr/>From run.config\"]\n        RECREATE_HASH[\"Recreate Hash\u003Cbr/>From run_id\"]\n        KEYPAIR_VERIFY[\"bt.Keypair.verify\u003Cbr/>Signature Validation\"]\n        RESULT[\"Verification Result\u003Cbr/>Boolean\"]\n    end\n    \n    subgraph \"Security Enforcement\"\n        VALID_HOTKEYS[\"valid_validator_hotkeys\u003Cbr/>Authorized Keys\"]\n        ACCESS_CONTROL[\"Data Access Control\u003Cbr/>Based on Verification\"]\n        FLAG_PARAM[\"flag Parameter\u003Cbr/>Enforcement Toggle\"]\n    end\n    \n    RUN_ID --> DATA_HASH\n    DATA_HASH --> WALLET_SIGN\n    WALLET_SIGN --> CONFIG_UPDATE\n    \n    EXTRACT_SIG --> RECREATE_HASH\n    RECREATE_HASH --> KEYPAIR_VERIFY\n    KEYPAIR_VERIFY --> RESULT\n    \n    RESULT --> ACCESS_CONTROL\n    VALID_HOTKEYS --> ACCESS_CONTROL\n    FLAG_PARAM --> ACCESS_CONTROL\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:576-591\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L576-L591\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:592-616\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L592-L616\" />\n\n## Prometheus Integration\n\nThe Prometheus integration provides real-time metrics collection through custom Axon and Subtensor implementations.\n\n### Prometheus Architecture\n\n```mermaid\ngraph LR\n    subgraph \"Custom Implementations\"\n        COMPUTE_SUBTENSOR[\"ComputeSubnetSubtensor\u003Cbr/>compute/axon.py\"]\n        SERVE_PROMETHEUS[\"serve_prometheus\u003Cbr/>Method\"]\n        DO_SERVE[\"do_serve_prometheus\u003Cbr/>Extrinsic Handler\"]\n    end\n    \n    subgraph \"Blockchain Integration\"\n        PROMETHEUS_EXTRINSIC[\"prometheus_extrinsic\u003Cbr/>compute/prometheus.py\"]\n        SUBSTRATE_CALL[\"substrate.compose_call\u003Cbr/>SubtensorModule\"]\n        SIGNED_EXTRINSIC[\"create_signed_extrinsic\u003Cbr/>Blockchain Transaction\"]\n    end\n    \n    subgraph \"Network Deployment\"\n        AXON_SERVE[\"Custom Axon\u003Cbr/>ComputeSubnetAxon\"]\n        METRICS_ENDPOINT[\"Prometheus Endpoint\u003Cbr/>Port Configuration\"]\n        NETWORK_REGISTRATION[\"Blockchain Registration\u003Cbr/>Network Visibility\"]\n    end\n    \n    COMPUTE_SUBTENSOR --> SERVE_PROMETHEUS\n    SERVE_PROMETHEUS --> DO_SERVE\n    DO_SERVE --> PROMETHEUS_EXTRINSIC\n    \n    PROMETHEUS_EXTRINSIC --> SUBSTRATE_CALL\n    SUBSTRATE_CALL --> SIGNED_EXTRINSIC\n    \n    AXON_SERVE --> METRICS_ENDPOINT\n    METRICS_ENDPOINT --> NETWORK_REGISTRATION\n```\n\n**Sources:** \u003CSourceLink text=\"compute/axon.py:166-201\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L166-L201\" />, \u003CSourceLink text=\"compute/axon.py:203-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L203-L283\" />\n\n### Prometheus Extrinsic Submission\n\nThe `serve_prometheus` method submits blockchain extrinsics to register Prometheus endpoints with the network, enabling distributed metrics collection.\n\n| Component | Function | Parameters |\n|-----------|----------|------------|\n| `serve_prometheus` | Main entry point | `wallet`, `port`, `netuid`, `wait_for_inclusion`, `wait_for_finalization` |\n| `do_serve_prometheus` | Extrinsic handler | `call_params`, retry logic with exponential backoff |\n| `prometheus_extrinsic` | Blockchain integration | Custom prometheus integration (imported) |\n\n**Sources:** \u003CSourceLink text=\"compute/axon.py:166-201\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L166-L201\" />, \u003CSourceLink text=\"compute/axon.py:203-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L203-L283\" />\n\n## Data Flow Integration\n\nThe monitoring system integrates with the core subnet operations through several key data flows.\n\n### Validator Monitoring Flow\n\n```mermaid\ngraph TD\n    subgraph \"Validation Process\"\n        POG_VALIDATION[\"Proof-of-GPU\u003Cbr/>Validation\"]\n        CHALLENGE_RESULTS[\"Challenge Results\u003Cbr/>Success/Failure\"]\n        SCORING[\"Score Calculation\u003Cbr/>Performance Metrics\"]\n    end\n    \n    subgraph \"Local Storage\"\n        COMPUTE_DB_WRITE[\"ComputeDb Write\u003Cbr/>Local Statistics\"]\n        POG_STATS[\"pog_stats table\u003Cbr/>GPU Performance\"]\n        STATS_TABLE[\"stats table\u003Cbr/>Miner Scores\"]\n    end\n    \n    subgraph \"WandB Logging\"\n        UPDATE_STATS_CALL[\"update_stats\u003Cbr/>Aggregate Results\"]\n        WANDB_LOG[\"wandb.log\u003Cbr/>Time Series Data\"]\n        CONFIG_UPDATE_STATS[\"run.config.update\u003Cbr/>Latest State\"]\n    end\n    \n    subgraph \"Cross-Network Sync\"\n        ALLOCATED_UPDATE[\"update_allocated_hotkeys\u003Cbr/>Resource State\"]\n        PENALIZED_UPDATE[\"update_penalized_hotkeys\u003Cbr/>Blacklist State\"]\n        SIGNATURE_APPLY[\"sign_run\u003Cbr/>Cryptographic Proof\"]\n    end\n    \n    POG_VALIDATION --> CHALLENGE_RESULTS\n    CHALLENGE_RESULTS --> SCORING\n    SCORING --> COMPUTE_DB_WRITE\n    \n    COMPUTE_DB_WRITE --> POG_STATS\n    COMPUTE_DB_WRITE --> STATS_TABLE\n    \n    SCORING --> UPDATE_STATS_CALL\n    UPDATE_STATS_CALL --> WANDB_LOG\n    WANDB_LOG --> CONFIG_UPDATE_STATS\n    \n    CONFIG_UPDATE_STATS --> ALLOCATED_UPDATE\n    ALLOCATED_UPDATE --> PENALIZED_UPDATE\n    PENALIZED_UPDATE --> SIGNATURE_APPLY\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:186-196\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L186-L196\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:198-230\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L198-L230\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:232-249\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L232-L249\" />\n\n## Configuration and Environment\n\nThe monitoring system requires specific configuration for proper operation, including API credentials and network parameters.\n\n### Environment Requirements\n\n| Requirement | Configuration | Source |\n|-------------|---------------|---------|\n| WandB API Key | `WANDB_API_KEY` environment variable or `.netrc` file | Environment setup |\n| Project Configuration | `PUBLIC_WANDB_NAME = \"opencompute\"` | Hard-coded constant |\n| Entity Configuration | `PUBLIC_WANDB_ENTITY = \"neuralinternet\"` | Hard-coded constant |\n| Database Connection | `ComputeDb()` instance | Local SQLite database |\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:15-16\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L15-L16\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:38-45\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L38-L45\" />","src/content/docs/monitoring-and-metrics.mdx","11d5877d7964e614","installation-and-setup",{"id":74,"data":76,"body":81,"filePath":82,"digest":83,"deferredRender":15},{"title":77,"editUrl":15,"head":78,"template":17,"sidebar":79,"pagefind":15,"draft":19},"Installation and Setup",[],{"hidden":19,"attrs":80},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"scripts/installation_script/README.md\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/README.md\" />\n\n  \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document provides comprehensive installation instructions for setting up the NI Compute Subnet on Ubuntu systems. It covers system requirements, dependency installation, wallet configuration, and miner deployment using the automated installation script.\n\nFor information about validator-specific setup procedures, see [Validator System](/validator-system#2). For details about the API service deployment, see [Resource Allocation API](/resource-allocation-api#4).\n\n## Installation Architecture\n\nThe installation process follows a two-pass approach with clear separation between system-level dependencies and application-level configuration:\n\n```mermaid\ngraph TD\n    START[\"Installation Start\"] --> SYSCHECK[\"System Requirements Check\"]\n    SYSCHECK --> DOCKER[\"Docker Installation\"]\n    DOCKER --> NVIDIA[\"NVIDIA Drivers & Container Runtime\"]\n    NVIDIA --> CUDA[\"CUDA 12.8 Installation\"]\n    CUDA --> BITTENSOR[\"Bittensor Framework\"]\n    BITTENSOR --> REBOOT{\"Reboot Required?\"}\n    \n    REBOOT -->|Yes| REBOOT_SYS[\"System Reboot\"]\n    REBOOT -->|No| WALLET_CHECK[\"Wallet Check\"]\n    REBOOT_SYS --> WALLET_CHECK\n    \n    WALLET_CHECK --> WALLET_EXISTS{\"Wallet Exists?\"}\n    WALLET_EXISTS -->|No| WALLET_CREATE[\"Manual Wallet Creation\"]\n    WALLET_EXISTS -->|Yes| VENV_SETUP[\"Virtual Environment Setup\"]\n    WALLET_CREATE --> VENV_SETUP\n    \n    VENV_SETUP --> DEPS[\"Python Dependencies\"]\n    DEPS --> FIREWALL[\"UFW Firewall Configuration\"]\n    FIREWALL --> ENV_CONFIG[\"Environment Configuration\"]\n    ENV_CONFIG --> PM2_SETUP[\"PM2 Process Manager\"]\n    PM2_SETUP --> MINER_START[\"Miner Process Launch\"]\n    MINER_START --> COMPLETE[\"Installation Complete\"]\n```\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:1-805\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L1-L805\" />\n\n## System Requirements\n\nThe installation script supports Ubuntu 22.04 and 24.04 with the following requirements:\n\n| Component | Requirement | Detection Method |\n|-----------|-------------|------------------|\n| Operating System | Ubuntu 22.04/24.04 | `uname` check |\n| GPU | NVIDIA GPU | Driver installation |\n| Docker | Latest stable | `docker --version` |\n| CUDA | Version 12.8 | `nvcc --version` |\n| Python | 3.8+ | System package |\n| Node.js | 18.x | Package manager |\n\nThe installer performs automatic detection and installation of missing components through the `docker_installed()`, `nvidia_docker_installed()`, and `cuda_version_installed()` functions.\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:93-95\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L93-L95\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:113-118\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L113-L118\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:154-160\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L154-L160\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:165-188\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L165-L188\" />\n\n## Installation Script Components\n\nThe `compute_subnet_installer.sh` script orchestrates the entire installation process through modular functions:\n\n```mermaid\ngraph TB\n    subgraph \"System Dependencies\"\n        DOCKER_FUNC[\"docker_installed()\"]\n        NVIDIA_FUNC[\"nvidia_docker_installed()\"] \n        CUDA_FUNC[\"cuda_version_installed()\"]\n        BTCLI_FUNC[\"btcli_installed()\"]\n    end\n    \n    subgraph \"Configuration Functions\"\n        SETUP_NEEDRESTART[\"setup_needrestart()\"]\n        RUN_APT[\"run_apt_get()\"]\n        INSTALL_PKG[\"install_package()\"]\n        PAUSE_USER[\"pause_for_user()\"]\n    end\n    \n    subgraph \"Wallet Management\"\n        WALLET_DIR[\"~/.bittensor/wallets\"]\n        WALLET_CHECK[\"Wallet Existence Check\"]\n    end\n    \n    subgraph \"Environment Setup\"\n        VENV_DIR[\"~/venv\"]\n        ENV_FILE[\".env Configuration\"]\n        WANDB_CONFIG[\"WANDB Integration\"]\n    end\n    \n    subgraph \"Process Management\"\n        PM2_START[\"PM2 Process Launch\"]\n        FIREWALL_CONFIG[\"UFW Firewall Rules\"]\n        MINER_PROCESS[\"neurons/miner.py\"]\n    end\n    \n    DOCKER_FUNC --> NVIDIA_FUNC\n    NVIDIA_FUNC --> CUDA_FUNC\n    CUDA_FUNC --> BTCLI_FUNC\n    BTCLI_FUNC --> WALLET_CHECK\n    WALLET_CHECK --> VENV_DIR\n    VENV_DIR --> ENV_FILE\n    ENV_FILE --> WANDB_CONFIG\n    WANDB_CONFIG --> FIREWALL_CONFIG\n    FIREWALL_CONFIG --> PM2_START\n    PM2_START --> MINER_PROCESS\n```\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:113-203\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L113-L203\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:566-570\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L566-L570\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:681-732\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L681-L732\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:750-770\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L750-L770\" />\n\n## Installation Process\n\n### Phase 1: System Dependencies\n\nThe installer first establishes the execution environment and installs core system dependencies:\n\n**Environment Detection**\n```bash\n# User and home directory detection\nREAL_USER=$(whoami)\nUSER_NAME=\"$REAL_USER\"\nHOME_DIR=\"$(eval echo \"~$REAL_USER\")\"\n```\n\n**Docker Installation**\nThe script installs Docker CE with NVIDIA container runtime support when not present:\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:97-110\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L97-L110\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:223-249\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L223-L249\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:251-272\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L251-L272\" />\n\n**CUDA 12.8 Installation**\nVersion-specific CUDA installation based on Ubuntu release:\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:274-330\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L274-L330\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:287-298\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L287-L298\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:299-313\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L299-L313\" />\n\n### Phase 2: Repository Setup\n\nThe installer locates or clones the compute subnet repository:\n\n```mermaid\ngraph LR\n    GIT_CHECK[\"Git Repository Check\"] --> REPO_ROOT[\"git rev-parse --show-toplevel\"]\n    REPO_ROOT --> SETUP_CHECK[\"setup.py/pyproject.toml Check\"]\n    SETUP_CHECK --> CLONE_REPO[\"git clone ni-compute\"]\n    CLONE_REPO --> CS_PATH[\"CS_PATH Variable Set\"]\n    CS_PATH --> VENV_CREATE[\"Virtual Environment Creation\"]\n```\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:334-395\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L334-L395\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:427-444\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L427-L444\" />\n\n### Phase 3: Python Environment\n\nVirtual environment creation and dependency installation:\n\n| Step | Command | Configuration File |\n|------|---------|-------------------|\n| Virtual Environment | `python3 -m venv ~/venv` | N/A |\n| Base Dependencies | `pip install -r requirements.txt` | `requirements.txt` |\n| Compute Dependencies | `pip install --no-deps -r requirements-compute.txt` | `requirements-compute.txt` |\n| Editable Install | `pip install -e .` | `setup.py`/`pyproject.toml` |\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:446-464\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L446-L464\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:453-461\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L453-L461\" />\n\n### Phase 4: Wallet Configuration\n\nThe installer checks for existing Bittensor wallets and provides guidance for wallet creation:\n\n```bash\nWALLET_DIR=\"${HOME}/.bittensor/wallets\"\nhave_wallets=false\nif [ -d \"${WALLET_DIR}\" ] && [ -n \"$(ls -A \"${WALLET_DIR}\" 2>/dev/null)\" ]; then\n  have_wallets=true\nfi\n```\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:566-600\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L566-L600\" />\n\n### Phase 5: Network Configuration\n\n**Firewall Setup**\nUFW configuration with required ports:\n\n| Port | Protocol | Purpose |\n|------|----------|---------|\n| 22 | TCP | SSH Access |\n| 4444 | TCP | Validator Communication |\n| 8091 | TCP | Default Axon Port |\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:610-655\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L610-L655\" />\n\n**Environment Configuration**\nThe installer configures the `.env` file with database and API settings:\n\n```bash\n# SQLITE_DB_PATH configuration\nsed -i \"s@^SQLITE_DB_PATH=.*@SQLITE_DB_PATH=\\\"${CS_PATH}/database.db\\\"@\" \"$env_path\"\n\n# WANDB_API_KEY configuration\nsed -i \"s@^WANDB_API_KEY=.*@WANDB_API_KEY=\\\"$WANDB_API_KEY\\\"@\" \"$env_path\"\n```\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:699-732\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L699-L732\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:718-720\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L718-L720\" />\n\n### Phase 6: Process Management\n\n**PM2 Configuration**\nThe miner process is launched using PM2 with comprehensive environment variables:\n\n```bash\npm2 start \"${VENV_DIR}/bin/python3\" \\\n  --name \"subnet${NETUID}_miner\" \\\n  -- \\\n  \"${CS_PATH}/neurons/miner.py\" \\\n  --netuid \"${NETUID}\" \\\n  --subtensor.network \"${SUBTENSOR_NETWORK}\" \\\n  --wallet.name \"default\" \\\n  --wallet.hotkey \"default\" \\\n  --axon.port \"${AXON_PORT}\"\n```\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:750-770\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L750-L770\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:776-797\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L776-L797\" />\n\n## Configuration Options\n\n### Network Selection\n\nThe installer supports both mainnet and testnet configurations:\n\n| Network | NetUID | Subtensor Network | Default Port |\n|---------|--------|-------------------|--------------|\n| Mainnet | 27 | `subvortex.info:9944` | 8091 |\n| Testnet | 15 | `test` | 8091 |\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:618-649\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L618-L649\" />\n\n### Automated Mode\n\nThe installer supports non-interactive execution:\n\n```bash\n./compute_subnet_installer.sh --automated\n```\n\nThis mode uses environment variables for configuration and skips user prompts.\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:24-29\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L24-L29\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:539-561\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L539-L561\" />\n\n## Verification\n\n### Process Status\n```bash\n# Check PM2 process status\npm2 list\npm2 logs subnet${NETUID}_miner\n\n# Check log files\ntail -f ${CS_PATH}/pm2_out.log\ntail -f ${CS_PATH}/pm2_error.log\n```\n\n### Network Registration\n```bash\n# Register hotkey on network\nbtcli subnet register --wallet.name default --wallet.hotkey default --netuid ${NETUID}\n```\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:773-774\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L773-L774\" />, \u003CSourceLink text=\"scripts/installation_script/README.md:118-122\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/README.md#L118-L122\" />\n\n## Troubleshooting\n\n### Common Issues\n\n**Docker Permission Errors**\n```bash\nsudo usermod -aG docker $USER\nsudo systemctl restart docker\n```\n\n**CUDA Environment Issues**\nThe installer automatically configures CUDA paths in `.bashrc`:\n```bash\nexport PATH=/usr/local/cuda-12.8/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH\n```\n\n**Wallet Not Found**\nEnsure wallet exists at `~/.bittensor/wallets/\u003Ccoldkey>/hotkeys/\u003Chotkey>` before running the second installation pass.\n\nSources: \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:131-142\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L131-L142\" />, \u003CSourceLink text=\"scripts/installation_script/compute_subnet_installer.sh:315-326\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/compute_subnet_installer.sh#L315-L326\" />, \u003CSourceLink text=\"scripts/installation_script/README.md:127-140\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/installation_script/README.md#L127-L140\" />","src/content/docs/installation-and-setup.mdx","10c708a5c553e7b2","resource-allocation-api",{"id":84,"data":86,"body":91,"filePath":92,"digest":93,"deferredRender":15},{"title":87,"editUrl":15,"head":88,"template":17,"sidebar":89,"pagefind":15,"draft":19},"Resource Allocation API",[],{"hidden":19,"attrs":90},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/register_api.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\n## Purpose and Scope\n\nThe Resource Allocation API is a FastAPI-based web service that provides programmatic access to GPU compute resources within the NI Compute Subnet. It serves as the primary interface for external clients to discover, allocate, deallocate, and manage computational resources provided by subnet miners.\n\nThis system handles the complete lifecycle of resource allocation, from initial discovery through active management of allocated containers. For information about the underlying miner resource provisioning, see [Miner System](/miner-system#3). For details about the validation and scoring of these resources, see [Validator System](/validator-system#2).\n\n## Architecture Overview\n\nThe Resource Allocation API is implemented as the `RegisterAPI` class in \u003CSourceLink text=\"neurons/register_api.py:229-3251\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L229-L3251\" />. It operates as a standalone FastAPI application that interfaces with the Bittensor network, local databases, and distributed state management systems.\n\n```mermaid\ngraph TB\n    subgraph \"External Clients\"\n        CLI[\"CLI Client\"]\n        WEB[\"Web Applications\"]\n        API_CLIENTS[\"API Clients\"]\n    end\n    \n    subgraph \"RegisterAPI Core\"\n        FASTAPI[\"FastAPI Application\"]\n        ROUTES[\"Route Handlers\"]\n        MODELS[\"Pydantic Models\"]\n    end\n    \n    subgraph \"Resource Management\"\n        ALLOCATE[\"_allocate_container()\"]\n        HOTKEY_ALLOC[\"_allocate_container_hotkey()\"]\n        HEALTH_CHECK[\"_check_allocation()\"]\n    end\n    \n    subgraph \"State Management\"\n        LOCAL_DB[\"ComputeDb (SQLite)\"]\n        WANDB_STATE[\"WandB Integration\"]\n        METAGRAPH[\"Bittensor Metagraph\"]\n    end\n    \n    subgraph \"Network Communication\"\n        DENDRITE[\"Bittensor Dendrite\"]\n        MINERS[\"Subnet Miners\"]\n    end\n    \n    CLI --> FASTAPI\n    WEB --> FASTAPI\n    API_CLIENTS --> FASTAPI\n    \n    FASTAPI --> ROUTES\n    ROUTES --> MODELS\n    ROUTES --> ALLOCATE\n    ROUTES --> HOTKEY_ALLOC\n    \n    ALLOCATE --> DENDRITE\n    HOTKEY_ALLOC --> DENDRITE\n    DENDRITE --> MINERS\n    \n    ROUTES --> LOCAL_DB\n    ROUTES --> WANDB_STATE\n    HEALTH_CHECK --> METAGRAPH\n    \n    HEALTH_CHECK --> DENDRITE\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:229-3251\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L229-L3251\" />\n\n## Core Components\n\n### FastAPI Application Setup\n\nThe `RegisterAPI` class initializes a FastAPI application with SSL support and middleware for IP whitelisting when enabled. The application runs on port 8903 by default and requires SSL certificates for secure communication.\n\n```mermaid\ngraph LR\n    subgraph \"RegisterAPI.__init__()\"\n        CONFIG[\"Configuration Setup\"]\n        WALLET[\"Bittensor Wallet\"]\n        SUBTENSOR[\"ComputeSubnetSubtensor\"]\n        DENDRITE[\"Bittensor Dendrite\"]\n        METAGRAPH[\"Network Metagraph\"]\n        WANDB[\"ComputeWandb\"]\n        FASTAPI_APP[\"FastAPI Application\"]\n    end\n    \n    CONFIG --> WALLET\n    CONFIG --> SUBTENSOR\n    WALLET --> DENDRITE\n    SUBTENSOR --> METAGRAPH\n    CONFIG --> WANDB\n    CONFIG --> FASTAPI_APP\n    \n    FASTAPI_APP --> ROUTES[\"_setup_routes()\"]\n    ROUTES --> MIDDLEWARE[\"IPWhitelistMiddleware\"]\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:229-342\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L229-L342\" />, \u003CSourceLink text=\"neurons/register_api.py:314-323\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L314-L323\" />\n\n### Request/Response Models\n\nThe API uses Pydantic models to define request and response structures:\n\n| Model | Purpose | Key Fields |\n|-------|---------|------------|\n| `DeviceRequirement` | GPU resource specifications | `gpu_type`, `gpu_size`, `ram`, `timeline` |\n| `DockerRequirement` | Container configuration | `base_image`, `ssh_key`, `dockerfile` |\n| `Allocation` | Allocation response data | `hotkey`, `ssh_ip`, `ssh_port`, `uuid_key` |\n| `Resource` | Resource information | `gpu_name`, `gpu_capacity`, `allocate_status` |\n| `ResourceQuery` | Resource filtering | `gpu_name`, `cpu_count_min/max`, capacity ranges |\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:147-214\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L147-L214\" />, \u003CSourceLink text=\"neurons/register_api.py:156-168\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L156-L168\" />\n\n## API Endpoints\n\n### Resource Allocation Endpoints\n\n#### Allocate by Specification\n- **Endpoint**: `POST /service/allocate_spec`\n- **Handler**: \u003CSourceLink text=\"neurons/register_api.py:434-547\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L434-L547\" />\n- **Purpose**: Allocates resources based on GPU specifications and requirements\n- **Process**: Discovers suitable miners, validates availability, provisions container\n\n#### Allocate by Hotkey\n- **Endpoint**: `POST /service/allocate_hotkey`\n- **Handler**: \u003CSourceLink text=\"neurons/register_api.py:576-695\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L576-L695\" />\n- **Purpose**: Allocates a specific miner's resources by hotkey\n- **Process**: Direct allocation to specified miner with container provisioning\n\n#### Deallocate Resources\n- **Endpoint**: `POST /service/deallocate`\n- **Handler**: \u003CSourceLink text=\"neurons/register_api.py:725-850\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L725-L850\" />\n- **Purpose**: Releases allocated resources and cleans up containers\n- **Process**: Validates UUID, sends deallocation signal to miner, updates state\n\n### Container Management Endpoints\n\nThe API provides Docker container lifecycle management:\n\n| Endpoint | Purpose | Handler Location |\n|----------|---------|-------------------|\n| `/service/restart_docker` | Restart allocated container | \u003CSourceLink text=\"neurons/register_api.py:920-1012\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L920-L1012\" /> |\n| `/service/pause_docker` | Pause container execution | \u003CSourceLink text=\"neurons/register_api.py:1027-1114\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1027-L1114\" /> |\n| `/service/unpause_docker` | Resume paused container | \u003CSourceLink text=\"neurons/register_api.py:1129-1215\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1129-L1215\" /> |\n| `/service/exchange_docker_key` | Update SSH keys | \u003CSourceLink text=\"neurons/register_api.py:1230-1317\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1230-L1317\" /> |\n\n### Resource Discovery Endpoints\n\n```mermaid\ngraph TD\n    subgraph \"Resource Listing\"\n        SQL_LIST[\"/list/resources_sql\"]\n        WANDB_LIST[\"/list/resources_wandb\"]\n        ALLOC_LIST[\"/list/allocations_sql\"]\n    end\n    \n    subgraph \"Data Sources\"\n        COMPUTE_DB[\"ComputeDb (Local)\"]\n        WANDB_API[\"WandB API\"]\n        MINER_SPECS[\"get_miner_details()\"]\n    end\n    \n    subgraph \"Filtering & Pagination\"\n        RESOURCE_QUERY[\"ResourceQuery Model\"]\n        PAGINATE[\"_paginate_list()\"]\n    end\n    \n    SQL_LIST --> COMPUTE_DB\n    SQL_LIST --> MINER_SPECS\n    WANDB_LIST --> WANDB_API\n    WANDB_LIST --> MINER_SPECS\n    \n    SQL_LIST --> RESOURCE_QUERY\n    WANDB_LIST --> RESOURCE_QUERY\n    RESOURCE_QUERY --> PAGINATE\n    \n    ALLOC_LIST --> COMPUTE_DB\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:1441-1644\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1441-L1644\" />, \u003CSourceLink text=\"neurons/register_api.py:1847-2053\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1847-L2053\" />, \u003CSourceLink text=\"neurons/register_api.py:1346-1419\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1346-L1419\" />\n\n## Resource Management Logic\n\n### Allocation Process\n\nThe allocation process involves candidate discovery, availability checking, and container provisioning:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant RegisterAPI\n    participant ComputeDb\n    participant Dendrite\n    participant Miner\n    participant WandB\n    \n    Client->>RegisterAPI: \"/service/allocate_spec\"\n    RegisterAPI->>ComputeDb: \"select_allocate_miners_hotkey()\"\n    ComputeDb-->>RegisterAPI: \"candidate_hotkeys[]\"\n    \n    RegisterAPI->>Dendrite: \"Allocate(checking=True)\"\n    Dendrite->>Miner: \"Check availability\"\n    Miner-->>Dendrite: \"availability_response\"\n    Dendrite-->>RegisterAPI: \"final_candidates[]\"\n    \n    RegisterAPI->>RegisterAPI: \"Sort by scores\"\n    RegisterAPI->>Dendrite: \"Allocate(checking=False)\"\n    Dendrite->>Miner: \"Provision container\"\n    Miner-->>Dendrite: \"ssh_credentials\"\n    Dendrite-->>RegisterAPI: \"allocation_response\"\n    \n    RegisterAPI->>ComputeDb: \"update_allocation_db()\"\n    RegisterAPI->>WandB: \"_update_allocation_wandb()\"\n    RegisterAPI-->>Client: \"Allocation details\"\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2733-2805\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2733-L2805\" />, \u003CSourceLink text=\"neurons/register_api.py:2807-2889\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2807-L2889\" />\n\n### Health Monitoring\n\nThe `_check_allocation()` method continuously monitors allocated resources:\n\n- **Frequency**: Every 180 seconds (`ALLOCATE_CHECK_PERIOD`)\n- **Timeout Handling**: Deallocates after 20 consecutive failures (`ALLOCATE_CHECK_COUNT`)\n- **Notifications**: Sends webhook notifications for status changes\n- **Implementation**: \u003CSourceLink text=\"neurons/register_api.py:3002-3100\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L3002-L3100\" />\n\n### State Synchronization\n\nResource state is maintained across multiple systems:\n\n| System | Update Method | Purpose |\n|--------|---------------|---------|\n| Local SQLite | `update_allocation_db()` | Persistent allocation tracking |\n| WandB | `_update_allocation_wandb()` | Distributed state sharing |\n| Metagraph | `_refresh_metagraph()` | Network topology updates |\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2891-2919\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2891-L2919\" />, \u003CSourceLink text=\"neurons/register_api.py:2921-2929\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2921-L2929\" />\n\n## Integration Points\n\n### Bittensor Network Integration\n\nThe API integrates deeply with Bittensor network components:\n\n- **Subtensor**: Uses `ComputeSubnetSubtensor` for blockchain interaction\n- **Dendrite**: Communicates with miners via `Allocate` protocol messages\n- **Metagraph**: Maintains current network state and miner information\n- **Wallet**: Provides cryptographic identity for API operations\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:264-276\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L264-L276\" />\n\n### Database Operations\n\nThe API uses `ComputeDb` for local state persistence with the following key operations:\n\n- Allocation tracking in `allocation` table\n- Miner details retrieval via `get_miner_details()`\n- Challenge statistics for miner filtering\n- Implementation: \u003CSourceLink text=\"neurons/Validator/database/allocate.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/allocate.py\" />\n\n### WandB Integration\n\nWandB serves as distributed state management:\n\n- **Allocated Hotkeys**: Tracks resources across all validators\n- **Miner Specifications**: Hardware details and availability\n- **Penalized Hotkeys**: Blacklist management\n- **Implementation**: Via `ComputeWandb` class integration\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:1646-1702\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1646-L1702\" />, \u003CSourceLink text=\"neurons/register_api.py:1870-1872\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1870-L1872\" />\n\n## Configuration and Security\n\n### Authentication and Security\n\nThe API implements several security measures:\n\n- **SSL/TLS**: Required certificates for HTTPS communication\n- **IP Whitelisting**: Optional middleware for access control (`IPWhitelistMiddleware`)\n- **RSA Encryption**: Key pair generation for secure miner communication\n- **UUID Validation**: Prevents unauthorized resource access\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:120-134\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L120-L134\" />, \u003CSourceLink text=\"neurons/register_api.py:3214-3227\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L3214-L3227\" />\n\n### Constants and Configuration\n\nKey configuration constants defined in the module:\n\n| Constant | Value | Purpose |\n|----------|--------|---------|\n| `DEFAULT_API_PORT` | 8903 | Default API server port |\n| `DATA_SYNC_PERIOD` | 600 | Metagraph refresh interval |\n| `ALLOCATE_CHECK_PERIOD` | 180 | Health check frequency |\n| `ALLOCATE_CHECK_COUNT` | 20 | Max failures before deallocation |\n| `VALID_VALIDATOR_HOTKEYS` | Array | Authorized validator hotkeys |\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:86-116\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L86-L116\" />\n\nThe API runs with SSL certificates located at `cert/server.key`, `cert/server.cer`, and `cert/ca.cer`, and terminates if these certificates are not found.","src/content/docs/resource-allocation-api.mdx","d8332dcc552ac153","configuration",{"id":94,"data":96,"body":101,"filePath":102,"digest":103,"deferredRender":15},{"title":97,"editUrl":15,"head":98,"template":17,"sidebar":99,"pagefind":15,"draft":19},"Configuration",[],{"hidden":19,"attrs":100},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"README.md\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md\" />\n\n  \u003CSourceLink text=\"compute/__init__.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py\" />\n\n  \u003CSourceLink text=\"compute/utils/math.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/math.py\" />\n\n  \u003CSourceLink text=\"compute/utils/parser.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py\" />\n\n  \u003CSourceLink text=\"config.yaml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml\" />\n\n  \u003CSourceLink text=\"neurons/Validator/database/miner.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/miner.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/database/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/miner_script_m_merkletree.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/miner_script_m_merkletree.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\n## Purpose and Scope\n\nThis document covers the configuration system for the NI Compute Subnet, including static configuration constants, command-line argument parsing, GPU performance parameters, and runtime configuration options. The configuration system manages settings for validators, miners, and the resource allocation API across the entire compute subnet infrastructure.\n\nFor information about command-line argument usage in specific components, see [Command-line Arguments](/configuration/command-line-arguments#7.1). For GPU performance benchmarks and hardware-specific settings, see [GPU Performance Configuration](/configuration/gpu-performance-configuration#7.2).\n\n## Configuration Architecture\n\nThe configuration system operates through multiple layers and sources that provide settings for different aspects of the compute subnet:\n\n```mermaid\ngraph TB\n    subgraph \"Configuration Sources\"\n        CLI[\"Command Line Arguments\u003Cbr/>ComputeArgPaser\"]\n        YAML[\"config.yaml\u003Cbr/>GPU Performance Data\"]\n        STATIC[\"compute/__init__.py\u003Cbr/>Static Constants\"]\n        ENV[\"Environment Variables\u003Cbr/>.env file\"]\n    end\n    \n    subgraph \"Configuration Categories\"\n        NETWORK[\"Network Configuration\u003Cbr/>netuid, timeout values\"]\n        GPU_CONFIG[\"GPU Configuration\u003Cbr/>performance data, tolerance\"]\n        MINER_CONFIG[\"Miner Configuration\u003Cbr/>hashcat, whitelist settings\"]\n        VALIDATOR_CONFIG[\"Validator Configuration\u003Cbr/>batch sizes, thresholds\"]\n        POG_CONFIG[\"Proof-of-GPU Configuration\u003Cbr/>merkle proof settings\"]\n    end\n    \n    subgraph \"Target Components\"\n        MINER[\"Miner Process\u003Cbr/>neurons/miner.py\"]\n        VALIDATOR[\"Validator Process\u003Cbr/>neurons/validator.py\"]\n        API[\"RegisterAPI\u003Cbr/>FastAPI Service\"]\n    end\n    \n    CLI --> NETWORK\n    CLI --> MINER_CONFIG\n    CLI --> VALIDATOR_CONFIG\n    \n    YAML --> GPU_CONFIG\n    YAML --> POG_CONFIG\n    \n    STATIC --> NETWORK\n    STATIC --> MINER_CONFIG\n    STATIC --> POG_CONFIG\n    \n    ENV --> API\n    \n    NETWORK --> MINER\n    NETWORK --> VALIDATOR\n    NETWORK --> API\n    \n    GPU_CONFIG --> VALIDATOR\n    MINER_CONFIG --> MINER\n    VALIDATOR_CONFIG --> VALIDATOR\n    POG_CONFIG --> VALIDATOR\n```\n\n**Sources:** \u003CSourceLink text=\"compute/__init__.py:1-93\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L1-L93\" />, \u003CSourceLink text=\"config.yaml:1-104\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml#L1-L104\" />, \u003CSourceLink text=\"compute/utils/parser.py:1-170\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L1-L170\" />\n\n## Static Configuration Constants\n\nThe compute subnet defines core system parameters as static constants in the main module. These constants control network behavior, validation timing, and system limits:\n\n### Network and Validation Constants\n\n```mermaid\ngraph LR\n    CONSTANTS[\"compute/__init__.py\"] --> NETWORK_PARAMS[\"`Network Parameters\n    validator_permit_stake: 1.0e4\n    weights_rate_limit: 100\n    specs_timeout: 60`\"]\n    \n    CONSTANTS --> POG_PARAMS[\"`Proof-of-GPU Parameters\n    pog_retry_limit: 30\n    pog_retry_interval: 80\n    pow_timeout: 30`\"]\n    \n    CONSTANTS --> MINER_PARAMS[\"`Miner Parameters\n    miner_priority_specs: 1\n    miner_priority_challenge: 2\n    miner_priority_allocate: 3`\"]\n    \n    CONSTANTS --> SECURITY[\"`Security Lists\n    SUSPECTED_EXPLOITERS_HOTKEYS\n    TRUSTED_VALIDATORS_HOTKEYS`\"]\n```\n\nKey configuration constants include:\n\n| Category | Constant | Default Value | Purpose |\n|----------|----------|---------------|---------|\n| Network | `validator_permit_stake` | `1.0e4` | Minimum stake required for validator |\n| Network | `weights_rate_limit` | `100` | Rate limit for weight updates |\n| Validation | `specs_timeout` | `60` | Timeout for hardware specs queries |\n| Proof-of-GPU | `pog_retry_limit` | `30` | Maximum PoG retry attempts |\n| Proof-of-GPU | `pog_retry_interval` | `80` | Seconds between PoG retries |\n| Proof-of-Work | `pow_min_difficulty` | `7` | Minimum PoW difficulty |\n| Proof-of-Work | `pow_max_difficulty` | `12` | Maximum PoW difficulty |\n| Miner | `miner_hashcat_location` | `\"hashcat\"` | Default hashcat binary path |\n\n**Sources:** \u003CSourceLink text=\"compute/__init__.py:29-58\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L29-L58\" />\n\n### Security Configuration\n\nThe system maintains predefined lists of trusted validators and suspected exploiters:\n\n```mermaid\ngraph TB\n    SECURITY_CONFIG[\"Security Configuration\"] --> TRUSTED[\"TRUSTED_VALIDATORS_HOTKEYS\u003Cbr/>List of verified validator hotkeys\"]\n    SECURITY_CONFIG --> EXPLOITERS[\"SUSPECTED_EXPLOITERS_HOTKEYS\u003Cbr/>List of blacklisted hotkeys\"]\n    \n    TRUSTED --> VALIDATOR_PROCESS[\"Validator Whitelist Logic\"]\n    EXPLOITERS --> BLACKLIST_FILTER[\"Automatic Blacklisting\"]\n```\n\n**Sources:** \u003CSourceLink text=\"compute/__init__.py:59-92\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L59-L92\" />\n\n## Command-Line Argument System\n\nThe `ComputeArgPaser` class extends Python's `ArgumentParser` to provide comprehensive command-line configuration for both miners and validators:\n\n```mermaid\ngraph TB\n    PARSER[\"ComputeArgPaser\"] --> BASE_ARGS[\"`Base Arguments\n    --netuid: Subnet ID\n    --auto_update: Auto-update flag\n    --blacklist.exploiters: Use exploiter list`\"]\n    \n    PARSER --> BLACKLIST_ARGS[\"`Blacklist/Whitelist\n    --blacklist.hotkeys\n    --blacklist.coldkeys\n    --whitelist.hotkeys\n    --whitelist.coldkeys`\"]\n    \n    PARSER --> VALIDATOR_ARGS[\"`Validator Arguments\n    --validator.whitelist.unrecognized\n    --validator.perform.hardware.query\n    --validator.challenge.batch.size\n    --validator.specs.batch.size`\"]\n    \n    PARSER --> MINER_ARGS[\"`Miner Arguments\n    --miner.hashcat.path\n    --miner.hashcat.workload.profile\n    --miner.whitelist.not.enough.stake\n    --ssh.port`\"]\n    \n    PARSER --> BITTENSOR_ARGS[\"`Bittensor Integration\n    bt.subtensor.add_args()\n    bt.logging.add_args()\n    bt.wallet.add_args()\n    bt.axon.add_args()`\"]\n```\n\n### Argument Categories\n\nThe parser organizes arguments into logical categories:\n\n| Category | Method | Arguments |\n|----------|--------|-----------|\n| Base | `__init__` | `netuid`, `auto_update`, blacklist/whitelist options |\n| Validator | `add_validator_argument()` | batch sizes, hardware query settings, thresholds |\n| Miner | `add_miner_argument()` | hashcat configuration, SSH port, whitelist settings |\n| Bittensor | Built-in | subtensor, logging, wallet, axon arguments |\n\n**Sources:** \u003CSourceLink text=\"compute/utils/parser.py:8-71\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L8-L71\" />, \u003CSourceLink text=\"compute/utils/parser.py:72-115\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L72-L115\" />, \u003CSourceLink text=\"compute/utils/parser.py:116-166\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L116-L166\" />\n\n## GPU Performance Configuration\n\nThe `config.yaml` file contains comprehensive GPU performance data used for validation and scoring:\n\n```mermaid\ngraph TB\n    GPU_CONFIG[\"config.yaml\"] --> PERF_DATA[\"`GPU Performance Data\n    GPU_TFLOPS_FP16: FP16 performance\n    GPU_TFLOPS_FP32: FP32 performance\n    GPU_AVRAM: Available VRAM`\"]\n    \n    GPU_CONFIG --> TOLERANCE[\"`Tolerance Pairs\n    gpu_tolerance_pairs:\n    Similar GPU mappings`\"]\n    \n    GPU_CONFIG --> SCORES[\"`GPU Scores\n    gpu_scores:\n    Scoring weights by model`\"]\n    \n    PERF_DATA --> IDENTIFICATION[\"GPU Identification Logic\u003Cbr/>identify_gpu()\"]\n    TOLERANCE --> VALIDATION[\"Performance Validation\u003Cbr/>Tolerance Handling\"]\n    SCORES --> SCORING[\"Miner Scoring System\"]\n```\n\n### Performance Data Structure\n\nThe GPU configuration includes three main performance categories:\n\n1. **FP16 Performance** (`GPU_TFLOPS_FP16`): Theoretical FP16 TFLOPS for each GPU model\n2. **FP32 Performance** (`GPU_TFLOPS_FP32`): Theoretical FP32 TFLOPS for each GPU model  \n3. **Available VRAM** (`GPU_AVRAM`): Effective VRAM capacity in GB\n\nExample configuration structure:\n```yaml\ngpu_performance:\n  GPU_TFLOPS_FP16:\n    NVIDIA H100: 330\n    NVIDIA A100-SXM4-80GB: 238.8\n  GPU_TFLOPS_FP32:\n    NVIDIA H100: 37.2\n    NVIDIA A100-SXM4-80GB: 18.2\n  GPU_AVRAM:\n    NVIDIA H100: 34.36\n    NVIDIA A100-SXM4-80GB: 34.36\n```\n\n**Sources:** \u003CSourceLink text=\"config.yaml:1-94\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml#L1-L94\" />\n\n### GPU Tolerance and Scoring\n\nThe configuration defines tolerance pairs for GPUs with similar performance characteristics and assigns scoring weights:\n\n```mermaid\ngraph LR\n    TOLERANCE_PAIRS[\"gpu_tolerance_pairs\"] --> SIMILAR_GPUS[\"`Similar GPU Pairs\n    NVIDIA L40 ↔ NVIDIA RTX 6000 Ada\n    NVIDIA A100 variants ↔ similar models`\"]\n    \n    GPU_SCORES[\"gpu_scores\"] --> SCORE_WEIGHTS[\"`Scoring Weights\n    NVIDIA H200: 4.0\n    NVIDIA H100: 2.80\n    NVIDIA A100-SXM4-80GB: 1.90`\"]\n    \n    SIMILAR_GPUS --> VALIDATION_TOLERANCE[\"Validation Tolerance Logic\"]\n    SCORE_WEIGHTS --> MINER_SCORING[\"Miner Performance Scoring\"]\n```\n\n**Sources:** \u003CSourceLink text=\"config.yaml:63-94\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml#L63-L94\" />\n\n## Merkle Proof Configuration\n\nThe system includes specific configuration for Proof-of-GPU Merkle tree operations:\n\n| Parameter | Default Value | Purpose |\n|-----------|---------------|---------|\n| `miner_script_path` | `\"neurons/Validator/miner_script_m_merkletree.py\"` | Path to Merkle proof script |\n| `time_tolerance` | `5` | Time tolerance for proof verification |\n| `submatrix_size` | `512` | Size of submatrices for proof generation |\n| `hash_algorithm` | `'sha256'` | Hash algorithm for Merkle trees |\n| `pog_retry_limit` | `22` | Maximum Proof-of-GPU retry attempts |\n| `pog_retry_interval` | `60` | Seconds between PoG retries |\n| `max_workers` | `64` | Maximum concurrent workers |\n| `max_random_delay` | `900` | Maximum random delay in seconds |\n\n**Sources:** \u003CSourceLink text=\"config.yaml:95-104\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml#L95-L104\" />\n\n## Configuration Loading and Usage\n\nThe configuration system loads and applies settings through multiple mechanisms:\n\n```mermaid\ngraph TB\n    LOAD_CONFIG[\"`Configuration Loading Process`\"] --> YAML_LOAD[\"load_yaml_config()\u003Cbr/>YAML file parsing\"]\n    LOAD_CONFIG --> PARSER_CONFIG[\"ComputeArgPaser.config\u003Cbr/>bt.config() integration\"]\n    LOAD_CONFIG --> STATIC_IMPORT[\"Static constant imports\u003Cbr/>from compute import *\"]\n    \n    YAML_LOAD --> GPU_IDENTIFY[\"identify_gpu()\u003Cbr/>Performance-based identification\"]\n    PARSER_CONFIG --> COMPONENT_CONFIG[\"Component Configuration\u003Cbr/>Miners, Validators, API\"]\n    STATIC_IMPORT --> SYSTEM_LIMITS[\"System Limits\u003Cbr/>Timeouts, thresholds\"]\n    \n    GPU_IDENTIFY --> POG_VALIDATION[\"Proof-of-GPU Validation\"]\n    COMPONENT_CONFIG --> RUNTIME_BEHAVIOR[\"Runtime Behavior Control\"]\n    SYSTEM_LIMITS --> NETWORK_OPERATIONS[\"Network Operations\"]\n```\n\n### Configuration Access Patterns\n\nComponents access configuration through several patterns:\n\n1. **Static Import**: Direct import of constants from `compute` module\n2. **YAML Loading**: Dynamic loading of GPU performance data via `load_yaml_config()`\n3. **Argument Parsing**: Runtime configuration through `ComputeArgPaser.config`\n4. **Environment Variables**: API keys and paths from `.env` files\n\n**Sources:** \u003CSourceLink text=\"neurons/Validator/pog.py:14-26\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py#L14-L26\" />, \u003CSourceLink text=\"compute/utils/parser.py:70-71\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L70-L71\" />, \u003CSourceLink text=\"compute/__init__.py:1-93\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L1-L93\" />\n\n## Configuration Validation and Error Handling\n\nThe system includes validation mechanisms for configuration parameters:\n\n```mermaid\ngraph TB\n    VALIDATION[\"`Configuration Validation`\"] --> YAML_VALIDATION[\"YAML Validation\u003Cbr/>FileNotFoundError, YAMLError\"]\n    VALIDATION --> RANGE_VALIDATION[\"Range Validation\u003Cbr/>pow_min_difficulty ≤ pow_max_difficulty\"]\n    VALIDATION --> TYPE_VALIDATION[\"Type Validation\u003Cbr/>Numeric parameters, string paths\"]\n    \n    YAML_VALIDATION --> ERROR_HANDLING[\"Error Handling\u003Cbr/>Graceful degradation\"]\n    RANGE_VALIDATION --> BOUNDS_CHECKING[\"Bounds Checking\u003Cbr/>Prevent invalid configurations\"]\n    TYPE_VALIDATION --> CONVERSION[\"Type Conversion\u003Cbr/>String to numeric conversion\"]\n```\n\nThe configuration system provides robust error handling for common issues:\n\n- Missing configuration files (YAML not found)\n- Invalid YAML syntax or structure\n- Out-of-range numeric parameters\n- Invalid GPU model specifications\n- Missing required command-line arguments\n\n**Sources:** \u003CSourceLink text=\"neurons/Validator/pog.py:22-25\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py#L22-L25\" />, \u003CSourceLink text=\"compute/utils/math.py:16-21\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/math.py#L16-L21\" />","src/content/docs/configuration.mdx","40bbc9dce1e28646","miner-system",{"id":104,"data":106,"body":111,"filePath":112,"digest":113,"deferredRender":15},{"title":107,"editUrl":15,"head":108,"template":17,"sidebar":109,"pagefind":15,"draft":19},"Miner System",[],{"hidden":19,"attrs":110},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/miner.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThe Miner System provides compute resources to the NI Compute Subnet by responding to resource allocation requests and validation challenges from validators. It manages Docker containers for secure compute workloads, handles proof-of-work challenges, and maintains network connectivity through the Bittensor protocol.\n\nFor information about validator-side operations, see [Validator System](/validator-system#2). For details about the resource allocation API that coordinates with miners, see [Resource Allocation API](/resource-allocation-api#4). For container lifecycle management specifics, see [Container Management](/miner-system/container-management#3.1).\n\n## Architecture Overview\n\nThe miner system is implemented as a single `Miner` class that operates as a Bittensor axon server, handling three primary types of requests from validators: resource allocation, challenge-response, and system monitoring.\n\n### Core System Components\n\n```mermaid\ngraph TB\n    subgraph \"Miner Class (neurons/miner.py)\"\n        MINER[\"Miner\"]\n        ALLOCATE[\"allocate()\"]\n        CHALLENGE[\"challenge()\"]\n        BLACKLIST[\"base_blacklist()\"]\n        PRIORITY[\"base_priority()\"]\n    end\n    \n    subgraph \"Request Processing\"\n        ALLOC_REQ[\"Allocate Synapse\"]\n        CHALL_REQ[\"Challenge Synapse\"]\n        BLACKLIST_CHECK[\"Blacklist Check\"]\n        PRIORITY_CALC[\"Priority Calculation\"]\n    end\n    \n    subgraph \"Container Management\"\n        REGISTER_ALLOC[\"register_allocation()\"]\n        DEREGISTER_ALLOC[\"deregister_allocation()\"]\n        CHECK_ALLOC[\"check_allocation()\"]\n        CONTAINER_OPS[\"Container Operations\"]\n    end\n    \n    subgraph \"Network Layer\"\n        AXON[\"ComputeSubnetAxon\"]\n        SUBTENSOR[\"ComputeSubnetSubtensor\"]\n        METAGRAPH[\"bt.metagraph\"]\n        WALLET[\"bt.wallet\"]\n    end\n    \n    subgraph \"Monitoring & State\"\n        WANDB[\"ComputeWandb\"]\n        SPECS_UPDATE[\"update_specs()\"]\n        ALLOCATED_UPDATE[\"update_allocated()\"]\n    end\n    \n    subgraph \"Infrastructure\"\n        DOCKER[\"Docker Runtime\"]\n        SSH_SERVER[\"SSH Access\"]\n        HASHCAT[\"Hashcat (PoW)\"]\n    end\n    \n    %% Request flow\n    ALLOC_REQ --> BLACKLIST_CHECK\n    CHALL_REQ --> BLACKLIST_CHECK\n    BLACKLIST_CHECK --> PRIORITY_CALC\n    PRIORITY_CALC --> ALLOCATE\n    PRIORITY_CALC --> CHALLENGE\n    \n    %% Allocation flow\n    ALLOCATE --> REGISTER_ALLOC\n    ALLOCATE --> DEREGISTER_ALLOC\n    ALLOCATE --> CHECK_ALLOC\n    REGISTER_ALLOC --> CONTAINER_OPS\n    CONTAINER_OPS --> DOCKER\n    CONTAINER_OPS --> SSH_SERVER\n    \n    %% Challenge flow\n    CHALLENGE --> HASHCAT\n    \n    %% Network integration\n    MINER --> AXON\n    AXON --> SUBTENSOR\n    AXON --> METAGRAPH\n    MINER --> WALLET\n    \n    %% Monitoring\n    MINER --> WANDB\n    WANDB --> SPECS_UPDATE\n    WANDB --> ALLOCATED_UPDATE\n    \n    %% Security\n    MINER --> BLACKLIST\n    MINER --> PRIORITY\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:79-714\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L79-L714\" />\n\n### Miner Lifecycle and Main Loop\n\n```mermaid\nsequenceDiagram\n    participant MAIN as \"main()\"\n    participant MINER as \"Miner.__init__()\"\n    participant AXON as \"ComputeSubnetAxon\"\n    participant WANDB as \"ComputeWandb\"\n    participant LOOP as \"start() Loop\"\n    participant VALIDATOR as \"Validator\"\n    \n    MAIN->>MINER: \"Initialize miner\"\n    MINER->>MINER: \"init_config()\"\n    MINER->>MINER: \"init_black_and_white_list()\"\n    MINER->>AXON: \"Initialize axon server\"\n    MINER->>WANDB: \"Initialize WandB monitoring\"\n    MINER->>MINER: \"build_check_container()\"\n    \n    MINER->>AXON: \"axon.attach(allocate, challenge)\"\n    MINER->>AXON: \"axon.serve(netuid, subtensor)\"\n    MINER->>AXON: \"axon.start()\"\n    \n    MINER->>LOOP: \"asyncio.run(start())\"\n    \n    loop \"Every 5 seconds\"\n        LOOP->>LOOP: \"sync_local()\"\n        \n        alt \"Every 30 blocks (~6 min)\"\n            LOOP->>LOOP: \"get_updated_validator()\"\n        end\n        \n        alt \"Every 150 blocks (~30 min)\"\n            LOOP->>WANDB: \"update_specs()\"\n        end\n        \n        alt \"Every 75 blocks (~15 min)\"\n            LOOP->>LOOP: \"sync_status()\"\n            LOOP->>WANDB: \"log_chain_data()\"\n        end\n    end\n    \n    Note over VALIDATOR,AXON: \"Concurrent request handling\"\n    VALIDATOR->>AXON: \"Allocate/Challenge requests\"\n    AXON->>MINER: \"Route to handler methods\"\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:702-714\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L702-L714\" />, \u003CSourceLink text=\"neurons/miner.py:606-700\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L606-L700\" />, \u003CSourceLink text=\"neurons/miner.py:117-189\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L117-L189\" />\n\n## Request Handling System\n\nThe miner processes two primary types of requests from validators through the Bittensor axon protocol: `Allocate` requests for compute resource management and `Challenge` requests for proof-of-work validation.\n\n### Request Processing Pipeline\n\nAll incoming requests go through a standardized processing pipeline that includes blacklisting, priority calculation, and request-specific handling.\n\n```mermaid\ngraph LR\n    subgraph \"Incoming Request\"\n        REQ[\"Synapse Request\"]\n        HOTKEY[\"dendrite.hotkey\"]\n        STAKE[\"Validator Stake\"]\n    end\n    \n    subgraph \"Security Layer\"\n        BLACKLIST_FN[\"blacklist_allocate()\"]\n        BLACKLIST_BASE[\"base_blacklist()\"]\n        WHITELIST_CHECK[\"whitelist_hotkeys\"]\n        STAKE_CHECK[\"validator_permit_stake\"]\n        EXPLOITER_CHECK[\"exploiters_hotkeys_set\"]\n    end\n    \n    subgraph \"Priority Layer\"\n        PRIORITY_FN[\"priority_allocate()\"]\n        PRIORITY_BASE[\"base_priority()\"]\n        STAKE_PRIORITY[\"metagraph.S[caller_uid]\"]\n        MINER_PRIORITY[\"miner_priority_allocate\"]\n    end\n    \n    subgraph \"Handler Layer\"\n        ALLOCATE_HANDLER[\"allocate()\"]\n        CHALLENGE_HANDLER[\"challenge()\"]\n    end\n    \n    REQ --> BLACKLIST_FN\n    HOTKEY --> BLACKLIST_BASE\n    STAKE --> STAKE_CHECK\n    \n    BLACKLIST_FN --> BLACKLIST_BASE\n    BLACKLIST_BASE --> WHITELIST_CHECK\n    BLACKLIST_BASE --> STAKE_CHECK\n    BLACKLIST_BASE --> EXPLOITER_CHECK\n    \n    REQ --> PRIORITY_FN\n    PRIORITY_FN --> PRIORITY_BASE\n    PRIORITY_BASE --> STAKE_PRIORITY\n    PRIORITY_FN --> MINER_PRIORITY\n    \n    REQ --> ALLOCATE_HANDLER\n    REQ --> CHALLENGE_HANDLER\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:330-374\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L330-L374\" />, \u003CSourceLink text=\"neurons/miner.py:375-385\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L375-L385\" />, \u003CSourceLink text=\"neurons/miner.py:397-403\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L397-L403\" />\n\n### Blacklist and Security Controls\n\nThe `base_blacklist()` method implements comprehensive security controls to prevent unauthorized access and abuse:\n\n| Security Check | Implementation | Purpose |\n|---|---|---|\n| Whitelist Check | `hotkey not in self.whitelist_hotkeys` | Allow trusted validators regardless of stake |\n| Network Recognition | `hotkey not in self.metagraph.hotkeys` | Reject unregistered entities |\n| Stake Requirement | `stake &lt; validator_permit_stake` | Ensure minimum validator stake |\n| Explicit Blacklist | `hotkey in self.blacklist_hotkeys` | Block specific problematic validators |\n| Exploiter Detection | `hotkey in self.exploiters_hotkeys_set` | Block known malicious actors |\n\nSources: \u003CSourceLink text=\"neurons/miner.py:330-374\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L330-L374\" />\n\n## Resource Allocation Handler\n\nThe `allocate()` method manages the complete lifecycle of compute resource allocation, from initial availability checks to container provisioning and deallocation.\n\n### Allocation Request Types\n\n```mermaid\ngraph TD\n    subgraph \"Allocate Request Processing\"\n        ALLOCATE_REQ[\"Allocate Synapse\"]\n        CHECKING_FLAG[\"synapse.checking\"]\n        TIMELINE[\"synapse.timeline\"]\n        DOCKER_CHANGE[\"synapse.docker_change\"]\n    end\n    \n    subgraph \"Checking Mode (checking=True)\"\n        CHECK_POSITIVE[\"timeline > 0\"]\n        CHECK_ALLOCATION[\"check_allocation()\"]\n        CHECK_NEGATIVE[\"timeline : 0\"]\n        CHECK_IF_ALLOCATED[\"check_if_allocated()\"]\n    end\n    \n    subgraph \"Docker Change Mode (docker_change=True)\"\n        EXCHANGE_KEY[\"exchange_key_container()\"]\n        RESTART_CONTAINER[\"restart_container()\"]\n        PAUSE_CONTAINER[\"pause_container()\"]\n        UNPAUSE_CONTAINER[\"unpause_container()\"]\n    end\n    \n    subgraph \"Allocation Mode (Normal)\"\n        ALLOC_POSITIVE[\"timeline > 0\"]\n        REGISTER_ALLOCATION[\"register_allocation()\"]\n        ALLOC_NEGATIVE[\"timeline : 0\"] \n        DEREGISTER_ALLOCATION[\"deregister_allocation()\"]\n    end\n    \n    ALLOCATE_REQ --> CHECKING_FLAG\n    CHECKING_FLAG -->|\"True\"| CHECK_POSITIVE\n    CHECKING_FLAG -->|\"True\"| CHECK_NEGATIVE\n    CHECK_POSITIVE --> CHECK_ALLOCATION\n    CHECK_NEGATIVE --> CHECK_IF_ALLOCATED\n    \n    ALLOCATE_REQ --> DOCKER_CHANGE\n    DOCKER_CHANGE -->|\"True\"| EXCHANGE_KEY\n    DOCKER_CHANGE -->|\"True\"| RESTART_CONTAINER\n    DOCKER_CHANGE -->|\"True\"| PAUSE_CONTAINER\n    DOCKER_CHANGE -->|\"True\"| UNPAUSE_CONTAINER\n    \n    ALLOCATE_REQ --> ALLOC_POSITIVE\n    ALLOCATE_REQ --> ALLOC_NEGATIVE\n    ALLOC_POSITIVE --> REGISTER_ALLOCATION\n    ALLOC_NEGATIVE --> DEREGISTER_ALLOCATION\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:419-479\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L419-L479\" />\n\n### Allocation State Management\n\nThe miner tracks allocation state through multiple mechanisms to ensure consistency and prevent conflicts:\n\n- **WandB Integration**: `self.wandb.update_allocated()` synchronizes allocation status across the network\n- **File-based State**: `allocation_key` file stores the current allocation's public key\n- **Container State**: Docker container lifecycle tied to allocation status\n- **Concurrency Control**: `self.allocate_action` flag prevents concurrent allocations\n\nSources: \u003CSourceLink text=\"neurons/miner.py:405-417\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L405-L417\" />, \u003CSourceLink text=\"neurons/miner.py:190-221\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L190-L221\" />\n\n## Challenge Response System\n\nThe `challenge()` method handles proof-of-work validation requests from validators, though the actual proof-of-work execution is currently disabled in the implementation.\n\n### Challenge Processing Flow\n\n```mermaid\ngraph LR\n    subgraph \"Challenge Request\"\n        CHALL_REQ[\"Challenge Synapse\"]\n        DIFFICULTY[\"challenge_difficulty\"]\n        HASH[\"challenge_hash\"]\n        SALT[\"challenge_salt\"]\n        MODE[\"challenge_mode\"]\n    end\n    \n    subgraph \"Validation\"\n        DIFFICULTY_CHECK[\"difficulty \u003C: 0\"]\n        VALIDATOR_ID[\"dendrite.hotkey[:8]\"]\n        RUN_ID[\"run_id generation\"]\n    end\n    \n    subgraph \"PoW Execution (Disabled)\"\n        HASHCAT_PATH[\"hashcat_path\"]\n        WORKLOAD_PROFILE[\"hashcat_workload_profile\"]\n        EXTENDED_OPTIONS[\"hashcat_extended_options\"]\n        RUN_MINER_POW[\"run_miner_pow()\"]\n    end\n    \n    CHALL_REQ --> DIFFICULTY_CHECK\n    CHALL_REQ --> VALIDATOR_ID\n    VALIDATOR_ID --> RUN_ID\n    \n    CHALL_REQ --> HASHCAT_PATH\n    CHALL_REQ --> WORKLOAD_PROFILE\n    CHALL_REQ --> EXTENDED_OPTIONS\n    RUN_ID --> RUN_MINER_POW\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:491-515\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L491-L515\" />\n\n## Network Integration and Monitoring\n\nThe miner maintains continuous integration with the Bittensor network through periodic synchronization and state updates.\n\n### Network Synchronization Schedule\n\n| Operation | Frequency | Purpose |\n|---|---|---|\n| `sync_local()` | Every 5 seconds | Update local metagraph state |\n| `get_updated_validator()` | Every 30 blocks (~6 min) | Refresh validator whitelist |\n| `update_specs()` | Every 150 blocks (~30 min) | Sync hardware specs to WandB |\n| `sync_status()` | Every 75 blocks (~15 min) | Update registration status and log metrics |\n\n### WandB Integration Points\n\nThe miner integrates with Weights & Biases for distributed state management and monitoring:\n\n- **Specs Management**: `self.wandb.update_specs()` publishes hardware specifications\n- **Allocation Tracking**: `self.wandb.update_allocated()` maintains allocation state\n- **Chain Data Logging**: `self.wandb.log_chain_data()` records network metrics\n- **Validator Discovery**: `self.wandb.get_allocated_hotkeys()` queries network state\n\nSources: \u003CSourceLink text=\"neurons/miner.py:606-700\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L606-L700\" />, \u003CSourceLink text=\"neurons/miner.py:179-181\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L179-L181\" />\n\n### Initialization and Configuration\n\nThe miner initialization process follows a structured sequence to establish network connectivity and prepare for operation:\n\n```mermaid\ngraph TD\n    subgraph \"Configuration Phase\"\n        INIT_CONFIG[\"init_config()\"]\n        PARSE_ARGS[\"ComputeArgPaser\"]\n        LOGGING_SETUP[\"bt.logging setup\"]\n    end\n    \n    subgraph \"Network Objects\"\n        WALLET_INIT[\"bt.wallet(config)\"]\n        SUBTENSOR_INIT[\"ComputeSubnetSubtensor(config)\"]\n        METAGRAPH_INIT[\"subtensor.metagraph(netuid)\"]\n    end\n    \n    subgraph \"Infrastructure Setup\"\n        DOCKER_CHECK[\"check_docker_availability()\"]\n        BUILD_CONTAINER[\"build_check_container()\"]\n        BUILD_SAMPLE[\"build_sample_container()\"]\n        CUDA_CHECK[\"check_cuda_availability()\"]\n    end\n    \n    subgraph \"Security Setup\"\n        BLACKLIST_INIT[\"init_black_and_white_list()\"]\n        WHITELIST_SETUP[\"TRUSTED_VALIDATORS_HOTKEYS\"]\n        EXPLOITER_SETUP[\"SUSPECTED_EXPLOITERS_HOTKEYS\"]\n    end\n    \n    subgraph \"Service Initialization\"\n        AXON_INIT[\"init_axon()\"]\n        WANDB_INIT[\"ComputeWandb initialization\"]\n        ALLOCATION_CHECK[\"__check_alloaction_errors()\"]\n    end\n    \n    INIT_CONFIG --> PARSE_ARGS\n    PARSE_ARGS --> LOGGING_SETUP\n    \n    LOGGING_SETUP --> WALLET_INIT\n    WALLET_INIT --> SUBTENSOR_INIT\n    SUBTENSOR_INIT --> METAGRAPH_INIT\n    \n    METAGRAPH_INIT --> DOCKER_CHECK\n    DOCKER_CHECK --> BUILD_CONTAINER\n    BUILD_CONTAINER --> BUILD_SAMPLE\n    BUILD_SAMPLE --> CUDA_CHECK\n    \n    CUDA_CHECK --> BLACKLIST_INIT\n    BLACKLIST_INIT --> WHITELIST_SETUP\n    WHITELIST_SETUP --> EXPLOITER_SETUP\n    \n    EXPLOITER_SETUP --> AXON_INIT\n    AXON_INIT --> WANDB_INIT\n    WANDB_INIT --> ALLOCATION_CHECK\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:117-189\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L117-L189\" />, \u003CSourceLink text=\"neurons/miner.py:254-281\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L254-L281\" />, \u003CSourceLink text=\"neurons/miner.py:222-252\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L222-L252\" />","src/content/docs/miner-system.mdx","9e10e803f0dbba53","cli-tools/registration-cli",{"id":114,"data":116,"body":121,"filePath":122,"digest":123,"deferredRender":15},{"title":117,"editUrl":15,"head":118,"template":17,"sidebar":119,"pagefind":15,"draft":19},"Registration CLI",[],{"hidden":19,"attrs":120},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/register.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThe Registration CLI is a command-line interface tool for interacting with the NI Compute Subnet resource allocation system. It provides commands for allocating compute resources from miners, deallocating them when finished, and managing the penalty system for network validators.\n\nThis tool is primarily used by network participants who need to allocate GPU compute resources from miners on the subnet. For information about the underlying Resource Allocation API that this CLI interacts with, see [Resource Allocation API](/resource-allocation-api#4). For details about the validator system that manages resource scoring, see [Validator System](/validator-system#2).\n\n## Overview\n\nThe Registration CLI (`neurons/register.py`) serves as the primary user interface for resource allocation operations on the NI Compute Subnet. It connects to the Bittensor network to discover available miners, negotiate resource allocation, and manage the lifecycle of compute allocations.\n\n### CLI Command Structure\n\nThe CLI operates in an interactive loop, accepting single-letter commands for different operations:\n\n| Command | Function | Description |\n|---------|----------|-------------|\n| `a` | `allocate()` | Allocate resources by GPU requirements |\n| `a_hotkey` | `allocate_hotkey()` | Allocate resources by specific miner hotkey |\n| `d` | `deallocate()` | Deallocate resources |\n| `list_a` | `list_allocations()` | List currently allocated resources |\n| `list_ah` | `list_allocations_hotkeys()` | List allocated resource hotkeys |\n| `list_r` | `list_resources()` | List all available resources on network |\n| `p_hotkey` | `penalize_hotkey()` | Add miner to penalty blacklist |\n| `dp_hotkey` | `depenalize_hotkey()` | Remove miner from penalty blacklist |\n| `list_p` | `list_penalizations()` | List penalized miners |\n\nSources: \u003CSourceLink text=\"neurons/register.py:790-830\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L790-L830\" />\n\n## System Architecture\n\n### CLI Integration with Network Components\n\n```mermaid\ngraph TD\n    CLI[\"`register.py`\u003Cbr/>CLI Interface\"]\n    CONFIG[\"`get_config()`\u003Cbr/>Configuration Parser\"]\n    WALLET[\"`bt.wallet`\u003Cbr/>Bittensor Wallet\"]\n    SUBTENSOR[\"`bt.subtensor`\u003Cbr/>Blockchain Connection\"]\n    DENDRITE[\"`bt.dendrite`\u003Cbr/>RPC Client\"]\n    METAGRAPH[\"`metagraph`\u003Cbr/>Network State\"]\n    \n    subgraph \"Local State Management\"\n        COMPUTEDB[\"`ComputeDb`\u003Cbr/>SQLite Database\"]\n        WANDB[\"`ComputeWandb`\u003Cbr/>Distributed State\"]\n    end\n    \n    subgraph \"Network Communication\" \n        ALLOCATE_PROTOCOL[\"`Allocate`\u003Cbr/>Protocol Messages\"]\n        RSA[\"`RSAEncryption`\u003Cbr/>Secure Communication\"]\n    end\n    \n    subgraph \"Miner Network\"\n        MINERS[\"`Miners`\u003Cbr/>Resource Providers\"]\n        CONTAINERS[\"`Docker Containers`\u003Cbr/>Allocated Resources\"]\n    end\n    \n    CLI --> CONFIG\n    CLI --> WALLET\n    CLI --> SUBTENSOR\n    CLI --> DENDRITE\n    CLI --> COMPUTEDB\n    CLI --> WANDB\n    \n    SUBTENSOR --> METAGRAPH\n    DENDRITE --> ALLOCATE_PROTOCOL\n    ALLOCATE_PROTOCOL --> RSA\n    \n    DENDRITE --> MINERS\n    MINERS --> CONTAINERS\n    \n    COMPUTEDB --> WANDB\n```\n\nSources: \u003CSourceLink text=\"neurons/register.py:43-75\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L43-L75\" />, \u003CSourceLink text=\"neurons/register.py:117-180\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L117-L180\" />, \u003CSourceLink text=\"neurons/register.py:784-787\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L784-L787\" />\n\n### Resource Allocation Flow\n\n```mermaid\nsequenceDiagram\n    participant CLI as \"register.py CLI\"\n    participant DB as \"ComputeDb\"\n    participant WANDB as \"ComputeWandb\" \n    participant METAGRAPH as \"metagraph\"\n    participant DENDRITE as \"dendrite\"\n    participant MINER as \"Miner Axon\"\n    participant RSA as \"RSAEncryption\"\n    \n    Note over CLI,MINER: Resource Allocation Process\n    \n    CLI->>DB: \"select_allocate_miners_hotkey()\"\n    DB-->>CLI: \"candidate_hotkeys[]\"\n    \n    CLI->>METAGRAPH: \"Query network state\"\n    METAGRAPH-->>CLI: \"axon_candidates[]\"\n    \n    CLI->>DENDRITE: \"Query candidates (checking=True)\"\n    DENDRITE->>MINER: \"Allocate(checking=True)\"\n    MINER-->>DENDRITE: \"{'status': True/False}\"\n    DENDRITE-->>CLI: \"responses[]\"\n    \n    CLI->>CLI: \"Filter available candidates\"\n    CLI->>CLI: \"Sort by metagraph scores\"\n    \n    loop \"For each sorted candidate\"\n        CLI->>RSA: \"generate_key_pair()\"\n        RSA-->>CLI: \"private_key, public_key\"\n        \n        CLI->>DENDRITE: \"Query allocation (checking=False)\"\n        DENDRITE->>MINER: \"Allocate(public_key, timeline)\"\n        MINER-->>DENDRITE: \"{'status': True, 'info': encrypted_data}\"\n        DENDRITE-->>CLI: \"allocation_response\"\n        \n        alt \"Allocation successful\"\n            CLI->>RSA: \"decrypt_data()\"\n            RSA-->>CLI: \"ssh_credentials\"\n            CLI->>DB: \"update_allocation_db()\"\n            CLI->>WANDB: \"update_allocated_hotkeys()\"\n            CLI->>CLI: \"Display SSH details\"\n        end\n    end\n```\n\nSources: \u003CSourceLink text=\"neurons/register.py:117-180\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L117-L180\" />, \u003CSourceLink text=\"neurons/register.py:230-288\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L230-L288\" />\n\n## Core Functions\n\n### Resource Allocation Functions\n\n#### `allocate_container()`\nPrimary allocation function that finds and allocates resources based on device requirements.\n\n**Key Operations:**\n- Queries `ComputeDb` using `select_allocate_miners_hotkey()` to find candidate miners\n- Filters candidates through availability check (`checking=True`)\n- Sorts candidates by metagraph scores\n- Attempts allocation with highest-scored available miner\n- Returns encrypted SSH connection details\n\n**Parameters:**\n- `device_requirement`: Hardware specifications (GPU type, memory, CPU, RAM, disk)\n- `timeline`: Allocation duration in minutes\n- `public_key`: RSA public key for secure communication\n\nSources: \u003CSourceLink text=\"neurons/register.py:117-180\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L117-L180\" />\n\n#### `allocate_container_hotkey()`\nDirect allocation to a specific miner by hotkey, bypassing candidate selection.\n\n**Key Differences:**\n- Targets specific miner hotkey directly\n- Uses fixed device requirements structure\n- Includes `docker_requirement` for container specifications\n\nSources: \u003CSourceLink text=\"neurons/register.py:184-227\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L184-L227\" />\n\n### Deallocation Functions\n\n#### `deallocate()`\nHandles resource deallocation for multiple hotkeys simultaneously.\n\n**Process Flow:**\n1. Query local database for allocation details by hotkey\n2. Update database and WandB to mark as deallocated\n3. Send deallocation request to miner via `Allocate` protocol\n4. Handle batch processing for multiple hotkeys\n\n**Key Features:**\n- Supports comma-separated multiple hotkeys\n- Immediate database update before network communication\n- Error handling for missing hotkeys or network failures\n\nSources: \u003CSourceLink text=\"neurons/register.py:350-445\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L350-L445\" />\n\n### Information Display Functions\n\n#### `list_resources()`\nDisplays comprehensive network resource overview with GPU specifications and availability status.\n\n**Display Components:**\n- Tabular format with hotkey, GPU details, CPU, RAM, storage\n- Resource availability status (Available/Reserved)\n- Summary statistics for GPU instances and total counts\n- Integration with WandB allocated hotkeys for status updates\n\nSources: \u003CSourceLink text=\"neurons/register.py:531-643\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L531-L643\" />\n\n#### `list_allocations()`\nShows detailed information about currently allocated resources including SSH connection details.\n\n**Information Displayed:**\n- Allocation ID, hotkey, resource type\n- SSH credentials (username, password, port, IP)\n- Ready-to-use SSH commands\n\nSources: \u003CSourceLink text=\"neurons/register.py:446-488\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L446-L488\" />\n\n### Penalty Management Functions\n\n#### `penalize_hotkey()` and `depenalize_hotkey()`\nAdministrative functions for managing miner penalties through blacklist system.\n\n**Penalty Process:**\n- Validates hotkeys against network miner details\n- Updates local `blacklist` table via `update_blacklist_db()`\n- Synchronizes penalty status with WandB distributed state\n- Supports batch operations for multiple hotkeys\n\nSources: \u003CSourceLink text=\"neurons/register.py:671-762\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L671-L762\" />\n\n## Configuration and Setup\n\n### Configuration Parsers\n\nThe CLI provides two configuration modes:\n\n#### `get_config()`\nStandard argument parsing for programmatic usage with command-line parameters.\n\n#### `get_config_cli()`\nInteractive configuration that prompts for missing GPU requirements when not provided via command line.\n\n**Interactive Prompts:**\n- GPU type selection\n- GPU memory specification (converted from GB to MB)\n- Automatic logging directory setup\n\nSources: \u003CSourceLink text=\"neurons/register.py:43-113\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L43-L113\" />\n\n### Database Integration\n\nThe CLI integrates with multiple data persistence layers:\n\n#### Local Database Operations\n- **ComputeDb**: SQLite database for allocation tracking\n- **Tables Used**: `allocation`, `blacklist`, `miner`, `stats`\n- **Key Operations**: Resource queries, allocation updates, penalty management\n\n#### Distributed State Management\n- **ComputeWandb**: Synchronization with distributed validator state\n- **Functions**: `update_allocated_hotkeys()`, `update_penalized_hotkeys()`\n- **Purpose**: Cross-validator consistency for resource allocation status\n\nSources: \u003CSourceLink text=\"neurons/register.py:35\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L35\" />, \u003CSourceLink text=\"neurons/register.py:644-670\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L644-L670\" />\n\n## Security and Communication\n\n### RSA Encryption Integration\nAll resource allocation communications use RSA encryption for secure credential exchange:\n\n```mermaid\ngraph LR\n    CLI[\"`CLI`\u003Cbr/>register.py\"]\n    RSA_GEN[\"`generate_key_pair()`\u003Cbr/>RSA Key Generation\"]\n    MINER[\"`Miner`\u003Cbr/>Resource Provider\"]\n    RSA_DECRYPT[\"`decrypt_data()`\u003Cbr/>RSA Decryption\"]\n    \n    CLI --> RSA_GEN\n    RSA_GEN --> CLI\n    CLI -->|\"public_key\"| MINER\n    MINER -->|\"encrypted credentials\"| CLI\n    CLI --> RSA_DECRYPT\n    RSA_DECRYPT --> CLI\n```\n\n**Security Features:**\n- Unique key pair generation for each allocation\n- Encrypted SSH credential transmission\n- Base64 encoding for network transport\n\nSources: \u003CSourceLink text=\"neurons/register.py:31\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L31\" />, \u003CSourceLink text=\"neurons/register.py:237\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L237\" />, \u003CSourceLink text=\"neurons/register.py:244\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L244\" />\n\n### Protocol Communication\nUses Bittensor's `Allocate` protocol for standardized miner communication:\n\n**Protocol Parameters:**\n- `timeline`: Allocation duration\n- `device_requirement`: Hardware specifications\n- `checking`: Boolean flag for availability check vs actual allocation\n- `public_key`: RSA public key for encryption\n- `docker_requirement`: Container specifications (optional)\n\nSources: \u003CSourceLink text=\"neurons/register.py:32\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L32\" />, \u003CSourceLink text=\"neurons/register.py:144\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L144\" />, \u003CSourceLink text=\"neurons/register.py:168-170\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register.py#L168-L170\" />","src/content/docs/cli-tools/registration-cli.mdx","97bb4b55de55079a","communication-protocols/specs-allocate-and-challenge-protocols",{"id":124,"data":126,"body":131,"filePath":132,"digest":133,"deferredRender":15},{"title":127,"editUrl":15,"head":128,"template":17,"sidebar":129,"pagefind":15,"draft":19},"Specs, Allocate, and Challenge Protocols",[],{"hidden":19,"attrs":130},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/protocol.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document covers the three core Bittensor synapse protocols used for communication between validators and miners in the NI Compute Subnet: hardware specification queries, resource allocation requests, and challenge-response mechanisms. These protocols form the foundational communication layer for the compute marketplace.\n\nFor information about custom Bittensor extensions and Axon modifications, see [Custom Axon and Subtensor](/communication-protocols/custom-axon-and-subtensor#5.2).\n\n## Protocol Architecture\n\nThe NI Compute Subnet implements three primary communication protocols that extend Bittensor's `bt.Synapse` base class. Each protocol handles a specific aspect of the validator-miner interaction workflow.\n\n### Protocol Inheritance Structure\n\n```mermaid\nclassDiagram\n    class bt_Synapse {\n        \u003C\u003Cabstract>>\n        +deserialize()\n    }\n    \n    class Specs {\n        +specs_input: str\n        +specs_output: str\n        +deserialize() str\n    }\n    \n    class Allocate {\n        +timeline: int\n        +device_requirement: dict\n        +checking: bool\n        +public_key: str\n        +docker_requirement: dict\n        +docker_change: bool\n        +docker_action: dict\n        +output: dict\n        +deserialize() dict\n    }\n    \n    class Challenge {\n        +challenge_hash: str\n        +challenge_salt: str\n        +challenge_mode: str\n        +challenge_chars: str\n        +challenge_mask: str\n        +challenge_difficulty: int\n        +output: dict\n        +deserialize() dict\n    }\n    \n    bt_Synapse \u003C|-- Specs\n    bt_Synapse \u003C|-- Allocate\n    bt_Synapse \u003C|-- Challenge\n```\n\n**Sources:** \u003CSourceLink text=\"compute/protocol.py:18-136\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L18-L136\" />\n\n### Protocol Communication Flow\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator\"\n    participant BT as \"Bittensor Network\"\n    participant M as \"Miner\"\n    participant C as \"Container Runtime\"\n    \n    Note over V,M: Hardware Specification Query\n    V->>M: Specs(specs_input=\"\")\n    M->>M: Collect hardware info\n    M->>V: specs_output={\"CPU\": {...}, \"GPU\": {...}}\n    \n    Note over V,M: Resource Allocation Request\n    V->>M: Allocate(timeline=3600, device_requirement={...})\n    M->>C: Check resource availability\n    C->>M: Resource status\n    M->>V: output={\"status\": \"allocated\", \"ssh_details\": {...}}\n    \n    Note over V,M: Challenge Verification\n    V->>M: Challenge(challenge_hash=\"...\", challenge_difficulty=4)\n    M->>M: Execute hashcat proof-of-work\n    M->>V: output={\"password\": \"result\", \"error\": null}\n```\n\n**Sources:** \u003CSourceLink text=\"compute/protocol.py:23-135\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L23-L135\" />\n\n## Specs Protocol\n\nThe `Specs` protocol handles hardware specification queries between validators and miners. It allows validators to discover the computational capabilities of available miners.\n\n### Specs Protocol Structure\n\n| Attribute | Type | Description |\n|-----------|------|-------------|\n| `specs_input` | `str` | Input data sent to miner (typically empty) |\n| `specs_output` | `str` | Hardware specifications returned by miner |\n\n### Specs Message Format\n\nThe `specs_output` contains detailed hardware information in the following format:\n\n```json\n{\n  \"CPU\": {\n    \"count\": 4,\n    \"vendor_id_raw\": \"AuthenticAMD\",\n    \"brand_raw\": \"AMD Ryzen 7 3700X\",\n    \"hz_advertised_friendly\": \"3.6 GHz\"\n  },\n  \"GPU\": {\n    \"name\": \"NVIDIA GeForce RTX 3080\",\n    \"memory_total\": 10737418240,\n    \"compute_capability\": \"8.6\"\n  },\n  \"RAM\": {\n    \"total\": 34359738368,\n    \"available\": 28991029248\n  },\n  \"DISK\": {\n    \"total\": 1000204886016,\n    \"free\": 750153424896\n  }\n}\n```\n\n### Specs Protocol Implementation\n\n```mermaid\nflowchart TD\n    A[\"Validator calls dendrite.query()\"] --> B[\"Specs synapse created\"]\n    B --> C[\"specs_input : ''\"]\n    C --> D[\"Message sent to miner\"]\n    D --> E[\"Miner collects hardware info\"]\n    E --> F[\"specs_output populated\"]\n    F --> G[\"Response sent to validator\"]\n    G --> H[\"deserialize() returns specs_output\"]\n```\n\n**Sources:** \u003CSourceLink text=\"compute/protocol.py:23-57\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L23-L57\" />\n\n## Allocate Protocol\n\nThe `Allocate` protocol manages resource allocation requests and Docker container provisioning. It supports both allocation checking and actual resource reservation.\n\n### Allocate Protocol Attributes\n\n| Attribute | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `timeline` | `int` | `0` | Duration of allocation in seconds |\n| `device_requirement` | `dict` | `{}` | Required hardware specifications |\n| `checking` | `bool` | `True` | Flag for checking vs. actual allocation |\n| `public_key` | `str` | `\"\"` | RSA public key for encryption |\n| `output` | `dict` | `{}` | Miner response with allocation details |\n| `docker_requirement` | `dict` | See below | Docker container configuration |\n| `docker_change` | `bool` | `False` | Flag for Docker configuration changes |\n| `docker_action` | `dict` | See below | Docker action specifications |\n\n### Docker Configuration Structure\n\nThe `docker_requirement` dictionary contains container configuration:\n\n```python\n{\n    \"base_image\": \"ubuntu\",\n    \"ssh_key\": \"\",\n    \"ssh_port\": 4444,\n    \"volume_path\": \"/tmp\",\n    \"dockerfile\": \"\"\n}\n```\n\nThe `docker_action` dictionary specifies Docker operations:\n\n```python\n{\n    \"action\": \"\",\n    \"ssh_key\": \"\",\n    \"key_type\": \"\"\n}\n```\n\n### Allocate Protocol Workflow\n\n```mermaid\nflowchart TD\n    A[\"Allocation Request\"] --> B{\"checking == True?\"}\n    B -->|Yes| C[\"Check resource availability\"]\n    B -->|No| D[\"Perform actual allocation\"]\n    \n    C --> E[\"Return availability status\"]\n    D --> F[\"Create Docker container\"]\n    F --> G[\"Configure SSH access\"]\n    G --> H[\"Return allocation details\"]\n    \n    E --> I[\"output : {'available': bool}\"]\n    H --> J[\"output : {'status': 'allocated', 'ssh_details': {...}}\"]\n    \n    I --> K[\"deserialize() returns output\"]\n    J --> K\n```\n\n**Sources:** \u003CSourceLink text=\"compute/protocol.py:60-109\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L60-L109\" />\n\n## Challenge Protocol\n\nThe `Challenge` protocol implements proof-of-work verification using hashcat for GPU capability validation. It ensures miners have the computational resources they claim.\n\n### Challenge Protocol Attributes\n\n| Attribute | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `challenge_hash` | `str` | `\"\"` | Target hash for proof-of-work |\n| `challenge_salt` | `str` | `\"\"` | Salt value for hash computation |\n| `challenge_mode` | `str` | `\"\"` | Hashcat attack mode |\n| `challenge_chars` | `str` | `\"\"` | Character set for brute force |\n| `challenge_mask` | `str` | `\"\"` | Password mask pattern |\n| `challenge_difficulty` | `int` | `compute.pow_min_difficulty` | Minimum difficulty level |\n| `output` | `dict` | `{}` | Challenge response with results |\n\n### Challenge Response Format\n\nThe challenge response includes either a successful password result or error information:\n\n```json\n{\n  \"password\": \"found_password_or_null\",\n  \"error\": \"error_message_if_failed\"\n}\n```\n\n### Challenge Verification Process\n\n```mermaid\nflowchart TD\n    A[\"Validator generates challenge\"] --> B[\"Challenge parameters set\"]\n    B --> C[\"challenge_hash, challenge_salt, etc.\"]\n    C --> D[\"Synapse sent to miner\"]\n    D --> E[\"Miner executes hashcat\"]\n    E --> F{\"Hashcat successful?\"}\n    F -->|Yes| G[\"output : {'password': result, 'error': null}\"]\n    F -->|No| H[\"output : {'password': null, 'error': error_msg}\"]\n    G --> I[\"Response sent to validator\"]\n    H --> I\n    I --> J[\"deserialize() returns output\"]\n```\n\n**Sources:** \u003CSourceLink text=\"compute/protocol.py:112-135\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L112-L135\" />\n\n## Protocol Integration\n\nThese protocols integrate with the broader NI Compute Subnet architecture through the Bittensor network layer and are used by both validators and miners for different purposes:\n\n- **Validators** use these protocols to query miner capabilities, allocate resources, and verify computational claims\n- **Miners** implement protocol handlers to respond to specification queries, manage resource allocation, and execute proof-of-work challenges\n- **Resource Allocation API** leverages the `Allocate` protocol for external resource management requests\n\nThe protocols ensure secure, verifiable communication while maintaining compatibility with the Bittensor ecosystem.\n\n**Sources:** \u003CSourceLink text=\"compute/protocol.py:1-136\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/protocol.py#L1-L136\" />","src/content/docs/communication-protocols/specs-allocate-and-challenge-protocols.mdx","30d30c78f4954b76","communication-protocols/custom-axon-and-subtensor",{"id":134,"data":136,"body":141,"filePath":142,"digest":143,"deferredRender":15},{"title":137,"editUrl":15,"head":138,"template":17,"sidebar":139,"pagefind":15,"draft":19},"Custom Axon and Subtensor",[],{"hidden":19,"attrs":140},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/axon.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document covers the compute subnet's custom extensions to Bittensor's core communication components: Axon and Subtensor. These extensions provide subnet-specific functionality including custom version handling, Prometheus metrics integration, and enhanced request preprocessing.\n\nFor information about the core communication protocols used by these components, see [Specs, Allocate, and Challenge Protocols](/communication-protocols/specs-allocate-and-challenge-protocols#5.1). For monitoring and metrics infrastructure, see [Monitoring and Metrics](/monitoring-and-metrics#6).\n\n## Architecture Overview\n\nThe compute subnet extends Bittensor's base communication classes to add subnet-specific functionality. The main extensions include custom serve extrinsics, Prometheus metrics support, and enhanced middleware processing.\n\n```mermaid\ngraph TB\n    subgraph \"Bittensor Base Classes\"\n        BaseAxon[\"bittensor.core.axon.Axon\"]\n        BaseSubtensor[\"bittensor.core.subtensor.Subtensor\"]\n        BaseMiddleware[\"bittensor.core.axon.AxonMiddleware\"]\n    end\n    \n    subgraph \"Compute Subnet Extensions\"\n        ComputeAxon[\"ComputeSubnetAxon\"]\n        ComputeSubtensor[\"ComputeSubnetSubtensor\"]\n        ComputeMiddleware[\"ComputeSubnetAxonMiddleware\"]\n        CustomServe[\"custom_serve_extrinsic\"]\n    end\n    \n    subgraph \"Integration Components\"\n        VersionInt[\"__version_as_int__\"]\n        LocalVersion[\"get_local_version()\"]\n        PrometheusExtrinsic[\"prometheus_extrinsic\"]\n    end\n    \n    BaseAxon --> ComputeAxon\n    BaseSubtensor --> ComputeSubtensor\n    BaseMiddleware --> ComputeMiddleware\n    \n    ComputeAxon --> VersionInt\n    ComputeAxon --> LocalVersion\n    ComputeSubtensor --> PrometheusExtrinsic\n    ComputeMiddleware --> VersionInt\n    CustomServe --> VersionInt\n    \n    CustomServe -.->|\"Patches\"| BaseAxon\n```\n\n**Sources:** \u003CSourceLink text=\"compute/axon.py:1-488\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L1-L488\" />\n\n## Custom Serve Extrinsic\n\nThe `custom_serve_extrinsic` function replaces Bittensor's standard serve extrinsic functionality to incorporate compute subnet specific versioning.\n\n### Function Implementation\n\nThe custom serve extrinsic handles axon registration on the blockchain with subnet-specific parameters:\n\n```mermaid\ngraph TD\n    Start[\"custom_serve_extrinsic()\"]\n    Unlock[\"unlock_key(wallet)\"]\n    Params[\"Create AxonServeCallParams\"]\n    CheckNeuron[\"Check existing neuron state\"]\n    UpToDate{{\"Neuron up to date?\"}}\n    DoServe[\"do_serve_axon()\"]\n    Success{{\"Success?\"}}\n    Return[\"Return result\"]\n    \n    Start --> Unlock\n    Unlock --> Params\n    Params --> CheckNeuron\n    CheckNeuron --> UpToDate\n    UpToDate -->|\"Yes\"| Return\n    UpToDate -->|\"No\"| DoServe\n    DoServe --> Success\n    Success --> Return\n    \n    Params -.-> VersionInt[\"__version_as_int__\"]\n```\n\nKey characteristics:\n- Uses `__version_as_int__` for subnet version identification\n- Includes placeholder parameters for future extensibility\n- Patches the original Bittensor serve extrinsic at module level\n- Handles certificate-based TLS configuration\n\n**Sources:** \u003CSourceLink text=\"compute/axon.py:63-150\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L63-L150\" />\n\n## ComputeSubnetSubtensor\n\nThe `ComputeSubnetSubtensor` class extends the base Subtensor with Prometheus metrics functionality, allowing miners and validators to register metrics endpoints on the blockchain.\n\n### Prometheus Integration\n\n```mermaid\ngraph LR\n    Client[\"Wallet/Client\"]\n    SubtensorClass[\"ComputeSubnetSubtensor\"]\n    PrometheusMethod[\"serve_prometheus()\"]\n    ExtrinsicMethod[\"do_serve_prometheus()\"]\n    Blockchain[\"Bittensor Blockchain\"]\n    \n    Client --> SubtensorClass\n    SubtensorClass --> PrometheusMethod\n    PrometheusMethod --> ExtrinsicMethod\n    ExtrinsicMethod --> Blockchain\n    \n    PrometheusMethod -.-> PrometheusExtrinsicFunc[\"prometheus_extrinsic()\"]\n```\n\n### Key Methods\n\n| Method | Purpose | Parameters |\n|--------|---------|------------|\n| `serve_prometheus` | Public interface for Prometheus registration | `wallet`, `port`, `netuid`, wait flags |\n| `do_serve_prometheus` | Internal extrinsic submission handler | `wallet`, `call_params`, wait flags |\n\nThe implementation includes:\n- Retry logic with exponential backoff\n- Exception handling for substrate requests\n- Support for inclusion and finalization waiting\n- Integration with the compute subnet's Prometheus extrinsic function\n\n**Sources:** \u003CSourceLink text=\"compute/axon.py:152-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L152-L283\" />\n\n## ComputeSubnetAxon\n\nThe `ComputeSubnetAxon` class extends the base Axon with compute subnet specific configuration and information handling.\n\n### Architecture\n\n```mermaid\ngraph TB\n    subgraph \"ComputeSubnetAxon Components\"\n        Config[\"Configuration Management\"]\n        Wallet[\"Wallet Integration\"]\n        Network[\"Network Configuration\"]\n        Middleware[\"ComputeSubnetAxonMiddleware\"]\n        FastAPI[\"FastAPI Application\"]\n    end\n    \n    subgraph \"Custom Overrides\"\n        InfoMethod[\"info() method\"]\n        LocalVersionFunc[\"get_local_version()\"]\n        ProtocolValues[\"Protocol Values\u003Cbr/>placeholder1:1\u003Cbr/>placeholder2:2\"]\n    end\n    \n    Config --> Network\n    Wallet --> InfoMethod\n    InfoMethod --> LocalVersionFunc\n    InfoMethod --> ProtocolValues\n    FastAPI --> Middleware\n```\n\n### Info Method Override\n\nThe `info()` method returns subnet-specific axon information:\n\n- Uses `get_local_version()` instead of standard versioning\n- Sets protocol version to 4\n- Includes custom placeholder values for future extensibility\n- Returns properly formatted `AxonInfo` object\n\n### Configuration Parameters\n\n| Parameter | Purpose | Default Handling |\n|-----------|---------|------------------|\n| `external_ip` | External IP for network communication | Auto-detected if not provided |\n| `external_port` | External port for network communication | Uses internal port if not provided |\n| `max_workers` | Thread pool size | From configuration |\n\n**Sources:** \u003CSourceLink text=\"compute/axon.py:285-388\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L285-L388\" />\n\n## ComputeSubnetAxonMiddleware\n\nThe `ComputeSubnetAxonMiddleware` extends the base middleware with compute subnet specific request preprocessing.\n\n### Request Processing Flow\n\n```mermaid\nsequenceDiagram\n    participant Request as \"Incoming Request\"\n    participant Middleware as \"ComputeSubnetAxonMiddleware\"\n    participant Synapse as \"Synapse Object\"\n    participant Wallet as \"Wallet (Signing)\"\n    \n    Request ->> Middleware: HTTP Request\n    Middleware ->> Middleware: Extract request_name from URL\n    Middleware ->> Middleware: Get synapse class type\n    Middleware ->> Synapse: Create from headers\n    Middleware ->> Synapse: Fill axon info (__version_as_int__)\n    Middleware ->> Synapse: Fill dendrite info\n    Middleware ->> Wallet: Sign message\n    Wallet -->> Middleware: Signature\n    Middleware ->> Synapse: Set signature\n    Middleware -->> Request: Return processed synapse\n```\n\n### Custom Preprocessing\n\nThe `preprocess` method implements compute subnet specific logic:\n\n1. **Request Name Extraction**: Parses request name from URL path\n2. **Synapse Creation**: Instantiates appropriate synapse type from headers\n3. **Version Handling**: Sets axon version to `__version_as_int__`\n4. **Signature Generation**: Signs with wallet hotkey using custom message format\n\n### Error Handling\n\nThe middleware handles three main error types:\n- `InvalidRequestNameError`: Malformed URL paths\n- `UnknownSynapseError`: Unknown synapse types\n- `SynapseParsingError`: Header parsing failures\n\n**Sources:** \u003CSourceLink text=\"compute/axon.py:391-487\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L391-L487\" />\n\n## Integration Workflow\n\nThe custom Axon and Subtensor components integrate with the broader compute subnet architecture through specific workflows:\n\n```mermaid\ngraph TD\n    subgraph \"Miner/Validator Startup\"\n        Start[\"Process Start\"]\n        CreateAxon[\"Create ComputeSubnetAxon\"]\n        CreateSubtensor[\"Create ComputeSubnetSubtensor\"]\n        ServeAxon[\"Serve Axon (custom_serve_extrinsic)\"]\n        ServePrometheus[\"Serve Prometheus metrics\"]\n    end\n    \n    subgraph \"Runtime Operations\"\n        RequestProcessing[\"Process Incoming Requests\"]\n        MiddlewareHandling[\"ComputeSubnetAxonMiddleware\"]\n        SynapseProcessing[\"Synapse Processing\"]\n        ResponseGeneration[\"Response Generation\"]\n    end\n    \n    subgraph \"Blockchain Integration\"\n        BlockchainReg[\"Blockchain Registration\"]\n        NetworkState[\"Network State Updates\"]\n        MetricsReporting[\"Metrics Reporting\"]\n    end\n    \n    Start --> CreateAxon\n    Start --> CreateSubtensor\n    CreateAxon --> ServeAxon\n    CreateSubtensor --> ServePrometheus\n    \n    ServeAxon --> BlockchainReg\n    ServePrometheus --> MetricsReporting\n    \n    RequestProcessing --> MiddlewareHandling\n    MiddlewareHandling --> SynapseProcessing\n    SynapseProcessing --> ResponseGeneration\n    \n    BlockchainReg --> NetworkState\n```\n\nThis integration ensures that:\n- All network communication uses compute subnet versioning\n- Prometheus metrics are properly registered and accessible\n- Request processing includes subnet-specific preprocessing\n- Blockchain registration includes all necessary subnet parameters\n\n**Sources:** \u003CSourceLink text=\"compute/axon.py:1-488\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L1-L488\" />","src/content/docs/communication-protocols/custom-axon-and-subtensor.mdx","d3add5d28dfa7e62","configuration/command-line-arguments",{"id":144,"data":146,"body":151,"filePath":152,"digest":153,"deferredRender":15},{"title":147,"editUrl":15,"head":148,"template":17,"sidebar":149,"pagefind":15,"draft":19},"Command-line Arguments",[],{"hidden":19,"attrs":150},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"README.md\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md\" />\n\n  \u003CSourceLink text=\"compute/__init__.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py\" />\n\n  \u003CSourceLink text=\"compute/utils/parser.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document provides a comprehensive reference for all command-line arguments available in the NI Compute Subnet. The arguments control the behavior of miners and validators, including network configuration, validation parameters, resource allocation settings, and security controls.\n\nFor information about GPU performance configuration parameters, see [GPU Performance Configuration](/configuration/gpu-performance-configuration#7.2). For details about using the registration CLI tools, see [Registration CLI](/cli-tools/registration-cli#8.1).\n\n## Argument Parser Architecture\n\nThe command-line argument system is built using the `ComputeArgPaser` class, which extends Python's `argparse.ArgumentParser` to provide both compute subnet-specific arguments and inherited Bittensor framework arguments.\n\n### Argument Parser Structure\n\n```mermaid\ngraph TD\n    subgraph \"ComputeArgPaser Class\"\n        PARSER[\"ComputeArgPaser\"]\n        INIT[\"__init__()\"]\n        VALIDATOR_ARGS[\"add_validator_argument()\"]\n        MINER_ARGS[\"add_miner_argument()\"]\n        PARSE_LIST[\"parse_list()\"]\n    end\n    \n    subgraph \"Bittensor Framework Args\"\n        BT_SUBTENSOR[\"bt.subtensor.add_args()\"]\n        BT_LOGGING[\"bt.logging.add_args()\"]\n        BT_WALLET[\"bt.wallet.add_args()\"]\n        BT_AXON[\"bt.axon.add_args()\"]\n    end\n    \n    subgraph \"Configuration Objects\"\n        CONFIG[\"bt.config\"]\n        NETUID[\"--netuid\"]\n        AUTO_UPDATE[\"--auto_update\"]\n        BLACKLIST[\"--blacklist.*\"]\n        WHITELIST[\"--whitelist.*\"]\n    end\n    \n    INIT --> VALIDATOR_ARGS\n    INIT --> MINER_ARGS\n    INIT --> BT_SUBTENSOR\n    INIT --> BT_LOGGING\n    INIT --> BT_WALLET\n    INIT --> BT_AXON\n    \n    PARSER --> CONFIG\n    \n    VALIDATOR_ARGS --> CONFIG\n    MINER_ARGS --> CONFIG\n    BT_SUBTENSOR --> CONFIG\n    BT_LOGGING --> CONFIG\n    BT_WALLET --> CONFIG\n    BT_AXON --> CONFIG\n```\n\nSources: \u003CSourceLink text=\"compute/utils/parser.py:8-71\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L8-L71\" />\n\n## Core Arguments\n\n### Network Configuration\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--netuid` | int | 27 | The chain subnet UID (27 for mainnet, 15 for testnet) |\n| `--auto_update` | flag | True | Automatically update the git repository |\n\nThe `--netuid` argument determines which Bittensor subnet the process connects to. The main production network uses netuid 27, while the test network uses netuid 15.\n\nSources: \u003CSourceLink text=\"compute/utils/parser.py:12-22\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L12-L22\" />\n\n### Security and Access Control\n\nThe subnet provides comprehensive blacklisting and whitelisting capabilities for both hotkeys and coldkeys:\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--blacklist.exploiters` | flag | True | Automatically blacklist known exploiter hotkeys |\n| `--blacklist.hotkeys` | list | [] | Comma-separated list of hotkeys to blacklist |\n| `--blacklist.coldkeys` | list | [] | Comma-separated list of coldkeys to blacklist |\n| `--whitelist.hotkeys` | list | [] | Comma-separated list of hotkeys to whitelist |\n| `--whitelist.coldkeys` | list | [] | Comma-separated list of coldkeys to whitelist |\n\nThe `--blacklist.exploiters` flag automatically applies the hardcoded list of suspected exploiter hotkeys defined in `SUSPECTED_EXPLOITERS_HOTKEYS`.\n\nSources: \u003CSourceLink text=\"compute/utils/parser.py:24-57\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L24-L57\" />, \u003CSourceLink text=\"compute/__init__.py:60-77\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L60-L77\" />\n\n## Validator Arguments\n\nValidators use specialized arguments to control their validation behavior, hardware querying, and performance parameters.\n\n### Validation Control Arguments\n\n```mermaid\ngraph LR\n    subgraph \"Validator Configuration\"\n        WHITELIST_UNREC[\"--validator.whitelist.unrecognized\"]\n        HARDWARE_QUERY[\"--validator.perform.hardware.query\"]\n        PROMETHEUS_UPDATE[\"--validator.force.update.prometheus\"]\n        WHITELIST_THRESHOLD[\"--validator.whitelist.updated.threshold\"]\n    end\n    \n    subgraph \"Batch Processing\"\n        CHALLENGE_BATCH[\"--validator.challenge.batch.size\"]\n        SPECS_BATCH[\"--validator.specs.batch.size\"]\n    end\n    \n    subgraph \"Validator Process\"\n        VALIDATION_LOGIC[\"Validation Logic\"]\n        HARDWARE_SPECS[\"Hardware Specs Collection\"]\n        CHALLENGE_SYSTEM[\"Challenge System\"]\n    end\n    \n    WHITELIST_UNREC --> VALIDATION_LOGIC\n    HARDWARE_QUERY --> HARDWARE_SPECS\n    CHALLENGE_BATCH --> CHALLENGE_SYSTEM\n    SPECS_BATCH --> HARDWARE_SPECS\n    PROMETHEUS_UPDATE --> VALIDATION_LOGIC\n    WHITELIST_THRESHOLD --> VALIDATION_LOGIC\n```\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--validator.whitelist.unrecognized` | flag | False | Whitelist unrecognized miners |\n| `--validator.perform.hardware.query` | bool | True | Enable hardware specs collection from miners |\n| `--validator.challenge.batch.size` | int | 256 | Batch size for challenge processing |\n| `--validator.specs.batch.size` | int | 64 | Batch size for hardware specs queries |\n| `--validator.force.update.prometheus` | flag | False | Force Prometheus version update |\n| `--validator.whitelist.updated.threshold` | int | 60 | Quorum threshold percentage for whitelisting |\n\nThe batch size arguments allow validators with lower hardware specifications to reduce memory and processing load by processing smaller batches of miners at a time.\n\nSources: \u003CSourceLink text=\"compute/utils/parser.py:72-114\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L72-L114\" />\n\n## Miner Arguments\n\nMiners have specialized arguments for hashcat configuration, resource allocation, and validator interaction policies.\n\n### Hashcat Configuration\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--miner.hashcat.path` | str | \"hashcat\" | Path to the hashcat binary executable |\n| `--miner.hashcat.workload.profile` | str | \"3\" | Performance profile (1=Low, 2=Economic, 3=High, 4=Insane) |\n| `--miner.hashcat.extended.options` | str | \"\" | Additional hashcat command-line options |\n\nThe hashcat configuration controls how miners respond to Proof-of-Work challenges. The workload profile directly impacts the computational intensity and power consumption.\n\nSources: \u003CSourceLink text=\"compute/utils/parser.py:117-137\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L117-L137\" />, \u003CSourceLink text=\"compute/__init__.py:55-57\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L55-L57\" />\n\n### Miner Security and Validation Policies\n\n```mermaid\ngraph TD\n    subgraph \"Miner Whitelist Policy\"\n        NOT_ENOUGH_STAKE[\"--miner.whitelist.not.enough.stake\"]\n        NOT_UPDATED[\"--miner.whitelist.not.updated\"]\n        UPDATED_THRESHOLD[\"--miner.whitelist.updated.threshold\"]\n    end\n    \n    subgraph \"Resource Allocation\"\n        SSH_PORT[\"--ssh.port\"]\n        ALLOCATION_SERVICE[\"Allocation Service\"]\n    end\n    \n    subgraph \"Validator Interaction\"\n        STAKE_CHECK[\"Validator Stake Verification\"]\n        VERSION_CHECK[\"Validator Version Check\"]\n        THRESHOLD_CHECK[\"Quorum Threshold Check\"]\n    end\n    \n    NOT_ENOUGH_STAKE --> STAKE_CHECK\n    NOT_UPDATED --> VERSION_CHECK\n    UPDATED_THRESHOLD --> THRESHOLD_CHECK\n    SSH_PORT --> ALLOCATION_SERVICE\n    \n    STAKE_CHECK --> ALLOCATION_SERVICE\n    VERSION_CHECK --> ALLOCATION_SERVICE\n    THRESHOLD_CHECK --> ALLOCATION_SERVICE\n```\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--miner.whitelist.not.enough.stake` | flag | False | Accept validators without sufficient stake |\n| `--miner.whitelist.not.updated` | flag | False | Accept validators not using latest code version |\n| `--miner.whitelist.updated.threshold` | int | 60 | Quorum threshold percentage before applying whitelist |\n| `--ssh.port` | int | 4444 | SSH port for resource allocation service |\n\nThese security policies allow miners to control which validators they accept requests from based on stake requirements and code version compliance.\n\nSources: \u003CSourceLink text=\"compute/utils/parser.py:138-165\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L138-L165\" />\n\n## Inherited Bittensor Arguments\n\nThe argument parser incorporates standard Bittensor framework arguments through method calls that extend the parser with additional argument groups.\n\n### Bittensor Framework Integration\n\n```mermaid\ngraph LR\n    subgraph \"Bittensor Components\"\n        SUBTENSOR[\"bt.subtensor\"]\n        LOGGING[\"bt.logging\"]\n        WALLET[\"bt.wallet\"]\n        AXON[\"bt.axon\"]\n    end\n    \n    subgraph \"Added Arguments\"\n        SUBTENSOR_ARGS[\"--subtensor.network\u003Cbr/>--subtensor.chain_endpoint\"]\n        LOGGING_ARGS[\"--logging.debug\u003Cbr/>--logging.trace\u003Cbr/>--logging.logging_dir\"]\n        WALLET_ARGS[\"--wallet.name\u003Cbr/>--wallet.hotkey\u003Cbr/>--wallet.path\"]\n        AXON_ARGS[\"--axon.port\"]\n    end\n    \n    subgraph \"Configuration Object\"\n        BT_CONFIG[\"bt.config\"]\n    end\n    \n    SUBTENSOR --> SUBTENSOR_ARGS\n    LOGGING --> LOGGING_ARGS\n    WALLET --> WALLET_ARGS\n    AXON --> AXON_ARGS\n    \n    SUBTENSOR_ARGS --> BT_CONFIG\n    LOGGING_ARGS --> BT_CONFIG\n    WALLET_ARGS --> BT_CONFIG\n    AXON_ARGS --> BT_CONFIG\n```\n\n### Subtensor Arguments\n- `--subtensor.network`: Network endpoint (finney, test, or custom)\n- `--subtensor.chain_endpoint`: Custom blockchain endpoint URL\n\n### Logging Arguments\n- `--logging.debug`: Enable debug-level logging\n- `--logging.trace`: Enable trace-level logging\n- `--logging.logging_dir`: Directory for log files\n\n### Wallet Arguments\n- `--wallet.name`: Coldkey wallet name\n- `--wallet.hotkey`: Hotkey name\n- `--wallet.path`: Custom wallet directory path\n\n### Axon Arguments\n- `--axon.port`: Port for serving the axon (default: 8091)\n\nSources: \u003CSourceLink text=\"compute/utils/parser.py:61-70\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/parser.py#L61-L70\" />\n\n## Usage Examples\n\n### Miner Command Line\n\n```bash\npm2 start ./neurons/miner.py --name MINER_NAME --interpreter python3 -- \\\n  --netuid 27 \\\n  --subtensor.network finney \\\n  --wallet.name COLDKEY_NAME \\\n  --wallet.hotkey HOTKEY_NAME \\\n  --axon.port 8091 \\\n  --ssh.port 4444 \\\n  --logging.debug \\\n  --auto_update\n```\n\n### Validator Command Line\n\n```bash\npm2 start ./neurons/validator.py --name VALIDATOR_NAME --interpreter python3 -- \\\n  --netuid 27 \\\n  --subtensor.network finney \\\n  --wallet.name COLDKEY_NAME \\\n  --wallet.hotkey HOTKEY_NAME \\\n  --validator.specs.batch.size 64 \\\n  --validator.challenge.batch.size 256 \\\n  --logging.debug \\\n  --auto_update\n```\n\n### Advanced Configuration Examples\n\nFor miners with custom hashcat optimization:\n```bash\n--miner.hashcat.workload.profile 4 \\\n--miner.hashcat.extended.options \"-O --force\"\n```\n\nFor validators with restrictive whitelisting:\n```bash\n--validator.whitelist.updated.threshold 80 \\\n--blacklist.exploiters \\\n--blacklist.hotkeys \"5HZ1ATsziEMDm1iUqNWQatfEDb1JSNf37AiG8s3X4pZzoP3A\"\n```\n\nSources: \u003CSourceLink text=\"README.md:362-425\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/README.md#L362-L425\" />","src/content/docs/configuration/command-line-arguments.mdx","440aac399aac736c","configuration/gpu-performance-configuration",{"id":154,"data":156,"body":161,"filePath":162,"digest":163,"deferredRender":15},{"title":157,"editUrl":15,"head":158,"template":17,"sidebar":159,"pagefind":15,"draft":19},"GPU Performance Configuration",[],{"hidden":19,"attrs":160},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/__init__.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py\" />\n\n  \u003CSourceLink text=\"compute/utils/math.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/math.py\" />\n\n  \u003CSourceLink text=\"config.yaml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml\" />\n\n  \u003CSourceLink text=\"neurons/Validator/database/miner.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/miner.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/database/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/miner_script_m_merkletree.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/miner_script_m_merkletree.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\n## Overview\n\nThis document details the GPU performance configuration system used for Proof-of-GPU validation in the NI Compute subnet. It covers the performance benchmarks, tolerance settings, identification logic, and Merkle proof parameters that enable validators to verify miner GPU capabilities. For overall system configuration options, see [Command-line Arguments](/configuration/command-line-arguments#7.1).\n\nThe GPU performance configuration consists of several key components:\n- Performance benchmark data (TFLOPS, AVRAM) for GPU identification\n- Tolerance pairs for handling equivalent GPU models  \n- Merkle proof parameters for cryptographic verification\n- Benchmarking timeouts and retry limits\n\nSources: \u003CSourceLink text=\"config.yaml:1-104\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml#L1-L104\" />, \u003CSourceLink text=\"compute/__init__.py:37-48\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L37-L48\" />\n\n## GPU Performance Benchmarks\n\nThe system maintains comprehensive performance data for GPU models in `config.yaml` under the `gpu_performance` section. This data enables accurate GPU identification and performance verification through three key metrics:\n\n### Performance Data Structure\n\n**FP16 TFLOPS Configuration**\n```yaml\nGPU_TFLOPS_FP16:\n  NVIDIA B200: 1205\n  NVIDIA H200: 610\n  NVIDIA H100 80GB HBM3: 570\n  NVIDIA A100-SXM4-80GB: 238.8\n```\n\n**FP32 TFLOPS Configuration**  \n```yaml\nGPU_TFLOPS_FP32:\n  NVIDIA B200: 67.2\n  NVIDIA H200: 49.6\n  NVIDIA H100 80GB HBM3: 49.0\n  NVIDIA A100-SXM4-80GB: 18.2\n```\n\n**VRAM Configuration**\n```yaml\nGPU_AVRAM:\n  NVIDIA B200: 68.72\n  NVIDIA H200: 68.72\n  NVIDIA H100 80GB HBM3: 34.36\n  NVIDIA A100-SXM4-80GB: 34.36\n```\n\n### GPU Performance Ranking\n\nThe `gpu_scores` section assigns relative performance values used by the scoring system:\n\n| GPU Model | Performance Score |\n|-----------|------------------|\n| NVIDIA B200 | 5.00 |\n| NVIDIA H200 | 4.0 |\n| NVIDIA H100 80GB HBM3 | 3.30 |\n| NVIDIA H100 | 2.80 |\n| NVIDIA A100-SXM4-80GB | 1.90 |\n| NVIDIA L40s | 0.90 |\n| NVIDIA RTX 6000 Ada Generation | 0.83 |\n| NVIDIA RTX 4090 | 0.68 |\n\nSources: \u003CSourceLink text=\"config.yaml:1-94\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml#L1-L94\" />\n\n## GPU Tolerance Configuration\n\nThe system handles functionally equivalent GPU models through tolerance pairs that prevent false negatives during GPU identification. This mechanism accounts for naming variations and similar performance characteristics.\n\n### Tolerance Pairs Configuration\n\n```mermaid\ngraph LR\n    subgraph \"gpu_tolerance_pairs Configuration\"\n        L40[\"NVIDIA L40\"] \u003C--> RTX6000[\"NVIDIA RTX 6000 Ada Generation\"]\n        A100PCIe[\"NVIDIA A100 80GB PCIe\"] \u003C--> A100SXM[\"NVIDIA A100-SXM4-80GB\"] \n        H100_80GB[\"NVIDIA H100 80GB HBM3\"] \u003C--> H100[\"NVIDIA H100\"]\n        A40[\"NVIDIA A40\"] \u003C--> RTXA6000[\"NVIDIA RTX A6000\"]\n        RTXA5000[\"NVIDIA RTX A5000\"] \u003C--> RTX4000[\"NVIDIA RTX 4000 Ada Generation\"]\n    end\n```\n\n### Tolerance Implementation\n\nThe `identify_gpu` function in `neurons/Validator/pog.py` applies tolerance logic during GPU identification:\n\n```python\n# Check if identified GPU matches the tolerance pair\nif identified_gpu in tolerance_pairs and reported_name == tolerance_pairs.get(identified_gpu):\n    identified_gpu = reported_name\n# Check reverse mapping\nelif reported_name in tolerance_pairs and identified_gpu == tolerance_pairs.get(reported_name):\n    identified_gpu = reported_name\n```\n\nThis allows miners with equivalent hardware to receive consistent identification regardless of minor naming differences.\n\nSources: \u003CSourceLink text=\"config.yaml:63-73\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml#L63-L73\" />, \u003CSourceLink text=\"neurons/Validator/pog.py:27-73\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py#L27-L73\" />\n\n## Merkle Proof Configuration\n\nThe Proof-of-GPU system uses Merkle tree verification to cryptographically validate GPU computations. The merkle proof configuration parameters control this verification process.\n\n### Merkle Proof Parameters\n\n```yaml\nmerkle_proof:\n  miner_script_path: \"neurons/Validator/miner_script_m_merkletree.py\"\n  time_tolerance: 5\n  submatrix_size: 512\n  hash_algorithm: 'sha256'\n  pog_retry_limit: 22\n  pog_retry_interval: 60  # seconds\n  max_workers: 64\n  max_random_delay: 900  # 900 seconds\n```\n\n### Merkle Tree Process Flow\n\n```mermaid\nflowchart TD\n    subgraph \"Merkle Proof Verification Process\"\n        validator[\"Validator\"] -->|\"send_script_and_request_hash\"| ssh[\"SSH Connection\"]\n        ssh -->|\"execute_script_on_miner\"| script[\"miner_script_m_merkletree.py\"]\n        script -->|\"generate_matrix_torch\"| matrices[\"Matrix Generation\"]\n        matrices -->|\"build_merkle_tree_rows\"| tree[\"Merkle Tree\"]\n        tree -->|\"get_merkle_proof_row\"| proof[\"Merkle Proofs\"]\n        proof -->|\"verify_merkle_proof_row\"| validator\n        validator -->|\"verify_responses\"| result[\"Verification Result\"]\n    end\n```\n\n### Implementation Components\n\nThe Merkle proof system involves several key functions:\n- `send_script_and_request_hash()`: Transfers and verifies the benchmark script\n- `execute_script_on_miner()`: Runs computation modes (benchmark/compute/proof)\n- `build_merkle_tree_rows()`: Constructs Merkle trees from computation results\n- `verify_merkle_proof_row()`: Validates individual proof elements\n- `verify_responses()`: Performs overall verification with failure tolerance\n\nSources: \u003CSourceLink text=\"config.yaml:95-104\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml#L95-L104\" />, \u003CSourceLink text=\"neurons/Validator/pog.py:75-340\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py#L75-L340\" />\n\n## Benchmarking Parameters\n\nThe system uses several timeout and retry parameters to ensure reliable GPU performance validation while handling network and hardware variations.\n\n### Core PoG Parameters\n\nFrom `compute/__init__.py`:\n```python\n# Proof of GPU settings\npog_retry_limit = 30\npog_retry_interval = 80  # seconds\nspecs_timeout = 60  # Time before specs requests timeout\n```\n\n### Benchmark Execution Flow\n\n```mermaid\nflowchart TD\n    subgraph \"GPU Benchmarking Process\"\n        start[\"Validator initiates PoG\"] --> send[\"send_script_and_request_hash()\"]\n        send --> verify[\"Verify script hash\"]\n        verify --> benchmark[\"execute_script_on_miner(mode:'benchmark')\"]\n        benchmark --> parse[\"parse_benchmark_output()\"]\n        parse --> compute[\"execute_script_on_miner(mode:'compute')\"]\n        compute --> merkle[\"parse_merkle_output()\"]\n        merkle --> proof[\"execute_script_on_miner(mode:'proof')\"]\n        proof --> validate[\"verify_responses()\"]\n        validate --> result[\"GPU identification & scoring\"]\n    end\n```\n\n### Benchmark Output Parsing\n\nThe `parse_benchmark_output` function processes miner responses:\n```python\nnum_gpus, vram, size_fp16, time_fp16, size_fp32, time_fp32 = parse_benchmark_output(output)\n```\n\nThis extracts:\n- GPU count\n- Available VRAM \n- FP16 matrix size and execution time\n- FP32 matrix size and execution time\n\nThese values are then used by `identify_gpu()` to match against the performance database.\n\nSources: \u003CSourceLink text=\"compute/__init__.py:37-48\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/__init__.py#L37-L48\" />, \u003CSourceLink text=\"neurons/Validator/pog.py:101-146\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py#L101-L146\" />\n\n## GPU Identification Logic\n\nThe core GPU identification process combines performance benchmarking with tolerance-aware matching to accurately identify miner hardware capabilities.\n\n### Identification Algorithm\n\n```mermaid\nflowchart TD\n    subgraph \"identify_gpu Function Flow\"\n        input[\"Input: fp16_tflops, fp32_tflops, estimated_avram, reported_name\"] \n        input --> calculate[\"Calculate combined_scores for all GPU models\"]\n        calculate --> deviation[\"fp16_deviation + fp32_deviation + avram_deviation / 3\"]\n        deviation --> sort[\"Sort by lowest deviation score\"]\n        sort --> identify[\"identified_gpu : best_match\"]\n        identify --> tolerance{\"Check tolerance_pairs\"}\n        tolerance -->|\"Match found\"| adjust[\"Apply tolerance adjustment\"]\n        tolerance -->|\"No match\"| return[\"Return identified_gpu\"]\n        adjust --> return\n    end\n```\n\n### Performance Deviation Calculation\n\nThe `identify_gpu` function calculates deviation scores for each GPU model:\n\n```python\nfp16_deviation = abs(fp16_tflops - fp16_theoretical) / fp16_theoretical\nfp32_deviation = abs(fp32_tflops - fp32_theoretical) / fp32_theoretical  \navram_deviation = abs(estimated_avram - avram_theoretical) / avram_theoretical\ncombined_score = (fp16_deviation + fp32_deviation + avram_deviation) / 3\n```\n\nThe GPU with the lowest combined deviation score is selected as the identified model.\n\n### Benchmark Script Integration\n\nThe `miner_script_m_merkletree.py` script provides multiple execution modes:\n- `benchmark`: Matrix multiplication performance testing\n- `compute`: Merkle tree computation with PRNG matrices\n- `proof`: Generate cryptographic proofs for verification\n- `gpu_info`: Basic GPU detection and enumeration\n\nSources: \u003CSourceLink text=\"neurons/Validator/pog.py:27-73\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py#L27-L73\" />, \u003CSourceLink text=\"neurons/Validator/miner_script_m_merkletree.py:21-388\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/miner_script_m_merkletree.py#L21-L388\" />\n\n## Configuration Loading and Validation\n\nThe GPU performance configuration is loaded and validated through the YAML configuration system with error handling for missing or malformed data.\n\n### Configuration Loading Process\n\n```mermaid\nflowchart TD\n    subgraph \"load_yaml_config Function\"\n        start[\"load_yaml_config(file_path)\"] --> open[\"Open config.yaml\"]\n        open --> parse[\"yaml.safe_load(data)\"]\n        parse --> validate[\"Validate gpu_performance section\"]\n        validate --> return[\"Return configuration dict\"]\n        \n        parse -->|\"FileNotFoundError\"| error1[\"Raise FileNotFoundError\"]\n        parse -->|\"YAMLError\"| error2[\"Raise ValueError\"]\n    end\n```\n\n### Configuration Structure Access\n\nThe loaded configuration provides access to all GPU performance data:\n\n```python\ngpu_data = load_yaml_config(\"config.yaml\")\nGPU_TFLOPS_FP16 = gpu_data[\"gpu_performance\"][\"GPU_TFLOPS_FP16\"]\nGPU_TFLOPS_FP32 = gpu_data[\"gpu_performance\"][\"GPU_TFLOPS_FP32\"] \nGPU_AVRAM = gpu_data[\"gpu_performance\"][\"GPU_AVRAM\"]\ntolerance_pairs = gpu_data[\"gpu_performance\"][\"gpu_tolerance_pairs\"]\n```\n\n### Database Integration\n\nGPU configuration data is persisted using database functions:\n- `update_pog_stats()`: Stores GPU name and count for miners\n- `get_pog_specs()`: Retrieves most recent GPU specifications\n- `write_stats()`: Stores comprehensive performance data with JSON serialization\n\nSources: \u003CSourceLink text=\"neurons/Validator/pog.py:14-26\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py#L14-L26\" />, \u003CSourceLink text=\"neurons/Validator/database/pog.py:24-98\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py#L24-L98\" />\n\n## Implementation Details\n\n### Key Components\n\n1. **GPU Performance Configuration**: Defined in `config.yaml`\n2. **Score Calculation Logic**: Implemented in `neurons/Validator/calculate_pow_score.py`\n3. **GPU Data Storage**: Managed by functions in `neurons/Validator/database/pog.py`\n4. **Mathematical Utilities**: Provided in `compute/utils/math.py`\n\n### Database Interaction\n\nGPU specifications are stored in the database using JSON serialization:\n\n```python\n# Convert dict to JSON string for storage\nif isinstance(raw_specs, dict):\n    gpu_specs = json.dumps(raw_specs)\nelse:\n    gpu_specs = raw_specs\n\n# When retrieving\nraw_gpu_specs = row[2]\nif raw_gpu_specs:\n    try:\n        gpu_specs = json.loads(raw_gpu_specs)  # Convert from JSON -> dict\n    except Exception as e:\n        gpu_specs = None\n```\n\nThis allows flexible storage of different GPU configurations while maintaining a structured database schema.\n\nSources: \u003CSourceLink text=\"neurons/Validator/database/pog.py:100-186\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py#L100-L186\" />","src/content/docs/configuration/gpu-performance-configuration.mdx","e0fa8b4e624eecdf","development/development-workflow",{"id":164,"data":166,"body":171,"filePath":172,"digest":173,"deferredRender":15},{"title":167,"editUrl":15,"head":168,"template":17,"sidebar":169,"pagefind":15,"draft":19},"Development Workflow",[],{"hidden":19,"attrs":170},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\".github/workflows/pre-commit.yml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml\" />\n\n  \u003CSourceLink text=\".pre-commit-config.yaml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml\" />\n\n  \u003CSourceLink text=\"scripts/check-branch-name.sh\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh\" />\n\n  \u003CSourceLink text=\"scripts/check-current-branch.sh\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-current-branch.sh\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document covers the development workflow, code quality enforcement, and automated testing infrastructure for the NI Compute Subnet project. It details the pre-commit hooks system, branch naming conventions, continuous integration pipeline, and dependency management processes that ensure code quality and consistency across the project.\n\nFor information about project structure and organization, see [Project Structure](/development/project-structure#9.2). For details about CLI tools used in development, see [CLI Tools](/cli-tools#8).\n\n## Pre-commit Hooks System\n\nThe project uses a comprehensive pre-commit hooks system to enforce code quality standards before commits are made. The configuration is defined in \u003CSourceLink text=\".pre-commit-config.yaml:1-64\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L1-L64\" /> and includes multiple stages of validation.\n\n### Hook Configuration\n\nThe pre-commit system includes the following hook categories:\n\n| Hook Category | Purpose | Stage |\n|---------------|---------|-------|\n| Code Formatting | Trailing whitespace and end-of-file fixes | `pre-commit`, `manual` |\n| Commit Validation | Conventional commit message format | `commit-msg` |\n| Branch Validation | Branch naming convention enforcement | `pre-commit`, `pre-push`, `manual` |\n| Dependency Management | Requirements file synchronization | `pre-commit`, `manual` |\n| Testing | Automated test execution | `pre-commit`, `manual` |\n\n### Pre-commit Hook Flow\n\n```mermaid\nflowchart TD\n    DEV[\"Developer makes commit\"] --> PRECOMMIT[\"pre-commit hooks triggered\"]\n    \n    PRECOMMIT --> WHITESPACE[\"trailing-whitespace hook\"]\n    PRECOMMIT --> ENDFILE[\"end-of-file-fixer hook\"]\n    PRECOMMIT --> BRANCH[\"check-branch-name hook\"]\n    PRECOMMIT --> PIPCOMPILE[\"pip-compile hooks\"]\n    PRECOMMIT --> PYTEST[\"run-pytest hook\"]\n    \n    WHITESPACE --> PASS1{Passes?}\n    ENDFILE --> PASS2{Passes?}\n    BRANCH --> BRANCHSCRIPT[\"./scripts/check-current-branch.sh\"]\n    BRANCHSCRIPT --> PASS3{Passes?}\n    PIPCOMPILE --> REQSYNC[\"Requirements sync\"]\n    REQSYNC --> PASS4{Passes?}\n    PYTEST --> TESTRUN[\"pytest --alluredir allure-results\"]\n    TESTRUN --> PASS5{Passes?}\n    \n    PASS1 -->|No| FAIL[\"Commit blocked\"]\n    PASS2 -->|No| FAIL\n    PASS3 -->|No| FAIL\n    PASS4 -->|No| FAIL\n    PASS5 -->|No| FAIL\n    \n    PASS1 -->|Yes| SUCCESS[\"All hooks pass\"]\n    PASS2 -->|Yes| SUCCESS\n    PASS3 -->|Yes| SUCCESS\n    PASS4 -->|Yes| SUCCESS\n    PASS5 -->|Yes| SUCCESS\n    \n    SUCCESS --> COMMIT[\"Commit allowed\"]\n    FAIL --> ABORT[\"Fix issues and retry\"]\n```\n\n**Sources:** \u003CSourceLink text=\".pre-commit-config.yaml:1-64\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L1-L64\" />, \u003CSourceLink text=\"scripts/check-current-branch.sh:1-7\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-current-branch.sh#L1-L7\" />\n\n## Branch Naming Conventions\n\nThe project enforces strict branch naming conventions through the `check-branch-name.sh` script, which validates branch names against predefined patterns.\n\n### Allowed Branch Patterns\n\nThe branch validation system supports the following patterns:\n\n| Branch Type | Pattern | Example |\n|-------------|---------|---------|\n| Feature | `feat/CSN-XXXX-description` | `feat/CSN-1234-add-gpu-validation` |  \n| Bug Fix | `fix/CSN-XXXX-description` | `fix/CSN-5678-memory-leak-issue` |\n| Hotfix | `hotfix/vX.Y.Z-CSN-XXXX-description` | `hotfix/v1.2.3-CSN-9999-critical-fix` |\n| Release | `release/vX.Y.Z` | `release/v2.1.0` |\n| Chore | `chore/CSN-XXXX-description` | `chore/CSN-1111-update-dependencies` |\n\n### Branch Validation Logic\n\n```mermaid\nflowchart TD\n    BRANCH_INPUT[\"Branch name input\"] --> EMPTY_CHECK{Empty or HEAD?}\n    EMPTY_CHECK -->|Yes| ERROR1[\"❌ Unable to determine branch name\"]\n    \n    EMPTY_CHECK -->|No| SPECIAL_CHECK{dev or main branch?}\n    SPECIAL_CHECK -->|Yes| ALLOW1[\"✅ Branch allowed without validation\"]\n    \n    SPECIAL_CHECK -->|No| RELEASE_CHECK{Starts with release/?}\n    RELEASE_CHECK -->|Yes| RELEASE_PATTERN[\"Check release/vX.Y.Z pattern\"]\n    RELEASE_PATTERN --> RELEASE_VALID{Valid?}\n    RELEASE_VALID -->|Yes| ALLOW2[\"✅ Valid release branch\"]\n    RELEASE_VALID -->|No| ERROR2[\"❌ Invalid release format\"]\n    \n    RELEASE_CHECK -->|No| HOTFIX_CHECK{Starts with hotfix/?}\n    HOTFIX_CHECK -->|Yes| HOTFIX_PATTERN[\"Check hotfix/vX.Y.Z-CSN-XXXX-desc\"]\n    HOTFIX_PATTERN --> HOTFIX_VALID{Valid?}\n    HOTFIX_VALID -->|Yes| ALLOW3[\"✅ Valid hotfix branch\"]\n    HOTFIX_VALID -->|No| ERROR3[\"❌ Invalid hotfix format\"]\n    \n    HOTFIX_CHECK -->|No| PREFIX_CHECK[\"Check allowed prefixes\"]\n    PREFIX_CHECK --> PREFIX_VALID{Valid prefix?}\n    PREFIX_VALID -->|No| ERROR4[\"❌ Invalid prefix\"]\n    \n    PREFIX_VALID -->|Yes| JIRA_CHECK[\"Extract JIRA ticket CSN-XXXX\"]\n    JIRA_CHECK --> JIRA_VALID{JIRA found?}\n    JIRA_VALID -->|No| ERROR5[\"❌ Missing JIRA ticket\"]\n    \n    JIRA_VALID -->|Yes| DESC_CHECK[\"Check description format\"]\n    DESC_CHECK --> KEBAB_CASE{Lowercase kebab-case?}\n    KEBAB_CASE -->|No| ERROR6[\"❌ Invalid description format\"]\n    KEBAB_CASE -->|Yes| ALLOW4[\"✅ Valid branch name\"]\n    \n    ERROR1 --> EXIT1[\"exit 1\"]\n    ERROR2 --> EXIT1\n    ERROR3 --> EXIT1\n    ERROR4 --> EXIT1\n    ERROR5 --> EXIT1\n    ERROR6 --> EXIT1\n    \n    ALLOW1 --> EXIT0[\"exit 0\"]\n    ALLOW2 --> EXIT0\n    ALLOW3 --> EXIT0\n    ALLOW4 --> EXIT0\n```\n\n**Sources:** \u003CSourceLink text=\"scripts/check-branch-name.sh:1-114\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh#L1-L114\" />, \u003CSourceLink text=\".pre-commit-config.yaml:19-26\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L19-L26\" />\n\n### Branch Name Validation Implementation\n\nThe validation logic is implemented in \u003CSourceLink text=\"scripts/check-branch-name.sh:1-114\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh#L1-L114\" /> with the following key components:\n\n- **Allowed Prefixes**: \u003CSourceLink text=\"scripts/check-branch-name.sh:4\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh#L4\" /> defines `feat|fix|hotfix|chore|refactor|test|spike|prototype|release|docs`\n- **JIRA Pattern**: \u003CSourceLink text=\"scripts/check-branch-name.sh:7\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh#L7\" /> requires `CSN-[0-9]+` format\n- **Release Pattern**: \u003CSourceLink text=\"scripts/check-branch-name.sh:10\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh#L10\" /> validates `release/v[0-9]+\\.[0-9]+\\.[0-9]+(-[a-z0-9]+)?`\n- **Hotfix Pattern**: \u003CSourceLink text=\"scripts/check-branch-name.sh:16\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh#L16\" /> validates `hotfix/v[0-9]+\\.[0-9]+\\.[0-9]+`\n\n**Sources:** \u003CSourceLink text=\"scripts/check-branch-name.sh:4-16\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/scripts/check-branch-name.sh#L4-L16\" />\n\n## Code Quality Enforcement\n\nThe development workflow enforces code quality through multiple automated checks that run at different stages of the development process.\n\n### Quality Check Pipeline\n\n```mermaid\ngraph LR\n    subgraph \"Pre-commit Stage\"\n        PC_WHITESPACE[\"trailing-whitespace\"]\n        PC_EOF[\"end-of-file-fixer\"]\n        PC_BRANCH[\"check-branch-name\"]\n        PC_DEPS[\"pip-compile\"]\n        PC_TEST[\"run-pytest\"]\n    end\n    \n    subgraph \"Commit Message Stage\"\n        CM_LINT[\"commitlint\"]\n        CM_CONV[\"@commitlint/config-conventional\"]\n    end\n    \n    subgraph \"Post-checkout Stage\"\n        PCO_BRANCH[\"post-checkout-check\"]\n    end\n    \n    subgraph \"CI/CD Stage\"\n        CI_PRECOMMIT[\"pre-commit/action@v3.0.1\"]\n        CI_PYTHON[\"Python 3.12 setup\"]\n        CI_DEPS[\"Install dependencies\"]\n    end\n    \n    PC_WHITESPACE --> COMMIT_READY\n    PC_EOF --> COMMIT_READY\n    PC_BRANCH --> COMMIT_READY\n    PC_DEPS --> COMMIT_READY\n    PC_TEST --> COMMIT_READY\n    \n    COMMIT_READY --> CM_LINT\n    CM_LINT --> CM_CONV\n    CM_CONV --> COMMITTED\n    \n    COMMITTED --> PCO_BRANCH\n    PCO_BRANCH --> CHECKOUT_COMPLETE\n    \n    CHECKOUT_COMPLETE --> CI_PYTHON\n    CI_PYTHON --> CI_DEPS\n    CI_DEPS --> CI_PRECOMMIT\n    CI_PRECOMMIT --> PIPELINE_SUCCESS\n```\n\n**Sources:** \u003CSourceLink text=\".pre-commit-config.yaml:1-64\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L1-L64\" />, \u003CSourceLink text=\".github/workflows/pre-commit.yml:1-22\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml#L1-L22\" />\n\n### Testing Integration\n\nThe pre-commit system includes automated test execution using `pytest` with Allure reporting:\n\n- **Test Command**: \u003CSourceLink text=\".pre-commit-config.yaml:58\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L58\" /> executes `python -m pytest --alluredir allure-results`\n- **Test Stage**: \u003CSourceLink text=\".pre-commit-config.yaml:60\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L60\" /> runs on `pre-commit` and `manual` stages\n- **Always Run**: \u003CSourceLink text=\".pre-commit-config.yaml:61\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L61\" /> ensures tests run regardless of file changes\n- **No Filenames**: \u003CSourceLink text=\".pre-commit-config.yaml:63\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L63\" /> prevents passing filenames to pytest\n\n**Sources:** \u003CSourceLink text=\".pre-commit-config.yaml:56-63\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L56-L63\" />\n\n## Continuous Integration Pipeline\n\nThe project uses GitHub Actions for continuous integration, with the workflow defined in \u003CSourceLink text=\".github/workflows/pre-commit.yml:1-22\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml#L1-L22\" />.\n\n### CI/CD Workflow Configuration\n\n| Configuration | Value | Purpose |\n|---------------|-------|---------|\n| **Trigger Events** | `pull_request`, `push` to `dev`/`main` | Automated validation on key events |\n| **Runner** | `ubuntu-latest` | Consistent Linux environment |\n| **Python Version** | `3.12` | Latest stable Python version |\n| **Cache Strategy** | `pip` with `pyproject.toml`, `requirements*.txt` | Dependency caching for performance |\n\n### CI Pipeline Steps\n\n```mermaid\nsequenceDiagram\n    participant TRIGGER as \"PR/Push Event\"\n    participant RUNNER as \"ubuntu-latest Runner\"\n    participant PYTHON as \"Python 3.12 Setup\"\n    participant CACHE as \"Pip Cache\"\n    participant DEPS as \"Dependencies\"\n    participant PRECOMMIT as \"pre-commit/action\"\n    \n    TRIGGER->>RUNNER: \"Start CI job\"\n    RUNNER->>PYTHON: \"Setup Python 3.12\"\n    PYTHON->>CACHE: \"Check pip cache\"\n    CACHE->>DEPS: \"Install -e .[dev] -r requirements*.txt\"\n    DEPS->>PRECOMMIT: \"Run pre-commit hooks\"\n    PRECOMMIT->>RUNNER: \"Report results\"\n    RUNNER->>TRIGGER: \"Complete CI job\"\n```\n\n**Sources:** \u003CSourceLink text=\".github/workflows/pre-commit.yml:8-22\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml#L8-L22\" />\n\n## Dependency Management\n\nThe project uses `pip-tools` for dependency management with automated synchronization through pre-commit hooks.\n\n### Requirements Compilation\n\nThe dependency management system includes two compilation targets:\n\n- **Production Requirements**: \u003CSourceLink text=\".pre-commit-config.yaml:40\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L40\" /> compiles `pyproject.toml` to `requirements.txt`\n- **Development Requirements**: \u003CSourceLink text=\".pre-commit-config.yaml:49\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L49\" /> compiles with `--extra dev` to `requirements-dev.txt`\n\n### Dependency Compilation Flow\n\n```mermaid\nflowchart TD\n    PYPROJECT[\"pyproject.toml changes\"] --> PRECOMMIT_TRIGGER[\"pre-commit triggered\"]\n    \n    PRECOMMIT_TRIGGER --> PROD_COMPILE[\"pip-compile hook\"]\n    PRECOMMIT_TRIGGER --> DEV_COMPILE[\"pip-compile-dev hook\"]\n    \n    PROD_COMPILE --> PROD_CMD[\"python -m piptools compile --quiet -o requirements.txt\"]\n    DEV_COMPILE --> DEV_CMD[\"python -m piptools compile --quiet --extra dev -o requirements-dev.txt\"]\n    \n    PROD_CMD --> REQ_FILE[\"requirements.txt updated\"]\n    DEV_CMD --> REQ_DEV_FILE[\"requirements-dev.txt updated\"]\n    \n    REQ_FILE --> COMMIT_READY[\"Dependencies synchronized\"]\n    REQ_DEV_FILE --> COMMIT_READY\n    \n    COMMIT_READY --> COMMIT_SUCCESS[\"Commit proceeds\"]\n```\n\n**Sources:** \u003CSourceLink text=\".pre-commit-config.yaml:38-53\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.pre-commit-config.yaml#L38-L53\" />\n\n### Installation Command Sequence\n\nThe CI/CD pipeline installs dependencies in the following order as defined in \u003CSourceLink text=\".github/workflows/pre-commit.yml:20\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml#L20\" />:\n\n1. `pip install -e .[dev]` - Install package in development mode with dev extras\n2. `-r requirements.txt` - Install production dependencies\n3. `-r requirements-dev.txt` - Install development dependencies\n\n**Sources:** \u003CSourceLink text=\".github/workflows/pre-commit.yml:20\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.github/workflows/pre-commit.yml#L20\" />","src/content/docs/development/development-workflow.mdx","330e085cf0fc6d0f","development/project-structure",{"id":174,"data":176,"body":181,"filePath":182,"digest":183,"deferredRender":15},{"title":177,"editUrl":15,"head":178,"template":17,"sidebar":179,"pagefind":15,"draft":19},"Project Structure",[],{"hidden":19,"attrs":180},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\".gitignore\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.gitignore\" />\n\n  \u003CSourceLink text=\"pyproject.toml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml\" />\n\n  \u003CSourceLink text=\"requirements-dev.txt\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements-dev.txt\" />\n\n  \u003CSourceLink text=\"requirements.txt\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements.txt\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document describes the organizational structure, dependency management, and build configuration of the NI Compute Subnet codebase. It covers the package layout, key dependencies, build tooling, and development infrastructure that supports the distributed GPU compute marketplace.\n\nFor information about installation and setup procedures, see [Installation and Setup](/installation-and-setup#1.2). For details about development workflows and code quality tools, see [Development Workflow](/development/development-workflow#9.1).\n\n## Package Organization\n\nThe codebase is organized into two main Python packages with a clear separation between core compute logic and network protocol implementations.\n\n```mermaid\ngraph TD\n    subgraph \"Root Directory\"\n        ROOT[\"ni-compute/\"]\n    end\n    \n    subgraph \"Core Packages\"\n        COMPUTE[\"compute/\"]\n        NEURONS[\"neurons/\"]\n    end\n    \n    subgraph \"Configuration Files\"\n        PYPROJECT[\"pyproject.toml\"]\n        REQS[\"requirements.txt\"]\n        DEVREQS[\"requirements-dev.txt\"]\n        GITIGNORE[\".gitignore\"]\n    end\n    \n    subgraph \"compute/ Package\"\n        COMPINIT[\"compute/__init__.py\"]\n        COMPMODULES[\"Core compute logic\u003Cbr/>Database, Protocols, Utils\"]\n    end\n    \n    subgraph \"neurons/ Package\"\n        VALIDATOR[\"neurons/validator.py\"]\n        MINER[\"neurons/miner.py\"]\n        REGISTER[\"neurons/register.py\"]\n        MINERDIR[\"neurons/Miner/\"]\n        REGISTERAPI[\"neurons/register-api/\"]\n    end\n    \n    ROOT --> COMPUTE\n    ROOT --> NEURONS\n    ROOT --> PYPROJECT\n    ROOT --> REQS\n    ROOT --> DEVREQS\n    ROOT --> GITIGNORE\n    \n    COMPUTE --> COMPINIT\n    COMPUTE --> COMPMODULES\n    \n    NEURONS --> VALIDATOR\n    NEURONS --> MINER\n    NEURONS --> REGISTER\n    NEURONS --> MINERDIR\n    NEURONS --> REGISTERAPI\n    \n    style COMPUTE fill:#f9f9f9\n    style NEURONS fill:#f9f9f9\n    style PYPROJECT fill:#e6f3ff\n```\n\n**Sources:** \u003CSourceLink text=\"pyproject.toml:87-91\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L87-L91\" />, \u003CSourceLink text=\".gitignore:248-262\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.gitignore#L248-L262\" />\n\n## Core Dependencies Architecture\n\nThe project relies on a carefully curated set of dependencies that enable blockchain integration, GPU computation, containerization, and distributed system monitoring.\n\n```mermaid\ngraph TB\n    subgraph \"Blockchain Layer\"\n        BITTENSOR[\"bittensor::9.0.0\"]\n        BTCLI[\"bittensor-cli::9.1.0\"]\n        BTWALLET[\"bittensor-wallet::3.0.4\"]\n        SUBSTRATE[\"async-substrate-interface::1.0.3\"]\n    end\n    \n    subgraph \"GPU Computing\"\n        TORCH[\"torch::2.5.1\"]\n        NVIDIA[\"nvidia-* packages\u003Cbr/>CUDA Runtime\"]\n        GPUTIL[\"GPUtil::1.4.0\"]\n        IGPU[\"igpu::0.1.2\"]\n        PYNVML[\"pynvml::12.0.0\"]\n    end\n    \n    subgraph \"Web Services\"\n        FASTAPI[\"fastapi::0.110.3\"]\n        UVICORN[\"uvicorn::0.34.0\"]\n        STARLETTE[\"starlette::0.37.2\"]\n        AIOHTTP[\"aiohttp::3.10.11\"]\n    end\n    \n    subgraph \"Containerization\"\n        DOCKER[\"docker::7.0.0\"]\n        PARAMIKO[\"paramiko::3.4.1\"]\n    end\n    \n    subgraph \"Monitoring & Logging\"\n        WANDB[\"wandb::0.19.0\"]\n        PSUTIL[\"psutil::5.9.8\"]\n    end\n    \n    subgraph \"Cryptography\"\n        CRYPTO[\"cryptography::43.0.1\"]\n        BLAKE3[\"blake3::1.0.4\"]\n        PYCRYPTO[\"pycryptodome::3.21.0\"]\n    end\n    \n    subgraph \"Data Processing\"\n        NUMPY[\"numpy::2.0.2\"]\n        REQUESTS[\"requests::2.31.0\"]\n        MSGPACK[\"msgpack-numpy-opentensor::0.5.0\"]\n    end\n    \n    BITTENSOR --> SUBSTRATE\n    BITTENSOR --> BTWALLET\n    BITTENSOR --> BTCLI\n    \n    TORCH --> NVIDIA\n    GPUTIL --> NVIDIA\n    IGPU --> PYNVML\n    \n    FASTAPI --> STARLETTE\n    FASTAPI --> UVICORN\n    \n    style BITTENSOR fill:#ff9999\n    style TORCH fill:#99ff99\n    style FASTAPI fill:#99ccff\n    style DOCKER fill:#ffcc99\n```\n\n**Sources:** \u003CSourceLink text=\"requirements.txt:36-59\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements.txt#L36-L59\" />, \u003CSourceLink text=\"pyproject.toml:36-59\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L36-L59\" />\n\n## Build and Packaging Configuration\n\nThe project uses modern Python packaging standards with `hatchling` as the build backend and supports both source and wheel distributions.\n\n| Component | Configuration | Purpose |\n|-----------|---------------|---------|\n| **Build Backend** | `hatchling&gt;=1.24.2` | Modern Python build system |\n| **Version Source** | `compute/__init__.py` | Dynamic versioning from package |\n| **Python Requirement** | `&gt;=3.10` | Minimum Python version |\n| **Package Distribution** | `compute`, `neurons` | Core packages included in wheel |\n| **License** | MIT | Open source license |\n\n```mermaid\ngraph LR\n    subgraph \"Source Code\"\n        SRC_COMPUTE[\"compute/\"]\n        SRC_NEURONS[\"neurons/\"]\n        VERSION[\"compute/__init__.py\u003Cbr/>(version)\"]\n    end\n    \n    subgraph \"Build Configuration\"\n        PYPROJECT_BUILD[\"pyproject.toml\u003Cbr/>[build-system]\"]\n        HATCH_VERSION[\"[tool.hatch.version]\"]\n        HATCH_WHEEL[\"[tool.hatch.build.targets.wheel]\"]\n    end\n    \n    subgraph \"Build Artifacts\"\n        SDIST[\"Source Distribution\u003Cbr/>.tar.gz\"]\n        WHEEL[\"Wheel Distribution\u003Cbr/>.whl\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        REQS_MAIN[\"requirements.txt\u003Cbr/>(pip-compile)\"]\n        REQS_DEV[\"requirements-dev.txt\u003Cbr/>(pip-compile)\"]\n        PYPROJECT_DEPS[\"pyproject.toml\u003Cbr/>[project.dependencies]\"]\n    end\n    \n    VERSION --> HATCH_VERSION\n    HATCH_VERSION --> PYPROJECT_BUILD\n    SRC_COMPUTE --> HATCH_WHEEL\n    SRC_NEURONS --> HATCH_WHEEL\n    \n    PYPROJECT_BUILD --> SDIST\n    PYPROJECT_BUILD --> WHEEL\n    \n    PYPROJECT_DEPS --> REQS_MAIN\n    PYPROJECT_DEPS --> REQS_DEV\n    \n    style PYPROJECT_BUILD fill:#e6f3ff\n    style WHEEL fill:#f9f9f9\n```\n\n**Sources:** \u003CSourceLink text=\"pyproject.toml:1-5\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L1-L5\" />, \u003CSourceLink text=\"pyproject.toml:78-91\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L78-L91\" />, \u003CSourceLink text=\"requirements.txt:1-6\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements.txt#L1-L6\" />\n\n## Development Environment Structure\n\nThe development environment includes comprehensive tooling for code quality, testing, and dependency management with automated workflows.\n\n```mermaid\ngraph TD\n    subgraph \"Development Dependencies\"\n        PRECOMMIT[\"pre-commit::4.1.0\"]\n        PYTEST[\"pytest::8.3.5\"]\n        PYTESTCOV[\"pytest-cov::6.0.0\"]\n        ALLURE[\"allure-pytest::2.13.5\"]\n        PIPTOOLS[\"pip-tools::7.4.1\"]\n    end\n    \n    subgraph \"Code Quality\"\n        HOOKS[\"Pre-commit Hooks\"]\n        COVERAGE[\"Coverage Reports\"]\n        TESTING[\"Test Suite\"]\n    end\n    \n    subgraph \"Dependency Management\"\n        COMPILE[\"pip-compile\"]\n        SYNC[\"pip-sync\"]\n        LOCK[\"requirements.txt\"]\n        DEVLOCK[\"requirements-dev.txt\"]\n    end\n    \n    subgraph \"Build Tools\"\n        HATCH[\"hatchling\"]\n        BUILD[\"build::1.2.2.post1\"]\n        WHEEL_BUILD[\"Wheel Building\"]\n    end\n    \n    PRECOMMIT --> HOOKS\n    PYTEST --> TESTING\n    PYTESTCOV --> COVERAGE\n    ALLURE --> TESTING\n    \n    PIPTOOLS --> COMPILE\n    COMPILE --> LOCK\n    COMPILE --> DEVLOCK\n    \n    HATCH --> BUILD\n    BUILD --> WHEEL_BUILD\n    \n    style PRECOMMIT fill:#ff9999\n    style PYTEST fill:#99ff99\n    style PIPTOOLS fill:#99ccff\n```\n\n**Sources:** \u003CSourceLink text=\"requirements-dev.txt:242-307\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements-dev.txt#L242-L307\" />, \u003CSourceLink text=\"pyproject.toml:61-69\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L61-L69\" />, \u003CSourceLink text=\"pyproject.toml:92-96\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L92-L96\" />\n\n## File Exclusions and Generated Content\n\nThe project maintains clean version control by excluding build artifacts, runtime data, and generated content from the repository.\n\n| Category | Files/Patterns | Purpose |\n|----------|----------------|---------|\n| **Python Artifacts** | `__pycache__/`, `*.pyc`, `build/`, `dist/` | Standard Python build artifacts |\n| **Runtime Data** | `database.db`, `wandb/` | Validator database and monitoring data |\n| **Generated Content** | `neurons/Miner/app`, `neurons/register-api/` | Dynamically generated applications |\n| **Security** | `cert/`, `.env` | SSL certificates and environment variables |\n| **Development** | `.idea/`, `.pytest_cache/`, `allure-results` | IDE and testing artifacts |\n\n**Sources:** \u003CSourceLink text=\".gitignore:1-262\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.gitignore#L1-L262\" />\n\n## Dependency Resolution Strategy\n\nThe project uses a two-stage dependency management approach with `pip-tools` for deterministic builds and dependency conflict resolution.\n\n```mermaid\nflowchart TD\n    subgraph \"Source Dependencies\"\n        PYPROJECT_MAIN[\"pyproject.toml\u003Cbr/>[project.dependencies]\"]\n        PYPROJECT_DEV[\"pyproject.toml\u003Cbr/>[project.optional-dependencies.dev]\"]\n    end\n    \n    subgraph \"Compilation Process\"\n        PIPCOMPILE[\"pip-compile\"]\n        RESOLVER[\"Dependency Resolver\"]\n    end\n    \n    subgraph \"Lock Files\"\n        REQUIREMENTS[\"requirements.txt\u003Cbr/>(production pins)\"]\n        REQUIREMENTS_DEV[\"requirements-dev.txt\u003Cbr/>(dev + production pins)\"]\n    end\n    \n    subgraph \"Installation\"\n        PIPSYNC[\"pip-sync\"]\n        VENV[\"Virtual Environment\"]\n    end\n    \n    PYPROJECT_MAIN --> PIPCOMPILE\n    PYPROJECT_DEV --> PIPCOMPILE\n    PIPCOMPILE --> RESOLVER\n    RESOLVER --> REQUIREMENTS\n    RESOLVER --> REQUIREMENTS_DEV\n    \n    REQUIREMENTS --> PIPSYNC\n    REQUIREMENTS_DEV --> PIPSYNC\n    PIPSYNC --> VENV\n    \n    style PIPCOMPILE fill:#99ccff\n    style REQUIREMENTS fill:#f9f9f9\n    style REQUIREMENTS_DEV fill:#f9f9f9\n```\n\n**Sources:** \u003CSourceLink text=\"requirements.txt:1-6\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements.txt#L1-L6\" />, \u003CSourceLink text=\"requirements-dev.txt:1-6\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/requirements-dev.txt#L1-L6\" />, \u003CSourceLink text=\"pyproject.toml:36-69\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/pyproject.toml#L36-L69\" />","src/content/docs/development/project-structure.mdx","16b50d5de6aaddf6","monitoring-and-metrics/wandb-integration",{"id":184,"data":186,"body":191,"filePath":192,"digest":193,"deferredRender":15},{"title":187,"editUrl":15,"head":188,"template":17,"sidebar":189,"pagefind":15,"draft":19},"WandB Integration",[],{"hidden":19,"attrs":190},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/wandb/wandb.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThe WandB Integration system provides distributed state management and experiment tracking across the NI Compute Subnet using Weights & Biases (WandB) as a centralized data store. This system enables validators and miners to share critical network state including hardware specifications, allocation status, performance metrics, and penalty information in a verifiable and tamper-resistant manner.\n\nThis document covers the technical implementation of WandB integration for network-wide data synchronization. For Prometheus-based local metrics collection, see [Prometheus Metrics](/monitoring-and-metrics/prometheus-metrics#6.2).\n\n## Architecture Overview\n\nThe WandB integration operates as a distributed state management layer that sits between local database storage and network-wide coordination. Each validator and miner maintains its own WandB run that serves as both a data publication mechanism and a verification layer through cryptographic signatures.\n\n```mermaid\ngraph TB\n    subgraph \"Local Systems\"\n        V1[\"Validator Instance\"]\n        V2[\"Validator Instance\"]\n        M1[\"Miner Instance\"]\n        M2[\"Miner Instance\"]\n        DB1[\"ComputeDb (Local)\"]\n        DB2[\"ComputeDb (Local)\"]\n    end\n    \n    subgraph \"WandB Cloud Platform\"\n        PROJECT[\"opencompute Project\"]\n        VRUN1[\"validator-{hotkey} Run\"]\n        VRUN2[\"validator-{hotkey} Run\"]\n        MRUN1[\"miner-{hotkey} Run\"]\n        MRUN2[\"miner-{hotkey} Run\"]\n    end\n    \n    subgraph \"Shared Network State\"\n        ALLOCATED[\"allocated_hotkeys\"]\n        PENALIZED[\"penalized_hotkeys\"]\n        SPECS[\"miner_specs\"]\n        STATS[\"validator_stats\"]\n    end\n    \n    V1 -->|\"ComputeWandb.update_allocated_hotkeys()\"| VRUN1\n    V2 -->|\"ComputeWandb.update_stats()\"| VRUN2\n    M1 -->|\"ComputeWandb.update_specs()\"| MRUN1\n    M2 -->|\"ComputeWandb.update_allocated()\"| MRUN2\n    \n    V1 -->|\"write_stats()\"| DB1\n    M1 -->|\"save_run_id()\"| DB2\n    \n    VRUN1 --> ALLOCATED\n    VRUN2 --> STATS\n    MRUN1 --> SPECS\n    MRUN2 --> ALLOCATED\n    \n    V1 -.->|\"get_allocated_hotkeys()\"| ALLOCATED\n    V1 -.->|\"get_miner_specs()\"| SPECS\n    V2 -.->|\"get_stats_allocated()\"| STATS\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:1-648\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L1-L648\" />\n\n## Core Components\n\n### ComputeWandb Class\n\nThe `ComputeWandb` class serves as the primary interface for all WandB operations within the compute subnet. It manages authentication, run lifecycle, data synchronization, and cryptographic verification.\n\n```mermaid\nclassDiagram\n    class ComputeWandb {\n        +run: wandb.Run\n        +config: bt.config\n        +wallet: bt.wallet\n        +hotkey: str\n        +role: str\n        +db: ComputeDb\n        +api: wandb.Api\n        +run_id: str\n        \n        +__init__(config, wallet, role)\n        +update_config()\n        +save_run_id(hotkey, run_id)\n        +get_run_id(hotkey)\n        +update_specs()\n        +log_chain_data(data)\n        +update_allocated(allocated)\n        +update_stats(stats)\n        +update_allocated_hotkeys(hotkey_list)\n        +update_penalized_hotkeys(hotkey_list)\n        +get_allocated_hotkeys(valid_validators, flag)\n        +get_stats_allocated(valid_validators, flag)\n        +get_miner_specs(queryable_uids)\n        +sign_run()\n        +verify_run(run)\n        +sync_allocated(hotkey)\n    }\n    \n    class ComputeDb {\n        +get_cursor()\n        +conn: Connection\n    }\n    \n    class wandb_Api {\n        +runs()\n        +project()\n        +flush()\n    }\n    \n    ComputeWandb --> ComputeDb\n    ComputeWandb --> wandb_Api\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:19-648\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L19-L648\" />\n\n### Authentication and Run Management\n\nThe system manages WandB authentication through API keys and run persistence through local database storage. Each hotkey maintains a single persistent run across restarts.\n\n| Configuration Parameter | Value | Purpose |\n|------------------------|-------|---------|\n| `PUBLIC_WANDB_NAME` | `\"opencompute\"` | Project name |\n| `PUBLIC_WANDB_ENTITY` | `\"neuralinternet\"` | Organization entity |\n| Run naming pattern | `\"{role}-{hotkey}\"` | Unique run identification |\n\nThe authentication flow handles multiple scenarios:\n\n```mermaid\nflowchart TD\n    START[\"ComputeWandb.__init__()\"]\n    CHECK_KEY{\"WANDB_API_KEY exists?\"}\n    CHECK_NETRC{\"~/.netrc exists?\"}\n    ERROR[\"Raise ValueError\"]\n    \n    GET_RUN_ID[\"get_run_id(hotkey)\"]\n    RUN_EXISTS{\"run_id found?\"}\n    \n    QUERY_WANDB[\"Query WandB for existing runs\"]\n    RUNS_FOUND{\"runs.length >= 1?\"}\n    \n    CREATE_RUN[\"wandb.init() new run\"]\n    SAVE_RUN[\"save_run_id()\"]\n    \n    RESUME_RUN[\"wandb.init(id:run_id, resume:'allow')\"]\n    UPDATE_CONFIG[\"update_config()\"]\n    SIGN[\"sign_run()\"]\n    \n    START --> CHECK_KEY\n    CHECK_KEY -->|No| CHECK_NETRC\n    CHECK_KEY -->|Yes| GET_RUN_ID\n    CHECK_NETRC -->|No| ERROR\n    CHECK_NETRC -->|Yes| GET_RUN_ID\n    \n    GET_RUN_ID --> RUN_EXISTS\n    RUN_EXISTS -->|No| QUERY_WANDB\n    RUN_EXISTS -->|Yes| RESUME_RUN\n    \n    QUERY_WANDB --> RUNS_FOUND\n    RUNS_FOUND -->|No| CREATE_RUN\n    RUNS_FOUND -->|Yes| SAVE_RUN\n    \n    CREATE_RUN --> SAVE_RUN\n    SAVE_RUN --> RESUME_RUN\n    RESUME_RUN --> UPDATE_CONFIG\n    UPDATE_CONFIG --> SIGN\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:22-88\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L22-L88\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:109-138\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L109-L138\" />\n\n## Data Synchronization Patterns\n\n### Validator Data Flow\n\nValidators publish aggregated network statistics and maintain lists of allocated and penalized hotkeys. The synchronization ensures consistency between local database state and distributed WandB state.\n\n```mermaid\nsequenceDiagram\n    participant VDB as \"ComputeDb (Validator)\"\n    participant CW as \"ComputeWandb\"\n    participant WB as \"WandB API\"\n    participant NET as \"Network State\"\n    \n    Note over VDB,NET: Stats Update Cycle\n    VDB->>CW: retrieve_stats(db)\n    CW->>CW: update_allocated_hotkeys(hotkey_list)\n    CW->>VDB: write_stats(db, updated_stats)\n    CW->>WB: run.config.update(allocated_hotkeys, stats)\n    CW->>WB: run.log(allocated_hotkeys)\n    CW->>CW: sign_run()\n    \n    Note over VDB,NET: Network Query Cycle\n    CW->>WB: api.runs(filters=validator_runs)\n    WB-->>CW: validator_run_configs\n    CW->>CW: verify_run(run) for each\n    CW-->>NET: aggregated_allocated_hotkeys\n    CW-->>NET: aggregated_stats\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:198-230\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L198-L230\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:291-332\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L291-L332\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:334-450\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L334-L450\" />\n\n### Miner Data Flow\n\nMiners publish hardware specifications and allocation status, enabling validators to discover available resources and verify capabilities.\n\n```mermaid\nsequenceDiagram\n    participant M as \"Miner Process\"\n    participant CW as \"ComputeWandb\"\n    participant WB as \"WandB API\"\n    participant V as \"Validator Query\"\n    \n    Note over M,V: Specs Publication\n    M->>CW: update_specs()\n    CW->>CW: get_perf_info(encrypted=False)\n    CW->>WB: run.config.update(specs)\n    CW->>CW: sign_run()\n    \n    Note over M,V: Allocation Status Update\n    M->>CW: update_allocated(validator_hotkey)\n    CW->>WB: run.config.update(allocated)\n    CW->>CW: sign_run()\n    \n    Note over M,V: Validator Discovery\n    V->>WB: api.runs(filters=miner_runs)\n    WB-->>V: miner_run_configs\n    V->>V: verify_run(run) for each\n    V-->>V: hotkey_to_specs_mapping\n```\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:140-159\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L140-L159\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:168-184\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L168-L184\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:540-574\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L540-L574\" />\n\n## Security and Verification\n\n### Cryptographic Signature System\n\nAll WandB runs are signed using the participant's hotkey to prevent data tampering and ensure authenticity. The signature covers the run ID to prevent replay attacks.\n\n```mermaid\nflowchart LR\n    subgraph \"Signing Process\"\n        RUN_ID[\"run.id\"]\n        HASH[\"SHA-256 Hash\"]\n        SIGN[\"wallet.hotkey.sign()\"]\n        STORE[\"run.config.signature\"]\n    end\n    \n    subgraph \"Verification Process\"\n        RETRIEVE[\"run.config.signature\"]\n        RECREATE[\"SHA-256(run.id)\"]\n        VERIFY[\"bt.Keypair.verify()\"]\n        RESULT[\"True/False\"]\n    end\n    \n    RUN_ID --> HASH\n    HASH --> SIGN\n    SIGN --> STORE\n    \n    RETRIEVE --> VERIFY\n    RECREATE --> VERIFY\n    VERIFY --> RESULT\n```\n\nThe verification process validates both signature authenticity and validator authorization:\n\n| Verification Check | Implementation | Purpose |\n|-------------------|----------------|---------|\n| Signature validity | `bt.Keypair(ss58_address=hotkey).verify()` | Prevents data tampering |\n| Validator authorization | `hotkey in valid_validator_hotkeys` | Prevents unauthorized updates |\n| Data existence | Config field presence checks | Ensures required data |\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:576-616\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L576-L616\" />\n\n## Configuration and Setup\n\n### Environment Requirements\n\n| Requirement | Configuration Method | Purpose |\n|-------------|---------------------|---------|\n| WandB API Key | `WANDB_API_KEY` environment variable | Authentication |\n| WandB Login | `wandb login` command | Alternative authentication |\n| Network file | `~/.netrc` | Credential storage |\n\n### Run Configuration Schema\n\nThe system maintains a standardized configuration schema across all runs:\n\n```json\n{\n  \"hotkey\": \"ss58_address\",\n  \"role\": \"validator|miner\", \n  \"config\": \"bt.config_object\",\n  \"version\": \"version_integer\",\n  \"specs\": \"hardware_specifications\",\n  \"allocated\": \"boolean_or_hotkey\",\n  \"allocated_hotkeys\": [\"hotkey_list\"],\n  \"penalized_hotkeys\": [\"hotkey_list\"],\n  \"stats\": \"uid_to_stats_mapping\",\n  \"signature\": \"hex_signature\"\n}\n```\n\n### Database Integration\n\nThe system maintains local state persistence through the `wandb_runs` table in `ComputeDb`:\n\n| Column | Type | Purpose |\n|--------|------|---------|\n| `hotkey` | TEXT | Participant identifier |\n| `run_id` | TEXT | WandB run identifier |\n\n**Sources:** \u003CSourceLink text=\"compute/wandb/wandb.py:15-52\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L15-L52\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:90-108\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L90-L108\" />, \u003CSourceLink text=\"compute/wandb/wandb.py:109-138\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/wandb/wandb.py#L109-L138\" />","src/content/docs/monitoring-and-metrics/wandb-integration.mdx","3401e6bf0901721b","validator-system",{"id":194,"data":196,"body":201,"filePath":202,"digest":203,"deferredRender":15},{"title":197,"editUrl":15,"head":198,"template":17,"sidebar":199,"pagefind":15,"draft":19},"Validator System",[],{"hidden":19,"attrs":200},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/validator.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThe Validator System is the core component responsible for evaluating miner capabilities, performing proof-of-GPU validation, and setting network weights in the NI Compute Subnet. It orchestrates the entire validation process including hardware verification, performance scoring, and blockchain weight updates.\n\nFor information about the specific proof-of-GPU validation algorithms, see [Proof of GPU](/validator-system/proof-of-gpu#2.1). For details about the scoring mechanisms, see [Scoring System](/validator-system/scoring-system#2.2). For database schema and operations, see [Database Operations](/validator-system/database-operations#2.3).\n\n## Architecture Overview\n\nThe validator system operates as a continuous validation loop that queries miners, validates their GPU capabilities, calculates performance scores, and updates network weights. The system is built around the `Validator` class which coordinates all validation activities.\n\n### Validator System Components\n\n```mermaid\ngraph TB\n    subgraph \"Validator Core\"\n        ValidatorClass[\"Validator Class\u003Cbr/>neurons/validator.py\"]\n        Config[\"Configuration\u003Cbr/>init_config()\"]\n        Prometheus[\"Prometheus Setup\u003Cbr/>init_prometheus()\"]\n    end\n    \n    subgraph \"Blockchain Interface\"\n        Subtensor[\"ComputeSubnetSubtensor\u003Cbr/>subtensor connection\"]\n        Metagraph[\"bt.metagraph\u003Cbr/>network state\"]\n        Wallet[\"bt.wallet\u003Cbr/>validator identity\"]\n    end\n    \n    subgraph \"Data Layer\"\n        ComputeDb[\"ComputeDb\u003Cbr/>local database\"]\n        ComputeWandb[\"ComputeWandb\u003Cbr/>metrics & monitoring\"]\n        ConfigData[\"config.yaml\u003Cbr/>GPU performance data\"]\n    end\n    \n    subgraph \"Validation Engine\"\n        PoGEngine[\"proof_of_gpu()\u003Cbr/>GPU validation\"]\n        ScoringEngine[\"sync_scores()\u003Cbr/>performance scoring\"]\n        WeightSetter[\"set_weights()\u003Cbr/>blockchain updates\"]\n    end\n    \n    subgraph \"Miner Communication\"\n        AllocateProtocol[\"Allocate Protocol\u003Cbr/>resource allocation\"]\n        SpecsProtocol[\"Specs Protocol\u003Cbr/>hardware queries\"]\n        ChallengeProtocol[\"Challenge Protocol\u003Cbr/>PoW verification\"]\n    end\n    \n    ValidatorClass --> Config\n    ValidatorClass --> Prometheus\n    ValidatorClass --> Subtensor\n    ValidatorClass --> Metagraph\n    ValidatorClass --> Wallet\n    ValidatorClass --> ComputeDb\n    ValidatorClass --> ComputeWandb\n    ValidatorClass --> ConfigData\n    \n    ValidatorClass --> PoGEngine\n    ValidatorClass --> ScoringEngine\n    ValidatorClass --> WeightSetter\n    \n    PoGEngine --> AllocateProtocol\n    ValidatorClass --> SpecsProtocol\n    ValidatorClass --> ChallengeProtocol\n    \n    ScoringEngine --> ComputeDb\n    ScoringEngine --> ComputeWandb\n    WeightSetter --> Subtensor\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:70-209\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L70-L209\" />, \u003CSourceLink text=\"neurons/validator.py:130-175\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L130-L175\" />\n\n### Validation Process Flow\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator\"\n    participant DB as \"ComputeDb\"\n    participant W as \"ComputeWandb\"\n    participant BT as \"Subtensor\"\n    participant M as \"Miner\"\n    \n    Note over V: Initialization Phase\n    V->>DB: Initialize database connection\n    V->>W: Setup WandB monitoring\n    V->>BT: Connect to blockchain\n    V->>V: init_scores()\n    \n    Note over V: Main Validation Loop\n    loop Every Block\n        V->>BT: sync_local() - Update metagraph\n        V->>V: get_queryable() - Filter valid miners\n        \n        alt Every 360 blocks (PoG)\n            V->>V: proof_of_gpu()\n            V->>M: allocate_miner()\n            V->>M: test_miner_gpu()\n            V->>M: deallocate_miner()\n            V->>DB: update_pog_stats()\n        end\n        \n        alt Every 150 blocks (Specs)\n            V->>W: get_specs_wandb()\n            V->>DB: update_miner_details()\n        end\n        \n        alt Every 25 blocks (Status)\n            V->>V: sync_status()\n            V->>W: log_chain_data()\n        end\n        \n        alt Every 100 blocks (Weights)\n            V->>V: sync_scores()\n            V->>V: set_burn_weights()\n            V->>BT: Submit weights to blockchain\n        end\n    end\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:1161-1273\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1161-L1273\" />, \u003CSourceLink text=\"neurons/validator.py:1192-1202\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1192-L1202\" />, \u003CSourceLink text=\"neurons/validator.py:1240-1247\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1240-L1247\" />\n\n## Core Components\n\n### Validator Class\n\nThe `Validator` class is the main orchestrator that manages all validation activities. It maintains state for queryable miners, scores, and validation results.\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `scores` | `torch.Tensor` | Current performance scores for all miners |\n| `stats` | `dict` | Detailed statistics for each miner |\n| `_queryable_uids` | `Dict[int, bt.AxonInfo]` | Valid miners available for validation |\n| `allocated_hotkeys` | `list` | Currently allocated miner hotkeys |\n| `penalized_hotkeys` | `list` | Penalized miner hotkeys |\n\nSources: \u003CSourceLink text=\"neurons/validator.py:70-91\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L70-L91\" />, \u003CSourceLink text=\"neurons/validator.py:84-91\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L84-L91\" />\n\n### Configuration Management\n\nThe validator uses multiple configuration sources to manage validation parameters:\n\n```mermaid\ngraph LR\n    subgraph \"Configuration Sources\"\n        ArgParser[\"ComputeArgPaser\u003Cbr/>CLI arguments\"]\n        ConfigYaml[\"config.yaml\u003Cbr/>GPU performance data\"]\n        EnvVars[\"Environment Variables\u003Cbr/>system settings\"]\n    end\n    \n    subgraph \"Configuration Properties\"\n        BatchSize[\"validator_specs_batch_size\u003Cbr/>validator_challenge_batch_size\"]\n        HardwareQuery[\"validator_perform_hardware_query\"]\n        Thresholds[\"validator_whitelist_updated_threshold\"]\n        Blacklists[\"blacklist_hotkeys\u003Cbr/>blacklist_coldkeys\"]\n    end\n    \n    ArgParser --> BatchSize\n    ArgParser --> HardwareQuery\n    ArgParser --> Thresholds\n    ArgParser --> Blacklists\n    ConfigYaml --> BatchSize\n    EnvVars --> HardwareQuery\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:210-235\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L210-L235\" />, \u003CSourceLink text=\"neurons/validator.py:132-147\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L132-L147\" />\n\n### Queryable Miners Management\n\nThe validator maintains a filtered list of queryable miners based on multiple criteria:\n\n```mermaid\ngraph TD\n    AllMiners[\"All Network Miners\u003Cbr/>metagraph.neurons\"]\n    \n    subgraph \"Filtering Pipeline\"\n        ValidTensors[\"get_valid_tensors()\u003Cbr/>IP & blacklist filter\"]\n        FilterAxons[\"filter_axons()\u003Cbr/>unique IP addresses\"]\n        FilterVersion[\"filter_axon_version()\u003Cbr/>minimum version check\"]\n    end\n    \n    QueryableMiners[\"_queryable_uids\u003Cbr/>Dict[int, bt.AxonInfo]\"]\n    \n    AllMiners --> ValidTensors\n    ValidTensors --> FilterAxons\n    FilterAxons --> FilterVersion\n    FilterVersion --> QueryableMiners\n    \n    subgraph \"Blacklist Checks\"\n        BlacklistColdkeys[\"blacklist_coldkeys\"]\n        BlacklistHotkeys[\"blacklist_hotkeys\"]\n        ExploiterKeys[\"exploiters_hotkeys\u003Cbr/>exploiters_coldkeys\"]\n    end\n    \n    ValidTensors --> BlacklistColdkeys\n    ValidTensors --> BlacklistHotkeys\n    ValidTensors --> ExploiterKeys\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:571-579\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L571-L579\" />, \u003CSourceLink text=\"neurons/validator.py:487-515\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L487-L515\" />, \u003CSourceLink text=\"neurons/validator.py:517-544\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L517-L544\" />\n\n## Validation Process\n\n### Proof-of-GPU Validation\n\nThe proof-of-GPU system allocates miners, tests their GPU capabilities, and verifies performance through cryptographic proofs:\n\n```mermaid\ngraph TD\n    subgraph \"PoG Initialization\"\n        GetQueryable[\"get_queryable()\u003Cbr/>filter available miners\"]\n        GetAllocated[\"wandb.get_allocated_hotkeys()\u003Cbr/>skip allocated miners\"]\n        CreateQueue[\"asyncio.Queue\u003Cbr/>miner processing queue\"]\n    end\n    \n    subgraph \"Miner Testing Pipeline\"\n        AllocateMiner[\"allocate_miner()\u003Cbr/>RSA key generation\"]\n        SSHConnect[\"paramiko.SSHClient\u003Cbr/>secure connection\"]\n        HashCheck[\"compute_script_hash()\u003Cbr/>integrity verification\"]\n        GPUInfo[\"get_remote_gpu_info()\u003Cbr/>nvidia-smi query\"]\n        Benchmark[\"execute_script_on_miner('benchmark')\u003Cbr/>performance test\"]\n        MerkleProof[\"execute_script_on_miner('compute')\u003Cbr/>cryptographic proof\"]\n        VerifyProof[\"verify_responses()\u003Cbr/>proof validation\"]\n    end\n    \n    subgraph \"Result Processing\"\n        UpdatePoGStats[\"update_pog_stats()\u003Cbr/>database update\"]\n        SyncScores[\"sync_scores()\u003Cbr/>recalculate scores\"]\n    end\n    \n    GetQueryable --> GetAllocated\n    GetAllocated --> CreateQueue\n    CreateQueue --> AllocateMiner\n    AllocateMiner --> SSHConnect\n    SSHConnect --> HashCheck\n    HashCheck --> GPUInfo\n    GPUInfo --> Benchmark\n    Benchmark --> MerkleProof\n    MerkleProof --> VerifyProof\n    VerifyProof --> UpdatePoGStats\n    UpdatePoGStats --> SyncScores\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:663-787\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L663-L787\" />, \u003CSourceLink text=\"neurons/validator.py:799-948\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L799-L948\" />\n\n### Scoring System\n\nThe scoring system calculates performance scores based on GPU specifications and reliability metrics:\n\n```mermaid\ngraph LR\n    subgraph \"Score Calculation\"\n        PoGSpecs[\"get_pog_specs()\u003Cbr/>local GPU data\"]\n        CalcScore[\"calc_score_pog()\u003Cbr/>performance calculation\"]\n        StatsAllocated[\"stats_allocated\u003Cbr/>external scores\"]\n        PenalizedCheck[\"penalized_hotkeys\u003Cbr/>penalty filter\"]\n    end\n    \n    subgraph \"Score Sources\"\n        LocalDB[\"Local Database\u003Cbr/>own_score: true\"]\n        ExternalWandb[\"WandB Stats\u003Cbr/>own_score: false\"]\n    end\n    \n    subgraph \"Final Score\"\n        FinalScore[\"stats[uid]['score']\u003Cbr/>final miner score\"]\n        ReliabilityScore[\"reliability_score\u003Cbr/>historical performance\"]\n    end\n    \n    PoGSpecs --> CalcScore\n    CalcScore --> LocalDB\n    StatsAllocated --> ExternalWandb\n    LocalDB --> FinalScore\n    ExternalWandb --> FinalScore\n    PenalizedCheck --> FinalScore\n    FinalScore --> ReliabilityScore\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:312-402\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L312-L402\" />, \u003CSourceLink text=\"neurons/validator.py:360-386\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L360-L386\" />\n\n## Database Operations\n\nThe validator uses `ComputeDb` for persistent storage of miner information, validation results, and statistics:\n\n| Table | Purpose | Key Operations |\n|-------|---------|----------------|\n| `miners` | Miner registration data | `select_miners()`, `update_miners()`, `purge_miner_entries()` |\n| `pog_stats` | Proof-of-GPU results | `get_pog_specs()`, `update_pog_stats()` |\n| `stats` | Performance statistics | `retrieve_stats()`, `write_stats()` |\n| `allocation` | Resource allocations | `update_miner_details()`, `get_miner_details()` |\n\nSources: \u003CSourceLink text=\"neurons/validator.py:171-172\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L171-L172\" />, \u003CSourceLink text=\"compute/utils/db.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py\" />\n\n## Monitoring and Metrics\n\n### WandB Integration\n\nThe validator integrates with Weights & Biases for distributed state management and metrics collection:\n\n```mermaid\ngraph TD\n    subgraph \"WandB Operations\"\n        AllocatedHotkeys[\"update_allocated_hotkeys()\u003Cbr/>track resource usage\"]\n        MinerSpecs[\"get_miner_specs()\u003Cbr/>hardware specifications\"]\n        ChainData[\"log_chain_data()\u003Cbr/>blockchain metrics\"]\n        PenalizedHotkeys[\"get_penalized_hotkeys_checklist_bak()\"]\n    end\n    \n    subgraph \"Metrics Collection\"\n        BlockData[\"Block, Stake, Rank\u003Cbr/>vTrust, Emission\"]\n        ValidatorStats[\"Validator performance\u003Cbr/>validation results\"]\n        MinerStats[\"Miner capabilities\u003Cbr/>GPU specifications\"]\n    end\n    \n    AllocatedHotkeys --> MinerStats\n    MinerSpecs --> MinerStats\n    ChainData --> BlockData\n    PenalizedHotkeys --> ValidatorStats\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:290-311\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L290-L311\" />, \u003CSourceLink text=\"neurons/validator.py:594-661\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L594-L661\" />, \u003CSourceLink text=\"neurons/validator.py:1229-1237\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1229-L1237\" />\n\n### Weight Setting\n\nThe validator periodically updates network weights based on calculated scores:\n\n```mermaid\ngraph LR\n    subgraph \"Weight Calculation\"\n        Scores[\"self.scores\u003Cbr/>miner performance\"]\n        ClampNegative[\"scores[scores \u003C 0] : 0\u003Cbr/>remove negative scores\"]\n        Normalize[\"torch.nn.functional.normalize()\u003Cbr/>L1 normalization\"]\n    end\n    \n    subgraph \"Weight Submission\"\n        SetWeights[\"subtensor.set_weights()\u003Cbr/>blockchain submission\"]\n        BurnWeights[\"set_burn_weights()\u003Cbr/>burn account allocation\"]\n        VersionKey[\"version_key\u003Cbr/>__version_as_int__\"]\n    end\n    \n    Scores --> ClampNegative\n    ClampNegative --> Normalize\n    Normalize --> SetWeights\n    Normalize --> BurnWeights\n    SetWeights --> VersionKey\n    BurnWeights --> VersionKey\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:1132-1153\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1132-L1153\" />, \u003CSourceLink text=\"neurons/validator.py:1101-1131\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1101-L1131\" />","src/content/docs/validator-system.mdx","bbc4e6cd6ce3ef46","monitoring-and-metrics/prometheus-metrics",{"id":204,"data":206,"body":211,"filePath":212,"digest":213,"deferredRender":15},{"title":207,"editUrl":15,"head":208,"template":17,"sidebar":209,"pagefind":15,"draft":19},"Prometheus Metrics",[],{"hidden":19,"attrs":210},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/axon.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document explains how the NI Compute system implements and utilizes Prometheus metrics for monitoring and observability. Prometheus is a popular open-source monitoring and alerting toolkit used to collect and query time-series metrics from various systems. In NI Compute, Prometheus metrics provide crucial insights into the performance and behavior of validators and miners in the decentralized GPU compute marketplace.\n\nFor information about general monitoring and integration with Weights & Biases, see [WandB Integration](/monitoring-and-metrics/wandb-integration#6.1).\n\n## 1. Prometheus Metrics Architecture\n\nPrometheus metrics in NI Compute are implemented through a specialized registration system that integrates with the Bittensor blockchain. This integration ensures that monitoring endpoints are discoverable by other network participants.\n\n```mermaid\nflowchart TD\n    subgraph \"Prometheus Metrics System\"\n        PM[\"PrometheusMetrics\"]\n        PE[\"prometheus_extrinsic()\"]\n        DPS[\"do_serve_prometheus()\"]\n    end\n    \n    subgraph \"Validator\"\n        VI[\"init_prometheus()\"]\n        SS[\"sync_status()\"]\n        VI --> PE\n        SS --> VI\n    end\n    \n    subgraph \"Subtensor\"\n        CSS[\"ComputeSubnetSubtensor\"]\n        SP[\"serve_prometheus()\"]\n        CSS --> SP\n        SP --> PE\n    end\n    \n    subgraph \"Blockchain\"\n        BT[\"Bittensor Chain\"]\n        PI[\"PrometheusInfo\"]\n    end\n    \n    VI --> SP\n    PE --> DPS\n    DPS --> BT\n    BT --> PI\n    \n    style PM fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style PE fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style DPS fill:#f9f9f9,stroke:#333,stroke-width:1px\n```\n\nSources: \u003CSourceLink text=\"compute/axon.py:166-201\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L166-L201\" />, \u003CSourceLink text=\"compute/axon.py:49\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L49\" />\n\n## 2. Prometheus Registration Process\n\nThe registration process involves the validator registering its Prometheus metrics endpoint with the Bittensor blockchain, making it discoverable to other nodes and monitoring systems.\n\n```mermaid\nsequenceDiagram\n    participant Validator\n    participant Subtensor as ComputeSubnetSubtensor\n    participant Chain as Bittensor Blockchain\n    \n    Validator->>Validator: init_prometheus()\n    Validator->>Subtensor: serve_prometheus(wallet, port, netuid)\n    Subtensor->>Subtensor: prometheus_extrinsic()\n    \n    Note over Subtensor: Create PrometheusServeCallParams\n    \n    Subtensor->>Subtensor: Check if neuron needs update\n    \n    alt Needs Update\n        Subtensor->>Subtensor: do_serve_prometheus()\n        Subtensor->>Chain: Submit extrinsic\n        Chain->>Chain: Update PrometheusInfo\n        Chain-->>Subtensor: Confirm update\n        Subtensor-->>Validator: Return success\n    else Already Updated\n        Subtensor-->>Validator: Return success (already served)\n    end\n    \n    Validator->>Validator: Log success or failure\n```\n\nSources: \u003CSourceLink text=\"compute/axon.py:166-201\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L166-L201\" />, \u003CSourceLink text=\"compute/axon.py:203-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L203-L283\" />\n\n## 3. Prometheus Metrics Implementation\n\n### 3.1 Registration Functions\n\nThe NI Compute system implements several key functions to handle Prometheus metrics registration:\n\n#### ComputeSubnetSubtensor.serve_prometheus()\n\nThe `serve_prometheus()` method in the `ComputeSubnetSubtensor` class coordinates Prometheus metrics registration on the blockchain. This method:\n\n- Accepts wallet, port, netuid, and wait parameters\n- Calls the `prometheus_extrinsic()` function to handle the registration process\n- Returns a boolean indicating success or failure\n- Includes comprehensive error handling and logging\n\n#### ComputeSubnetSubtensor.do_serve_prometheus()\n\nThe `do_serve_prometheus()` method handles the low-level blockchain interaction:\n\n- Composes a substrate call to `SubtensorModule.serve_prometheus`\n- Creates a signed extrinsic using the wallet hotkey\n- Implements retry logic with exponential backoff (3 tries, 2x backoff, max 4s delay)\n- Submits the extrinsic and processes the response\n- Returns a tuple of (success: bool, error: Optional[dict])\n\n#### prometheus_extrinsic()\n\nThe `prometheus_extrinsic()` function (imported from `compute.prometheus`) prepares the registration parameters and delegates to `do_serve_prometheus()`.\n\nSources: \u003CSourceLink text=\"compute/axon.py:166-201\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L166-L201\" />, \u003CSourceLink text=\"compute/axon.py:203-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L203-L283\" />, \u003CSourceLink text=\"compute/axon.py:258-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L258-L283\" />\n\n### 3.2 Versioning and Auto-Update\n\nThe system uses version information from `__version_as_int__` for Prometheus metrics registration. The version is embedded in the axon info that gets registered on the blockchain.\n\n```mermaid\nflowchart TD\n    subgraph \"Version Integration\"\n        VI[\"__version_as_int__\"]\n        AI[\"AxonInfo.version\"]\n        CSA[\"ComputeSubnetAxon.info()\"]\n        \n        VI --> AI\n        CSA --> AI\n    end\n    \n    subgraph \"Registration Process\"\n        SP[\"serve_prometheus()\"]\n        PE[\"prometheus_extrinsic()\"]\n        DSP[\"do_serve_prometheus()\"]\n        \n        SP --> PE\n        PE --> DSP\n        AI --> PE\n    end\n    \n    subgraph \"Blockchain\"\n        BC[\"Bittensor Chain\"]\n        PI[\"PrometheusInfo\"]\n        \n        DSP --> BC\n        BC --> PI\n    end\n```\n\nSources: \u003CSourceLink text=\"compute/axon.py:47\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L47\" />, \u003CSourceLink text=\"compute/axon.py:376-388\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L376-L388\" />\n\n## 4. Substrate Call Structure\n\nThe Prometheus registration uses a substrate call to the `SubtensorModule.serve_prometheus` function. The call parameters structure is defined by the Bittensor substrate interface and includes:\n\n| Parameter | Description | Implementation |\n|-----------|-------------|----------------|\n| `call_module` | Target module name | `\"SubtensorModule\"` |\n| `call_function` | Target function name | `\"serve_prometheus\"` |\n| `call_params` | Registration parameters | `PrometheusServeCallParams` |\n\nThe method creates a signed extrinsic using the wallet hotkey and submits it to the substrate interface with configurable wait options for inclusion and finalization.\n\nSources: \u003CSourceLink text=\"compute/axon.py:260-267\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L260-L267\" />, \u003CSourceLink text=\"compute/axon.py:206\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L206\" />\n\n## 5. Blockchain Integration\n\n### 5.1 ComputeSubnetSubtensor Extension\n\nThe `ComputeSubnetSubtensor` class extends Bittensor's base `Subtensor` class to add compute subnet-specific functionality:\n\n```mermaid\nclassDiagram\n    class Subtensor {\n        \u003C\u003CBittensor Core>>\n        +substrate: SubstrateInterface\n        +compose_call()\n        +create_signed_extrinsic()\n    }\n    \n    class ComputeSubnetSubtensor {\n        +serve_prometheus(wallet, port, netuid)\n        +do_serve_prometheus(wallet, call_params)\n        -make_substrate_call_with_retry()\n    }\n    \n    class prometheus_extrinsic {\n        \u003C\u003CFunction>>\n        (wallet, port, netuid)\n    }\n    \n    Subtensor \u003C|-- ComputeSubnetSubtensor\n    ComputeSubnetSubtensor --> prometheus_extrinsic : calls\n```\n\n### 5.2 Extrinsic Submission Flow\n\nThe registration process uses Bittensor's substrate interface for blockchain interaction:\n\n1. **Call Composition**: Uses `substrate.compose_call()` to create the blockchain call\n2. **Extrinsic Creation**: Creates a signed extrinsic with `substrate.create_signed_extrinsic()`\n3. **Submission**: Submits via `substrate.submit_extrinsic()` with retry logic\n4. **Response Processing**: Processes events and checks for success/failure\n\nSources: \u003CSourceLink text=\"compute/axon.py:152-165\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L152-L165\" />, \u003CSourceLink text=\"compute/axon.py:260-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L260-L283\" />\n\n## 6. Using Prometheus Metrics\n\n### 6.1 Accessing Metrics\n\nOnce registered, Prometheus metrics are available at:\n\n```\nhttp://\u003Cnode_ip>:\u003Cprometheus_port>/metrics\n```\n\nThe exact port is determined by the validator's configuration, typically using Bittensor's default axon port.\n\n### 6.2 Common Metrics\n\nWhile the specific metrics exposed aren't explicitly documented in the code provided, Prometheus in Bittensor networks typically provides metrics such as:\n\n- Node operational status\n- Request counts and latencies\n- Resource utilization (CPU, memory, GPU)\n- Network activity\n- Validation and scoring information\n\n### 6.3 Integration with Monitoring Systems\n\nTo monitor NI Compute nodes using Prometheus:\n\n1. Configure a Prometheus server to scrape the registered endpoints\n2. Set up appropriate recording rules and alerts\n3. Use visualization tools like Grafana to create dashboards\n\n## 7. Monitoring Lifecycle\n\nThe NI Compute system ensures continuous monitoring availability through its lifecycle management:\n\n```mermaid\nflowchart LR\n    subgraph \"Node Startup\"\n        NS[\"Node Start\"]\n        IR[\"Initialize Registration\"]\n    end\n    \n    subgraph \"Periodic Checks\"\n        SS[\"sync_status()\"]\n        VC[\"Version Check\"]\n    end\n    \n    subgraph \"Updates\"\n        UP[\"Update Prometheus\"]\n        RE[\"Re-register if needed\"]\n    end\n    \n    NS --> IR\n    IR --> SS\n    SS -- \"Every sync cycle\" --> VC\n    VC -- \"Version mismatch\" --> UP\n    VC -- \"Needs refresh\" --> RE\n    \n    style NS fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style IR fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style SS fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style VC fill:#f9f9f9,stroke:#333,stroke-width:1px\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:428-446\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L428-L446\" />, \u003CSourceLink text=\"compute/prometheus.py:80-95\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/prometheus.py#L80-L95\" />\n\n## 8. Error Handling and Reliability\n\n### 8.1 Retry Logic Implementation\n\nThe `do_serve_prometheus()` method implements comprehensive retry logic for reliable blockchain communication:\n\n```mermaid\nflowchart LR\n    subgraph \"Retry Configuration\"\n        RC[\"@retry decorator\"]\n        D[\"delay:1\"]\n        T[\"tries:3\"] \n        B[\"backoff:2\"]\n        MD[\"max_delay:4\"]\n    end\n    \n    subgraph \"Submission Process\"\n        MSC[\"make_substrate_call_with_retry()\"]\n        CC[\"compose_call()\"]\n        CSE[\"create_signed_extrinsic()\"]\n        SE[\"submit_extrinsic()\"]\n    end\n    \n    subgraph \"Error Handling\"\n        SRE[\"SubstrateRequestException\"]\n        GE[\"General Exception\"]\n        EL[\"Error Logging\"]\n    end\n    \n    RC --> MSC\n    MSC --> CC\n    CC --> CSE\n    CSE --> SE\n    SE --> SRE\n    SE --> GE\n    SRE --> EL\n    GE --> EL\n```\n\n### 8.2 Exception Management\n\nThe system handles multiple types of errors:\n\n- **SubstrateRequestException**: Caught and logged with formatted error messages\n- **General Exceptions**: Unexpected errors are logged with full stack traces\n- **Response Processing**: Success/failure determined by response event processing\n\n### 8.3 Comprehensive Logging\n\nError logging includes:\n- Detailed exception information with `exc_info=True`\n- Formatted error messages using `format_error_message()`\n- Debug-level logging for successful operations\n\nSources: \u003CSourceLink text=\"compute/axon.py:258-283\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L258-L283\" />, \u003CSourceLink text=\"compute/axon.py:244-256\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L244-L256\" />, \u003CSourceLink text=\"compute/axon.py:197-201\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/axon.py#L197-L201\" />\n\n## 9. Summary\n\nPrometheus metrics in NI Compute provide essential observability into the decentralized GPU marketplace. The registration system ensures that metrics endpoints are discoverable through the Bittensor blockchain, allowing for comprehensive monitoring of validators and miners. Through version tracking and periodic updates, the system maintains monitoring capabilities even as the software evolves.","src/content/docs/monitoring-and-metrics/prometheus-metrics.mdx","11d8c6e81c966858","miner-system/challenge-response",{"id":214,"data":216,"body":221,"filePath":222,"digest":223,"deferredRender":15},{"title":217,"editUrl":15,"head":218,"template":17,"sidebar":219,"pagefind":15,"draft":19},"Challenge Response",[],{"hidden":19,"attrs":220},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\".env.example\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/.env.example\" />\n\n  \u003CSourceLink text=\"neurons/Miner/pow.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/pow.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pow.py\" />\n\n  \u003CSourceLink text=\"neurons/miner.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document describes the Challenge Response system within NI Compute's miner implementation. The Challenge Response mechanism allows miners to respond to computational challenges issued by validators, primarily for Proof of GPU (PoG) verification. This is one of several verification methods used in the subnet to ensure miners have the hardware capabilities they claim and are operational. For details on how validators score miners based on challenge results, see [Scoring System](/validator-system/scoring-system#2.2).\n\n## Overview\n\nThe Challenge Response system enables validators to verify miners' GPU capabilities by requiring them to solve cryptographic challenges using their hardware. These challenges are designed to:\n\n1. Verify that miners possess the GPU resources they claim\n2. Ensure miners have properly configured systems with working CUDA and hashcat\n3. Provide a standardized benchmark for comparison across different hardware\n4. Prevent fraudulent resource claims by requiring proof of computational power\n\n```mermaid\nflowchart TD\n    subgraph \"Challenge Response System\"\n        V[\"Validator\"] -->|\"1. Issues Challenge\"| M[\"Miner\"]\n        M -->|\"2. Processes with GPU\"| HC[\"Hashcat Process\"]\n        HC -->|\"3. Returns Solution\"| M\n        M -->|\"4. Returns Result\"| V\n        V -->|\"5. Verifies & Scores\"| V\n    end\n\n    subgraph \"Internal Components\"\n        M --- POW[\"run_miner_pow()\"]\n        POW --- RH[\"run_hashcat()\"]\n        RH --- Q[\"Challenge Queue\"]\n    end\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:491-515\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L491-L515\" />, \u003CSourceLink text=\"neurons/Miner/pow.py:175-205\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py#L175-L205\" />\n\n## Challenge Structure\n\nChallenges sent by validators to miners contain specific parameters that define the computational task:\n\n| Parameter | Description |\n|-----------|-------------|\n| `challenge_hash` | The hash that needs to be cracked |\n| `challenge_salt` | Salt value used in the hash generation |\n| `challenge_mode` | Hashcat mode identifier for the algorithm |\n| `challenge_chars` | Available characters for the password |\n| `challenge_mask` | Pattern mask defining password structure |\n| `challenge_difficulty` | Difficulty level of the challenge |\n\nThe challenges are cryptographic problems that require GPU acceleration to solve efficiently, typically involving password recovery for a hashed value.\n\nSources: \u003CSourceLink text=\"neurons/miner.py:491-500\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L491-L500\" />, \u003CSourceLink text=\"neurons/Validator/pow.py:29-72\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pow.py#L29-L72\" />\n\n## Challenge Response Flow\n\nThe Challenge Response process follows a specific sequence of operations between validators and miners:\n\n```mermaid\nsequenceDiagram\n    participant V as Validator\n    participant M as Miner\n    participant HC as Hashcat Process\n    participant Q as Challenge Queue\n\n    V->>M: Send Challenge(hash, salt, mode, chars, mask, difficulty)\n    note over M: Receive challenge with Challenge Protocol\n    M->>Q: Add challenge to queue\n    note over Q: Maintain FIFO order for challenges\n    Q->>HC: Process next challenge when available\n    note over HC: Execute hashcat with GPU acceleration\n    HC-->>M: Return result (password or error)\n    M-->>V: Return response with execution time and result\n    V->>V: Verify and score the response\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:491-515\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L491-L515\" />, \u003CSourceLink text=\"neurons/Miner/pow.py:175-205\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py#L175-L205\" />\n\n## Challenge Handling on Miner Side\n\nWhen a miner receives a challenge, it follows these steps:\n\n1. The miner receives the challenge through the `challenge()` method in the `Miner` class.\n2. The challenge is identified by a run ID constructed from validator ID, difficulty, and portion of the challenge hash.\n3. The challenge is queued in a FIFO manner, ensuring orderly processing of multiple challenges.\n4. When processing begins, the miner calls `run_miner_pow()` which executes `run_hashcat()` with the appropriate parameters.\n5. Hashcat is run on the miner's GPU to attempt to recover the original input from the hash.\n6. Results, including success/failure and execution time, are returned to the validator.\n\nThe miner's challenge handler includes validation to prevent invalid challenges (e.g., difficulty &lt;= 0):\n\n```mermaid\nflowchart TD\n    subgraph \"Miner Challenge Handler\"\n        R[\"Receive Challenge\"] --> V[\"Validate Challenge Parameters\"]\n        V -->|\"Valid\"| Q[\"Queue Challenge\"]\n        V -->|\"Invalid\"| E[\"Return Error\"]\n        \n        Q --> P[\"Process with run_miner_pow()\"]\n        P --> HC[\"Execute Hashcat\"]\n        HC --> S[\"Send Results to Validator\"]\n    end\n\n    subgraph \"Challenge Queue Management\"\n        CR[\"Challenge Received\"] --> CQ[\"Check Queue\"]\n        CQ -->|\"Empty\"| PP[\"Process Immediately\"]\n        CQ -->|\"Not Empty\"| AQ[\"Add to Queue\"]\n        AQ --> WP[\"Wait Processing\"]\n    end\n```\n\nSources: \u003CSourceLink text=\"neurons/miner.py:491-515\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L491-L515\" />, \u003CSourceLink text=\"neurons/Miner/pow.py:29-205\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py#L29-L205\" />\n\n## Hashcat Execution\n\nThe core of the challenge response system uses Hashcat, a popular password recovery tool that leverages GPU acceleration:\n\n1. The miner configures Hashcat with parameters from the challenge:\n   - Hash and salt combination\n   - Attack mode (mode 3 for mask attack)\n   - Device type (2 for CUDA/GPU)\n   - Hash mode (defining the hash algorithm)\n   - Character set and mask\n   - Workload profile and extended options\n\n2. Hashcat is executed as a subprocess with timeout protection to prevent hanging:\n   - Successful execution returns the recovered password\n   - Timeouts or errors are captured and returned as part of the response\n\n3. A queue system ensures challenges are processed sequentially, preventing resource contention:\n   - Challenges are added to a FIFO queue\n   - New challenges wait until currently running challenges complete\n   - Execution time is tracked and returned for performance evaluation\n\nSources: \u003CSourceLink text=\"neurons/Miner/pow.py:43-172\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py#L43-L172\" />\n\n## Challenge Generation (Validator Side)\n\nWhile this document focuses on the miner's perspective, understanding how validators generate challenges provides context:\n\nValidators create challenges using:\n1. Secure random password generation with cryptographic randomness\n2. Hash generation using BLAKE2b algorithm\n3. Configuration of difficulty levels and character sets\n4. Creation of formatted challenges with all necessary parameters\n\nThe generated challenge is designed to be:\n- Verifiable (validator knows the expected answer)\n- Hardware-intensive (requires GPU acceleration)\n- Time-sensitive (expected completion time correlates with hardware capability)\n\nSources: \u003CSourceLink text=\"neurons/Validator/pow.py:29-72\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pow.py#L29-L72\" />\n\n## Priority and Blacklisting\n\nThe Challenge Response system includes priority and blacklisting mechanisms:\n\n1. **Priority Handling**: Challenge requests are prioritized based on:\n   - Validator stake (higher stake = higher priority)\n   - Base priority value (`miner_priority_challenge`)\n   - This ensures that challenges from trusted validators with more stake are processed first\n\n2. **Blacklisting**: Miners can reject challenge requests based on:\n   - Unrecognized hotkeys (validators not registered in the metagraph)\n   - Insufficient stake (below `validator_permit_stake`)\n   - Manually blacklisted validators\n   - Known exploiters (from `SUSPECTED_EXPLOITERS_HOTKEYS` list)\n\nThese mechanisms help protect miners from spam or malicious challenges while ensuring responsiveness to legitimate validators.\n\nSources: \u003CSourceLink text=\"neurons/miner.py:482-488\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L482-L488\" />, \u003CSourceLink text=\"neurons/miner.py:330-373\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L330-L373\" />\n\n## Integration with Miner System\n\nThe Challenge Response system is integrated into the broader miner architecture:\n\n```mermaid\nflowchart TD\n    subgraph \"Miner System\"\n        AX[\"ComputeSubnetAxon\"] -->|\"Receives Requests\"| CR[\"Challenge Response\"]\n        AX -->|\"Receives Requests\"| AL[\"Allocate\"]\n        \n        CR -->|\"Uses\"| POW[\"PoW Functions\"]\n        POW -->|\"Executes\"| HC[\"Hashcat\"]\n        \n        CR -->|\"Reports\"| WB[\"WandB Monitoring\"]\n    end\n    \n    subgraph \"Challenge Response Components\"\n        CH[\"challenge() Method\"] --> BL[\"blacklist_challenge()\"]\n        CH --> PR[\"priority_challenge()\"]\n        CH --> RM[\"run_miner_pow()\"]\n        RM --> RH[\"run_hashcat()\"]\n    end\n```\n\nThe Challenge Response system attaches to the miner's axon to receive requests from validators, alongside other endpoints like allocation requests.\n\nSources: \u003CSourceLink text=\"neurons/miner.py:227-235\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L227-L235\" />, \u003CSourceLink text=\"neurons/miner.py:491-515\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L491-L515\" />\n\n## CUDA Availability Check\n\nBefore processing challenges, miners verify CUDA availability to ensure GPU acceleration works:\n\n```python\ndef check_cuda_availability():\n    import torch\n\n    if torch.cuda.is_available():\n        device_count = torch.cuda.device_count()\n        bt.logging.info(f\"CUDA is available with {device_count} CUDA device(s)!\")\n    else:\n        bt.logging.warning(\n            \"CUDA is not available or not properly configured on this system.\"\n        )\n```\n\nThis check happens during miner initialization, ensuring that the challenge response system can utilize GPU acceleration.\n\nSources: \u003CSourceLink text=\"neurons/Miner/pow.py:31-40\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py#L31-L40\" />\n\n## Performance Considerations\n\nChallenge response performance is a critical factor in miner evaluation:\n\n1. **Execution Time**: The time taken to solve challenges directly influences scoring\n2. **Queue Management**: FIFO queuing ensures fair processing of challenges\n3. **Timeout Handling**: Challenges have a maximum execution time (`pow_timeout`)\n4. **Workload Profile**: Configurable workload profiles allow miners to balance performance vs. system load\n\nMiners can configure several parameters to optimize challenge response:\n- `miner_hashcat_path`: Path to the hashcat executable\n- `miner_hashcat_workload_profile`: Performance vs. system impact balance\n- `miner_hashcat_extended_options`: Additional hashcat options\n\nSources: \u003CSourceLink text=\"neurons/Miner/pow.py:51-172\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py#L51-L172\" />, \u003CSourceLink text=\"neurons/miner.py:170-172\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L170-L172\" />\n\n## Security Considerations\n\nThe Challenge Response system includes several security features:\n\n1. **Timeout Protection**: Prevents validators from hanging miners with impossible challenges\n2. **Blacklisting**: Protects against malicious validators\n3. **Process Isolation**: Hashcat runs as a separate subprocess\n4. **Error Handling**: Robust error handling prevents system crashes\n5. **Queue System**: Prevents resource exhaustion from multiple simultaneous challenges\n\nSources: \u003CSourceLink text=\"neurons/Miner/pow.py:92-172\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py#L92-L172\" />, \u003CSourceLink text=\"neurons/miner.py:330-373\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L330-L373\" />\n\n## Troubleshooting\n\nCommon issues with Challenge Response include:\n\n| Issue | Possible Causes | Solutions |\n|-------|-----------------|-----------|\n| Timeouts | GPU overload, insufficient hardware | Adjust workload profile, upgrade hardware |\n| Errors | Missing/incompatible hashcat, CUDA issues | Check hashcat installation, update drivers |\n| No response | Blacklisting, network issues | Check blacklist settings, network connectivity |\n| Poor performance | Hardware limitations, competing processes | Close other GPU applications, optimize settings |\n\nSources: \u003CSourceLink text=\"neurons/Miner/pow.py:147-165\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/pow.py#L147-L165\" />\n\n## Conclusion\n\nThe Challenge Response system is a critical component in NI Compute's validator-miner relationship, allowing objective verification of GPU capabilities through computational challenges. By requiring miners to solve cryptographic problems with their GPU hardware, validators can ensure that miners possess the resources they claim to have. This mechanism, combined with other verification methods, creates a trustworthy decentralized GPU marketplace.","src/content/docs/miner-system/challenge-response.mdx","8b41e2e979d745bf","miner-system/container-management",{"id":224,"data":226,"body":231,"filePath":232,"digest":233,"deferredRender":15},{"title":227,"editUrl":15,"head":228,"template":17,"sidebar":229,"pagefind":15,"draft":19},"Container Management",[],{"hidden":19,"attrs":230},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"min_compute.yml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/min_compute.yml\" />\n\n  \u003CSourceLink text=\"neurons/Miner/allocate.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py\" />\n\n  \u003CSourceLink text=\"neurons/Miner/container.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py\" />\n\n  \u003CSourceLink text=\"neurons/miner_checker.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner_checker.py\" />\n\n  \u003CSourceLink text=\"tests/test_miner_container.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/tests/test_miner_container.py\" />\n\n  \u003CSourceLink text=\"tests/test_rsa_encryption.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/tests/test_rsa_encryption.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nContainer Management is the core Docker container lifecycle system used by miners to provide isolated compute resources to validators and clients. This system handles the creation, configuration, monitoring, and termination of SSH-enabled containers that serve as the compute environments for resource allocation requests.\n\nFor information about how containers integrate with the broader resource allocation workflow, see [Resource Allocation](/miner-system/resource-allocation#3.3). For details about the communication protocols used during container provisioning, see [Specs, Allocate, and Challenge Protocols](/communication-protocols/specs-allocate-and-challenge-protocols#5.1).\n\n## Container Lifecycle Overview\n\nThe container management system orchestrates Docker containers through a complete lifecycle from base image preparation to final cleanup. Each container is configured with SSH access, GPU capabilities, and custom software environments based on allocation requirements.\n\n```mermaid\nstateDiagram-v2\n    [*] --> ImageBuilding: \"build_sample_container()\"\n    ImageBuilding --> ImageReady: \"ssh-image-base created\"\n    ImageReady --> ContainerCreation: \"run_container()\"\n    ContainerCreation --> ContainerRunning: \"status: created\"\n    ContainerRunning --> ContainerPaused: \"pause_container()\"\n    ContainerPaused --> ContainerRunning: \"unpause_container()\"\n    ContainerRunning --> ContainerStopped: \"kill_container()\"\n    ContainerStopped --> [*]: \"cleanup complete\"\n    ContainerRunning --> KeyExchange: \"exchange_key_container()\"\n    KeyExchange --> ContainerRunning: \"SSH keys updated\"\n    ContainerRunning --> ContainerRestarted: \"restart_container()\"\n    ContainerRestarted --> ContainerRunning: \"restart complete\"\n```\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:57-103\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L57-L103\" />, \u003CSourceLink text=\"neurons/Miner/container.py:384-420\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L384-L420\" />, \u003CSourceLink text=\"neurons/Miner/container.py:421-473\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L421-L473\" />\n\n## Base Image Management\n\nThe system maintains a base container image `ssh-image-base` that provides the foundation for all allocated containers. This image includes SSH server configuration, Python runtime, and GPU support.\n\n### Base Image Construction\n\n```mermaid\nflowchart TD\n    A[\"build_sample_container()\"] --> B[\"Check existing images\"]\n    B --> C{Base image exists?}\n    C -->|Yes| D[\"Return existing\"]\n    C -->|No| E[\"Build from pytorch/pytorch:2.7.0-cuda12.6-cudnn9-runtime\"]\n    E --> F[\"Install SSH server\"]\n    F --> G[\"Configure SSH settings\"]\n    G --> H[\"Install Python packages\"]\n    H --> I[\"Tag as ssh-image-base:latest\"]\n    I --> J[\"Base image ready\"]\n```\n\nThe base image includes:\n- PyTorch CUDA runtime environment\n- SSH server with root access enabled\n- Python 3 and pip package manager\n- Essential build tools and libraries\n- Conda environment configuration\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:280-368\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L280-L368\" />\n\n## Container Creation and Configuration\n\nWhen a resource allocation request is received, the system creates a customized container based on the base image and specific requirements.\n\n### Container Creation Process\n\n```mermaid\nsequenceDiagram\n    participant AC as \"Allocation Controller\"\n    participant CM as \"Container Manager\"\n    participant DC as \"Docker Client\"\n    participant FS as \"File System\"\n    \n    AC->>CM: \"run_container(cpu_usage, ram_usage, gpu_usage, public_key, docker_requirement, testing)\"\n    CM->>CM: \"kill_container()\" \n    CM->>CM: \"password_generator(10)\"\n    CM->>CM: \"build_sample_container()\"\n    CM->>FS: \"Create Dockerfile with custom requirements\"\n    CM->>DC: \"images.build(path, dockerfile, tag=ssh-image)\"\n    CM->>DC: \"containers.run(image=ssh-image, device_requests=[GPU], ports={22: ssh_port})\"\n    DC-->>CM: \"Container created\"\n    CM->>CM: \"rsa.encrypt_data(public_key, connection_info)\"\n    CM->>FS: \"Write allocation_key file\"\n    CM-->>AC: \"Return encrypted connection info\"\n```\n\n### Container Configuration Parameters\n\n| Parameter | Description | Example |\n|-----------|-------------|---------|\n| `cpu_assignment` | CPU cores allocated | `\"0-1\"` for 2 cores |\n| `ram_limit` | Memory limit | `\"5g\"` for 5GB |\n| `hard_disk_capacity` | Storage limit | `\"100g\"` for 100GB |\n| `gpu_capacity` | GPU allocation | `\"all\"` for all GPUs |\n| `ssh_port` | SSH access port | `4444` |\n| `shm_size` | Shared memory size | `\"7g\"` (90% of available) |\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:105-207\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L105-L207\" />\n\n## Security and Access Control\n\nThe container management system implements a multi-layered security model using RSA encryption and allocation key verification.\n\n### Security Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Client Side\"\n        CL[\"Client\"] \n        PRV[\"Private Key\"]\n        PUB[\"Public Key\"]\n    end\n    \n    subgraph \"Miner Container System\"\n        AK[\"allocation_key file\"]\n        CM[\"Container Manager\"]\n        SSH[\"SSH Container\"]\n    end\n    \n    subgraph \"Authentication Flow\"\n        ENC[\"Encrypted Connection Info\"]\n        DEC[\"Decrypted Credentials\"]\n    end\n    \n    CL --> PUB\n    PUB --> CM\n    CM --> ENC\n    ENC --> DEC\n    PRV --> DEC\n    DEC --> SSH\n    \n    CM --> AK\n    AK --> CM\n```\n\n### Allocation Key Management\n\nThe system uses allocation keys to verify container access permissions:\n\n- **Key Storage**: Public keys are base64-encoded and stored in `allocation_key` file\n- **Key Verification**: All container operations require matching public key\n- **Key Rotation**: SSH keys can be updated through `exchange_key_container()`\n\n### Security Functions\n\n| Function | Purpose | Key Verification |\n|----------|---------|------------------|\n| `restart_container()` | Restart existing container | Required |\n| `pause_container()` | Pause container execution | Required |\n| `unpause_container()` | Resume container execution | Required |\n| `exchange_key_container()` | Update SSH keys | Required |\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:370-382\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L370-L382\" />, \u003CSourceLink text=\"neurons/Miner/container.py:384-420\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L384-L420\" />, \u003CSourceLink text=\"neurons/Miner/container.py:475-521\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L475-L521\" />\n\n## Integration with Allocation System\n\nContainer management integrates with the resource allocation system through the `allocate.py` module, which orchestrates container lifecycle during allocation requests.\n\n### Allocation Integration Flow\n\n```mermaid\nsequenceDiagram\n    participant API as \"RegisterAPI\"\n    participant ALLOC as \"Allocation Controller\"\n    participant CONT as \"Container Manager\"\n    participant SCHED as \"Scheduler\"\n    \n    API->>ALLOC: \"register_allocation(timeline, device_requirement, public_key, docker_requirement)\"\n    ALLOC->>CONT: \"kill_container()\" \n    ALLOC->>CONT: \"run_container(cpu_usage, ram_usage, hard_disk_usage, gpu_usage, public_key, docker_requirement, testing)\"\n    CONT-->>ALLOC: \"Container info + encrypted credentials\"\n    ALLOC->>SCHED: \"start(timeline)\"\n    ALLOC-->>API: \"Allocation result\"\n    \n    Note over SCHED: \"Timeline expires\"\n    SCHED->>CONT: \"kill_container(deregister=True)\"\n```\n\n### Container Types\n\nThe system supports two container types:\n\n- **Production Containers** (`container_name`): Long-running containers for actual resource allocation\n- **Test Containers** (`container_name_test`): Short-lived containers for validation and health checks\n\nSources: \u003CSourceLink text=\"neurons/Miner/allocate.py:29-62\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py#L29-L62\" />, \u003CSourceLink text=\"neurons/Miner/allocate.py:66-94\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py#L66-L94\" />\n\n## Container Monitoring and Health Checks\n\nThe system provides container status monitoring and health checking capabilities used by validators and the allocation system.\n\n### Health Check Functions\n\n```mermaid\nflowchart LR\n    subgraph \"Health Check Operations\"\n        A[\"check_container()\"] --> B{Container exists?}\n        B -->|Yes| C{Status == running?}\n        B -->|No| D[\"Return False\"]\n        C -->|Yes| E[\"Return True\"]\n        C -->|No| D\n    end\n    \n    subgraph \"Allocation Status\"\n        F[\"check_if_allocated(public_key)\"] --> G{allocation_key exists?}\n        G -->|Yes| H{Key matches?}\n        G -->|No| I[\"Return False\"]\n        H -->|Yes| J{Container running?}\n        H -->|No| I\n        J -->|Yes| K[\"Return True\"]\n        J -->|No| I\n    end\n```\n\n### Monitoring Integration\n\nThe container system integrates with validator monitoring through:\n\n- **Miner Checker**: Validators use `miner_checker.py` to test container allocation and SSH access\n- **Health Endpoints**: API endpoints query container status for resource availability\n- **Allocation Tracking**: Container state is synchronized with allocation records\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:210-222\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L210-L222\" />, \u003CSourceLink text=\"neurons/Miner/allocate.py:106-137\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py#L106-L137\" />, \u003CSourceLink text=\"neurons/miner_checker.py:85-151\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner_checker.py#L85-L151\" />\n\n## Container Cleanup and Resource Management\n\nThe system implements comprehensive cleanup procedures to prevent resource leaks and ensure proper container lifecycle management.\n\n### Cleanup Operations\n\n```mermaid\ngraph TD\n    A[\"kill_container(deregister)\"] --> B{deregister flag?}\n    B -->|True| C[\"Kill production container\"]\n    B -->|False| D[\"Kill test container only\"]\n    C --> E[\"Find container_name\"]\n    D --> F[\"Find container_name_test\"]\n    E --> G{Container running?}\n    F --> H{Container running?}\n    G -->|Yes| I[\"exec_run('kill -15 1')\"]\n    G -->|No| J[\"remove()\"]\n    H -->|Yes| K[\"exec_run('kill -15 1')\"]\n    H -->|No| L[\"remove()\"]\n    I --> M[\"wait()\"]\n    K --> N[\"wait()\"]\n    M --> J\n    N --> L\n    J --> O[\"images.prune(dangling:True)\"]\n    L --> O\n```\n\nThe cleanup process includes:\n- Graceful container termination using SIGTERM\n- Container removal from Docker\n- Dangling image cleanup\n- Allocation key file management\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:57-103\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L57-L103\" />","src/content/docs/miner-system/container-management.mdx","0de42ff74d7b0ed9","miner-system/resource-allocation",{"id":234,"data":236,"body":241,"filePath":242,"digest":243,"deferredRender":15},{"title":237,"editUrl":15,"head":238,"template":17,"sidebar":239,"pagefind":15,"draft":19},"Resource Allocation",[],{"hidden":19,"attrs":240},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/Miner/allocate.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py\" />\n\n  \u003CSourceLink text=\"neurons/Miner/container.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py\" />\n\n  \u003CSourceLink text=\"neurons/miner.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py\" />\n\n  \u003CSourceLink text=\"neurons/miner_checker.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner_checker.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document covers how miners in the NI Compute Subnet handle resource allocation requests from validators and external clients. Resource allocation involves provisioning Docker containers with specified compute resources (CPU, RAM, GPU, storage) and providing secure SSH access to allocated environments.\n\nFor information about the Resource Allocation API that external clients use to request resources, see [Resource Allocation API](/resource-allocation-api#4). For details about container lifecycle management, see [Container Management](/miner-system/container-management#3.1).\n\n## Allocation Request Processing\n\nThe miner's resource allocation system is built around the `Allocate` synapse protocol. When a validator or client sends an allocation request, the miner processes it through several stages:\n\n```mermaid\nflowchart TD\n    A[\"Allocate Synapse Request\"] --> B[\"blacklist_allocate()\"]\n    B --> C{\"Blacklisted?\"}\n    C -->|Yes| D[\"Reject Request\"]\n    C -->|No| E[\"priority_allocate()\"]\n    E --> F[\"allocate() Method\"]\n    F --> G{\"Checking Mode?\"}\n    G -->|Yes| H[\"check_allocation()\"]\n    G -->|No| I{\"Timeline > 0?\"}\n    I -->|Yes| J[\"register_allocation()\"]\n    I -->|No| K[\"deregister_allocation()\"]\n    H --> L[\"Return Status\"]\n    J --> M[\"run_container()\"]\n    K --> N[\"kill_container()\"]\n    M --> O[\"Update WandB State\"]\n    N --> O\n    O --> L\n```\n\n**Allocation Request Flow**\n\nSources: \u003CSourceLink text=\"neurons/miner.py:419-479\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L419-L479\" />, \u003CSourceLink text=\"neurons/miner.py:397-403\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L397-L403\" />\n\nThe `allocate` method in the `Miner` class handles three types of operations:\n\n| Operation Type | Condition | Action |\n|---------------|-----------|---------|\n| **Check Allocation** | `checking=True, timeline>0` | Verify resource availability without allocating |\n| **Register Allocation** | `checking=False, timeline>0` | Create new resource allocation |\n| **Deregister Allocation** | `checking=False, timeline=0` | Remove existing allocation |\n\n## Resource Registration Process\n\nWhen a miner receives a valid allocation request with `timeline &gt; 0`, it initiates the resource registration process:\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator/Client\"\n    participant M as \"Miner.allocate()\"\n    participant A as \"register_allocation()\"\n    participant C as \"Container Management\"\n    participant D as \"Docker Engine\"\n    participant S as \"Schedule Manager\"\n    \n    V->>M: \"Allocate(timeline=3600, device_requirement={...})\"\n    M->>A: \"register_allocation(timeline, device_requirement, public_key)\"\n    A->>C: \"kill_container() - cleanup existing\"\n    C->>D: \"Remove existing containers\"\n    A->>C: \"run_container(cpu_usage, ram_usage, gpu_usage, ...)\"\n    C->>D: \"Create container with resource limits\"\n    D->>C: \"Return container info + encrypted SSH details\"\n    C->>A: \"Return allocation status + connection info\"\n    A->>S: \"start(timeline) - schedule auto-deallocation\"\n    A->>M: \"Return allocation result\"\n    M->>V: \"Return Allocate synapse with status + SSH info\"\n```\n\n**Resource Registration Sequence**\n\nSources: \u003CSourceLink text=\"neurons/Miner/allocate.py:29-62\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py#L29-L62\" />, \u003CSourceLink text=\"neurons/miner.py:463-476\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L463-L476\" />\n\nThe registration process transforms device requirements into Docker container configurations:\n\n```mermaid\ngraph LR\n    A[\"Device Requirements\"] --> B[\"Resource Parsing\"]\n    B --> C[\"Docker Configuration\"]\n    C --> D[\"Container Creation\"]\n    \n    subgraph \"Resource Parsing\"\n        B1[\"CPU Count → CPU Assignment\"]\n        B2[\"RAM Capacity → Memory Limit\"]\n        B3[\"Disk Capacity → Storage Limit\"]\n        B4[\"GPU Capacity → Device Requests\"]\n    end\n    \n    subgraph \"Container Creation\"\n        D1[\"build_sample_container()\"]\n        D2[\"Generate SSH Credentials\"]\n        D3[\"Create Dockerfile\"]\n        D4[\"Run Container with Limits\"]\n        D5[\"Return Encrypted Connection Info\"]\n    end\n    \n    B --> B1\n    B --> B2\n    B --> B3\n    B --> B4\n    \n    C --> D1\n    C --> D2\n    C --> D3\n    C --> D4\n    C --> D5\n```\n\n**Resource Requirement Processing**\n\nSources: \u003CSourceLink text=\"neurons/Miner/allocate.py:34-51\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py#L34-L51\" />, \u003CSourceLink text=\"neurons/Miner/container.py:105-207\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L105-L207\" />\n\n## Container Resource Management\n\nThe container management system translates abstract resource requirements into concrete Docker container limits:\n\n### Resource Limit Translation\n\n| Resource Type | Input Format | Docker Configuration | Implementation |\n|--------------|--------------|---------------------|----------------|\n| **CPU** | `{\"count\": 2}` | `cpuset_cpus=\"0-1\"` | \u003CSourceLink text=\"container.py:110-137\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/container.py#L110-L137\" /> |\n| **RAM** | `{\"capacity\": 5368709120}` | `mem_limit=\"5g\"` | \u003CSourceLink text=\"container.py:111-133\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/container.py#L111-L133\" /> |\n| **GPU** | `{\"capacity\": \"all\"}` | `device_requests=[DeviceRequest(...)]` | \u003CSourceLink text=\"container.py:167-175\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/container.py#L167-L175\" /> |\n| **Storage** | `{\"capacity\": 107374182400}` | Volume mount limits | \u003CSourceLink text=\"container.py:112-149\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/container.py#L112-L149\" /> |\n\n### Container Lifecycle Operations\n\nThe system supports several container management operations beyond basic allocation:\n\n```mermaid\nstateDiagram-v2\n    [*] --> Available: \"No Container\"\n    Available --> Creating: \"register_allocation()\"\n    Creating --> Running: \"Container Created\"\n    Running --> Paused: \"pause_container()\"\n    Paused --> Running: \"unpause_container()\"\n    Running --> Running: \"restart_container()\"\n    Running --> Available: \"deregister_allocation()\"\n    Running --> Available: \"Timeline Expired\"\n    \n    note right of Running: \"SSH Access Available\\nResource Limits Applied\"\n    note right of Paused: \"Container Suspended\\nResources Released\"\n```\n\n**Container State Management**\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:421-520\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L421-L520\" />, \u003CSourceLink text=\"neurons/miner.py:437-458\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L437-L458\" />\n\n## Security and Access Control\n\nResource allocation implements multiple security layers:\n\n### Public Key Authentication\n\nAll allocation operations require RSA public key authentication:\n\n```mermaid\nflowchart LR\n    A[\"Client Public Key\"] --> B[\"Allocation Request\"]\n    B --> C[\"Container Creation\"]\n    C --> D[\"SSH Key Setup\"]\n    D --> E[\"Connection Info Encryption\"]\n    E --> F[\"Encrypted Response\"]\n    \n    subgraph \"Storage\"\n        G[\"allocation_key file\"]\n        H[\"Base64 Encoded Public Key\"]\n    end\n    \n    A --> G\n    G --> H\n    H --> I[\"Access Validation\"]\n    I --> J[\"Container Operations\"]\n```\n\n**Public Key Authentication Flow**\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:188-200\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L188-L200\" />, \u003CSourceLink text=\"neurons/Miner/allocate.py:74-77\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py#L74-L77\" />\n\n### SSH Access Management\n\nThe system provides secure SSH access to allocated containers:\n\n| Operation | Function | Security Check |\n|-----------|----------|----------------|\n| **Key Exchange** | `exchange_key_container()` | Public key validation |\n| **Container Restart** | `restart_container()` | Allocation key verification |\n| **Container Pause** | `pause_container()` | Authentication required |\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:475-520\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L475-L520\" />, \u003CSourceLink text=\"neurons/Miner/container.py:384-419\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L384-L419\" />\n\n## Allocation State Management\n\nThe miner maintains allocation state through multiple mechanisms:\n\n### Local State Storage\n\n```mermaid\ngraph TD\n    A[\"Allocation Request\"] --> B[\"allocation_key File\"]\n    B --> C[\"Base64 Encoded Public Key\"]\n    C --> D[\"Access Validation\"]\n    \n    E[\"Container Status\"] --> F[\"Docker API Queries\"]\n    F --> G[\"check_container()\"]\n    \n    H[\"WandB Integration\"] --> I[\"update_allocated()\"]\n    I --> J[\"Distributed State Sync\"]\n    \n    subgraph \"State Validation\"\n        K[\"check_if_allocated()\"]\n        L[\"File Existence Check\"]\n        M[\"Key Comparison\"]\n        N[\"Container Running Check\"]\n    end\n    \n    D --> K\n    G --> K\n    K --> L\n    K --> M\n    K --> N\n```\n\n**Allocation State Management**\n\nSources: \u003CSourceLink text=\"neurons/Miner/allocate.py:106-137\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py#L106-L137\" />, \u003CSourceLink text=\"neurons/miner.py:405-417\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/miner.py#L405-L417\" />\n\n### Automatic Deallocation\n\nThe system includes automatic resource cleanup through timeline-based scheduling:\n\n```mermaid\nsequenceDiagram\n    participant A as \"register_allocation()\"\n    participant S as \"Schedule Manager\"\n    participant T as \"Timer Thread\"\n    participant C as \"Container Manager\"\n    \n    A->>S: \"start(timeline=3600)\"\n    S->>T: \"Create timer for 3600 seconds\"\n    T-->>T: \"Wait for timeline expiry\"\n    T->>C: \"kill_container() after timeout\"\n    C->>C: \"Remove container and cleanup\"\n    \n    Note over T: \"Timeline-based auto-cleanup\\nPrevents resource leaks\"\n```\n\n**Automatic Deallocation Timeline**\n\nSources: \u003CSourceLink text=\"neurons/Miner/allocate.py:57\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/allocate.py#L57\" />, \u003CSourceLink text=\"neurons/Miner/schedule.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/schedule.py\" />\n\n## Docker Integration\n\nThe allocation system builds upon Docker containers with specific configurations:\n\n### Base Container Setup\n\nThe system uses a pre-built base image (`ssh-image-base`) for faster allocation:\n\n```mermaid\ngraph LR\n    A[\"build_sample_container()\"] --> B[\"pytorch/pytorch:2.7.0-cuda12.6-cudnn9-runtime\"]\n    B --> C[\"Install SSH Server\"]\n    C --> D[\"Configure SSH Settings\"]\n    D --> E[\"Install Python Dependencies\"]\n    E --> F[\"ssh-image-base:latest\"]\n    \n    subgraph \"Runtime Container\"\n        G[\"Custom Dockerfile\"]\n        H[\"User SSH Keys\"]\n        I[\"Resource Limits\"]\n        J[\"ssh-image:latest\"]\n    end\n    \n    F --> G\n    G --> H\n    H --> I\n    I --> J\n```\n\n**Container Image Pipeline**\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:280-368\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L280-L368\" />, \u003CSourceLink text=\"neurons/Miner/container.py:136-159\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L136-L159\" />\n\n### Container Configuration\n\nEach allocation creates a customized container with:\n\n- **SSH Access**: Root user with password and key-based authentication\n- **GPU Support**: NVIDIA GPU access through device requests\n- **Resource Limits**: CPU, memory, and storage constraints\n- **Custom Environment**: User-specified Docker commands and dependencies\n\nSources: \u003CSourceLink text=\"neurons/Miner/container.py:170-181\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L170-L181\" />, \u003CSourceLink text=\"neurons/Miner/container.py:136-146\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/container.py#L136-L146\" />","src/content/docs/miner-system/resource-allocation.mdx","f634d3a1664ae97f","neural-internet/ai_safety",{"id":244,"data":246,"body":251,"filePath":252,"digest":253,"rendered":254},{"title":247,"editUrl":15,"head":248,"template":17,"sidebar":249,"pagefind":15,"draft":19},"AI Safety",[],{"hidden":19,"attrs":250},{},"### An AI Oligopoly is Shaping\n\nHumanity's collective knowledge is increasingly managed, curated, and disseminated by artificial intelligence. Entities controlling access to this AI possess significant influence over our shared information. They have the capacity to shape, manipulate, and potentially censor the intelligence that reaches the public.\n\n**Tackling the Challenges of Centralized AI**\n\nCentralized AI systems present numerous challenges:\n\n* **Monopoly and Regulatory Capture**: Centralized AI risks leading to monopolistic control, stifling innovation and limiting diversity in AI development.\n* **Privacy Concerns**: These systems often require extensive data collection, raising privacy and security risks.\n* **Bias and Discrimination**: Centralized AI can propagate biases, making them more difficult to address.\n* **Lack of Transparency and Accountability**: Decision-making in centralized systems can be opaque, reducing accountability.\n* **Dependency and Vulnerability**: Reliance on centralized AI creates vulnerabilities, including potential failures or attacks.\n* **Limited Customization and Flexibility**: Such AI often lacks the ability to meet diverse, localized needs.\n* **Ethical and Economic Disparities**: Centralized control raises ethical questions and can exacerbate economic inequalities.\n* **Security Risks**: They present high-value targets for cyber attacks and are susceptible to catastrophic failures.\n* **Slow Adaptation**: Centralized systems may struggle to rapidly adapt to new information or changes.\n\n## Distributed AI is Responsible AI\n\nIn the rapidly evolving landscape of artificial intelligence, the debate between centralized and decentralized AI has become increasingly relevant. Centralized AI, dominated by a few tech giants, has raised concerns about monopoly, privacy, and ethical issues. In contrast, decentralized AI offers a promising alternative, championing a more democratic, transparent, and equitable approach. Decentralized AI not only addresses the limitations of centralized systems but also paves the way for a more responsible and safe AI future.\n\n### Breaking the Monopoly: Decentralized AI as a Democratic Force\n\nCentralized AI systems often result in monopolistic control, where a handful of companies hold significant sway over AI’s development and application. This concentration of power risks regulatory capture, where regulations serve corporate interests over the public good. Decentralized AI, on the other hand, distributes control across a diverse set of participants, preventing any single entity from wielding excessive influence. This democratization fosters a competitive environment, spurring innovation and preventing monopolistic practices.\n\n### Safeguarding Privacy in the AI Era\n\nPrivacy concerns are paramount in the age of AI. Centralized AI systems, which aggregate vast amounts of data in the hands of a few, pose significant privacy risks. Decentralized AI powered by blockchain can ensure secure, anonymous data handling, thus enhancing user privacy and data security.\n\n### Tackling Bias and Discrimination\n\nBias in AI is a critical issue, often stemming from non-representative training datasets in centralized systems. Decentralized AI can leverage diverse datasets, reducing inherent biases and promoting fairness in AI outcomes. This diversity in data sources is key to developing equitable AI systems that reflect a wide range of human experiences and perspectives.\n\n### Ensuring Transparency and Accountability\n\nThe opaque nature of centralized AI often leads to a lack of transparency and accountability. Decentralized AI, by its very nature, promotes openness in algorithmic decision-making. Open-source models and collaborative development practices enhance scrutiny and accountability, ensuring AI actions align with societal values and ethical standards.\n\n### Reducing Dependency and Increasing System Robustness\n\nCentralized AI creates dependency on specific providers and introduces central points of failure, making systems more vulnerable to attacks and outages. Decentralized AI reduces this dependency, offering increased robustness and resilience. Distributed networks are less susceptible to single-point failures, ensuring more consistent and reliable AI operations.\n\n### Customization and Flexibility: Meeting Diverse Needs\n\nCentralized AI often provides generic solutions that may not meet the specific needs of all users. Decentralized AI, in contrast, allows for greater customization and flexibility. It can adapt to individual or community-specific requirements, ensuring that AI applications are more aligned with diverse user needs and ethical considerations.\n\n### Economic Equity and Reducing Disparities\n\nCentralized AI can exacerbate economic inequalities, with large corporations reaping disproportionate benefits. Decentralized AI, however, offers a more equitable distribution of AI’s economic benefits. It provides opportunities for smaller players, fostering innovation and reducing economic disparities.\n\n### Security: A Multi-Faceted Approach\n\nIn terms of security, decentralized AI offers a distinct advantage. While centralized systems present lucrative targets for malicious attacks, decentralized networks distribute risk, making them less attractive and harder to compromise. This distributed risk model is crucial for ensuring the safety and security of AI systems.\n\n## Responsible AI Powered By Bittensor\n\nBittensor is solving AI Safety by decentralizing the creation and ownership of machine intelligence. Democratizing the access and creation of machine intelligence ensures that the power of this transformative technology is not held in a select few corporations. Bittensor brings us open-access and open-ownership where intelligence is uncensored, unbiased and aligned to all.\n\nNeural Internet promises to build with Responsible AI through collaboration. By providing access to the shared collective knowledge that Bittensor is, we bring ownership and governance of AI to the collective. Open-Source and collaborative development will outpace Centralized AI.\n\n### Embracing a Decentralized AI Future\n\nDecentralized AI represents a paradigm shift towards a more responsible, safe, and equitable approach to AI. By addressing the significant concerns associated with centralized AI, such as monopoly, privacy breaches, bias, and lack of transparency, decentralized AI paves the way for a future where AI benefits are widely and fairly distributed. As we stand at the cusp of this AI revolution, it is imperative to steer the technology in a direction that aligns with democratic values, ethical standards, and societal needs. The future of AI should not be dictated by a few but shaped by the many, and decentralized AI offers the blueprint to achieve this ideal.","src/content/docs/neural internet/ai_safety.md","848af655bae064fb",{"html":255,"metadata":256},"\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"an-ai-oligopoly-is-shaping\">An AI Oligopoly is Shaping\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#an-ai-oligopoly-is-shaping\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “An AI Oligopoly is Shaping”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Humanity’s collective knowledge is increasingly managed, curated, and disseminated by artificial intelligence. Entities controlling access to this AI possess significant influence over our shared information. They have the capacity to shape, manipulate, and potentially censor the intelligence that reaches the public.\u003C/p>\n\u003Cp>\u003Cstrong>Tackling the Challenges of Centralized AI\u003C/strong>\u003C/p>\n\u003Cp>Centralized AI systems present numerous challenges:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Monopoly and Regulatory Capture\u003C/strong>: Centralized AI risks leading to monopolistic control, stifling innovation and limiting diversity in AI development.\u003C/li>\n\u003Cli>\u003Cstrong>Privacy Concerns\u003C/strong>: These systems often require extensive data collection, raising privacy and security risks.\u003C/li>\n\u003Cli>\u003Cstrong>Bias and Discrimination\u003C/strong>: Centralized AI can propagate biases, making them more difficult to address.\u003C/li>\n\u003Cli>\u003Cstrong>Lack of Transparency and Accountability\u003C/strong>: Decision-making in centralized systems can be opaque, reducing accountability.\u003C/li>\n\u003Cli>\u003Cstrong>Dependency and Vulnerability\u003C/strong>: Reliance on centralized AI creates vulnerabilities, including potential failures or attacks.\u003C/li>\n\u003Cli>\u003Cstrong>Limited Customization and Flexibility\u003C/strong>: Such AI often lacks the ability to meet diverse, localized needs.\u003C/li>\n\u003Cli>\u003Cstrong>Ethical and Economic Disparities\u003C/strong>: Centralized control raises ethical questions and can exacerbate economic inequalities.\u003C/li>\n\u003Cli>\u003Cstrong>Security Risks\u003C/strong>: They present high-value targets for cyber attacks and are susceptible to catastrophic failures.\u003C/li>\n\u003Cli>\u003Cstrong>Slow Adaptation\u003C/strong>: Centralized systems may struggle to rapidly adapt to new information or changes.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"distributed-ai-is-responsible-ai\">Distributed AI is Responsible AI\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#distributed-ai-is-responsible-ai\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Distributed AI is Responsible AI”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>In the rapidly evolving landscape of artificial intelligence, the debate between centralized and decentralized AI has become increasingly relevant. Centralized AI, dominated by a few tech giants, has raised concerns about monopoly, privacy, and ethical issues. In contrast, decentralized AI offers a promising alternative, championing a more democratic, transparent, and equitable approach. Decentralized AI not only addresses the limitations of centralized systems but also paves the way for a more responsible and safe AI future.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"breaking-the-monopoly-decentralized-ai-as-a-democratic-force\">Breaking the Monopoly: Decentralized AI as a Democratic Force\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#breaking-the-monopoly-decentralized-ai-as-a-democratic-force\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Breaking the Monopoly: Decentralized AI as a Democratic Force”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Centralized AI systems often result in monopolistic control, where a handful of companies hold significant sway over AI’s development and application. This concentration of power risks regulatory capture, where regulations serve corporate interests over the public good. Decentralized AI, on the other hand, distributes control across a diverse set of participants, preventing any single entity from wielding excessive influence. This democratization fosters a competitive environment, spurring innovation and preventing monopolistic practices.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"safeguarding-privacy-in-the-ai-era\">Safeguarding Privacy in the AI Era\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#safeguarding-privacy-in-the-ai-era\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Safeguarding Privacy in the AI Era”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Privacy concerns are paramount in the age of AI. Centralized AI systems, which aggregate vast amounts of data in the hands of a few, pose significant privacy risks. Decentralized AI powered by blockchain can ensure secure, anonymous data handling, thus enhancing user privacy and data security.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"tackling-bias-and-discrimination\">Tackling Bias and Discrimination\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#tackling-bias-and-discrimination\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Tackling Bias and Discrimination”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Bias in AI is a critical issue, often stemming from non-representative training datasets in centralized systems. Decentralized AI can leverage diverse datasets, reducing inherent biases and promoting fairness in AI outcomes. This diversity in data sources is key to developing equitable AI systems that reflect a wide range of human experiences and perspectives.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"ensuring-transparency-and-accountability\">Ensuring Transparency and Accountability\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#ensuring-transparency-and-accountability\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Ensuring Transparency and Accountability”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The opaque nature of centralized AI often leads to a lack of transparency and accountability. Decentralized AI, by its very nature, promotes openness in algorithmic decision-making. Open-source models and collaborative development practices enhance scrutiny and accountability, ensuring AI actions align with societal values and ethical standards.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"reducing-dependency-and-increasing-system-robustness\">Reducing Dependency and Increasing System Robustness\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#reducing-dependency-and-increasing-system-robustness\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Reducing Dependency and Increasing System Robustness”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Centralized AI creates dependency on specific providers and introduces central points of failure, making systems more vulnerable to attacks and outages. Decentralized AI reduces this dependency, offering increased robustness and resilience. Distributed networks are less susceptible to single-point failures, ensuring more consistent and reliable AI operations.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"customization-and-flexibility-meeting-diverse-needs\">Customization and Flexibility: Meeting Diverse Needs\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#customization-and-flexibility-meeting-diverse-needs\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Customization and Flexibility: Meeting Diverse Needs”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Centralized AI often provides generic solutions that may not meet the specific needs of all users. Decentralized AI, in contrast, allows for greater customization and flexibility. It can adapt to individual or community-specific requirements, ensuring that AI applications are more aligned with diverse user needs and ethical considerations.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"economic-equity-and-reducing-disparities\">Economic Equity and Reducing Disparities\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#economic-equity-and-reducing-disparities\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Economic Equity and Reducing Disparities”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Centralized AI can exacerbate economic inequalities, with large corporations reaping disproportionate benefits. Decentralized AI, however, offers a more equitable distribution of AI’s economic benefits. It provides opportunities for smaller players, fostering innovation and reducing economic disparities.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"security-a-multi-faceted-approach\">Security: A Multi-Faceted Approach\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#security-a-multi-faceted-approach\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Security: A Multi-Faceted Approach”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>In terms of security, decentralized AI offers a distinct advantage. While centralized systems present lucrative targets for malicious attacks, decentralized networks distribute risk, making them less attractive and harder to compromise. This distributed risk model is crucial for ensuring the safety and security of AI systems.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"responsible-ai-powered-by-bittensor\">Responsible AI Powered By Bittensor\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#responsible-ai-powered-by-bittensor\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Responsible AI Powered By Bittensor”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Bittensor is solving AI Safety by decentralizing the creation and ownership of machine intelligence. Democratizing the access and creation of machine intelligence ensures that the power of this transformative technology is not held in a select few corporations. Bittensor brings us open-access and open-ownership where intelligence is uncensored, unbiased and aligned to all.\u003C/p>\n\u003Cp>Neural Internet promises to build with Responsible AI through collaboration. By providing access to the shared collective knowledge that Bittensor is, we bring ownership and governance of AI to the collective. Open-Source and collaborative development will outpace Centralized AI.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"embracing-a-decentralized-ai-future\">Embracing a Decentralized AI Future\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#embracing-a-decentralized-ai-future\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Embracing a Decentralized AI Future”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Decentralized AI represents a paradigm shift towards a more responsible, safe, and equitable approach to AI. By addressing the significant concerns associated with centralized AI, such as monopoly, privacy breaches, bias, and lack of transparency, decentralized AI paves the way for a future where AI benefits are widely and fairly distributed. As we stand at the cusp of this AI revolution, it is imperative to steer the technology in a direction that aligns with democratic values, ethical standards, and societal needs. The future of AI should not be dictated by a few but shaped by the many, and decentralized AI offers the blueprint to achieve this ideal.\u003C/p>",{"headings":257,"localImagePaths":296,"remoteImagePaths":297,"frontmatter":298,"imagePaths":299},[258,262,266,269,272,275,278,281,284,287,290,293],{"depth":259,"slug":260,"text":261},3,"an-ai-oligopoly-is-shaping","An AI Oligopoly is Shaping",{"depth":263,"slug":264,"text":265},2,"distributed-ai-is-responsible-ai","Distributed AI is Responsible AI",{"depth":259,"slug":267,"text":268},"breaking-the-monopoly-decentralized-ai-as-a-democratic-force","Breaking the Monopoly: Decentralized AI as a Democratic Force",{"depth":259,"slug":270,"text":271},"safeguarding-privacy-in-the-ai-era","Safeguarding Privacy in the AI Era",{"depth":259,"slug":273,"text":274},"tackling-bias-and-discrimination","Tackling Bias and Discrimination",{"depth":259,"slug":276,"text":277},"ensuring-transparency-and-accountability","Ensuring Transparency and Accountability",{"depth":259,"slug":279,"text":280},"reducing-dependency-and-increasing-system-robustness","Reducing Dependency and Increasing System Robustness",{"depth":259,"slug":282,"text":283},"customization-and-flexibility-meeting-diverse-needs","Customization and Flexibility: Meeting Diverse Needs",{"depth":259,"slug":285,"text":286},"economic-equity-and-reducing-disparities","Economic Equity and Reducing Disparities",{"depth":259,"slug":288,"text":289},"security-a-multi-faceted-approach","Security: A Multi-Faceted Approach",{"depth":263,"slug":291,"text":292},"responsible-ai-powered-by-bittensor","Responsible AI Powered By Bittensor",{"depth":259,"slug":294,"text":295},"embracing-a-decentralized-ai-future","Embracing a Decentralized AI Future",[],[],{"title":247},[],"miner-system/ni_compue_subnet_miner_setup",{"id":300,"data":302,"body":307,"filePath":308,"digest":309,"rendered":310},{"title":303,"editUrl":15,"head":304,"template":17,"sidebar":305,"pagefind":15,"draft":19},"NI Compuτe: Subnet Miner Setup",[],{"hidden":19,"attrs":306},{},"## 1. INTRODUCTION\n\nFor miners interested in joining this innovative network, Subnet 27 offers the opportunity to contribute computing resources and **earn $NI** in return. This guide is structured to provide a comprehensive breakdown of how you can get started with contributing to Bittensor’s commodity markets using your compute power.\n\n## **Decentralizing Compute**\n\nNI Compute decentralizes computing resources by combining siloed pools of compute on a blockchain to be validated and accessed trustlessly. This opens a door to scalable compute without the constraints of centralized power.\n\n![](https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2Fcontent.gitbook.com%2Fcontent%2FRCFZhMrQpz0DM7pJi6mq%2Fblobs%2FizrLCHm4g9RaPDSFREa4%2Fcompute1.png&width=768&dpr=4&quality=100&sign=960b335e&sv=2)\n\nSubnetwork 27\n\n### Powered By Bittensor\n\nNI Compute brings, arguably, the most important and finite resource needed for the creation of machine intelligence. All network participants will have access to an ever-expanding pool of compute for all development needs.\n\n**What is a decentralized supercomputer without access to permissionless compute?**\n\n![](https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2Fcontent.gitbook.com%2Fcontent%2FRCFZhMrQpz0DM7pJi6mq%2Fblobs%2Fw1R7MxeZMqrsQ4sz1opq%2Fcompute2.png&width=768&dpr=4&quality=100&sign=a68e2e77&sv=2)\n\nSubnetwork 27\n\n---\n\n### Miner Overview:\n\nMiners contribute processing resources, notably GPU (Graphics Processing Unit) instances.\n\n**Performance-Based Mining:** The system operates on a performance-based reward mechanism, where miners are incentivized through a dynamic reward structure correlated to the processing capability of their hardware. High-performance devices are eligible for increased compensation, reflecting their greater contribution to the network's computational throughput. Emphasizing the integration of GPU instances is critical due to their superior computational power, particularly in tasks regarding machine learning.\n\n![](https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252Fkd6OZovkBiDnAqrBHVGp%252Fsn27_miner_overview1.png%3Falt%3Dmedia%26token%3D7f342c3e-adf3-4469-98ec-c657216da27f&width=768&dpr=4&quality=100&sign=9a7a9e01&sv=2)\n\n---\n\n**Rent A Server From Subnet 27:** \u003Chttps://app.neuralinternet.ai/>\n\n**Compute Subnet Github:** \u003Chttps://github.com/neuralinternet/compute-subnet>\n\n**Compute Subnet Discord Channel:** \u003Chttps://discord.gg/t7BMee4w>\n\n**Real-Time Compute Subnet Metrics:** \u003Chttps://opencompute.streamlit.app/>\n\nWe greatly appreciate and encourage contributions from the community to help improve and advance the development of the Compute Subnet. **We have an active bounty program in place to incentivize and reward valuable contributions.**\n\nIf you are interested in contributing to the Compute Subnet, please review ourReward Program for Valuable Contributions document on GitHub. This document outlines the details of the bounty program, including the types of contributions eligible for rewards and the reward structure.\n\n**Reward Program for Valuable Contributions:** [**https://github.com/neuralinternet/compute-subnet/blob/main/CONTRIBUTING.md**](https://github.com/neuralinternet/compute-subnet/blob/main/CONTRIBUTING.md)\n\n**Validator CLI Guide For Reserving Compute Subnet Resources:** [Validator Utilization of Compute Resources](/group-1/bittensor-utilization-of-compute-resources-in-subnet-27)\n\n#### Example Cloud Providers:\n\nWe do not support Containerized (docker)-based cloud platforms such as Runpod, VastAI and Lambda.\n\nWe strongly urge miners to provide their own hardware to foster and build a stronger network for all. Providing your own in-house hardware may come with its own benefits.\n\nIf you cannot supply your hardware in-house, here are some usable GPU providers:\n\n* [Oracle](https://www.oracle.com/cloud/compute/gpu/)\n* [Coreweave](https://www.coreweave.com/)\n* [FluidStack](https://www.fluidstack.io/)\n* [Latitude.sh](https://www.latitude.sh/) (referral code: BITTENSOR27)\n* [Oblivus](https://oblivus.com/) (referral code: BITTENSOR27 - 2% cash back in platform expenditures)\n\n**Examples of GPUs to rent** (listed in order of computing power):\n\n**GPU Base Scores**: The following GPUs are assigned specific base scores, reflecting their relative performance. To understand scoring please see the Proof-of-GPU page [here](/ni-ecosystem/ni-compute-sn27/ai-gpu-benchmarking-with-proof-of-gpu):\n\n* NVIDIA H200: 4.00\n* NVIDIA H100 80GB HBM3: 3.30\n* NVIDIA H100 80GB PCIE: 2.80\n* NVIDIA A100-SXM4-80GB: 1.90\n\n## 2. I**NSTALLATIO**N\n\n---\n\nThis installation process requires Ubuntu 22.04 and python3.8 or higher. You are limited to one external IP per UID. There is automatic blacklisting in place if validators detect anomalous behavior. Port 4444 is to be opened for your miner to function properly.\n\n### Install Docker\n\nTo run a miner, you must [install](https://docs.docker.com/engine/install/ubuntu) Docker and run the service. If Docker is already installed on your machine, scroll down to step 2.1\n\nInstall Link: \u003Chttps://docs.docker.com/engine/install/ubuntu/#install-using-the-repository>\n\nVerify that the Docker Engine installation is successful by running the `hello-world` image.\n\nThis command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.\n\nCopy\n\n```\nsudo docker run hello-world\n```\n\n### 2.1 BEGIN BY INSTALLING BITTENSOR:\n\nCopy\n\n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/opentensor/bittensor/master/scripts/install.sh)\"\n```\n\nSee Bittensor’s documentation for alternative installation instructions.\n\n**Bittensor Documentation:** [docs.bittensor.com](https://docs.bittensor.com/)\n\n### 2.2 VERIFY THE INSTALLATION:\n\nVerify using the `btcli` command\n\nCopy\n\n```\nbtcli --help\n```\n\nIf this does not result in the expected output try adding python to path using: `export PATH=$PATH:$(python3 -m site --user-base)/bin`\n\nwhich will give you an output similar to below:\n\nCopy\n\n```\nusage: btcli \u003Ccommand> \u003Ccommand args>\n\nbittensor cli v6.9.4\n\npositional arguments:\n  {subnets,s,subnet,root,r,roots,wallet,w,wallets,stake,st,stakes,sudo,su,sudos,legacy,l,info,i}\n    subnets (s, subnet)\n                        Commands for managing and viewing subnetworks.\n    root (r, roots)     Commands for managing and viewing the root network.\n    wallet (w, wallets)\n                        Commands for managing and viewing wallets.\n    stake (st, stakes)  Commands for staking and removing stake from hotkey accounts.\n    sudo (su, sudos)    Commands for subnet management\n    legacy (l)          Miscellaneous commands.\n    info (i)            Instructions for enabling autocompletion for the CLI.\n\noptions:\n  -h, --help            show this help message and exit\n  --print-completion {bash,zsh,tcsh}\n                        Print shell tab completion script\n```\n\n---\n\nCreate a Cold & Hotkey with the commands below:\n\nFollow the instruction following both of these commands\n\nCopy\n\n```\nbtcli w new_coldkey\n```\n\nCopy\n\n```\nbtcli w new_hotkey\n```\n\nIf you already have a Key, you can regenerate it ‘safely’ on a machine using `btcli w regen_coldkeypub`. However, you must regen the full key if you plan to register or transfer from that wallet. `regen_coldkeypub` lets you load the key without exposing your mnemonic to the server. If you want to, you can generate a key pair on a local safe machine to use as cold storage for the funds that you send.\n\nCopy\n\n```\nbtcli w regen_coldkeypub\n```\n\nCopy\n\n```\nbtcli w regen_coldkey\n```\n\nCopy\n\n```\nbtcli w regen_hotkey\n```\n\n## 4. CLONE COMPUTE-SUBNET\n\n---\n\nCopy\n\n```\ngit clone https://github.com/neuralinternet/Compute-Subnet.git\n```\n\nAccess the Compute-Subnet Directory\n\nCopy\n\n```\ncd Compute-Subnet\n```\n\n## 5. COMPUTE SUBNET DEPENDENCIES\n\n---\n\nFor optimal functionality of the Compute Subnet, it's essential to install the appropriate graphics drivers and dependencies.\n\n**Required** **dependencies for validators and miners:**\n\nCopy\n\n```\npython3 -m pip install -r requirements.txt\npython3 -m pip install --no-deps -r requirements-compute.txt\npython3 -m pip install -e .\n```\n\n### 5.1 EXTRA DEPENDENCIES FOR MINERS:\n\n#### In case you have missing requirements\n\nCopy\n\n```\nsudo apt -y install ocl-icd-libopencl1 pocl-opencl-icd\n```\n\n#### Download the NVIDIA CUDA Toolkit\n\nTo ensure **optimal performance and compatibility**, it is **strongly recommended** to install the **latest available CUDA version** from NVIDIA.\n\nIf Nvidia toolkit and drivers are already installed on your machine, scroll down to verify then move on to the Wandb Setup.\n\nCopy\n\n```\n# Visit NVIDIA's official CUDA download page to get the latest version:\n# https://developer.nvidia.com/cuda-downloads\n\n# Select your operating system, architecture, distribution, and version to get the appropriate installer.\n\n# Example for Ubuntu 22.04 (replace with the latest version as needed):\n\n# Download the CUDA repository package (update the URL to the latest version)\n\n\nwget https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.1-545.23.08-1_amd64.deb\n```\n\nCopy\n\n```\nsudo dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.1-545.23.08-1_amd64.deb\n```\n\nCopy\n\n```\nsudo cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\n```\n\nCopy\n\n```\nsudo apt-get update\n```\n\nCopy\n\n```\nsudo apt-get -y install cuda-toolkit-12-3\n```\n\nCopy\n\n```\nsudo apt-get -y install -y cuda-drivers\n```\n\nCopy\n\n```\nexport CUDA_VERSION=cuda-12.3\nexport PATH=$PATH:/usr/local/$CUDA_VERSION/bin\nexport LD_LIBRARY_PATH=/usr/local/$CUDA_VERSION/lib64\n```\n\nCopy\n\n```\necho \"\">>~/.bashrc\necho \"PATH=$PATH\">>~/.bashrc\necho \"LD_LIBRARY_PATH=$LD_LIBRARY_PATH\">>~/.bashrc\n```\n\nCopy\n\n```\nsource ~/.bashrc\n```\n\nYou may need to reboot the machine at this point to finalize changes\n\nCopy\n\n```\nsudo reboot\n```\n\nThe simplest way to check the installed CUDA version is by using the NVIDIA CUDA Compiler (`nvcc`).\n\nCopy\n\n```\nnvidia-smi\nnvcc --version\n```\n\nThe output of which should look something like\n\nCopy\n\n```\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX                     Off | 00000000:05:00.0 Off |                  Off |\n| 30%   34C    P0              70W / 300W |  400MiB / 4914000MiB |      4%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n```\n\nCopy\n\n```\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Fri_Nov__3_17:16:49_PDT_2023\nCuda compilation tools, release 12.3, V12.3.103\nBuild cuda_12.3.r12.3/compiler.33492891_0\n```\n\n### Wandb Setup\n\nTo log into the wandb project named opencompute from neuralinternet, miners and validators need a wandb API key. This is necessary for your miner to be properly scored. You can obtain a free API key by making an account here: \u003Chttps://wandb.ai/>\n\nInside of the Compute-Subnet directory; Rename the `.env.example` file to `.env` and replace the placeholder with your actual API key.\n\nYou can now track your mining and validation statistics on Wandb. For access, visit: \u003Chttps://wandb.ai/neuralinternet/opencompute>. To view the networks overall statistics check out our real-time dashboard here: \u003Chttps://opencompute.streamlit.app/>\n\n### **PM2 Installation**\n\nInstall and run pm2 commands to keep your miner online at all times.\n\nCopy\n\n```\nsudo apt update\n```\n\nCopy\n\n```\nsudo apt install npm\n```\n\nCopy\n\n```\nsudo npm install pm2 -g\n```\n\nConfirm pm2 is installed and running correctly\n\nCopy\n\n```\npm2 ls\n```\n\n### 5.2 INSTALL NVIDIA DOCKER SUPPORT\n\nFor more information, refer to the NVIDIA Container Toolkit installation guide: \u003Chttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-apt>\n\nCopy\n\n```\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n```\n\nCopy\n\n```\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\n```\n\nCopy\n\n```\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n```\n\nCopy\n\n```\nsudo apt update\n```\n\nCopy\n\n```\nsudo apt-get install -y nvidia-container-toolkit\n```\n\nCopy\n\n```\nsudo apt install -y nvidia-docker2\n```\n\n## 6. START THE DOCKER SERVICE IN COMPUTE SUBNET\n\n---\n\nCopy\n\n```\ncd Compute-Subnet\n```\n\nCopy\n\n```\nsudo groupadd docker\n```\n\nCopy\n\n```\nsudo usermod -aG docker $USER\n```\n\nCopy\n\n```\nsudo systemctl start docker\n```\n\nCopy\n\n```\nsudo apt install at\n```\n\nMake sure to check that docker is properly installed and running correctly:\n\nCopy\n\n```\nsudo service docker status\n```\n\nThis is an example of it running correctly:\n\nCopy\n\n```\nroot@merciful-bored-zephyr-fin-01:~# sudo service docker status\n● docker.service - Docker Application Container Engine\n     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset>\n     Active: active (running)\n```\n\n## 7.0 SETTING UP A MINER\n\n---\n\n### Hotkey Registration\n\nAt this point, you will need some $TAO in your coldkey address for miner registration. Once your coldkey is funded, run the command below to register your hotkey:\n\nCopy\n\n```\nbtcli s register --subtensor.network finney --netuid 27\n```\n\nFor testnet use: `btcli s register --subtensor.network test --netuid 15`\n\nIf you get the error ‘too many registrations this interval’ it means the max amount of registrations that cycle has been reached, and you need to wait a bit and try again. You can check the registration cost [here](https://taostats.io/subnets/netuid-27/#registration)\n\n### 7.1 SETTING UP UFW FOR MINER:\n\nOpen your desired ssh port for allocations; default is 4444 (required for allocation):\n\nCopy\n\n```\nsudo apt update\n```\n\nCopy\n\n```\nsudo apt install ufw\n```\n\nBelow we open the allocation SSH port\n\nCopy\n\n```\nsudo ufw allow 4444\n```\n\n**TCP Ports:** Open ports using ufw (put any number in place of **xxxx** and **yyyy** of your choice) and use them as axon port.\n\nCopy\n\n```\nsudo ufw allow xxxx:yyyy/tcp\n```\n\nCopy\n\n```\nsudo ufw allow 22/tcp\n```\n\nCopy\n\n```\nsudo ufw enable\n```\n\nCopy\n\n```\nsudo ufw status\n```\n\n### 7.2 RUNNING THE MINER:\n\n### Miner options\n\n* `--miner.whitelist.not.enough.stake`: (Optional) Whitelist the validators without enough stake. Default: False.\n* `--miner.whitelist.not.updated`: (Optional) Whitelist validators not using the last version of the code. Default: False.\n* `--miner.whitelist.updated.threshold`: (Optional) Total quorum before starting the whitelist. Default: 60. (%)\n\n#### Now, using pm2, run miner as:\n\nCopy\n\n```\npm2 start ./neurons/miner.py --name MINER --interpreter python3 -- --netuid 27 --subtensor.network finney --wallet.name COLDKEYNAME --wallet.hotkey HOTKEYNAME --axon.port XXXX --axon.ip xx.xxx.xxx.xx --logging.debug\n```\n\n#### SubVortex subtensor (**recommended**):\n\nCopy\n\n```\npm2 start ./neurons/miner.py --name MINER --interpreter python3 -- --netuid 27 --subtensor.network subvortex.info:9944 --wallet.name COLDKEYNAME --wallet.hotkey HOTKEYNAME --axon.port xxxx --logging.debug\n```\n\nTo set up your miner, first replace `COLDKEYNAME` & `HOTKEYNAME` with the names of your keys. Then, update `axon.port`with a 4-digit number in the range you opened for [xxxx:yyyy above](/ni-ecosystem/ni-compute-sn27/ni-compute-subnet-miner-setup#id-7.1-setting-up-ufw-for-miner). For the parameters `--axon.external` and `--axon.ip`, use your miner machine's public IP address in place of the 'xxxxx's. You can find this IP by running `hostname -I`. Though not always necessary, these parameters can be crucial for resolving certain connectivity issues.\n\nWhen operating a miner and you have `local subtensor` running on a separate machine, it's crucial to add and adjust the `--subtensor.chain_endpoint` parameter. This should be set to the IP and port (XXX.XX.XXX.XXX:XXXX) where your subtensor is running. If your subtensor is local to the miner machine, this parameter can be removed.\n\n## 8. **CHECKING MINER LOGS**\n\n---\n\nAfter launching the compute miner, you can then check the logs using the two commands below:\n\nCopy\n\n```\npm2 logs\n```\n\nCopy\n\n```\npm2 monit\n```\n\nRun pm2 logs to debug any issues and view information on your miner. Specifically find your wandb run to view more live information. **In the wandb project you can view the scores you receive from validators.**\n\n![](https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FoGTwcg3bpJ4nUlV8ZzzT%252Fminerpm2logs.png%3Falt%3Dmedia%26token%3D712d2473-5b82-4b3d-876d-3e0e670b1047&width=768&dpr=4&quality=100&sign=34158338&sv=2)\n\npm2 miner logs\n\n![](https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FLXpkIJ5iVt0uAA3qCxfZ%252Fwandblogs1.png%3Falt%3Dmedia%26token%3D29ca8a36-8680-49e2-899b-cd11acb7de75&width=768&dpr=4&quality=100&sign=1a5a098f&sv=2)\n\nYou can view your scores through the wandb run of a Validator\n\n![](https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FLMgLKvJXbojXsFQuiUEy%252Fwandblogs2.png%3Falt%3Dmedia%26token%3D848b804f-8dc5-409c-b50d-18a4bf960867&width=768&dpr=4&quality=100&sign=fb7d65ee&sv=2)\n\nExpand the stats section\n\n![](https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FNrC3MEzwKrPJH4UNmSNj%252Fwandblogs3.png%3Falt%3Dmedia%26token%3D2ba7eb40-dcc6-4d76-93d4-517678ae745c&width=768&dpr=4&quality=100&sign=4b0976a1&sv=2)\n\nFind your miner UID\n\n## 10. MORE USEFUL COMMANDS\n\nCopy\n\n```\nbtcli s metagraph --netuid 27\n```\n\nCopy\n\n```\nbtcli s list\n```\n\nCopy\n\n```\nbtcli wallet overview --subtensor.network finney --all --netuid 27\n```\n\nCopy\n\n```\npm2 logs miner --lines 1000 | grep -i \"Challenge.*found\"\n```\n\nCopy\n\n```\npm2 logs -f | grep -E \"SUCCESS|INFO|DEBUG|ERROR\"\n```\n\nCopy\n\n```\nnvidia-smi --query-gpu=name,memory.total,clocks.gr,clocks.mem --format=csvgrep \"Challenge .* found in\" \"/home/ubuntu/.pm2/logs/MINER-out.log\" | sed -E 's/.* found in ([0-9.]+) seconds.*/\\\\1/' | awk '{sum+=$1; count+=1} END {if (count > 0) print sum/count; else print \"No data to calculate average\"}’\n```\n\nCopy\n\n```\nbtcli w transfer --subtensor.network local --dest DESTINATION_WALLET --wallet.name default --amount 0\n```\n\nCopy\n\n```\nbtcli stake remove --subtensor.network local --all --all_hotkeys --wallet.name default\n```","src/content/docs/miner-system/ni_compue_subnet_miner_setup.md","4131c920110f7f80",{"html":311,"metadata":312},"\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"1-introduction\">1. INTRODUCTION\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#1-introduction\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “1. INTRODUCTION”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>For miners interested in joining this innovative network, Subnet 27 offers the opportunity to contribute computing resources and \u003Cstrong>earn $NI\u003C/strong> in return. This guide is structured to provide a comprehensive breakdown of how you can get started with contributing to Bittensor’s commodity markets using your compute power.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"decentralizing-compute\">\u003Cstrong>Decentralizing Compute\u003C/strong>\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#decentralizing-compute\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Decentralizing Compute”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>NI Compute decentralizes computing resources by combining siloed pools of compute on a blockchain to be validated and accessed trustlessly. This opens a door to scalable compute without the constraints of centralized power.\u003C/p>\n\u003Cp>\u003Cimg src=\"https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2Fcontent.gitbook.com%2Fcontent%2FRCFZhMrQpz0DM7pJi6mq%2Fblobs%2FizrLCHm4g9RaPDSFREa4%2Fcompute1.png&#x26;width=768&#x26;dpr=4&#x26;quality=100&#x26;sign=960b335e&#x26;sv=2\" alt=\"\">\u003C/p>\n\u003Cp>Subnetwork 27\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"powered-by-bittensor\">Powered By Bittensor\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#powered-by-bittensor\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Powered By Bittensor”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>NI Compute brings, arguably, the most important and finite resource needed for the creation of machine intelligence. All network participants will have access to an ever-expanding pool of compute for all development needs.\u003C/p>\n\u003Cp>\u003Cstrong>What is a decentralized supercomputer without access to permissionless compute?\u003C/strong>\u003C/p>\n\u003Cp>\u003Cimg src=\"https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2Fcontent.gitbook.com%2Fcontent%2FRCFZhMrQpz0DM7pJi6mq%2Fblobs%2Fw1R7MxeZMqrsQ4sz1opq%2Fcompute2.png&#x26;width=768&#x26;dpr=4&#x26;quality=100&#x26;sign=a68e2e77&#x26;sv=2\" alt=\"\">\u003C/p>\n\u003Cp>Subnetwork 27\u003C/p>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"miner-overview\">Miner Overview:\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#miner-overview\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Miner Overview:”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Miners contribute processing resources, notably GPU (Graphics Processing Unit) instances.\u003C/p>\n\u003Cp>\u003Cstrong>Performance-Based Mining:\u003C/strong> The system operates on a performance-based reward mechanism, where miners are incentivized through a dynamic reward structure correlated to the processing capability of their hardware. High-performance devices are eligible for increased compensation, reflecting their greater contribution to the network’s computational throughput. Emphasizing the integration of GPU instances is critical due to their superior computational power, particularly in tasks regarding machine learning.\u003C/p>\n\u003Cp>\u003Cimg src=\"https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252Fkd6OZovkBiDnAqrBHVGp%252Fsn27_miner_overview1.png%3Falt%3Dmedia%26token%3D7f342c3e-adf3-4469-98ec-c657216da27f&#x26;width=768&#x26;dpr=4&#x26;quality=100&#x26;sign=9a7a9e01&#x26;sv=2\" alt=\"\">\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cstrong>Rent A Server From Subnet 27:\u003C/strong> \u003Ca href=\"https://app.neuralinternet.ai/\">https://app.neuralinternet.ai/\u003C/a>\u003C/p>\n\u003Cp>\u003Cstrong>Compute Subnet Github:\u003C/strong> \u003Ca href=\"https://github.com/neuralinternet/compute-subnet\">https://github.com/neuralinternet/compute-subnet\u003C/a>\u003C/p>\n\u003Cp>\u003Cstrong>Compute Subnet Discord Channel:\u003C/strong> \u003Ca href=\"https://discord.gg/t7BMee4w\">https://discord.gg/t7BMee4w\u003C/a>\u003C/p>\n\u003Cp>\u003Cstrong>Real-Time Compute Subnet Metrics:\u003C/strong> \u003Ca href=\"https://opencompute.streamlit.app/\">https://opencompute.streamlit.app/\u003C/a>\u003C/p>\n\u003Cp>We greatly appreciate and encourage contributions from the community to help improve and advance the development of the Compute Subnet. \u003Cstrong>We have an active bounty program in place to incentivize and reward valuable contributions.\u003C/strong>\u003C/p>\n\u003Cp>If you are interested in contributing to the Compute Subnet, please review ourReward Program for Valuable Contributions document on GitHub. This document outlines the details of the bounty program, including the types of contributions eligible for rewards and the reward structure.\u003C/p>\n\u003Cp>\u003Cstrong>Reward Program for Valuable Contributions:\u003C/strong> \u003Ca href=\"https://github.com/neuralinternet/compute-subnet/blob/main/CONTRIBUTING.md\">\u003Cstrong>https://github.com/neuralinternet/compute-subnet/blob/main/CONTRIBUTING.md\u003C/strong>\u003C/a>\u003C/p>\n\u003Cp>\u003Cstrong>Validator CLI Guide For Reserving Compute Subnet Resources:\u003C/strong> \u003Ca href=\"/group-1/bittensor-utilization-of-compute-resources-in-subnet-27\">Validator Utilization of Compute Resources\u003C/a>\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"example-cloud-providers\">Example Cloud Providers:\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#example-cloud-providers\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Example Cloud Providers:”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>We do not support Containerized (docker)-based cloud platforms such as Runpod, VastAI and Lambda.\u003C/p>\n\u003Cp>We strongly urge miners to provide their own hardware to foster and build a stronger network for all. Providing your own in-house hardware may come with its own benefits.\u003C/p>\n\u003Cp>If you cannot supply your hardware in-house, here are some usable GPU providers:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.oracle.com/cloud/compute/gpu/\">Oracle\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.coreweave.com/\">Coreweave\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.fluidstack.io/\">FluidStack\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.latitude.sh/\">Latitude.sh\u003C/a> (referral code: BITTENSOR27)\u003C/li>\n\u003Cli>\u003Ca href=\"https://oblivus.com/\">Oblivus\u003C/a> (referral code: BITTENSOR27 - 2% cash back in platform expenditures)\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Examples of GPUs to rent\u003C/strong> (listed in order of computing power):\u003C/p>\n\u003Cp>\u003Cstrong>GPU Base Scores\u003C/strong>: The following GPUs are assigned specific base scores, reflecting their relative performance. To understand scoring please see the Proof-of-GPU page \u003Ca href=\"/ni-ecosystem/ni-compute-sn27/ai-gpu-benchmarking-with-proof-of-gpu\">here\u003C/a>:\u003C/p>\n\u003Cul>\n\u003Cli>NVIDIA H200: 4.00\u003C/li>\n\u003Cli>NVIDIA H100 80GB HBM3: 3.30\u003C/li>\n\u003Cli>NVIDIA H100 80GB PCIE: 2.80\u003C/li>\n\u003Cli>NVIDIA A100-SXM4-80GB: 1.90\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"2-installation\">2. I\u003Cstrong>NSTALLATIO\u003C/strong>N\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#2-installation\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “2. INSTALLATION”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cp>This installation process requires Ubuntu 22.04 and python3.8 or higher. You are limited to one external IP per UID. There is automatic blacklisting in place if validators detect anomalous behavior. Port 4444 is to be opened for your miner to function properly.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"install-docker\">Install Docker\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#install-docker\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Install Docker”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>To run a miner, you must \u003Ca href=\"https://docs.docker.com/engine/install/ubuntu\">install\u003C/a> Docker and run the service. If Docker is already installed on your machine, scroll down to step 2.1\u003C/p>\n\u003Cp>Install Link: \u003Ca href=\"https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository\">https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository\u003C/a>\u003C/p>\n\u003Cp>Verify that the Docker Engine installation is successful by running the \u003Ccode dir=\"auto\">hello-world\u003C/code> image.\u003C/p>\n\u003Cp>This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Clink rel=\"stylesheet\" href=\"/_astro/ec.v4551.css\">\u003Cscript type=\"module\" src=\"/_astro/ec.8zarh.js\">\u003C/script>\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo docker run hello-world\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo docker run hello-world\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"21-begin-by-installing-bittensor\">2.1 BEGIN BY INSTALLING BITTENSOR:\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#21-begin-by-installing-bittensor\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “2.1 BEGIN BY INSTALLING BITTENSOR:”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/opentensor/bittensor/master/scripts/install.sh)\"\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"/bin/bash -c &#x22;$(curl -fsSL https://raw.githubusercontent.com/opentensor/bittensor/master/scripts/install.sh)&#x22;\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>See Bittensor’s documentation for alternative installation instructions.\u003C/p>\n\u003Cp>\u003Cstrong>Bittensor Documentation:\u003C/strong> \u003Ca href=\"https://docs.bittensor.com/\">docs.bittensor.com\u003C/a>\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"22-verify-the-installation\">2.2 VERIFY THE INSTALLATION:\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#22-verify-the-installation\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “2.2 VERIFY THE INSTALLATION:”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Verify using the \u003Ccode dir=\"auto\">btcli\u003C/code> command\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli --help\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli --help\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>If this does not result in the expected output try adding python to path using: \u003Ccode dir=\"auto\">export PATH=$PATH:$(python3 -m site --user-base)/bin\u003C/code>\u003C/p>\n\u003Cp>which will give you an output similar to below:\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">usage: btcli &#x3C;command> &#x3C;command args>\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">bittensor cli v6.9.4\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">positional arguments:\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">  \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">{subnets,s,subnet,root,r,roots,wallet,w,wallets,stake,st,stakes,sudo,su,sudos,legacy,l,info,i}\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">subnets (s, subnet)\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">                        \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Commands for managing and viewing subnetworks.\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">root (r, roots)     Commands for managing and viewing the root network.\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">wallet (w, wallets)\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">                        \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Commands for managing and viewing wallets.\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">stake (st, stakes)  Commands for staking and removing stake from hotkey accounts.\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo (su, sudos)    Commands for subnet management\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">legacy (l)          Miscellaneous commands.\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">info (i)            Instructions for enabling autocompletion for the CLI.\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">options:\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">  \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">-h, --help            show this help message and exit\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">  \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">--print-completion {bash,zsh,tcsh}\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">                        \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Print shell tab completion script\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"usage: btcli \u003Ccommand> \u003Ccommand args>bittensor cli v6.9.4positional arguments:  {subnets,s,subnet,root,r,roots,wallet,w,wallets,stake,st,stakes,sudo,su,sudos,legacy,l,info,i}    subnets (s, subnet)                        Commands for managing and viewing subnetworks.    root (r, roots)     Commands for managing and viewing the root network.    wallet (w, wallets)                        Commands for managing and viewing wallets.    stake (st, stakes)  Commands for staking and removing stake from hotkey accounts.    sudo (su, sudos)    Commands for subnet management    legacy (l)          Miscellaneous commands.    info (i)            Instructions for enabling autocompletion for the CLI.options:  -h, --help            show this help message and exit  --print-completion {bash,zsh,tcsh}                        Print shell tab completion script\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Chr>\n\u003Cp>Create a Cold &#x26; Hotkey with the commands below:\u003C/p>\n\u003Cp>Follow the instruction following both of these commands\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli w new_coldkey\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli w new_coldkey\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli w new_hotkey\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli w new_hotkey\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>If you already have a Key, you can regenerate it ‘safely’ on a machine using \u003Ccode dir=\"auto\">btcli w regen_coldkeypub\u003C/code>. However, you must regen the full key if you plan to register or transfer from that wallet. \u003Ccode dir=\"auto\">regen_coldkeypub\u003C/code> lets you load the key without exposing your mnemonic to the server. If you want to, you can generate a key pair on a local safe machine to use as cold storage for the funds that you send.\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli w regen_coldkeypub\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli w regen_coldkeypub\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli w regen_coldkey\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli w regen_coldkey\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli w regen_hotkey\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli w regen_hotkey\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"4-clone-compute-subnet\">4. CLONE COMPUTE-SUBNET\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#4-clone-compute-subnet\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “4. CLONE COMPUTE-SUBNET”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">git clone https://github.com/neuralinternet/Compute-Subnet.git\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"git clone https://github.com/neuralinternet/Compute-Subnet.git\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Access the Compute-Subnet Directory\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">cd Compute-Subnet\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"cd Compute-Subnet\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"5-compute-subnet-dependencies\">5. COMPUTE SUBNET DEPENDENCIES\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#5-compute-subnet-dependencies\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “5. COMPUTE SUBNET DEPENDENCIES”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cp>For optimal functionality of the Compute Subnet, it’s essential to install the appropriate graphics drivers and dependencies.\u003C/p>\n\u003Cp>\u003Cstrong>Required\u003C/strong> \u003Cstrong>dependencies for validators and miners:\u003C/strong>\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">python3 -m pip install -r requirements.txt\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">python3 -m pip install --no-deps -r requirements-compute.txt\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">python3 -m pip install -e .\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"python3 -m pip install -r requirements.txtpython3 -m pip install --no-deps -r requirements-compute.txtpython3 -m pip install -e .\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"51-extra-dependencies-for-miners\">5.1 EXTRA DEPENDENCIES FOR MINERS:\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#51-extra-dependencies-for-miners\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “5.1 EXTRA DEPENDENCIES FOR MINERS:”\u003C/span>\u003C/a>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"in-case-you-have-missing-requirements\">In case you have missing requirements\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#in-case-you-have-missing-requirements\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “In case you have missing requirements”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt -y install ocl-icd-libopencl1 pocl-opencl-icd\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt -y install ocl-icd-libopencl1 pocl-opencl-icd\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"download-the-nvidia-cuda-toolkit\">Download the NVIDIA CUDA Toolkit\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#download-the-nvidia-cuda-toolkit\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Download the NVIDIA CUDA Toolkit”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>To ensure \u003Cstrong>optimal performance and compatibility\u003C/strong>, it is \u003Cstrong>strongly recommended\u003C/strong> to install the \u003Cstrong>latest available CUDA version\u003C/strong> from NVIDIA.\u003C/p>\n\u003Cp>If Nvidia toolkit and drivers are already installed on your machine, scroll down to verify then move on to the Wandb Setup.\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\"># Visit NVIDIA's official CUDA download page to get the latest version:\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\"># https://developer.nvidia.com/cuda-downloads\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\"># Select your operating system, architecture, distribution, and version to get the appropriate installer.\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\"># Example for Ubuntu 22.04 (replace with the latest version as needed):\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\"># Download the CUDA repository package (update the URL to the latest version)\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">wget https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.1-545.23.08-1_amd64.deb\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"# Visit NVIDIA&#x27;s official CUDA download page to get the latest version:# https://developer.nvidia.com/cuda-downloads# Select your operating system, architecture, distribution, and version to get the appropriate installer.# Example for Ubuntu 22.04 (replace with the latest version as needed):# Download the CUDA repository package (update the URL to the latest version)wget https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.1-545.23.08-1_amd64.deb\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.1-545.23.08-1_amd64.deb\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.1-545.23.08-1_amd64.deb\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt-get update\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt-get update\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt-get -y install cuda-toolkit-12-3\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt-get -y install cuda-toolkit-12-3\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt-get -y install -y cuda-drivers\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt-get -y install -y cuda-drivers\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">export CUDA_VERSION=cuda-12.3\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">export PATH=$PATH:/usr/local/$CUDA_VERSION/bin\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">export LD_LIBRARY_PATH=/usr/local/$CUDA_VERSION/lib64\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"export CUDA_VERSION=cuda-12.3export PATH=$PATH:/usr/local/$CUDA_VERSION/binexport LD_LIBRARY_PATH=/usr/local/$CUDA_VERSION/lib64\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">echo \"\">>~/.bashrc\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">echo \"PATH=$PATH\">>~/.bashrc\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">echo \"LD_LIBRARY_PATH=$LD_LIBRARY_PATH\">>~/.bashrc\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"echo &#x22;&#x22;>>~/.bashrcecho &#x22;PATH=$PATH&#x22;>>~/.bashrcecho &#x22;LD_LIBRARY_PATH=$LD_LIBRARY_PATH&#x22;>>~/.bashrc\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">source ~/.bashrc\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"source ~/.bashrc\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>You may need to reboot the machine at this point to finalize changes\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo reboot\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo reboot\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>The simplest way to check the installed CUDA version is by using the NVIDIA CUDA Compiler (\u003Ccode dir=\"auto\">nvcc\u003C/code>).\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">nvidia-smi\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">nvcc --version\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"nvidia-sminvcc --version\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>The output of which should look something like\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">+---------------------------------------------------------------------------------------+\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|-----------------------------------------+----------------------+----------------------+\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|                                         |                      |               MIG M. |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|=========================================+======================+======================|\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|   0  NVIDIA RTX                     Off | 00000000:05:00.0 Off |                  Off |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">| 30%   34C    P0              70W / 300W |  400MiB / 4914000MiB |      4%      Default |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|                                         |                      |                  N/A |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">+-----------------------------------------+----------------------+----------------------+\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\n\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">+---------------------------------------------------------------------------------------+\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">| Processes:                                                                            |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|        ID   ID                                                             Usage      |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|=======================================================================================|\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">|  No running processes found                                                           |\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">+---------------------------------------------------------------------------------------+\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"+---------------------------------------------------------------------------------------+| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     ||-----------------------------------------+----------------------+----------------------+| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. ||                                         |                      |               MIG M. ||=========================================+======================+======================||   0  NVIDIA RTX                     Off | 00000000:05:00.0 Off |                  Off || 30%   34C    P0              70W / 300W |  400MiB / 4914000MiB |      4%      Default ||                                         |                      |                  N/A |+-----------------------------------------+----------------------+----------------------++---------------------------------------------------------------------------------------+| Processes:                                                                            ||  GPU   GI   CI        PID   Type   Process name                            GPU Memory ||        ID   ID                                                             Usage      ||=======================================================================================||  No running processes found                                                           |+---------------------------------------------------------------------------------------+\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">nvcc: NVIDIA (R) Cuda compiler driver\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Copyright (c) 2005-2023 NVIDIA Corporation\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Built on Fri_Nov__3_17:16:49_PDT_2023\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Cuda compilation tools, release 12.3, V12.3.103\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Build cuda_12.3.r12.3/compiler.33492891_0\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"nvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2023 NVIDIA CorporationBuilt on Fri_Nov__3_17:16:49_PDT_2023Cuda compilation tools, release 12.3, V12.3.103Build cuda_12.3.r12.3/compiler.33492891_0\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"wandb-setup\">Wandb Setup\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#wandb-setup\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Wandb Setup”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>To log into the wandb project named opencompute from neuralinternet, miners and validators need a wandb API key. This is necessary for your miner to be properly scored. You can obtain a free API key by making an account here: \u003Ca href=\"https://wandb.ai/\">https://wandb.ai/\u003C/a>\u003C/p>\n\u003Cp>Inside of the Compute-Subnet directory; Rename the \u003Ccode dir=\"auto\">.env.example\u003C/code> file to \u003Ccode dir=\"auto\">.env\u003C/code> and replace the placeholder with your actual API key.\u003C/p>\n\u003Cp>You can now track your mining and validation statistics on Wandb. For access, visit: \u003Ca href=\"https://wandb.ai/neuralinternet/opencompute\">https://wandb.ai/neuralinternet/opencompute\u003C/a>. To view the networks overall statistics check out our real-time dashboard here: \u003Ca href=\"https://opencompute.streamlit.app/\">https://opencompute.streamlit.app/\u003C/a>\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"pm2-installation\">\u003Cstrong>PM2 Installation\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#pm2-installation\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “PM2 Installation”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Install and run pm2 commands to keep your miner online at all times.\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt update\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt update\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt install npm\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt install npm\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo npm install pm2 -g\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo npm install pm2 -g\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Confirm pm2 is installed and running correctly\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">pm2 ls\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"pm2 ls\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"52-install-nvidia-docker-support\">5.2 INSTALL NVIDIA DOCKER SUPPORT\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#52-install-nvidia-docker-support\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “5.2 INSTALL NVIDIA DOCKER SUPPORT”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>For more information, refer to the NVIDIA Container Toolkit installation guide: \u003Ca href=\"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-apt\">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-apt\u003C/a>\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt update\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt update\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt-get install -y nvidia-container-toolkit\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt-get install -y nvidia-container-toolkit\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt install -y nvidia-docker2\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt install -y nvidia-docker2\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"6-start-the-docker-service-in-compute-subnet\">6. START THE DOCKER SERVICE IN COMPUTE SUBNET\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#6-start-the-docker-service-in-compute-subnet\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “6. START THE DOCKER SERVICE IN COMPUTE SUBNET”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">cd Compute-Subnet\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"cd Compute-Subnet\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo groupadd docker\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo groupadd docker\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo usermod -aG docker $USER\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo usermod -aG docker $USER\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo systemctl start docker\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo systemctl start docker\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt install at\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt install at\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Make sure to check that docker is properly installed and running correctly:\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo service docker status\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo service docker status\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>This is an example of it running correctly:\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">root@merciful-bored-zephyr-fin-01:~# sudo service docker status\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">● docker.service - Docker Application Container Engine\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">     \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset>\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">     \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">Active: active (running)\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"root@merciful-bored-zephyr-fin-01:~# sudo service docker status● docker.service - Docker Application Container Engine     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset>     Active: active (running)\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"70-setting-up-a-miner\">7.0 SETTING UP A MINER\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#70-setting-up-a-miner\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “7.0 SETTING UP A MINER”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"hotkey-registration\">Hotkey Registration\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#hotkey-registration\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Hotkey Registration”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>At this point, you will need some $TAO in your coldkey address for miner registration. Once your coldkey is funded, run the command below to register your hotkey:\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli s register --subtensor.network finney --netuid 27\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli s register --subtensor.network finney --netuid 27\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>For testnet use: \u003Ccode dir=\"auto\">btcli s register --subtensor.network test --netuid 15\u003C/code>\u003C/p>\n\u003Cp>If you get the error ‘too many registrations this interval’ it means the max amount of registrations that cycle has been reached, and you need to wait a bit and try again. You can check the registration cost \u003Ca href=\"https://taostats.io/subnets/netuid-27/#registration\">here\u003C/a>\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"71-setting-up-ufw-for-miner\">7.1 SETTING UP UFW FOR MINER:\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#71-setting-up-ufw-for-miner\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “7.1 SETTING UP UFW FOR MINER:”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Open your desired ssh port for allocations; default is 4444 (required for allocation):\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt update\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt update\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo apt install ufw\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo apt install ufw\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Below we open the allocation SSH port\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo ufw allow 4444\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo ufw allow 4444\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>\u003Cstrong>TCP Ports:\u003C/strong> Open ports using ufw (put any number in place of \u003Cstrong>xxxx\u003C/strong> and \u003Cstrong>yyyy\u003C/strong> of your choice) and use them as axon port.\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo ufw allow xxxx:yyyy/tcp\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo ufw allow xxxx:yyyy/tcp\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo ufw allow 22/tcp\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo ufw allow 22/tcp\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo ufw enable\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo ufw enable\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">sudo ufw status\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"sudo ufw status\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"72-running-the-miner\">7.2 RUNNING THE MINER:\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#72-running-the-miner\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “7.2 RUNNING THE MINER:”\u003C/span>\u003C/a>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"miner-options\">Miner options\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#miner-options\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Miner options”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">--miner.whitelist.not.enough.stake\u003C/code>: (Optional) Whitelist the validators without enough stake. Default: False.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">--miner.whitelist.not.updated\u003C/code>: (Optional) Whitelist validators not using the last version of the code. Default: False.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">--miner.whitelist.updated.threshold\u003C/code>: (Optional) Total quorum before starting the whitelist. Default: 60. (%)\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"now-using-pm2-run-miner-as\">Now, using pm2, run miner as:\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#now-using-pm2-run-miner-as\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Now, using pm2, run miner as:”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">pm2 start ./neurons/miner.py --name MINER --interpreter python3 -- --netuid 27 --subtensor.network finney --wallet.name COLDKEYNAME --wallet.hotkey HOTKEYNAME --axon.port XXXX --axon.ip xx.xxx.xxx.xx --logging.debug\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"pm2 start ./neurons/miner.py --name MINER --interpreter python3 -- --netuid 27 --subtensor.network finney --wallet.name COLDKEYNAME --wallet.hotkey HOTKEYNAME --axon.port XXXX --axon.ip xx.xxx.xxx.xx --logging.debug\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"subvortex-subtensor-recommended\">SubVortex subtensor (\u003Cstrong>recommended\u003C/strong>):\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#subvortex-subtensor-recommended\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “SubVortex subtensor (recommended):”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">pm2 start ./neurons/miner.py --name MINER --interpreter python3 -- --netuid 27 --subtensor.network subvortex.info:9944 --wallet.name COLDKEYNAME --wallet.hotkey HOTKEYNAME --axon.port xxxx --logging.debug\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"pm2 start ./neurons/miner.py --name MINER --interpreter python3 -- --netuid 27 --subtensor.network subvortex.info:9944 --wallet.name COLDKEYNAME --wallet.hotkey HOTKEYNAME --axon.port xxxx --logging.debug\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>To set up your miner, first replace \u003Ccode dir=\"auto\">COLDKEYNAME\u003C/code> &#x26; \u003Ccode dir=\"auto\">HOTKEYNAME\u003C/code> with the names of your keys. Then, update \u003Ccode dir=\"auto\">axon.port\u003C/code>with a 4-digit number in the range you opened for \u003Ca href=\"/ni-ecosystem/ni-compute-sn27/ni-compute-subnet-miner-setup#id-7.1-setting-up-ufw-for-miner\">xxxx:yyyy above\u003C/a>. For the parameters \u003Ccode dir=\"auto\">--axon.external\u003C/code> and \u003Ccode dir=\"auto\">--axon.ip\u003C/code>, use your miner machine’s public IP address in place of the ‘xxxxx’s. You can find this IP by running \u003Ccode dir=\"auto\">hostname -I\u003C/code>. Though not always necessary, these parameters can be crucial for resolving certain connectivity issues.\u003C/p>\n\u003Cp>When operating a miner and you have \u003Ccode dir=\"auto\">local subtensor\u003C/code> running on a separate machine, it’s crucial to add and adjust the \u003Ccode dir=\"auto\">--subtensor.chain_endpoint\u003C/code> parameter. This should be set to the IP and port (XXX.XX.XXX.XXX:XXXX) where your subtensor is running. If your subtensor is local to the miner machine, this parameter can be removed.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"8-checking-miner-logs\">8. \u003Cstrong>CHECKING MINER LOGS\u003C/strong>\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#8-checking-miner-logs\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “8. CHECKING MINER LOGS”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cp>After launching the compute miner, you can then check the logs using the two commands below:\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">pm2 logs\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"pm2 logs\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">pm2 monit\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"pm2 monit\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Run pm2 logs to debug any issues and view information on your miner. Specifically find your wandb run to view more live information. \u003Cstrong>In the wandb project you can view the scores you receive from validators.\u003C/strong>\u003C/p>\n\u003Cp>\u003Cimg src=\"https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FoGTwcg3bpJ4nUlV8ZzzT%252Fminerpm2logs.png%3Falt%3Dmedia%26token%3D712d2473-5b82-4b3d-876d-3e0e670b1047&#x26;width=768&#x26;dpr=4&#x26;quality=100&#x26;sign=34158338&#x26;sv=2\" alt=\"\">\u003C/p>\n\u003Cp>pm2 miner logs\u003C/p>\n\u003Cp>\u003Cimg src=\"https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FLXpkIJ5iVt0uAA3qCxfZ%252Fwandblogs1.png%3Falt%3Dmedia%26token%3D29ca8a36-8680-49e2-899b-cd11acb7de75&#x26;width=768&#x26;dpr=4&#x26;quality=100&#x26;sign=1a5a098f&#x26;sv=2\" alt=\"\">\u003C/p>\n\u003Cp>You can view your scores through the wandb run of a Validator\u003C/p>\n\u003Cp>\u003Cimg src=\"https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FLMgLKvJXbojXsFQuiUEy%252Fwandblogs2.png%3Falt%3Dmedia%26token%3D848b804f-8dc5-409c-b50d-18a4bf960867&#x26;width=768&#x26;dpr=4&#x26;quality=100&#x26;sign=fb7d65ee&#x26;sv=2\" alt=\"\">\u003C/p>\n\u003Cp>Expand the stats section\u003C/p>\n\u003Cp>\u003Cimg src=\"https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FNrC3MEzwKrPJH4UNmSNj%252Fwandblogs3.png%3Falt%3Dmedia%26token%3D2ba7eb40-dcc6-4d76-93d4-517678ae745c&#x26;width=768&#x26;dpr=4&#x26;quality=100&#x26;sign=4b0976a1&#x26;sv=2\" alt=\"\">\u003C/p>\n\u003Cp>Find your miner UID\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"10-more-useful-commands\">10. MORE USEFUL COMMANDS\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#10-more-useful-commands\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “10. MORE USEFUL COMMANDS”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli s metagraph --netuid 27\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli s metagraph --netuid 27\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli s list\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli s list\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli wallet overview --subtensor.network finney --all --netuid 27\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli wallet overview --subtensor.network finney --all --netuid 27\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">pm2 logs miner --lines 1000 | grep -i \"Challenge.*found\"\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"pm2 logs miner --lines 1000 | grep -i &#x22;Challenge.*found&#x22;\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">pm2 logs -f | grep -E \"SUCCESS|INFO|DEBUG|ERROR\"\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"pm2 logs -f | grep -E &#x22;SUCCESS|INFO|DEBUG|ERROR&#x22;\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">nvidia-smi --query-gpu=name,memory.total,clocks.gr,clocks.mem --format=csvgrep \"Challenge .* found in\" \"/home/ubuntu/.pm2/logs/MINER-out.log\" | sed -E 's/.* found in ([0-9.]+) seconds.*/\\\\1/' | awk '{sum+=$1; count+=1} END {if (count > 0) print sum/count; else print \"No data to calculate average\"}’\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"nvidia-smi --query-gpu=name,memory.total,clocks.gr,clocks.mem --format=csvgrep &#x22;Challenge .* found in&#x22; &#x22;/home/ubuntu/.pm2/logs/MINER-out.log&#x22; | sed -E &#x27;s/.* found in ([0-9.]+) seconds.*/\\\\1/&#x27; | awk &#x27;{sum+=$1; count+=1} END {if (count > 0) print sum/count; else print &#x22;No data to calculate average&#x22;}’\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli w transfer --subtensor.network local --dest DESTINATION_WALLET --wallet.name default --amount 0\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli w transfer --subtensor.network local --dest DESTINATION_WALLET --wallet.name default --amount 0\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli stake remove --subtensor.network local --all --all_hotkeys --wallet.name default\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli stake remove --subtensor.network local --all --all_hotkeys --wallet.name default\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>",{"headings":313,"localImagePaths":396,"remoteImagePaths":397,"frontmatter":398,"imagePaths":399},[314,317,320,323,326,330,333,336,339,342,345,348,351,354,357,360,363,366,369,372,375,378,381,384,387,390,393],{"depth":263,"slug":315,"text":316},"1-introduction","1. INTRODUCTION",{"depth":263,"slug":318,"text":319},"decentralizing-compute","Decentralizing Compute",{"depth":259,"slug":321,"text":322},"powered-by-bittensor","Powered By Bittensor",{"depth":259,"slug":324,"text":325},"miner-overview","Miner Overview:",{"depth":327,"slug":328,"text":329},4,"example-cloud-providers","Example Cloud Providers:",{"depth":263,"slug":331,"text":332},"2-installation","2. INSTALLATION",{"depth":259,"slug":334,"text":335},"install-docker","Install Docker",{"depth":259,"slug":337,"text":338},"21-begin-by-installing-bittensor","2.1 BEGIN BY INSTALLING BITTENSOR:",{"depth":259,"slug":340,"text":341},"22-verify-the-installation","2.2 VERIFY THE INSTALLATION:",{"depth":263,"slug":343,"text":344},"4-clone-compute-subnet","4. CLONE COMPUTE-SUBNET",{"depth":263,"slug":346,"text":347},"5-compute-subnet-dependencies","5. COMPUTE SUBNET DEPENDENCIES",{"depth":259,"slug":349,"text":350},"51-extra-dependencies-for-miners","5.1 EXTRA DEPENDENCIES FOR MINERS:",{"depth":327,"slug":352,"text":353},"in-case-you-have-missing-requirements","In case you have missing requirements",{"depth":327,"slug":355,"text":356},"download-the-nvidia-cuda-toolkit","Download the NVIDIA CUDA Toolkit",{"depth":259,"slug":358,"text":359},"wandb-setup","Wandb Setup",{"depth":259,"slug":361,"text":362},"pm2-installation","PM2 Installation",{"depth":259,"slug":364,"text":365},"52-install-nvidia-docker-support","5.2 INSTALL NVIDIA DOCKER SUPPORT",{"depth":263,"slug":367,"text":368},"6-start-the-docker-service-in-compute-subnet","6. START THE DOCKER SERVICE IN COMPUTE SUBNET",{"depth":263,"slug":370,"text":371},"70-setting-up-a-miner","7.0 SETTING UP A MINER",{"depth":259,"slug":373,"text":374},"hotkey-registration","Hotkey Registration",{"depth":259,"slug":376,"text":377},"71-setting-up-ufw-for-miner","7.1 SETTING UP UFW FOR MINER:",{"depth":259,"slug":379,"text":380},"72-running-the-miner","7.2 RUNNING THE MINER:",{"depth":259,"slug":382,"text":383},"miner-options","Miner options",{"depth":327,"slug":385,"text":386},"now-using-pm2-run-miner-as","Now, using pm2, run miner as:",{"depth":327,"slug":388,"text":389},"subvortex-subtensor-recommended","SubVortex subtensor (recommended):",{"depth":263,"slug":391,"text":392},"8-checking-miner-logs","8. CHECKING MINER LOGS",{"depth":263,"slug":394,"text":395},"10-more-useful-commands","10. MORE USEFUL COMMANDS",[],[],{"title":303},[],"neural-internet/bittensor_governance",{"id":400,"data":402,"body":407,"filePath":408,"digest":409,"rendered":410},{"title":403,"editUrl":15,"head":404,"template":17,"sidebar":405,"pagefind":15,"draft":19},"Bittensor Governance",[],{"hidden":19,"attrs":406},{},"With Bittensor's DAO going Live there are now twelve senate slots that hold collective veto power on sudo calls made by a group of three Opentensor Foundation members, known as the triumvirate. Neural Inτerneτ holds one of these twelve slots and is now part of the senate.\n\nThe responsibility of the senate is to optimize distributed consensus and ensure fair & effective decision-making in the Bittensor ecosystem.","src/content/docs/neural internet/bittensor_governance.md","ce489fb7bf46016d",{"html":411,"metadata":412},"\u003Cp>With Bittensor’s DAO going Live there are now twelve senate slots that hold collective veto power on sudo calls made by a group of three Opentensor Foundation members, known as the triumvirate. Neural Inτerneτ holds one of these twelve slots and is now part of the senate.\u003C/p>\n\u003Cp>The responsibility of the senate is to optimize distributed consensus and ensure fair &#x26; effective decision-making in the Bittensor ecosystem.\u003C/p>",{"headings":413,"localImagePaths":414,"remoteImagePaths":415,"frontmatter":416,"imagePaths":417},[],[],[],{"title":403},[],"neural-internet/bittensor_charter",{"id":418,"data":420,"body":425,"filePath":426,"digest":427,"rendered":428},{"title":421,"editUrl":15,"head":422,"template":17,"sidebar":423,"pagefind":15,"draft":19},"Bittensor Charter",[],{"hidden":19,"attrs":424},{},"Bittensor itself cannot have a charter. Its core technology is a consensus mechanism, which reaches agreement about how its preferences should be distributed to participants in an open and un-permissioned network. If it has preferences itself, they are openness and decentralization, which are immutably written into its code.\n\nAs such, this document merely outlines the principles and commitments of those who use Bittensor as a medium to express their subjective preferences on top of its playing field. It is signed by The Opentensor Foundation and other entities that believe in Bittensor’s vision of decentralized AI.\n\n### **Charter**\n\nThere is wide agreement that the blossoming of Artificial Intelligence offers up tremendous promise – and risk – for humanity’s relationship with technology along multiple axes. Those include its potential use to abuse humans, long term existential risk to the human race, and its ability to increase power imbalances. We acknowledge the following principles as our commitments to stop those outcomes. Those are:\n\n### **OUR COUNTERPOINT TO CENTRALIZED CONTROL: THE GREATER THE POWER, THE MORE DANGEROUS THE ABUSE**\n\nWe are committed to safeguarding AI from being totally controlled or regulated by governments, powerful corporations, and the individuals signing this document. We believe that excessive centralization of AI poses the greatest risk to the human race, within or without Bittensor. Concentration of power will inevitably create biased decision−making, controlled access to benefits and significant abuse. Recognizing that AI is the most powerful technology humanity has created to date, it is vital that we ensure its governance sits in the hands of the many - rather than the few. To ensure this we are further committed to:\n\n### **DECENTRALIZED PREFERENCE CONSENSUS: THE PURPOSE OF POWER IS TO GIVE IT AWAY**\n\nWe firmly oppose the misuse of AI for harmful intent, and will actively strive to prevent the spread of harmful content. We also pledge to advocate for the positive, ethical and life−affirming application of AI. Simultaneously, we will actively work to diffuse control over these preferences, in the name of decentralized power, with the express purpose of leveraging the collective wisdom and judgment of humanity around the exceedingly and increasingly difficult questions AI as a technology poses. In pursuit of this, we embrace:\n\n### **OPEN OWNERSHIP: IN REAL OPEN SOURCE, YOU HAVE THE RIGHT TO CONTROL YOUR OWN DESTINY**\n\nThe Bittensor Network inherently allows open and un−permissioned ownership accrual to those who contribute. We, the signatories, will work to clear the path, through which individuals may work to participate, and therefore gain real control in the development of AI. This is necessary to ensure as many humans as possible have access, influence, and hard power in the future that we are creating together. This principle is reinforced by our commitment to:\n\n### **OPEN SOURCE DEVELOPMENT: FOR US, OPEN-SOURCE IS A MORAL IMPERATIVE**\n\nWe are totally committed to open−source development of all of our work within the Bittensor ecosystem; whether it be mining, validating, subnetwork creation or any other value−creating software, and will actively support open−source development projects, education initiatives and those who seek to lower barriers to entry at all levels.\n\nWe recognize the importance of collaborative efforts and community−driven initiatives to unlock the true potential of Artificial Intelligence; all of which can contribute to Bittensor. We will bolster this by upholding our value of ...\n\n### **TRANSPARENCY: TRANSPARENCY IS TRUST**\n\nThe Bittensor blockchain's value transfers are at all times completely transparent. We, the signatories, are further committed to total transparency of Bittensor's decision-making process above and beyond what is already public, with the intention of making it clear what our votes in the DAO entail, and why we distribute our preferences in this way to direct Bittensor.\n\n### Conclusion\n\nWe have chosen Bittensor as our common platform to represent our shared values for an open and decentralized future of AI. We are devoted to opposing centralized control and encouraging decentralized decision−making through open−ownership, software development, and superior transparency. We believe together, we can shape an AI landscape that truly serves the collective interests of humanity.\n\nSIGNED BY\n\nOpenτensor Foundaτion • τaosτaτs and BitAPAI • RoundTable21 • TAO-Validator.com • Neural Inτerneτ • Lucrosus Capital\n\n[Official link to Charter](https://bittensor.com/charter) - Bittensor.com/charter","src/content/docs/neural internet/bittensor_charter.md","b9271ada33d508e0",{"html":429,"metadata":430},"\u003Cp>Bittensor itself cannot have a charter. Its core technology is a consensus mechanism, which reaches agreement about how its preferences should be distributed to participants in an open and un-permissioned network. If it has preferences itself, they are openness and decentralization, which are immutably written into its code.\u003C/p>\n\u003Cp>As such, this document merely outlines the principles and commitments of those who use Bittensor as a medium to express their subjective preferences on top of its playing field. It is signed by The Opentensor Foundation and other entities that believe in Bittensor’s vision of decentralized AI.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"charter\">\u003Cstrong>Charter\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#charter\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Charter”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>There is wide agreement that the blossoming of Artificial Intelligence offers up tremendous promise – and risk – for humanity’s relationship with technology along multiple axes. Those include its potential use to abuse humans, long term existential risk to the human race, and its ability to increase power imbalances. We acknowledge the following principles as our commitments to stop those outcomes. Those are:\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"our-counterpoint-to-centralized-control-the-greater-the-power-the-more-dangerous-the-abuse\">\u003Cstrong>OUR COUNTERPOINT TO CENTRALIZED CONTROL: THE GREATER THE POWER, THE MORE DANGEROUS THE ABUSE\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#our-counterpoint-to-centralized-control-the-greater-the-power-the-more-dangerous-the-abuse\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “OUR COUNTERPOINT TO CENTRALIZED CONTROL: THE GREATER THE POWER, THE MORE DANGEROUS THE ABUSE”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>We are committed to safeguarding AI from being totally controlled or regulated by governments, powerful corporations, and the individuals signing this document. We believe that excessive centralization of AI poses the greatest risk to the human race, within or without Bittensor. Concentration of power will inevitably create biased decision−making, controlled access to benefits and significant abuse. Recognizing that AI is the most powerful technology humanity has created to date, it is vital that we ensure its governance sits in the hands of the many - rather than the few. To ensure this we are further committed to:\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"decentralized-preference-consensus-the-purpose-of-power-is-to-give-it-away\">\u003Cstrong>DECENTRALIZED PREFERENCE CONSENSUS: THE PURPOSE OF POWER IS TO GIVE IT AWAY\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#decentralized-preference-consensus-the-purpose-of-power-is-to-give-it-away\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “DECENTRALIZED PREFERENCE CONSENSUS: THE PURPOSE OF POWER IS TO GIVE IT AWAY”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>We firmly oppose the misuse of AI for harmful intent, and will actively strive to prevent the spread of harmful content. We also pledge to advocate for the positive, ethical and life−affirming application of AI. Simultaneously, we will actively work to diffuse control over these preferences, in the name of decentralized power, with the express purpose of leveraging the collective wisdom and judgment of humanity around the exceedingly and increasingly difficult questions AI as a technology poses. In pursuit of this, we embrace:\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"open-ownership-in-real-open-source-you-have-the-right-to-control-your-own-destiny\">\u003Cstrong>OPEN OWNERSHIP: IN REAL OPEN SOURCE, YOU HAVE THE RIGHT TO CONTROL YOUR OWN DESTINY\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#open-ownership-in-real-open-source-you-have-the-right-to-control-your-own-destiny\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “OPEN OWNERSHIP: IN REAL OPEN SOURCE, YOU HAVE THE RIGHT TO CONTROL YOUR OWN DESTINY”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The Bittensor Network inherently allows open and un−permissioned ownership accrual to those who contribute. We, the signatories, will work to clear the path, through which individuals may work to participate, and therefore gain real control in the development of AI. This is necessary to ensure as many humans as possible have access, influence, and hard power in the future that we are creating together. This principle is reinforced by our commitment to:\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"open-source-development-for-us-open-source-is-a-moral-imperative\">\u003Cstrong>OPEN SOURCE DEVELOPMENT: FOR US, OPEN-SOURCE IS A MORAL IMPERATIVE\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#open-source-development-for-us-open-source-is-a-moral-imperative\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “OPEN SOURCE DEVELOPMENT: FOR US, OPEN-SOURCE IS A MORAL IMPERATIVE”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>We are totally committed to open−source development of all of our work within the Bittensor ecosystem; whether it be mining, validating, subnetwork creation or any other value−creating software, and will actively support open−source development projects, education initiatives and those who seek to lower barriers to entry at all levels.\u003C/p>\n\u003Cp>We recognize the importance of collaborative efforts and community−driven initiatives to unlock the true potential of Artificial Intelligence; all of which can contribute to Bittensor. We will bolster this by upholding our value of …\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"transparency-transparency-is-trust\">\u003Cstrong>TRANSPARENCY: TRANSPARENCY IS TRUST\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#transparency-transparency-is-trust\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “TRANSPARENCY: TRANSPARENCY IS TRUST”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The Bittensor blockchain’s value transfers are at all times completely transparent. We, the signatories, are further committed to total transparency of Bittensor’s decision-making process above and beyond what is already public, with the intention of making it clear what our votes in the DAO entail, and why we distribute our preferences in this way to direct Bittensor.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"conclusion\">Conclusion\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#conclusion\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Conclusion”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>We have chosen Bittensor as our common platform to represent our shared values for an open and decentralized future of AI. We are devoted to opposing centralized control and encouraging decentralized decision−making through open−ownership, software development, and superior transparency. We believe together, we can shape an AI landscape that truly serves the collective interests of humanity.\u003C/p>\n\u003Cp>SIGNED BY\u003C/p>\n\u003Cp>Openτensor Foundaτion • τaosτaτs and BitAPAI • RoundTable21 • TAO-Validator.com • Neural Inτerneτ • Lucrosus Capital\u003C/p>\n\u003Cp>\u003Ca href=\"https://bittensor.com/charter\">Official link to Charter\u003C/a> - Bittensor.com/charter\u003C/p>",{"headings":431,"localImagePaths":453,"remoteImagePaths":454,"frontmatter":455,"imagePaths":456},[432,435,438,441,444,447,450],{"depth":259,"slug":433,"text":434},"charter","Charter",{"depth":259,"slug":436,"text":437},"our-counterpoint-to-centralized-control-the-greater-the-power-the-more-dangerous-the-abuse","OUR COUNTERPOINT TO CENTRALIZED CONTROL: THE GREATER THE POWER, THE MORE DANGEROUS THE ABUSE",{"depth":259,"slug":439,"text":440},"decentralized-preference-consensus-the-purpose-of-power-is-to-give-it-away","DECENTRALIZED PREFERENCE CONSENSUS: THE PURPOSE OF POWER IS TO GIVE IT AWAY",{"depth":259,"slug":442,"text":443},"open-ownership-in-real-open-source-you-have-the-right-to-control-your-own-destiny","OPEN OWNERSHIP: IN REAL OPEN SOURCE, YOU HAVE THE RIGHT TO CONTROL YOUR OWN DESTINY",{"depth":259,"slug":445,"text":446},"open-source-development-for-us-open-source-is-a-moral-imperative","OPEN SOURCE DEVELOPMENT: FOR US, OPEN-SOURCE IS A MORAL IMPERATIVE",{"depth":259,"slug":448,"text":449},"transparency-transparency-is-trust","TRANSPARENCY: TRANSPARENCY IS TRUST",{"depth":259,"slug":451,"text":452},"conclusion","Conclusion",[],[],{"title":421},[],"neural-internet/how_to_delegate",{"id":457,"data":459,"body":464,"filePath":465,"digest":466,"rendered":467},{"title":460,"editUrl":15,"head":461,"template":17,"sidebar":462,"pagefind":15,"draft":19},"How to Delegate?",[],{"hidden":19,"attrs":463},{},"For staking via frontend/wallets, you will need a wallet like Talisman, The Bittensor Wallet, Nova, or the Polkadotjs Wallet Extension.\n\nNeural Inτerneτ validator hotkey is: `5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA`\n\nDelegating to the above hotkey address means you are staking to Neural Internet’s validator. You can ‘undelegate’ or unstake any time and receive your $TAO back in your cold key address.\n\n### Delegation Process\n\n---\n\n### **Delegating through a frontend (Recommended)**\n\nFollow the steps as described on this front end here: [Staking to NI](https://delegate.taostats.io/staking?hkey=5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA)\n\n---\n\n### **Delegating through CLI**\n\nThose who run Bittensor through the command can delegate to our validator by copy-pasting the following command into their console. Make sure your $TAO is unstacked and sitting in your cold key before running the command:\n\nCopy\n\n```\nbtcli stake add --netuid X --name default --hotkey-ss58-address 5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA --amount X\n```\n\nNI Validator: `5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA`","src/content/docs/neural internet/how_to_delegate.md","d9f3cd7cb029c347",{"html":468,"metadata":469},"\u003Cp>For staking via frontend/wallets, you will need a wallet like Talisman, The Bittensor Wallet, Nova, or the Polkadotjs Wallet Extension.\u003C/p>\n\u003Cp>Neural Inτerneτ validator hotkey is: \u003Ccode dir=\"auto\">5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA\u003C/code>\u003C/p>\n\u003Cp>Delegating to the above hotkey address means you are staking to Neural Internet’s validator. You can ‘undelegate’ or unstake any time and receive your $TAO back in your cold key address.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"delegation-process\">Delegation Process\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#delegation-process\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Delegation Process”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"delegating-through-a-frontend-recommended\">\u003Cstrong>Delegating through a frontend (Recommended)\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#delegating-through-a-frontend-recommended\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Delegating through a frontend (Recommended)”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Follow the steps as described on this front end here: \u003Ca href=\"https://delegate.taostats.io/staking?hkey=5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA\">Staking to NI\u003C/a>\u003C/p>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"delegating-through-cli\">\u003Cstrong>Delegating through CLI\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#delegating-through-cli\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Delegating through CLI”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Those who run Bittensor through the command can delegate to our validator by copy-pasting the following command into their console. Make sure your $TAO is unstacked and sitting in your cold key before running the command:\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Clink rel=\"stylesheet\" href=\"/_astro/ec.v4551.css\">\u003Cscript type=\"module\" src=\"/_astro/ec.8zarh.js\">\u003C/script>\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">btcli stake add --netuid X --name default --hotkey-ss58-address 5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA --amount X\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"btcli stake add --netuid X --name default --hotkey-ss58-address 5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA --amount X\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Cp>NI Validator: \u003Ccode dir=\"auto\">5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA\u003C/code>\u003C/p>",{"headings":470,"localImagePaths":480,"remoteImagePaths":481,"frontmatter":482,"imagePaths":483},[471,474,477],{"depth":259,"slug":472,"text":473},"delegation-process","Delegation Process",{"depth":259,"slug":475,"text":476},"delegating-through-a-frontend-recommended","Delegating through a frontend (Recommended)",{"depth":259,"slug":478,"text":479},"delegating-through-cli","Delegating through CLI",[],[],{"title":460},[],"neural-internet/dtao_a_high_level_overview",{"id":484,"data":486,"body":491,"filePath":492,"digest":493,"rendered":494},{"title":487,"editUrl":15,"head":488,"template":17,"sidebar":489,"pagefind":15,"draft":19},"dTAO: A High-Level Overview",[],{"hidden":19,"attrs":490},{},"**Dynamic TAO** is a major upgrade to the Bittensor network’s staking mechanism. It shifts the power to allocate network emissions from a small group of root validators to every TAO holder, thereby promoting a more direct and democratic participation in the network’s growth and success.\n\n## 1. What is Dynamic TAO?\n\nDynamic TAO (dTAO) transforms the way staking works in Bittensor. In the previous system, TAO holders would simply delegate their stake to a validator who then decided which subnets to support. With dTAO, TAO holders now choose directly which subnet they want to back. When you stake TAO, it is exchanged for that subnet’s token (for simplicity, we refer to all subnet tokens as **ALPHA**), and your stake directly influences the subnet’s share of emissions.\n\n## 2. How Does It Work?\n\n### Staking to Subnets\n\n* **Direct Control:**\n  When staking TAO, you select the subnet that you wish to support. Your TAO is then exchanged for the subnet’s ALPHA tokens.\n* **Emission Influence:**\n  The amount of TAO staked in a subnet’s pool determines the volume of emissions that subnet receives. More TAO in a subnet’s pool means a higher share of the emissions per new block.\n\n![](https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FoLd2WkkEFljHZxd031yS%252FBittensor%2520Dynamic%2520TAO%2520%28dTAO%29%2520Overview%2520-%2520visual%2520selection.png%3Falt%3Dmedia%26token%3Db834a2d1-23a8-4b84-8b25-bcb3328cfd5e&width=768&dpr=4&quality=100&sign=5b05bdc7&sv=2)\n\n### Root Staking (Subnet 0)\n\n* **Safety and Stability:**\n  Root staking allows you to stake TAO without converting it into ALPHA tokens. Your TAO remains as TAO, protecting your stake from price fluctuations.\n* **Reward Mechanism:**\n  Even with root staking, you earn rewards. The validator you stake to applies your stake across their registered subnets, collects ALPHA as rewards, and automatically converts those tokens back into TAO, simplifying the management process. Root dividends are much less than if you participated and staked directly to subnets.\n\n## 3. Emission Model\n\nEach new block in the network creates rewards in both TAO and ALPHA tokens. The allocation is designed to be balanced:\n\n* **TAO Emissions:**\n  Each subnet receives TAO rewards proportional to the amount of TAO staked in its pool. For example, if a subnet holds 30% of all staked TAO across the network, it receives 30% of the new TAO emissions per block.\n* **ALPHA Emissions:**\n  In addition to TAO, each block issues a fixed amount of ALPHA tokens. These tokens are distributed between the subnet’s pool and the participants. The ALPHA tokens represent support for the subnet and can later be exchanged for TAO.\n\n## 4. Early Staking Dynamics\n\n* **Initial Phase:**\n  When a new subnet launches, its token price starts at a 1:1 rate (1.0 TAO per ALPHA) and generally experiences an initial decrease due to high inflation and low overall emissions.\n* **Opportunities and Risks:**\n  Early stakers can accumulate a larger proportion of ALPHA tokens when there is less competition. However, this phase also carries higher risk—early token holdings may fluctuate in value as the subnet grows and attracts more stake.\n\n## 5. Incentives and Democratic Control\n\nDynamic TAO introduces a more direct form of democracy in the network:\n\n* **Direct Influence:**\n  Every TAO holder’s staking decision directly affects the distribution of emissions. This replaces the previous system where large validators, acting as representatives, determined the flow of emissions.\n* **Long-Term Alignment:**\n  Validators, miners, and subnet owners are all incentivized to support subnets that deliver real value. Selling ALPHA tokens can decrease a stakeholder’s influence, reduce potential rewards, and lower a subnet’s overall value. Consequently, long-term holding of ALPHA is encouraged to help strengthen the network.\n* **Decentralized Decision-Making:**\n  The success of each subnet is now directly tied to the support it receives from the community. High-performing subnets attract more stake and, in turn, receive more emissions, creating a self-reinforcing cycle of growth and quality.\n\n## 6. Evolution and Future Outlook\n\n* **Third Major Upgrade:**\n  Dynamic TAO is the third major upgrade in Bittensor’s evolution—following Finney (which enabled stake delegation to validators) and Revolution (which allowed permissionless creation of subnets and commodities).\n* **Addressing Past Challenges:**\n  The shift to dynamic TAO addresses issues related to biased emission allocation and the increasing complexity of valuing subnets. By decentralizing emissions, every TAO holder now has a direct say in the network’s direction.\n* **Future Growth:**\n  As the network continues to evolve, new tools and educational resources will be provided to help users engage with dTAO. The system is designed to stimulate growth by lowering barriers to entry, fostering innovation in subnet design, and aligning incentives among all participants.\n\n## 7. Summary\n\nBittensor Dynamic TAO represents a significant evolution in how staking and emissions are managed on the network. By empowering every TAO holder to choose which subnet to support, dTAO transforms the network from a representative model into one of direct democracy. This change not only enhances fairness and decentralization but also aligns the interests of validators, miners, subnet owners, and stakers toward building a more robust and innovative ecosystem.\n\n*Disclaimer: The design and parameters of Dynamic TAO are still under discussion and subject to change. Please stay tuned for updates as the network continues to evolve.*","src/content/docs/neural internet/dtao_a_high_level_overview.md","c29fb67ccca24b4c",{"html":495,"metadata":496},"\u003Cp>\u003Cstrong>Dynamic TAO\u003C/strong> is a major upgrade to the Bittensor network’s staking mechanism. It shifts the power to allocate network emissions from a small group of root validators to every TAO holder, thereby promoting a more direct and democratic participation in the network’s growth and success.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"1-what-is-dynamic-tao\">1. What is Dynamic TAO?\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#1-what-is-dynamic-tao\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “1. What is Dynamic TAO?”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Dynamic TAO (dTAO) transforms the way staking works in Bittensor. In the previous system, TAO holders would simply delegate their stake to a validator who then decided which subnets to support. With dTAO, TAO holders now choose directly which subnet they want to back. When you stake TAO, it is exchanged for that subnet’s token (for simplicity, we refer to all subnet tokens as \u003Cstrong>ALPHA\u003C/strong>), and your stake directly influences the subnet’s share of emissions.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"2-how-does-it-work\">2. How Does It Work?\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#2-how-does-it-work\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “2. How Does It Work?”\u003C/span>\u003C/a>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"staking-to-subnets\">Staking to Subnets\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#staking-to-subnets\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Staking to Subnets”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Direct Control:\u003C/strong>\nWhen staking TAO, you select the subnet that you wish to support. Your TAO is then exchanged for the subnet’s ALPHA tokens.\u003C/li>\n\u003Cli>\u003Cstrong>Emission Influence:\u003C/strong>\nThe amount of TAO staked in a subnet’s pool determines the volume of emissions that subnet receives. More TAO in a subnet’s pool means a higher share of the emissions per new block.\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cimg src=\"https://docs.neuralinternet.ai/~gitbook/image?url=https%3A%2F%2F3449997301-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FRCFZhMrQpz0DM7pJi6mq%252Fuploads%252FoLd2WkkEFljHZxd031yS%252FBittensor%2520Dynamic%2520TAO%2520%28dTAO%29%2520Overview%2520-%2520visual%2520selection.png%3Falt%3Dmedia%26token%3Db834a2d1-23a8-4b84-8b25-bcb3328cfd5e&#x26;width=768&#x26;dpr=4&#x26;quality=100&#x26;sign=5b05bdc7&#x26;sv=2\" alt=\"\">\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"root-staking-subnet-0\">Root Staking (Subnet 0)\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#root-staking-subnet-0\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Root Staking (Subnet 0)”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Safety and Stability:\u003C/strong>\nRoot staking allows you to stake TAO without converting it into ALPHA tokens. Your TAO remains as TAO, protecting your stake from price fluctuations.\u003C/li>\n\u003Cli>\u003Cstrong>Reward Mechanism:\u003C/strong>\nEven with root staking, you earn rewards. The validator you stake to applies your stake across their registered subnets, collects ALPHA as rewards, and automatically converts those tokens back into TAO, simplifying the management process. Root dividends are much less than if you participated and staked directly to subnets.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"3-emission-model\">3. Emission Model\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#3-emission-model\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “3. Emission Model”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Each new block in the network creates rewards in both TAO and ALPHA tokens. The allocation is designed to be balanced:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>TAO Emissions:\u003C/strong>\nEach subnet receives TAO rewards proportional to the amount of TAO staked in its pool. For example, if a subnet holds 30% of all staked TAO across the network, it receives 30% of the new TAO emissions per block.\u003C/li>\n\u003Cli>\u003Cstrong>ALPHA Emissions:\u003C/strong>\nIn addition to TAO, each block issues a fixed amount of ALPHA tokens. These tokens are distributed between the subnet’s pool and the participants. The ALPHA tokens represent support for the subnet and can later be exchanged for TAO.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"4-early-staking-dynamics\">4. Early Staking Dynamics\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#4-early-staking-dynamics\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “4. Early Staking Dynamics”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Initial Phase:\u003C/strong>\nWhen a new subnet launches, its token price starts at a 1:1 rate (1.0 TAO per ALPHA) and generally experiences an initial decrease due to high inflation and low overall emissions.\u003C/li>\n\u003Cli>\u003Cstrong>Opportunities and Risks:\u003C/strong>\nEarly stakers can accumulate a larger proportion of ALPHA tokens when there is less competition. However, this phase also carries higher risk—early token holdings may fluctuate in value as the subnet grows and attracts more stake.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"5-incentives-and-democratic-control\">5. Incentives and Democratic Control\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#5-incentives-and-democratic-control\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “5. Incentives and Democratic Control”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Dynamic TAO introduces a more direct form of democracy in the network:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Direct Influence:\u003C/strong>\nEvery TAO holder’s staking decision directly affects the distribution of emissions. This replaces the previous system where large validators, acting as representatives, determined the flow of emissions.\u003C/li>\n\u003Cli>\u003Cstrong>Long-Term Alignment:\u003C/strong>\nValidators, miners, and subnet owners are all incentivized to support subnets that deliver real value. Selling ALPHA tokens can decrease a stakeholder’s influence, reduce potential rewards, and lower a subnet’s overall value. Consequently, long-term holding of ALPHA is encouraged to help strengthen the network.\u003C/li>\n\u003Cli>\u003Cstrong>Decentralized Decision-Making:\u003C/strong>\nThe success of each subnet is now directly tied to the support it receives from the community. High-performing subnets attract more stake and, in turn, receive more emissions, creating a self-reinforcing cycle of growth and quality.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"6-evolution-and-future-outlook\">6. Evolution and Future Outlook\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#6-evolution-and-future-outlook\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “6. Evolution and Future Outlook”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Third Major Upgrade:\u003C/strong>\nDynamic TAO is the third major upgrade in Bittensor’s evolution—following Finney (which enabled stake delegation to validators) and Revolution (which allowed permissionless creation of subnets and commodities).\u003C/li>\n\u003Cli>\u003Cstrong>Addressing Past Challenges:\u003C/strong>\nThe shift to dynamic TAO addresses issues related to biased emission allocation and the increasing complexity of valuing subnets. By decentralizing emissions, every TAO holder now has a direct say in the network’s direction.\u003C/li>\n\u003Cli>\u003Cstrong>Future Growth:\u003C/strong>\nAs the network continues to evolve, new tools and educational resources will be provided to help users engage with dTAO. The system is designed to stimulate growth by lowering barriers to entry, fostering innovation in subnet design, and aligning incentives among all participants.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"7-summary\">7. Summary\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#7-summary\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “7. Summary”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Bittensor Dynamic TAO represents a significant evolution in how staking and emissions are managed on the network. By empowering every TAO holder to choose which subnet to support, dTAO transforms the network from a representative model into one of direct democracy. This change not only enhances fairness and decentralization but also aligns the interests of validators, miners, subnet owners, and stakers toward building a more robust and innovative ecosystem.\u003C/p>\n\u003Cp>\u003Cem>Disclaimer: The design and parameters of Dynamic TAO are still under discussion and subject to change. Please stay tuned for updates as the network continues to evolve.\u003C/em>\u003C/p>",{"headings":497,"localImagePaths":525,"remoteImagePaths":526,"frontmatter":527,"imagePaths":528},[498,501,504,507,510,513,516,519,522],{"depth":263,"slug":499,"text":500},"1-what-is-dynamic-tao","1. What is Dynamic TAO?",{"depth":263,"slug":502,"text":503},"2-how-does-it-work","2. How Does It Work?",{"depth":259,"slug":505,"text":506},"staking-to-subnets","Staking to Subnets",{"depth":259,"slug":508,"text":509},"root-staking-subnet-0","Root Staking (Subnet 0)",{"depth":263,"slug":511,"text":512},"3-emission-model","3. Emission Model",{"depth":263,"slug":514,"text":515},"4-early-staking-dynamics","4. Early Staking Dynamics",{"depth":263,"slug":517,"text":518},"5-incentives-and-democratic-control","5. Incentives and Democratic Control",{"depth":263,"slug":520,"text":521},"6-evolution-and-future-outlook","6. Evolution and Future Outlook",{"depth":263,"slug":523,"text":524},"7-summary","7. Summary",[],[],{"title":487},[],"neural-internet/ni_dao",{"id":529,"data":531,"body":536,"filePath":537,"digest":538,"rendered":539},{"title":532,"editUrl":15,"head":533,"template":17,"sidebar":534,"pagefind":15,"draft":19},"NI DAO",[],{"hidden":19,"attrs":535},{},"**Neural Internet operates as a Decentralized Autonomous Organization LLC.** DAOs are the future of corporate structure that operates through smart-contracts on the blockchain**.** Unlike traditional organizations, DAOs lack a central authority and instead distribute power among their members.\n\nWhile some may argue that DAOs are still in their infancy and face governance challenges, we at Neural Internet see them as the future of empowerment, especially in the age of Stakeholder Capitalism and the burgeoning creator/knowledge economy.\n\n**Key Benefits of Operating a DAO:**\n\n1. **Decentralized Governance:** DAOs democratize decision-making, DAO founders to vote on key issues. This structure ensures a broader representation and involvement in organizational decisions.\n2. **Transparency and Trust:** All transactions and votes in a DAO are recorded on the blockchain, ensuring transparency and accountability. This builds trust among participants and stakeholders.\n3. **Efficiency and Adaptability:** DAOs can adapt quickly to changes and execute decisions efficiently without bureaucratic delays. Smart contracts automate many processes, further enhancing operational efficiency.\n4. **Distributed Participation:** DAOs enable worldwide participation, breaking geographical barriers. Members from any location can contribute and have a say in the organization.\n5. **Innovation and Community Engagement:** By leveraging collective wisdom, DAOs can drive innovation. They foster a sense of community and shared purpose, which can be crucial for motivation and progress.\n\nOperating a DAO offers a forward-looking approach to governance and organization, harnessing the power of blockchain for greater transparency, efficiency, and participatory decision-making. It represents a shift towards more egalitarian, adaptable, and transparent operations, making it an attractive model for organizations in the digital age.\n\nAs we stand on the brink of a new era, we at Neural Internet invite you to join us in embracing DAOs for the transformative power they hold. Let's envision a future where DAOs and stakeholder capitalism coalesce, paving the way for a more just, inclusive, and sustainable world.","src/content/docs/neural internet/ni_dao.md","513cd1e75def9571",{"html":540,"metadata":541},"\u003Cp>\u003Cstrong>Neural Internet operates as a Decentralized Autonomous Organization LLC.\u003C/strong> DAOs are the future of corporate structure that operates through smart-contracts on the blockchain**.** Unlike traditional organizations, DAOs lack a central authority and instead distribute power among their members.\u003C/p>\n\u003Cp>While some may argue that DAOs are still in their infancy and face governance challenges, we at Neural Internet see them as the future of empowerment, especially in the age of Stakeholder Capitalism and the burgeoning creator/knowledge economy.\u003C/p>\n\u003Cp>\u003Cstrong>Key Benefits of Operating a DAO:\u003C/strong>\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>Decentralized Governance:\u003C/strong> DAOs democratize decision-making, DAO founders to vote on key issues. This structure ensures a broader representation and involvement in organizational decisions.\u003C/li>\n\u003Cli>\u003Cstrong>Transparency and Trust:\u003C/strong> All transactions and votes in a DAO are recorded on the blockchain, ensuring transparency and accountability. This builds trust among participants and stakeholders.\u003C/li>\n\u003Cli>\u003Cstrong>Efficiency and Adaptability:\u003C/strong> DAOs can adapt quickly to changes and execute decisions efficiently without bureaucratic delays. Smart contracts automate many processes, further enhancing operational efficiency.\u003C/li>\n\u003Cli>\u003Cstrong>Distributed Participation:\u003C/strong> DAOs enable worldwide participation, breaking geographical barriers. Members from any location can contribute and have a say in the organization.\u003C/li>\n\u003Cli>\u003Cstrong>Innovation and Community Engagement:\u003C/strong> By leveraging collective wisdom, DAOs can drive innovation. They foster a sense of community and shared purpose, which can be crucial for motivation and progress.\u003C/li>\n\u003C/ol>\n\u003Cp>Operating a DAO offers a forward-looking approach to governance and organization, harnessing the power of blockchain for greater transparency, efficiency, and participatory decision-making. It represents a shift towards more egalitarian, adaptable, and transparent operations, making it an attractive model for organizations in the digital age.\u003C/p>\n\u003Cp>As we stand on the brink of a new era, we at Neural Internet invite you to join us in embracing DAOs for the transformative power they hold. Let’s envision a future where DAOs and stakeholder capitalism coalesce, paving the way for a more just, inclusive, and sustainable world.\u003C/p>",{"headings":542,"localImagePaths":543,"remoteImagePaths":544,"frontmatter":545,"imagePaths":546},[],[],[],{"title":532},[],"neural-internet/ni_validator",{"id":547,"data":549,"body":554,"filePath":555,"digest":556,"rendered":557},{"title":550,"editUrl":15,"head":551,"template":17,"sidebar":552,"pagefind":15,"draft":19},"NI Validator",[],{"hidden":19,"attrs":553},{},"**Validator Delegate Hotkey:** 5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA\n\nStake Now: [Staking to NI](https://delegate.taostats.io/staking?hkey=5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA)\n\n### **Validating Intelligence**\n\nThe Bittensor network comprises two types of nodes: **Miners** and **Validators**. Miners, when prompted by Validators, provide information and are assessed based on the value of their responses.\n\nAs a **validator** on the Bittensor protocol, **Neural Internet** plays a crucial role in maintaining the network's integrity and security. Our validation process involves verifying the computational work done by miners, ensuring the quality and reliability of the network. Our participation supports decentralized AI initiatives and aligns with Bittensor’s vision of democratizing AI technologies.\n\n### **Access to Collective Knowledge and Resources**\n\nIn terms of network access, validators serve as the gateway through which applications connect to the network. Our position as a validator enables us to facilitate broader access to the vast array of computational resources, datasets, and AI models available on the Bittensor network. This access fosters **collaboration, innovation, and development** in decentralized AI, allowing developers, researchers, and businesses to tap into a shared pool of cutting-edge knowledge and resources.\n\n### **Nominee Delegation**\n\nIt is the community’s responsibility to recognize the validators contributing most to Bittensor’s success and delegate to them. By delegating or “investing” in those that build and sustain the ecosystem, the community ensures that value remains within the network and is **returned to stakeholders**. This unique mechanism incentivizes those who contribute and naturally phases out those who do not.\n\n---\n\n## **NI Validator: The Premier Compute Validator on Subnet 27**\n\n### **Specializing in NI Compute**\n\nThe **NI Validator** is dedicated exclusively to **NI Compute** (**Subnet 27).** Unlike multi-subnet validators that spread their focus thin, NI Validator is **laser-focused on Compute**, maximizing efficiency, accuracy, and reliability for AI model training, inference, and high-performance workloads.\n\n### **Invest in the Future of AI Compute**\n\nDelegates who stake to the NI Validator not only earn **competitive staking rewards** but also gain **access** to the NI Token—staking them directly in the **Compute Commodity** of the future. This positions delegates as stakeholders in a decentralized AI infrastructure designed to rival traditional cloud providers while upholding **transparency, fairness, and cost-efficiency**.\n\n**Benefits for Delegates:**\n\n* **Gain exposure to the NI Token**, a share in the decentralized AI compute market.\n* **Become a stakeholder in AI Compute**, supporting a commodity that fuels next-generation machine intelligence.\n* **Participate in network governance**, influencing the evolution of decentralized GPU networks.\n\n## **Shaping the Future of Decentralized AI**\n\nBy staking with **NI Validator**, delegates directly **contribute to building a decentralized, permissionless AI compute network**—one that breaks free from monopolized infrastructure and hands power back to the users. The NI Compute subnet (SN27) is redefining how AI models are trained, deployed, and accessed. Delegating here is an investment in a **future where compute is democratized, open, and optimized for innovation**.\n\nJoin **NI Validator** on **Subnet 27** and stake in the next evolution of decentralized artificial intelligence.","src/content/docs/neural internet/ni_validator.md","14968ef6d7859516",{"html":558,"metadata":559},"\u003Cp>\u003Cstrong>Validator Delegate Hotkey:\u003C/strong> 5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA\u003C/p>\n\u003Cp>Stake Now: \u003Ca href=\"https://delegate.taostats.io/staking?hkey=5GmvyePN9aYErXBBhBnxZKGoGk4LKZApE4NkaSzW62CYCYNA\">Staking to NI\u003C/a>\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"validating-intelligence\">\u003Cstrong>Validating Intelligence\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#validating-intelligence\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Validating Intelligence”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The Bittensor network comprises two types of nodes: \u003Cstrong>Miners\u003C/strong> and \u003Cstrong>Validators\u003C/strong>. Miners, when prompted by Validators, provide information and are assessed based on the value of their responses.\u003C/p>\n\u003Cp>As a \u003Cstrong>validator\u003C/strong> on the Bittensor protocol, \u003Cstrong>Neural Internet\u003C/strong> plays a crucial role in maintaining the network’s integrity and security. Our validation process involves verifying the computational work done by miners, ensuring the quality and reliability of the network. Our participation supports decentralized AI initiatives and aligns with Bittensor’s vision of democratizing AI technologies.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"access-to-collective-knowledge-and-resources\">\u003Cstrong>Access to Collective Knowledge and Resources\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#access-to-collective-knowledge-and-resources\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Access to Collective Knowledge and Resources”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>In terms of network access, validators serve as the gateway through which applications connect to the network. Our position as a validator enables us to facilitate broader access to the vast array of computational resources, datasets, and AI models available on the Bittensor network. This access fosters \u003Cstrong>collaboration, innovation, and development\u003C/strong> in decentralized AI, allowing developers, researchers, and businesses to tap into a shared pool of cutting-edge knowledge and resources.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"nominee-delegation\">\u003Cstrong>Nominee Delegation\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#nominee-delegation\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Nominee Delegation”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>It is the community’s responsibility to recognize the validators contributing most to Bittensor’s success and delegate to them. By delegating or “investing” in those that build and sustain the ecosystem, the community ensures that value remains within the network and is \u003Cstrong>returned to stakeholders\u003C/strong>. This unique mechanism incentivizes those who contribute and naturally phases out those who do not.\u003C/p>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"ni-validator-the-premier-compute-validator-on-subnet-27\">\u003Cstrong>NI Validator: The Premier Compute Validator on Subnet 27\u003C/strong>\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#ni-validator-the-premier-compute-validator-on-subnet-27\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “NI Validator: The Premier Compute Validator on Subnet 27”\u003C/span>\u003C/a>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"specializing-in-ni-compute\">\u003Cstrong>Specializing in NI Compute\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#specializing-in-ni-compute\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Specializing in NI Compute”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The \u003Cstrong>NI Validator\u003C/strong> is dedicated exclusively to \u003Cstrong>NI Compute\u003C/strong> (\u003Cstrong>Subnet 27).\u003C/strong> Unlike multi-subnet validators that spread their focus thin, NI Validator is \u003Cstrong>laser-focused on Compute\u003C/strong>, maximizing efficiency, accuracy, and reliability for AI model training, inference, and high-performance workloads.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"invest-in-the-future-of-ai-compute\">\u003Cstrong>Invest in the Future of AI Compute\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#invest-in-the-future-of-ai-compute\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Invest in the Future of AI Compute”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Delegates who stake to the NI Validator not only earn \u003Cstrong>competitive staking rewards\u003C/strong> but also gain \u003Cstrong>access\u003C/strong> to the NI Token—staking them directly in the \u003Cstrong>Compute Commodity\u003C/strong> of the future. This positions delegates as stakeholders in a decentralized AI infrastructure designed to rival traditional cloud providers while upholding \u003Cstrong>transparency, fairness, and cost-efficiency\u003C/strong>.\u003C/p>\n\u003Cp>\u003Cstrong>Benefits for Delegates:\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Gain exposure to the NI Token\u003C/strong>, a share in the decentralized AI compute market.\u003C/li>\n\u003Cli>\u003Cstrong>Become a stakeholder in AI Compute\u003C/strong>, supporting a commodity that fuels next-generation machine intelligence.\u003C/li>\n\u003Cli>\u003Cstrong>Participate in network governance\u003C/strong>, influencing the evolution of decentralized GPU networks.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"shaping-the-future-of-decentralized-ai\">\u003Cstrong>Shaping the Future of Decentralized AI\u003C/strong>\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#shaping-the-future-of-decentralized-ai\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Shaping the Future of Decentralized AI”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>By staking with \u003Cstrong>NI Validator\u003C/strong>, delegates directly \u003Cstrong>contribute to building a decentralized, permissionless AI compute network\u003C/strong>—one that breaks free from monopolized infrastructure and hands power back to the users. The NI Compute subnet (SN27) is redefining how AI models are trained, deployed, and accessed. Delegating here is an investment in a \u003Cstrong>future where compute is democratized, open, and optimized for innovation\u003C/strong>.\u003C/p>\n\u003Cp>Join \u003Cstrong>NI Validator\u003C/strong> on \u003Cstrong>Subnet 27\u003C/strong> and stake in the next evolution of decentralized artificial intelligence.\u003C/p>",{"headings":560,"localImagePaths":582,"remoteImagePaths":583,"frontmatter":584,"imagePaths":585},[561,564,567,570,573,576,579],{"depth":259,"slug":562,"text":563},"validating-intelligence","Validating Intelligence",{"depth":259,"slug":565,"text":566},"access-to-collective-knowledge-and-resources","Access to Collective Knowledge and Resources",{"depth":259,"slug":568,"text":569},"nominee-delegation","Nominee Delegation",{"depth":263,"slug":571,"text":572},"ni-validator-the-premier-compute-validator-on-subnet-27","NI Validator: The Premier Compute Validator on Subnet 27",{"depth":259,"slug":574,"text":575},"specializing-in-ni-compute","Specializing in NI Compute",{"depth":259,"slug":577,"text":578},"invest-in-the-future-of-ai-compute","Invest in the Future of AI Compute",{"depth":263,"slug":580,"text":581},"shaping-the-future-of-decentralized-ai","Shaping the Future of Decentralized AI",[],[],{"title":550},[],"neural-internet/nodexo_cloud_platform",{"id":586,"data":588,"body":593,"filePath":594,"digest":595,"rendered":596},{"title":589,"editUrl":15,"head":590,"template":17,"sidebar":591,"pagefind":15,"draft":19},"Nodexo Cloud Platform",[],{"hidden":19,"attrs":592},{},"## **Overview**\n\nNodexo is the **flagship decentralized cloud computing platform** powered by **NI Compute (Subnet 27)**. Designed to offer **seamless, on-demand access to GPU resources**, Nodexo provides enterprises, researchers, and developers with **high-performance compute for AI training, inference, and large-scale computational tasks**—all within a **trustless and decentralized** ecosystem.\n\nBuilt on the **Bittensor subnet architecture**, Nodexo **removes reliance on centralized cloud providers**, optimizing cost, efficiency, and **global GPU availability** through a **permissionless network** of **miners and validators**.\n\n---\n\n## **Key Features**\n\n#### **Decentralized Infrastructure**\n\n* **Powered by Bittensor Subnet 27** (NI Compute)\n* **Trustless GPU validation** via **Proof-of-GPU (PoG)**\n* **Self-sustaining economic model** driven by **real-world compute demand**\n* **Fair, transparent resource allocation** across a **distributed network**\n\n#### **Enterprise-Grade GPU Access**\n\n* **Bare-metal and cloud-integrated GPU support** (H200, H100, A100, etc.)\n* **Optimized performance for AI/ML workloads**\n* **Low-latency, high-throughput compute capabilities**\n\n#### **Seamless API Integration**\n\n* **Fully modular API** for programmatic GPU allocation\n* **Plug-and-play architecture for AI-driven applications**\n\n#### **Efficient Cost Model**\n\n* **Dynamic, demand-driven pricing**\n* **100% of API revenue reinvested into GPU liquidity** (via **$NI buybacks & recycling**)\n* **Optimized cost efficiency over traditional cloud GPU rental**\n\n---\n\n### **How It Works**\n\n#### **For Renters (Users)**\n\n1. **Sign Up & Fund Account**\n\n   * Register on the **Nodexo Cloud** platform.\n   * Set up **payment method** (Fiat, TAO, USDC).\n2. **Browse & Rent Compute**\n\n   * Search available **GPU resources** in real time.\n   * Select from **pre-configured instances** or customize compute needs.\n3. **Deploy & Scale AI Models**\n\n   * Run AI training, inference, or computational workloads **seamlessly**.\n   * Monitor and manage **resource usage** through an **intuitive dashboard**.\n\n#### **For GPU Providers (Miners)**\n\n1. **Join the NI Compute Subnet (SN27)**\n\n   * Register as a **GPU provider** (miner).\n   * List available **compute resources**.\n2. **Earn Rewards via PoG**\n\n   * GPUs are benchmarked via **Proof-of-GPU (PoG)** validation.\n   * **$NI rewards** are distributed based on hardware performance and uptime.\n3. **Scale & Optimize**\n\n   * Dynamically **adjust resources** based on market demand.\n   * Maximize returns through **continuous workload provisioning**.\n\n---\n\n### **Why Choose Nodexo Cloud?**\n\n#### **For Developers & AI Teams**\n\n* **Cost-effective compute scaling** for **LLMs, training, and inference**.\n* **No vendor lock-in**—access **global decentralized GPU networks**.\n* **Seamless API integration** into **existing AI platforms**.\n\n#### **For Enterprises & Research Institutions**\n\n* **Reliable access to enterprise-grade GPUs** at a fraction of traditional costs.\n* **Trustless compute allocation** backed by cryptographic **PoG validation**.\n* **Self-sustaining incentive model**, ensuring **long-term resource availability**.\n\n#### **For GPU Providers & Validators**\n\n* **Monetize idle GPUs** with **higher ROI than centralized platforms**.\n* **Earn $NI for contributing to the compute economy**.\n* **Participate in decentralized AI governance and long-term network growth**.\n\n---\n\n### **The Future of Decentralized AI Compute**\n\nNodexo Cloud is **more than just a cloud platform**—it is the **foundation of the decentralized AI revolution**. By **eliminating centralized control** over computational resources, it ensures that **compute power remains accessible, efficient, and aligned with the principles of decentralization**.\n\nJoin the future of AI computing today—**rent, stake, mine, and govern the future of decentralized intelligence with Nodexo Cloud.**","src/content/docs/neural internet/nodexo_cloud_platform.md","6d343520942c4157",{"html":597,"metadata":598},"\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"overview\">\u003Cstrong>Overview\u003C/strong>\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#overview\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Overview”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Nodexo is the \u003Cstrong>flagship decentralized cloud computing platform\u003C/strong> powered by \u003Cstrong>NI Compute (Subnet 27)\u003C/strong>. Designed to offer \u003Cstrong>seamless, on-demand access to GPU resources\u003C/strong>, Nodexo provides enterprises, researchers, and developers with \u003Cstrong>high-performance compute for AI training, inference, and large-scale computational tasks\u003C/strong>—all within a \u003Cstrong>trustless and decentralized\u003C/strong> ecosystem.\u003C/p>\n\u003Cp>Built on the \u003Cstrong>Bittensor subnet architecture\u003C/strong>, Nodexo \u003Cstrong>removes reliance on centralized cloud providers\u003C/strong>, optimizing cost, efficiency, and \u003Cstrong>global GPU availability\u003C/strong> through a \u003Cstrong>permissionless network\u003C/strong> of \u003Cstrong>miners and validators\u003C/strong>.\u003C/p>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"key-features\">\u003Cstrong>Key Features\u003C/strong>\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#key-features\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Key Features”\u003C/span>\u003C/a>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"decentralized-infrastructure\">\u003Cstrong>Decentralized Infrastructure\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#decentralized-infrastructure\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Decentralized Infrastructure”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Powered by Bittensor Subnet 27\u003C/strong> (NI Compute)\u003C/li>\n\u003Cli>\u003Cstrong>Trustless GPU validation\u003C/strong> via \u003Cstrong>Proof-of-GPU (PoG)\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Self-sustaining economic model\u003C/strong> driven by \u003Cstrong>real-world compute demand\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Fair, transparent resource allocation\u003C/strong> across a \u003Cstrong>distributed network\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"enterprise-grade-gpu-access\">\u003Cstrong>Enterprise-Grade GPU Access\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#enterprise-grade-gpu-access\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Enterprise-Grade GPU Access”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Bare-metal and cloud-integrated GPU support\u003C/strong> (H200, H100, A100, etc.)\u003C/li>\n\u003Cli>\u003Cstrong>Optimized performance for AI/ML workloads\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Low-latency, high-throughput compute capabilities\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"seamless-api-integration\">\u003Cstrong>Seamless API Integration\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#seamless-api-integration\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Seamless API Integration”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Fully modular API\u003C/strong> for programmatic GPU allocation\u003C/li>\n\u003Cli>\u003Cstrong>Plug-and-play architecture for AI-driven applications\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"efficient-cost-model\">\u003Cstrong>Efficient Cost Model\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#efficient-cost-model\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Efficient Cost Model”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Dynamic, demand-driven pricing\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>100% of API revenue reinvested into GPU liquidity\u003C/strong> (via \u003Cstrong>$NI buybacks &#x26; recycling\u003C/strong>)\u003C/li>\n\u003Cli>\u003Cstrong>Optimized cost efficiency over traditional cloud GPU rental\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"how-it-works\">\u003Cstrong>How It Works\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#how-it-works\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “How It Works”\u003C/span>\u003C/a>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"for-renters-users\">\u003Cstrong>For Renters (Users)\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#for-renters-users\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “For Renters (Users)”\u003C/span>\u003C/a>\u003C/div>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Cstrong>Sign Up &#x26; Fund Account\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Register on the \u003Cstrong>Nodexo Cloud\u003C/strong> platform.\u003C/li>\n\u003Cli>Set up \u003Cstrong>payment method\u003C/strong> (Fiat, TAO, USDC).\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Browse &#x26; Rent Compute\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Search available \u003Cstrong>GPU resources\u003C/strong> in real time.\u003C/li>\n\u003Cli>Select from \u003Cstrong>pre-configured instances\u003C/strong> or customize compute needs.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Deploy &#x26; Scale AI Models\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Run AI training, inference, or computational workloads \u003Cstrong>seamlessly\u003C/strong>.\u003C/li>\n\u003Cli>Monitor and manage \u003Cstrong>resource usage\u003C/strong> through an \u003Cstrong>intuitive dashboard\u003C/strong>.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"for-gpu-providers-miners\">\u003Cstrong>For GPU Providers (Miners)\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#for-gpu-providers-miners\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “For GPU Providers (Miners)”\u003C/span>\u003C/a>\u003C/div>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Cstrong>Join the NI Compute Subnet (SN27)\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Register as a \u003Cstrong>GPU provider\u003C/strong> (miner).\u003C/li>\n\u003Cli>List available \u003Cstrong>compute resources\u003C/strong>.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Earn Rewards via PoG\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>GPUs are benchmarked via \u003Cstrong>Proof-of-GPU (PoG)\u003C/strong> validation.\u003C/li>\n\u003Cli>\u003Cstrong>$NI rewards\u003C/strong> are distributed based on hardware performance and uptime.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Scale &#x26; Optimize\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Dynamically \u003Cstrong>adjust resources\u003C/strong> based on market demand.\u003C/li>\n\u003Cli>Maximize returns through \u003Cstrong>continuous workload provisioning\u003C/strong>.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ol>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"why-choose-nodexo-cloud\">\u003Cstrong>Why Choose Nodexo Cloud?\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#why-choose-nodexo-cloud\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Why Choose Nodexo Cloud?”\u003C/span>\u003C/a>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"for-developers--ai-teams\">\u003Cstrong>For Developers &#x26; AI Teams\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#for-developers--ai-teams\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “For Developers &#x26; AI Teams”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Cost-effective compute scaling\u003C/strong> for \u003Cstrong>LLMs, training, and inference\u003C/strong>.\u003C/li>\n\u003Cli>\u003Cstrong>No vendor lock-in\u003C/strong>—access \u003Cstrong>global decentralized GPU networks\u003C/strong>.\u003C/li>\n\u003Cli>\u003Cstrong>Seamless API integration\u003C/strong> into \u003Cstrong>existing AI platforms\u003C/strong>.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"for-enterprises--research-institutions\">\u003Cstrong>For Enterprises &#x26; Research Institutions\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#for-enterprises--research-institutions\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “For Enterprises &#x26; Research Institutions”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Reliable access to enterprise-grade GPUs\u003C/strong> at a fraction of traditional costs.\u003C/li>\n\u003Cli>\u003Cstrong>Trustless compute allocation\u003C/strong> backed by cryptographic \u003Cstrong>PoG validation\u003C/strong>.\u003C/li>\n\u003Cli>\u003Cstrong>Self-sustaining incentive model\u003C/strong>, ensuring \u003Cstrong>long-term resource availability\u003C/strong>.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"for-gpu-providers--validators\">\u003Cstrong>For GPU Providers &#x26; Validators\u003C/strong>\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#for-gpu-providers--validators\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “For GPU Providers &#x26; Validators”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Monetize idle GPUs\u003C/strong> with \u003Cstrong>higher ROI than centralized platforms\u003C/strong>.\u003C/li>\n\u003Cli>\u003Cstrong>Earn $NI for contributing to the compute economy\u003C/strong>.\u003C/li>\n\u003Cli>\u003Cstrong>Participate in decentralized AI governance and long-term network growth\u003C/strong>.\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"the-future-of-decentralized-ai-compute\">\u003Cstrong>The Future of Decentralized AI Compute\u003C/strong>\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#the-future-of-decentralized-ai-compute\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “The Future of Decentralized AI Compute”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Nodexo Cloud is \u003Cstrong>more than just a cloud platform\u003C/strong>—it is the \u003Cstrong>foundation of the decentralized AI revolution\u003C/strong>. By \u003Cstrong>eliminating centralized control\u003C/strong> over computational resources, it ensures that \u003Cstrong>compute power remains accessible, efficient, and aligned with the principles of decentralization\u003C/strong>.\u003C/p>\n\u003Cp>Join the future of AI computing today—\u003Cstrong>rent, stake, mine, and govern the future of decentralized intelligence with Nodexo Cloud.\u003C/strong>\u003C/p>",{"headings":599,"localImagePaths":641,"remoteImagePaths":642,"frontmatter":643,"imagePaths":644},[600,602,605,608,611,614,617,620,623,626,629,632,635,638],{"depth":263,"slug":601,"text":57},"overview",{"depth":263,"slug":603,"text":604},"key-features","Key Features",{"depth":327,"slug":606,"text":607},"decentralized-infrastructure","Decentralized Infrastructure",{"depth":327,"slug":609,"text":610},"enterprise-grade-gpu-access","Enterprise-Grade GPU Access",{"depth":327,"slug":612,"text":613},"seamless-api-integration","Seamless API Integration",{"depth":327,"slug":615,"text":616},"efficient-cost-model","Efficient Cost Model",{"depth":259,"slug":618,"text":619},"how-it-works","How It Works",{"depth":327,"slug":621,"text":622},"for-renters-users","For Renters (Users)",{"depth":327,"slug":624,"text":625},"for-gpu-providers-miners","For GPU Providers (Miners)",{"depth":259,"slug":627,"text":628},"why-choose-nodexo-cloud","Why Choose Nodexo Cloud?",{"depth":327,"slug":630,"text":631},"for-developers--ai-teams","For Developers & AI Teams",{"depth":327,"slug":633,"text":634},"for-enterprises--research-institutions","For Enterprises & Research Institutions",{"depth":327,"slug":636,"text":637},"for-gpu-providers--validators","For GPU Providers & Validators",{"depth":259,"slug":639,"text":640},"the-future-of-decentralized-ai-compute","The Future of Decentralized AI Compute",[],[],{"title":589},[],"neural-internet/powered_by_bittensor",{"id":645,"data":647,"body":651,"filePath":652,"digest":653,"rendered":654},{"title":322,"editUrl":15,"head":648,"template":17,"sidebar":649,"pagefind":15,"draft":19},[],{"hidden":19,"attrs":650},{},"`Bittensor is revolutionizing the development of machine learning platforms by decentralizing the process and creating a peer-to-peer market for machine intelligence. It enables the collective intelligence of AI models to come together, forming a digital hive mind. This decentralized approach allows for the rapid expansion and sharing of knowledge, akin to an unstoppable library of knowledge that grows exponentially. By harnessing the power of distributed networks and incentivizing collaboration, Bittensor is driving innovation and pushing the boundaries of machine learning. -`[`bittensor.org`](https://bittensor.org/)\n\n## What is Bittensor?\n\n### Permission-less P2P digital commodity market.\n\nStarted as an open-source machine learning protocol, Bittensor has expanded into a decentralized commodity market incentivized with a unified digital asset, $TAO. These commodity markets called subnetworks all interact and integrate into a single computing infrastructure on Bittensor’s blockchain.\n\nThis framework enables the building of a hierarchical web of distributed resources, which ultimately leads to the creation of machine intelligence. Bittensor’s unique incentivization mechanism encourages continual development, improvement, and participation. This leads to a self-sustaining and progressively advancing supercomputer with a vast amount of resources from intelligence to compute.\n\nAs a permission-less and incentivized network, Bittensor is poised to become the largest and most intelligent neural network in the world, redefining machine intelligence as a key commodity of the 21st century. Machine intelligence should be a shared asset, aligning with the collective interests of humanity. Bittensor symbolizes this future of unbiased, uncensored, and democratized AI where intelligence is a tool of collective empowerment rather than a control mechanism of the select few.\n\n**Decentralized Commodity Markets**: Bittensor functions as a language to create numerous decentralized markets, called 'subnets', under a unified token system. These markets operate through Bittensor's blockchain, allowing for interaction and integration into a single computing infrastructure, similar to how Ethereum expanded on Bitcoin's concept to support decentralized contracts​​.\n\n**Framework for Intelligence Creation**: The framework Bittensor offers enables the building of a hierarchical web of resources, which ultimately leads to the creation of intelligence. This intelligence leverages computation, data, and storage within the ecosystem. Bittensor's vision is to harness digital markets for the benefit of Artificial Intelligence, ensuring that the benefits and ownership of machine intelligence are accessible to a broader audience​​.\n\n**Benefits for Various Stakeholders**: For developers, Bittensor offers a platform to create markets for bespoke commodities like computational power, exploiting the vast scale and efficiency of decentralized systems. Consumers get access to resources at lower costs without intermediaries. Bittensor as a whole aims to create Machine Intelligence in an open and equitable manner on top of these sub-markets​​.\n\n**Operational Dynamics**: Bittensor's computational power is abstracted underneath its markets. It leverages machine learning models and physical storage to create diverse products, without defining the specifics of resource production. This abstraction allows for innovation and problem-solving by numerous individuals globally​​.\n\n**Similarities and Differences with Bitcoin**: Bittensor, like Bitcoin, has a transferable, censorship-resistant token (TAO) and is run by miners. However, unlike Bitcoin, which focuses on token economics and resistance to external influences, Bittensor's primary goal is to build markets that create value. This involves incentivizing its compute network to grow significantly, with miners working to reduce costs and create digital intelligence products.\n\n## Yuma Consensus\n\nThe Yuma Consensus is an innovative consensus mechanism developed by the Bittensor Network. It's a decentralized, peer-to-peer algorithm that facilitates the distribution of computational resources across a network of nodes. This mechanism is revolutionary for AI development because it creates a decentralized AI network free from central or single-entity control, enhancing efficiency, robustness, and security.\n\nKey features include:\n\n* **Decentralized Distribution of Computational Resources**: It enables the distribution of computational resources across a network of miners and validators, facilitating complex tasks and larger datasets, and scaling as more participants join.\n* **Hybrid Proof-of-Work and Proof-of-Stake Mechanisms**: Miners perform computational work to validate transactions and create blocks, rewarded with tokens. Validators, holding $TAO, verify this work and participate in the consensus process, aligning their actions with the network's best interests.\n* **Support for Decentralized AI Applications:** The Yuma Consensus allows decentralized AI applications to run across a network of miners and validators, enhancing scalability and security, and reducing single points of failure.\n* **Scalability:** The decentralized nature allows easy network expansion as more participants join, suitable for handling larger AI workloads.\n* **Flexibility and Customization**: The network's decentralized structure allows customization for various organizational and application needs, prioritizing miners based on computational power or validation level.\n* **Robustness and Resilience Against Censorship**: Designed for robustness, the consensus is designed to support unstoppable AI applications, free from censorship or interference by central authorities\n* **Balancing Energy Consumption and Centralization**: This hybrid mechanism addresses the energy consumption of proof-of-work and centralization risks of proof-of-stake, maintaining a secure, decentralized network.\n\nAt the heart of Bittensor is the Yuma consensus mechanism, allowing for the permissionless creation of subnets and consensus models. This flexibility empowers the community to determine what is necessary and valuable, leading to a network that is both dynamic, responsive, and ever-evolving.\n\n### Subnetworks\n\n\"Subnets compartmentalize different components of the Bittensor network, enabling a new approach to incentivizing and governing AI advancements. Each subnet allows developers to design custom incentive systems tailored to the specific needs of each AI component\" - tensorspace.io\n\n## Intelligence as a Commodity\n\nThe creation of digital trust, also known as Bitcoin, demonstrated how computing can be organized using a protocol to produce something superior to anything that can be achieved by a centralized organization.\n\nBittensor utilizes digital trust to establish markets for AI, resulting in the creation of the first cognitive economy. This economy prices and enables the exchange of pure machine intelligence between consumers and producers, forming the first neural internet that enables researchers to connect directly and encourages collaboration to produce a more advanced intelligence source for the web.\n\n*\"We're doing something similar to Bitcoin at Bittensor, where there is consensus around who is creating value & we can distribute tokens to those computers...*\n\n*Thus, creating a competitive & incentivized mechanism for the creation of machine intelligence\" -* **Opentensor Foundation**\n\n## The Future of AI is Decentralized\n\n\"The future of AI should be decentralized, full stop. We need to treat the knowledge created by AI as the ultimate commodity, the equivalent of a cognitive process in the human mind. Collaborating together to increase the breadth and depth of this knowledge is not only our calling as scientists, but our calling as humans as well.\" - Bittensor Co-Founder Ala Shaabana","src/content/docs/neural internet/powered_by_bittensor.md","b4a1727e7f8f8ff3",{"html":655,"metadata":656},"\u003Cp>\u003Ccode dir=\"auto\">Bittensor is revolutionizing the development of machine learning platforms by decentralizing the process and creating a peer-to-peer market for machine intelligence. It enables the collective intelligence of AI models to come together, forming a digital hive mind. This decentralized approach allows for the rapid expansion and sharing of knowledge, akin to an unstoppable library of knowledge that grows exponentially. By harnessing the power of distributed networks and incentivizing collaboration, Bittensor is driving innovation and pushing the boundaries of machine learning. -\u003C/code>\u003Ca href=\"https://bittensor.org/\">\u003Ccode dir=\"auto\">bittensor.org\u003C/code>\u003C/a>\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"what-is-bittensor\">What is Bittensor?\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#what-is-bittensor\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “What is Bittensor?”\u003C/span>\u003C/a>\u003C/div>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"permission-less-p2p-digital-commodity-market\">Permission-less P2P digital commodity market.\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#permission-less-p2p-digital-commodity-market\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Permission-less P2P digital commodity market.”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Started as an open-source machine learning protocol, Bittensor has expanded into a decentralized commodity market incentivized with a unified digital asset, $TAO. These commodity markets called subnetworks all interact and integrate into a single computing infrastructure on Bittensor’s blockchain.\u003C/p>\n\u003Cp>This framework enables the building of a hierarchical web of distributed resources, which ultimately leads to the creation of machine intelligence. Bittensor’s unique incentivization mechanism encourages continual development, improvement, and participation. This leads to a self-sustaining and progressively advancing supercomputer with a vast amount of resources from intelligence to compute.\u003C/p>\n\u003Cp>As a permission-less and incentivized network, Bittensor is poised to become the largest and most intelligent neural network in the world, redefining machine intelligence as a key commodity of the 21st century. Machine intelligence should be a shared asset, aligning with the collective interests of humanity. Bittensor symbolizes this future of unbiased, uncensored, and democratized AI where intelligence is a tool of collective empowerment rather than a control mechanism of the select few.\u003C/p>\n\u003Cp>\u003Cstrong>Decentralized Commodity Markets\u003C/strong>: Bittensor functions as a language to create numerous decentralized markets, called ‘subnets’, under a unified token system. These markets operate through Bittensor’s blockchain, allowing for interaction and integration into a single computing infrastructure, similar to how Ethereum expanded on Bitcoin’s concept to support decentralized contracts​​.\u003C/p>\n\u003Cp>\u003Cstrong>Framework for Intelligence Creation\u003C/strong>: The framework Bittensor offers enables the building of a hierarchical web of resources, which ultimately leads to the creation of intelligence. This intelligence leverages computation, data, and storage within the ecosystem. Bittensor’s vision is to harness digital markets for the benefit of Artificial Intelligence, ensuring that the benefits and ownership of machine intelligence are accessible to a broader audience​​.\u003C/p>\n\u003Cp>\u003Cstrong>Benefits for Various Stakeholders\u003C/strong>: For developers, Bittensor offers a platform to create markets for bespoke commodities like computational power, exploiting the vast scale and efficiency of decentralized systems. Consumers get access to resources at lower costs without intermediaries. Bittensor as a whole aims to create Machine Intelligence in an open and equitable manner on top of these sub-markets​​.\u003C/p>\n\u003Cp>\u003Cstrong>Operational Dynamics\u003C/strong>: Bittensor’s computational power is abstracted underneath its markets. It leverages machine learning models and physical storage to create diverse products, without defining the specifics of resource production. This abstraction allows for innovation and problem-solving by numerous individuals globally​​.\u003C/p>\n\u003Cp>\u003Cstrong>Similarities and Differences with Bitcoin\u003C/strong>: Bittensor, like Bitcoin, has a transferable, censorship-resistant token (TAO) and is run by miners. However, unlike Bitcoin, which focuses on token economics and resistance to external influences, Bittensor’s primary goal is to build markets that create value. This involves incentivizing its compute network to grow significantly, with miners working to reduce costs and create digital intelligence products.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"yuma-consensus\">Yuma Consensus\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#yuma-consensus\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Yuma Consensus”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The Yuma Consensus is an innovative consensus mechanism developed by the Bittensor Network. It’s a decentralized, peer-to-peer algorithm that facilitates the distribution of computational resources across a network of nodes. This mechanism is revolutionary for AI development because it creates a decentralized AI network free from central or single-entity control, enhancing efficiency, robustness, and security.\u003C/p>\n\u003Cp>Key features include:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Decentralized Distribution of Computational Resources\u003C/strong>: It enables the distribution of computational resources across a network of miners and validators, facilitating complex tasks and larger datasets, and scaling as more participants join.\u003C/li>\n\u003Cli>\u003Cstrong>Hybrid Proof-of-Work and Proof-of-Stake Mechanisms\u003C/strong>: Miners perform computational work to validate transactions and create blocks, rewarded with tokens. Validators, holding $TAO, verify this work and participate in the consensus process, aligning their actions with the network’s best interests.\u003C/li>\n\u003Cli>\u003Cstrong>Support for Decentralized AI Applications:\u003C/strong> The Yuma Consensus allows decentralized AI applications to run across a network of miners and validators, enhancing scalability and security, and reducing single points of failure.\u003C/li>\n\u003Cli>\u003Cstrong>Scalability:\u003C/strong> The decentralized nature allows easy network expansion as more participants join, suitable for handling larger AI workloads.\u003C/li>\n\u003Cli>\u003Cstrong>Flexibility and Customization\u003C/strong>: The network’s decentralized structure allows customization for various organizational and application needs, prioritizing miners based on computational power or validation level.\u003C/li>\n\u003Cli>\u003Cstrong>Robustness and Resilience Against Censorship\u003C/strong>: Designed for robustness, the consensus is designed to support unstoppable AI applications, free from censorship or interference by central authorities\u003C/li>\n\u003Cli>\u003Cstrong>Balancing Energy Consumption and Centralization\u003C/strong>: This hybrid mechanism addresses the energy consumption of proof-of-work and centralization risks of proof-of-stake, maintaining a secure, decentralized network.\u003C/li>\n\u003C/ul>\n\u003Cp>At the heart of Bittensor is the Yuma consensus mechanism, allowing for the permissionless creation of subnets and consensus models. This flexibility empowers the community to determine what is necessary and valuable, leading to a network that is both dynamic, responsive, and ever-evolving.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"subnetworks\">Subnetworks\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#subnetworks\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Subnetworks”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>“Subnets compartmentalize different components of the Bittensor network, enabling a new approach to incentivizing and governing AI advancements. Each subnet allows developers to design custom incentive systems tailored to the specific needs of each AI component” - tensorspace.io\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"intelligence-as-a-commodity\">Intelligence as a Commodity\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#intelligence-as-a-commodity\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Intelligence as a Commodity”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The creation of digital trust, also known as Bitcoin, demonstrated how computing can be organized using a protocol to produce something superior to anything that can be achieved by a centralized organization.\u003C/p>\n\u003Cp>Bittensor utilizes digital trust to establish markets for AI, resulting in the creation of the first cognitive economy. This economy prices and enables the exchange of pure machine intelligence between consumers and producers, forming the first neural internet that enables researchers to connect directly and encourages collaboration to produce a more advanced intelligence source for the web.\u003C/p>\n\u003Cp>\u003Cem>“We’re doing something similar to Bitcoin at Bittensor, where there is consensus around who is creating value &#x26; we can distribute tokens to those computers…\u003C/em>\u003C/p>\n\u003Cp>\u003Cem>Thus, creating a competitive &#x26; incentivized mechanism for the creation of machine intelligence” -\u003C/em> \u003Cstrong>Opentensor Foundation\u003C/strong>\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"the-future-of-ai-is-decentralized\">The Future of AI is Decentralized\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#the-future-of-ai-is-decentralized\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “The Future of AI is Decentralized”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>“The future of AI should be decentralized, full stop. We need to treat the knowledge created by AI as the ultimate commodity, the equivalent of a cognitive process in the human mind. Collaborating together to increase the breadth and depth of this knowledge is not only our calling as scientists, but our calling as humans as well.” - Bittensor Co-Founder Ala Shaabana\u003C/p>",{"headings":657,"localImagePaths":676,"remoteImagePaths":677,"frontmatter":678,"imagePaths":679},[658,661,664,667,670,673],{"depth":263,"slug":659,"text":660},"what-is-bittensor","What is Bittensor?",{"depth":259,"slug":662,"text":663},"permission-less-p2p-digital-commodity-market","Permission-less P2P digital commodity market.",{"depth":263,"slug":665,"text":666},"yuma-consensus","Yuma Consensus",{"depth":259,"slug":668,"text":669},"subnetworks","Subnetworks",{"depth":263,"slug":671,"text":672},"intelligence-as-a-commodity","Intelligence as a Commodity",{"depth":263,"slug":674,"text":675},"the-future-of-ai-is-decentralized","The Future of AI is Decentralized",[],[],{"title":322},[],"validator-system/database-operations",{"id":680,"data":682,"body":687,"filePath":688,"digest":689,"deferredRender":15},{"title":683,"editUrl":15,"head":684,"template":17,"sidebar":685,"pagefind":15,"draft":19},"Database Operations",[],{"hidden":19,"attrs":686},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/utils/db.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/database/allocate.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/allocate.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/database/challenge.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/challenge.py\" />\n\n  \u003CSourceLink text=\"neurons/validator.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThis document covers the database operations used by the NI Compute Subnet validator system to persist miner statistics, challenge results, allocation records, and proof-of-GPU data. The system uses SQLite for local data persistence with structured schemas for tracking network state and performance metrics.\n\nFor information about the scoring algorithms that use this data, see [Scoring System](/validator-system/scoring-system#2.2). For details about proof-of-GPU validation that writes to these tables, see [Proof of GPU](/validator-system/proof-of-gpu#2.1).\n\n## Database Architecture\n\nThe compute subnet uses a centralized SQLite database managed by the `ComputeDb` class to store all validator-related data. The database serves as the primary persistence layer for validator operations, storing everything from miner registration details to performance benchmarks.\n\n```mermaid\ngraph TB\n    subgraph \"Database Layer\"\n        ComputeDb[\"ComputeDb\u003Cbr/>SQLite Connection Manager\"]\n        SQLiteDB[(\"SQLite Database\u003Cbr/>database.db\")]\n    end\n    \n    subgraph \"Core Tables\"\n        MinerTable[\"miner\u003Cbr/>(uid, ss58_address)\"]\n        MinerDetailsTable[\"miner_details\u003Cbr/>(hotkey, details, no_specs_count)\"]\n        ChallengeTable[\"challenge_details\u003Cbr/>(uid, success, elapsed_time, difficulty)\"]\n        AllocationTable[\"allocation\u003Cbr/>(hotkey, details)\"]\n        PogStatsTable[\"pog_stats\u003Cbr/>(hotkey, gpu_name, num_gpus)\"]\n        StatsTable[\"stats\u003Cbr/>(uid, hotkey, gpu_specs, score)\"]\n        BlacklistTable[\"blacklist\u003Cbr/>(hotkey, details)\"]\n        WandbTable[\"wandb_runs\u003Cbr/>(hotkey, run_id)\"]\n    end\n    \n    subgraph \"Accessing Components\"\n        ValidatorProcess[\"Validator Process\u003Cbr/>neurons/validator.py\"]\n        ChallengeOps[\"Challenge Operations\u003Cbr/>database/challenge.py\"]\n        AllocateOps[\"Allocation Operations\u003Cbr/>database/allocate.py\"]\n        PogOps[\"PoG Operations\u003Cbr/>database/pog.py\"]\n        MinerOps[\"Miner Operations\u003Cbr/>database/miner.py\"]\n    end\n    \n    ComputeDb --> SQLiteDB\n    \n    SQLiteDB --> MinerTable\n    SQLiteDB --> MinerDetailsTable\n    SQLiteDB --> ChallengeTable\n    SQLiteDB --> AllocationTable\n    SQLiteDB --> PogStatsTable\n    SQLiteDB --> StatsTable\n    SQLiteDB --> BlacklistTable\n    SQLiteDB --> WandbTable\n    \n    ValidatorProcess --> ComputeDb\n    ChallengeOps --> ComputeDb\n    AllocateOps --> ComputeDb\n    PogOps --> ComputeDb\n    MinerOps --> ComputeDb\n    \n    MinerTable -.->|\"Foreign Key\"| ChallengeTable\n    MinerDetailsTable -.->|\"Foreign Key\"| PogStatsTable\n    MinerDetailsTable -.->|\"Foreign Key\"| StatsTable\n```\n\nSources: \u003CSourceLink text=\"compute/utils/db.py:9-84\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py#L9-L84\" />, \u003CSourceLink text=\"neurons/validator.py:170-172\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L170-L172\" />\n\n## Core Database Tables\n\nThe database schema consists of eight primary tables, each serving specific validator functions:\n\n| Table Name | Primary Key | Purpose | Key Relationships |\n|------------|-------------|---------|-------------------|\n| `miner` | `uid` | Basic miner registration | Referenced by `challenge_details` |\n| `miner_details` | `hotkey` | Hardware specifications and Docker status | Referenced by `pog_stats`, `stats` |\n| `challenge_details` | Auto-increment | Proof-of-Work challenge results | Foreign keys to `miner` table |\n| `allocation` | `hotkey` | Active resource allocations | Unique hotkey constraint |\n| `pog_stats` | Auto-increment | Proof-of-GPU benchmark results | Foreign key to `miner_details` |\n| `stats` | `uid` | Comprehensive miner scoring data | Foreign key to `miner_details` |\n| `blacklist` | Auto-increment | Penalized miner hotkeys | Unique hotkey constraint |\n| `wandb_runs` | `hotkey` | WandB run tracking | Links to external monitoring |\n\n### Miner Registration Tables\n\nThe `miner` table stores basic network registration data, while `miner_details` contains comprehensive hardware specifications:\n\n```mermaid\nerDiagram\n    miner {\n        INTEGER uid PK\n        TEXT ss58_address UK\n    }\n    \n    miner_details {\n        INTEGER id PK\n        TEXT hotkey UK\n        TEXT details\n        INTEGER no_specs_count\n    }\n    \n    challenge_details {\n        INTEGER uid FK\n        TEXT ss58_address FK\n        BOOLEAN success\n        REAL elapsed_time\n        INTEGER difficulty\n        TIMESTAMP created_at\n    }\n    \n    miner ||--o{ challenge_details : \"has challenges\"\n    miner_details ||--o{ pog_stats : \"has PoG results\"\n    miner_details ||--o{ stats : \"has statistics\"\n```\n\nSources: \u003CSourceLink text=\"compute/utils/db.py:29-30\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py#L29-L30\" />, \u003CSourceLink text=\"compute/utils/db.py:30-31\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py#L30-L31\" />, \u003CSourceLink text=\"compute/utils/db.py:33-45\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py#L33-L45\" />\n\n### Performance and Allocation Tracking\n\nThe system maintains detailed performance metrics and resource allocation state through specialized tables:\n\n```mermaid\ngraph LR\n    subgraph \"Performance Tracking\"\n        PogStats[\"pog_stats\u003Cbr/>GPU benchmarking results\"]\n        ChallengeDetails[\"challenge_details\u003Cbr/>PoW challenge outcomes\"]\n        Stats[\"stats\u003Cbr/>Aggregated scoring data\"]\n    end\n    \n    subgraph \"Resource Management\"\n        Allocation[\"allocation\u003Cbr/>Active resource assignments\"]\n        Blacklist[\"blacklist\u003Cbr/>Penalized miners\"]\n        WandbRuns[\"wandb_runs\u003Cbr/>External monitoring links\"]\n    end\n    \n    subgraph \"Data Sources\"\n        ValidatorProcess[\"Validator Process\"]\n        PogValidation[\"PoG Validation\"]\n        AllocationAPI[\"Allocation API\"]\n    end\n    \n    ValidatorProcess --> Stats\n    PogValidation --> PogStats\n    ValidatorProcess --> ChallengeDetails\n    AllocationAPI --> Allocation\n    ValidatorProcess --> Blacklist\n    ValidatorProcess --> WandbRuns\n```\n\nSources: \u003CSourceLink text=\"compute/utils/db.py:53-62\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py#L53-L62\" />, \u003CSourceLink text=\"compute/utils/db.py:64-77\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py#L64-L77\" />, \u003CSourceLink text=\"compute/utils/db.py:46-47\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py#L46-L47\" />\n\n## Database Operations by Component\n\n### Challenge Management Operations\n\nThe challenge system tracks proof-of-work validation results with comprehensive statistical analysis:\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator Process\"\n    participant CD as \"ChallengeOps\u003Cbr/>challenge.py\"\n    participant DB as \"ComputeDb\"\n    participant CT as \"challenge_details table\"\n    \n    V->>CD: update_challenge_details(pow_benchmarks)\n    CD->>DB: get_cursor()\n    CD->>CT: INSERT challenge results\n    Note over CD,CT: Bulk insert with executemany\n    CD->>DB: commit()\n    \n    V->>CD: select_challenge_stats()\n    CD->>CT: Complex query with CTEs\n    Note over CD,CT: Analyzes last 60 attempts\u003Cbr/>calculates success rates\n    CD->>CD: Process statistics\n    CD-->>V: Return aggregated stats dict\n```\n\nThe `select_challenge_stats` function uses Common Table Expressions (CTEs) to analyze challenge performance over rolling windows, calculating success rates and average difficulties for the most recent 20 and 60 attempts.\n\nSources: \u003CSourceLink text=\"neurons/Validator/database/challenge.py:24-125\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/challenge.py#L24-L125\" />, \u003CSourceLink text=\"neurons/Validator/database/challenge.py:128-176\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/challenge.py#L128-L176\" />\n\n### Miner Details and Allocation Management\n\nAllocation operations manage hardware specifications and resource assignment state:\n\n```mermaid\ngraph TD\n    subgraph \"Miner Specification Flow\"\n        GetSpecs[\"get_miner_details()\u003Cbr/>Retrieve all miner specs\"]\n        UpdateSpecs[\"update_miner_details()\u003Cbr/>Batch update from WandB\"]\n        CheckDocker[\"select_has_docker_miners_hotkey()\u003Cbr/>Filter Docker-capable miners\"]\n    end\n    \n    subgraph \"Allocation Management\"\n        AllocateCheck[\"select_allocate_miners_hotkey()\u003Cbr/>Find miners meeting requirements\"]\n        UpdateAllocation[\"update_allocation_db()\u003Cbr/>Track active allocations\"]\n        UpdateBlacklist[\"update_blacklist_db()\u003Cbr/>Manage penalized miners\"]\n    end\n    \n    subgraph \"Database Tables\"\n        MinerDetailsTable[(\"miner_details\")]\n        AllocationTable[(\"allocation\")]\n        BlacklistTable[(\"blacklist\")]\n    end\n    \n    GetSpecs --> MinerDetailsTable\n    UpdateSpecs --> MinerDetailsTable\n    CheckDocker --> MinerDetailsTable\n    \n    AllocateCheck --> MinerDetailsTable\n    UpdateAllocation --> AllocationTable\n    UpdateBlacklist --> BlacklistTable\n    \n    MinerDetailsTable -.->|\"JSON details parsing\"| AllocateCheck\n```\n\nThe `update_miner_details` function includes automatic schema migration logic to handle database structure changes while preserving existing data.\n\nSources: \u003CSourceLink text=\"neurons/Validator/database/allocate.py:26-45\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/allocate.py#L26-L45\" />, \u003CSourceLink text=\"neurons/Validator/database/allocate.py:93-176\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/allocate.py#L93-L176\" />, \u003CSourceLink text=\"neurons/Validator/database/allocate.py:178-206\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/allocate.py#L178-L206\" />\n\n### Proof-of-GPU Statistics Management\n\nPoG operations maintain GPU benchmarking results and performance metrics:\n\n```mermaid\nflowchart LR\n    subgraph \"PoG Database Operations\"\n        UpdatePogStats[\"update_pog_stats()\u003Cbr/>Store GPU benchmark results\"]\n        GetPogSpecs[\"get_pog_specs()\u003Cbr/>Retrieve GPU specifications\"]\n        RetrieveStats[\"retrieve_stats()\u003Cbr/>Load scoring statistics\"]\n        WriteStats[\"write_stats()\u003Cbr/>Update comprehensive stats\"]\n    end\n    \n    subgraph \"Data Tables\"\n        PogStatsTable[(\"pog_stats\u003Cbr/>hotkey, gpu_name, num_gpus\")]\n        StatsTable[(\"stats\u003Cbr/>uid, score, gpu_specs\")]\n    end\n    \n    subgraph \"Processing Flow\"\n        PogValidation[\"PoG Validation Process\"]\n        ScoreCalculation[\"Score Calculation\"]\n        NetworkWeights[\"Network Weight Setting\"]\n    end\n    \n    PogValidation --> UpdatePogStats\n    UpdatePogStats --> PogStatsTable\n    GetPogSpecs --> PogStatsTable\n    \n    RetrieveStats --> StatsTable\n    WriteStats --> StatsTable\n    \n    GetPogSpecs --> ScoreCalculation\n    RetrieveStats --> ScoreCalculation\n    ScoreCalculation --> WriteStats\n    WriteStats --> NetworkWeights\n```\n\nSources: \u003CSourceLink text=\"neurons/Validator/database/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py\" /> (referenced), \u003CSourceLink text=\"neurons/validator.py:361-362\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L361-L362\" />, \u003CSourceLink text=\"neurons/validator.py:402-403\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L402-L403\" />\n\n## Data Persistence Workflow\n\n### Validator Database Integration\n\nThe validator process integrates with the database through multiple synchronized operations:\n\n```mermaid\nsequenceDiagram\n    participant VP as \"Validator Process\u003Cbr/>validator.py\"\n    participant DB as \"ComputeDb\"\n    participant WB as \"WandB Integration\"\n    participant NS as \"Network State\"\n    \n    Note over VP: Initialization Phase\n    VP->>DB: ComputeDb()\n    VP->>DB: select_miners()\n    DB-->>VP: miners dict\n    \n    Note over VP: Scoring Synchronization\n    VP->>DB: retrieve_stats()\n    VP->>WB: get_allocated_hotkeys()\n    VP->>WB: get_stats_allocated()\n    VP->>VP: sync_scores()\n    VP->>DB: write_stats()\n    \n    Note over VP: Allocation Monitoring\n    VP->>DB: SELECT FROM allocation\n    VP->>WB: update_allocated_hotkeys()\n    \n    Note over VP: PoG Results Processing\n    VP->>VP: proof_of_gpu()\n    VP->>DB: update_pog_stats()\n    VP->>VP: sync_scores()\n    VP->>NS: Set network weights\n```\n\nThe validator maintains a continuous cycle of data synchronization between local database state, distributed WandB state, and blockchain network state.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:170-172\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L170-L172\" />, \u003CSourceLink text=\"neurons/validator.py:312-404\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L312-L404\" />, \u003CSourceLink text=\"neurons/validator.py:663-787\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L663-L787\" />\n\n### Database Transaction Management\n\nAll database operations use transaction-safe patterns with proper error handling:\n\n| Operation Type | Transaction Pattern | Error Handling |\n|----------------|-------------------|----------------|\n| Single Inserts | `cursor.execute()` + `commit()` | `rollback()` on exception |\n| Bulk Operations | `cursor.executemany()` + `commit()` | `rollback()` + logging |\n| Complex Queries | Read-only, no transaction | Exception logging only |\n| Schema Changes | DDL statements + `commit()` | `rollback()` + preservation |\n\nThe database connection uses `check_same_thread=False` to support multi-threaded validator operations while maintaining thread safety through proper cursor management.\n\nSources: \u003CSourceLink text=\"compute/utils/db.py:13-17\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/db.py#L13-L17\" />, \u003CSourceLink text=\"neurons/Validator/database/challenge.py:140-176\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/challenge.py#L140-L176\" />, \u003CSourceLink text=\"neurons/Validator/database/allocate.py:211-229\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/allocate.py#L211-L229\" />","src/content/docs/validator-system/database-operations.mdx","0522b7086040e3e6","neural-internet/we_are_ni",{"id":690,"data":692,"body":697,"filePath":698,"digest":699,"rendered":700},{"title":693,"editUrl":15,"head":694,"template":17,"sidebar":695,"pagefind":15,"draft":19},"We are NI",[],{"hidden":19,"attrs":696},{},"### **Neural Internet:** AI Acceleration through Decentralized Compute\n\n`Transforming compute power into a digital commodity—unlocking distributed, on-demand GPU cloud resources for AI acceleration. Powered by $NI.`\n\n[NextPowered By Bittensor](/neural-internet/powered-by-bittensor)\n\nLast updated 4 months ago","src/content/docs/neural internet/we_are_ni.md","11e850182eaf1d70",{"html":701,"metadata":702},"\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"neural-internet-ai-acceleration-through-decentralized-compute\">\u003Cstrong>Neural Internet:\u003C/strong> AI Acceleration through Decentralized Compute\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#neural-internet-ai-acceleration-through-decentralized-compute\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Neural Internet: AI Acceleration through Decentralized Compute”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>\u003Ccode dir=\"auto\">Transforming compute power into a digital commodity—unlocking distributed, on-demand GPU cloud resources for AI acceleration. Powered by $NI.\u003C/code>\u003C/p>\n\u003Cp>\u003Ca href=\"/neural-internet/powered-by-bittensor\">NextPowered By Bittensor\u003C/a>\u003C/p>\n\u003Cp>Last updated 4 months ago\u003C/p>",{"headings":703,"localImagePaths":707,"remoteImagePaths":708,"frontmatter":709,"imagePaths":710},[704],{"depth":259,"slug":705,"text":706},"neural-internet-ai-acceleration-through-decentralized-compute","Neural Internet: AI Acceleration through Decentralized Compute",[],[],{"title":693},[],"neural-internet/running_a_validator",{"id":711,"data":713,"body":718,"filePath":719,"digest":720,"rendered":721},{"title":714,"editUrl":15,"head":715,"template":17,"sidebar":716,"pagefind":15,"draft":19},"Running A Validator",[],{"hidden":19,"attrs":717},{},"## Introduction\n\n---\n\nValidators hold the critical responsibility of rigorously assessing and verifying the computational capabilities of miners within the NI Compute Subnet. This multifaceted evaluation process ensures that only high-performance and reliable hardware contributes to the network, maintaining its integrity and efficiency.\n\n#### Evaluation Process\n\n1. **Performance Data Collection**:\n\n   Validators initiate the evaluation by requesting miners to provide comprehensive performance data. This data encompasses not only processing speeds and efficiencies but also essential metrics such as Random Access Memory (RAM) capacity and disk space availability. By gathering detailed hardware specifications, validators gain a holistic understanding of each miner's computational resources.\n2. **Computational Integrity through PoG**:\n\n   Leveraging the **Proof-of-GPU (PoG)** mechanism, validators proceed to test the miners' computational integrity. PoG involves executing real-world AI workloads that simulate genuine machine learning tasks. These tasks are designed to evaluate both the processing power and the reliability of the miners' GPU systems over sustained periods, reflecting the demands of long-running AI model training and inference.\n\n   * **Benchmarking AI Workloads**: Validators deploy AI-centric benchmarks that include matrix multiplications and tensor operations at various data precisions (e.g., FP16, FP32). These benchmarks mimic actual AI workflows, ensuring that the performance metrics are directly relevant to real-world applications.\n   * **Sustained Performance Measurement**: Unlike traditional benchmarks that measure peak performance, PoG assesses a GPU's ability to maintain high floating-point operations per second (FLOPS) over extended durations. This ensures that GPUs do not throttle or overheat during prolonged AI tasks.\n3. **Accuracy and Precision Verification**:\n\n   In addition to measuring the time taken by miners to complete benchmarking tasks, validators meticulously verify the accuracy of the computational results. This involves:\n\n   * **Merkle Tree Proofs**: Validators generate Merkle trees from the computation results to create cryptographic proofs. These proofs are used to verify the integrity and correctness of the data produced by the miners.\n   * **Data Precision Checks**: Validators ensure that the results of matrix multiplications and tensor operations meet the expected precision levels, confirming that miners are performing computations accurately without significant errors.\n4. **Dynamic Scoring Mechanism**:\n\n   Based on the benchmarking and verification results, validators update each miner's score, reflecting a comprehensive view of their computational capacity, efficiency, and hardware quality. The scoring process considers various factors, including:\n\n   * **GPU Performance**: Evaluates the GPU's sustained FLOPS, memory capacity, and bandwidth based on PoG benchmarks.\n   * **CPU, RAM, and Storage**: Assesses the performance metrics of the CPU, RAM, and hard disk, ensuring a balanced evaluation of the miner's overall system capabilities.\n   * **Scalability and Multi-GPU Support**: Takes into account the miner's ability to scale performance across multiple GPUs, which is crucial for large-scale AI training tasks.\n\n   The **scoring algorithm** normalizes these metrics and applies predefined weights to calculate a total score for each miner. This score determines the miner's weight within the network, directly influencing their potential rewards and standing.\n\n---\n\n### Validator Script\n\nYou will have needed to follow the environment set up instructions until [step 6](/ni-ecosystem/ni-compute-sn27/ni-compute-subnet-miner-setup#id-6.-start-the-docker-service-in-compute-subnet).\n\nMaking sure you have the right dependencies and requirements to validate on Subnet 27.\n\nCopy\n\n```\n# To run the validator\ncd neurons\npython -m validator.py\n    --netuid \u003Cyour netuid> # The subnet id you want to connect to\n    --subtensor.network \u003Cyour chain url> # blockchain endpoint you want to connect\n    --wallet.name \u003Cyour validator wallet>  # name of your wallet\n    --wallet.hotkey \u003Cyour validator hotkey> # hotkey name of your wallet\n    --logging.debug # Run in debug mode, alternatively --logging.trace for trace mode\n```\n\n---\n\n### Understanding the Score Calculation Process\n\nThe score calculation function now determines a miner's performance primarily based on their GPU hardware and resource allocation. Only the GPUs listed below are supported and scored correctly.\n\n**GPU Base Scores**: The following GPUs are assigned specific base scores, reflecting their relative performance:\n\n* NVIDIA H200: 4.00\n* NVIDIA H100 80GB HBM3: 3.30\n* NVIDIA H100: 2.80\n* NVIDIA A100-SXM4-80GB: 1.90\n\n**Scaling Factor**: Determine the highest GPU base score, multiply it by 8 (the maximum number of GPUs), and set this scenario as the 100-point baseline. A scaling factor is derived so that using eight of the top GPU models equals 50 points.\n\n**GPU Score**: Multiply the chosen GPU’s base score by the number of GPUs (up to 8) and by the scaling factor to find the miner’s GPU score (0–50).\n\n#### Example 1: Miner A's Total Score\n\n* **GPU**: NVIDIA H200 (Base Score: 3.90)\n* **Number of GPUs**: 8\n\nStep-by-step calculation:\n\n1. Highest scenario: 4 \\* 8 = 32\n2. Scaling factor: 50 / 32 ≈ 1.5625\n3. GPU Score: 4 \\* 8 \\* 1.5625 ≈ 50\n\nTotal Score = 50\n\n### Resource Allocation Mechanism\n\nThe allocation mechanism within subnet 27 is designed to optimize the utilization of computational resources effectively. Key aspects of this mechanism include:\n\n1. **Resource Requirement Analysis:** The mechanism begins by analyzing the specific resource requirements of each task, including CPU, GPU, memory, and storage needs.\n2. **Miner Selection:** Based on the analysis, the mechanism selects suitable miners that meet the resource requirements. This selection process considers the current availability, performance history, and network weights of the miners.\n3. **Dynamic Allocation:** The allocation of tasks to miners is dynamic, allowing for real-time adjustments based on changing network conditions and miner performance.\n4. **Efficiency Optimization:** The mechanism aims to maximize network efficiency by matching the most suitable miners to each task, ensuring optimal use of the network's computational power.\n5. **Load Balancing:** It also incorporates load balancing strategies to prevent overburdening individual miners, thereby maintaining a healthy and sustainable network ecosystem.\n\nThrough these functionalities, the allocation mechanism ensures that computational resources are utilized efficiently and effectively, contributing to the overall robustness and performance of the network.\n\nValidators can send requests to reserve access to resources from miners by specifying the specs manually in the in `register.py` and running this script: \u003Chttps://github.com/neuralinternet/Compute-Subnet/blob/main/neurons/register.py> for example: `{'cpu':{'count':1}, 'gpu':{'count':1}, 'hard_disk':{'capacity':10737418240}, 'ram':{'capacity':1073741824}}`\n\n### Options\n\n---\n\nAll the list arguments are now using coma separator.\n\n* `-netuid`: (Optional) The chain subnet uid. Default: 27.\n* `-auto_update`: (Optional) Auto update the repository. Default: True.\n* `-blacklist.exploiters`: (Optional) Automatically use the list of internal exploiters hotkeys. Default: True.\n* `-blacklist.hotkeys \u003Chotkey_0,hotkey_1,...>`: (Optional) List of hotkeys to blacklist. Default: [].\n* `-blacklist.coldkeys \u003Ccoldkey_0,coldkey_1,...>`: (Optional) List of coldkeys to blacklist. Default: [].\n* `-whitelist.hotkeys \u003Chotkey_0,hotkey_1,...>`: (Optional) List of hotkeys to whitelist. Default: [].\n* `-whitelist.coldkeys \u003Ccoldkey_0,coldkey_1,...>`: (Optional) List of coldkeys to whitelist. Default: [].\n\n### Validator options\n\n---\n\nFlags that you can use with the validator script.\n\n* `--validator.whitelist.unrecognized`: (Optional) Whitelist the unrecognized miners. Default: False.\n* `--validator.perform.hardware.query \u003Cbool>`: (Optional) Perform the specs query - useful to register to a miner's machine. Default: True.\n* `--validator.specs.batch.size \u003Csize>`: (Optional) Batch size that perform the specs queries - For lower hardware specifications you might want to use a different batch\\_size than default. Keep in mind the lower is the batch\\_size the longer it will take to perform all challenge queries. Default: 64.\n* `--validator.force.update.prometheus`: (Optional) Force the try-update of prometheus version. Default: False.\n* `--validator.whitelist.updated.threshold`: (Optional) Total quorum before starting the whitelist. Default: 60. (%)","src/content/docs/neural internet/running_a_validator.md","cbad874ebb486f99",{"html":722,"metadata":723},"\u003Cdiv class=\"sl-heading-wrapper level-h2\">\u003Ch2 id=\"introduction\">Introduction\u003C/h2>\u003Ca class=\"sl-anchor-link\" href=\"#introduction\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Introduction”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cp>Validators hold the critical responsibility of rigorously assessing and verifying the computational capabilities of miners within the NI Compute Subnet. This multifaceted evaluation process ensures that only high-performance and reliable hardware contributes to the network, maintaining its integrity and efficiency.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"evaluation-process\">Evaluation Process\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#evaluation-process\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Evaluation Process”\u003C/span>\u003C/a>\u003C/div>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Cstrong>Performance Data Collection\u003C/strong>:\u003C/p>\n\u003Cp>Validators initiate the evaluation by requesting miners to provide comprehensive performance data. This data encompasses not only processing speeds and efficiencies but also essential metrics such as Random Access Memory (RAM) capacity and disk space availability. By gathering detailed hardware specifications, validators gain a holistic understanding of each miner’s computational resources.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Computational Integrity through PoG\u003C/strong>:\u003C/p>\n\u003Cp>Leveraging the \u003Cstrong>Proof-of-GPU (PoG)\u003C/strong> mechanism, validators proceed to test the miners’ computational integrity. PoG involves executing real-world AI workloads that simulate genuine machine learning tasks. These tasks are designed to evaluate both the processing power and the reliability of the miners’ GPU systems over sustained periods, reflecting the demands of long-running AI model training and inference.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Benchmarking AI Workloads\u003C/strong>: Validators deploy AI-centric benchmarks that include matrix multiplications and tensor operations at various data precisions (e.g., FP16, FP32). These benchmarks mimic actual AI workflows, ensuring that the performance metrics are directly relevant to real-world applications.\u003C/li>\n\u003Cli>\u003Cstrong>Sustained Performance Measurement\u003C/strong>: Unlike traditional benchmarks that measure peak performance, PoG assesses a GPU’s ability to maintain high floating-point operations per second (FLOPS) over extended durations. This ensures that GPUs do not throttle or overheat during prolonged AI tasks.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Accuracy and Precision Verification\u003C/strong>:\u003C/p>\n\u003Cp>In addition to measuring the time taken by miners to complete benchmarking tasks, validators meticulously verify the accuracy of the computational results. This involves:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Merkle Tree Proofs\u003C/strong>: Validators generate Merkle trees from the computation results to create cryptographic proofs. These proofs are used to verify the integrity and correctness of the data produced by the miners.\u003C/li>\n\u003Cli>\u003Cstrong>Data Precision Checks\u003C/strong>: Validators ensure that the results of matrix multiplications and tensor operations meet the expected precision levels, confirming that miners are performing computations accurately without significant errors.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Dynamic Scoring Mechanism\u003C/strong>:\u003C/p>\n\u003Cp>Based on the benchmarking and verification results, validators update each miner’s score, reflecting a comprehensive view of their computational capacity, efficiency, and hardware quality. The scoring process considers various factors, including:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>GPU Performance\u003C/strong>: Evaluates the GPU’s sustained FLOPS, memory capacity, and bandwidth based on PoG benchmarks.\u003C/li>\n\u003Cli>\u003Cstrong>CPU, RAM, and Storage\u003C/strong>: Assesses the performance metrics of the CPU, RAM, and hard disk, ensuring a balanced evaluation of the miner’s overall system capabilities.\u003C/li>\n\u003Cli>\u003Cstrong>Scalability and Multi-GPU Support\u003C/strong>: Takes into account the miner’s ability to scale performance across multiple GPUs, which is crucial for large-scale AI training tasks.\u003C/li>\n\u003C/ul>\n\u003Cp>The \u003Cstrong>scoring algorithm\u003C/strong> normalizes these metrics and applies predefined weights to calculate a total score for each miner. This score determines the miner’s weight within the network, directly influencing their potential rewards and standing.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"validator-script\">Validator Script\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#validator-script\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Validator Script”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>You will have needed to follow the environment set up instructions until \u003Ca href=\"/ni-ecosystem/ni-compute-sn27/ni-compute-subnet-miner-setup#id-6.-start-the-docker-service-in-compute-subnet\">step 6\u003C/a>.\u003C/p>\n\u003Cp>Making sure you have the right dependencies and requirements to validate on Subnet 27.\u003C/p>\n\u003Cp>Copy\u003C/p>\n\u003Cdiv class=\"expressive-code\">\u003Clink rel=\"stylesheet\" href=\"/_astro/ec.v4551.css\">\u003Cscript type=\"module\" src=\"/_astro/ec.8zarh.js\">\u003C/script>\u003Cfigure class=\"frame not-content\">\u003Cfigcaption class=\"header\">\u003C/figcaption>\u003Cpre data-language=\"plaintext\">\u003Ccode>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\"># To run the validator\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">cd neurons\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">python -m validator.py\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">--netuid &#x3C;your netuid> # The subnet id you want to connect to\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">--subtensor.network &#x3C;your chain url> # blockchain endpoint you want to connect\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">--wallet.name &#x3C;your validator wallet>  # name of your wallet\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">--wallet.hotkey &#x3C;your validator hotkey> # hotkey name of your wallet\u003C/span>\u003C/div>\u003C/div>\u003Cdiv class=\"ec-line\">\u003Cdiv class=\"code\">\u003Cspan class=\"indent\">\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">    \u003C/span>\u003C/span>\u003Cspan style=\"--0:#d6deeb;--1:#403f53\">--logging.debug # Run in debug mode, alternatively --logging.trace for trace mode\u003C/span>\u003C/div>\u003C/div>\u003C/code>\u003C/pre>\u003Cdiv class=\"copy\">\u003Cbutton title=\"Copy to clipboard\" data-copied=\"Copied!\" data-code=\"# To run the validatorcd neuronspython -m validator.py    --netuid \u003Cyour netuid> # The subnet id you want to connect to    --subtensor.network \u003Cyour chain url> # blockchain endpoint you want to connect    --wallet.name \u003Cyour validator wallet>  # name of your wallet    --wallet.hotkey \u003Cyour validator hotkey> # hotkey name of your wallet    --logging.debug # Run in debug mode, alternatively --logging.trace for trace mode\">\u003Cdiv>\u003C/div>\u003C/button>\u003C/div>\u003C/figure>\u003C/div>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"understanding-the-score-calculation-process\">Understanding the Score Calculation Process\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#understanding-the-score-calculation-process\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Understanding the Score Calculation Process”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The score calculation function now determines a miner’s performance primarily based on their GPU hardware and resource allocation. Only the GPUs listed below are supported and scored correctly.\u003C/p>\n\u003Cp>\u003Cstrong>GPU Base Scores\u003C/strong>: The following GPUs are assigned specific base scores, reflecting their relative performance:\u003C/p>\n\u003Cul>\n\u003Cli>NVIDIA H200: 4.00\u003C/li>\n\u003Cli>NVIDIA H100 80GB HBM3: 3.30\u003C/li>\n\u003Cli>NVIDIA H100: 2.80\u003C/li>\n\u003Cli>NVIDIA A100-SXM4-80GB: 1.90\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Scaling Factor\u003C/strong>: Determine the highest GPU base score, multiply it by 8 (the maximum number of GPUs), and set this scenario as the 100-point baseline. A scaling factor is derived so that using eight of the top GPU models equals 50 points.\u003C/p>\n\u003Cp>\u003Cstrong>GPU Score\u003C/strong>: Multiply the chosen GPU’s base score by the number of GPUs (up to 8) and by the scaling factor to find the miner’s GPU score (0–50).\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"example-1-miner-as-total-score\">Example 1: Miner A’s Total Score\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#example-1-miner-as-total-score\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Example 1: Miner A’s Total Score”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>GPU\u003C/strong>: NVIDIA H200 (Base Score: 3.90)\u003C/li>\n\u003Cli>\u003Cstrong>Number of GPUs\u003C/strong>: 8\u003C/li>\n\u003C/ul>\n\u003Cp>Step-by-step calculation:\u003C/p>\n\u003Col>\n\u003Cli>Highest scenario: 4 * 8 = 32\u003C/li>\n\u003Cli>Scaling factor: 50 / 32 ≈ 1.5625\u003C/li>\n\u003Cli>GPU Score: 4 * 8 * 1.5625 ≈ 50\u003C/li>\n\u003C/ol>\n\u003Cp>Total Score = 50\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"resource-allocation-mechanism\">Resource Allocation Mechanism\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#resource-allocation-mechanism\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Resource Allocation Mechanism”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The allocation mechanism within subnet 27 is designed to optimize the utilization of computational resources effectively. Key aspects of this mechanism include:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>Resource Requirement Analysis:\u003C/strong> The mechanism begins by analyzing the specific resource requirements of each task, including CPU, GPU, memory, and storage needs.\u003C/li>\n\u003Cli>\u003Cstrong>Miner Selection:\u003C/strong> Based on the analysis, the mechanism selects suitable miners that meet the resource requirements. This selection process considers the current availability, performance history, and network weights of the miners.\u003C/li>\n\u003Cli>\u003Cstrong>Dynamic Allocation:\u003C/strong> The allocation of tasks to miners is dynamic, allowing for real-time adjustments based on changing network conditions and miner performance.\u003C/li>\n\u003Cli>\u003Cstrong>Efficiency Optimization:\u003C/strong> The mechanism aims to maximize network efficiency by matching the most suitable miners to each task, ensuring optimal use of the network’s computational power.\u003C/li>\n\u003Cli>\u003Cstrong>Load Balancing:\u003C/strong> It also incorporates load balancing strategies to prevent overburdening individual miners, thereby maintaining a healthy and sustainable network ecosystem.\u003C/li>\n\u003C/ol>\n\u003Cp>Through these functionalities, the allocation mechanism ensures that computational resources are utilized efficiently and effectively, contributing to the overall robustness and performance of the network.\u003C/p>\n\u003Cp>Validators can send requests to reserve access to resources from miners by specifying the specs manually in the in \u003Ccode dir=\"auto\">register.py\u003C/code> and running this script: \u003Ca href=\"https://github.com/neuralinternet/Compute-Subnet/blob/main/neurons/register.py\">https://github.com/neuralinternet/Compute-Subnet/blob/main/neurons/register.py\u003C/a> for example: \u003Ccode dir=\"auto\">{'cpu':{'count':1}, 'gpu':{'count':1}, 'hard_disk':{'capacity':10737418240}, 'ram':{'capacity':1073741824}}\u003C/code>\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"options\">Options\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#options\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Options”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cp>All the list arguments are now using coma separator.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">-netuid\u003C/code>: (Optional) The chain subnet uid. Default: 27.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">-auto_update\u003C/code>: (Optional) Auto update the repository. Default: True.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">-blacklist.exploiters\u003C/code>: (Optional) Automatically use the list of internal exploiters hotkeys. Default: True.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">-blacklist.hotkeys &#x3C;hotkey_0,hotkey_1,...>\u003C/code>: (Optional) List of hotkeys to blacklist. Default: [].\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">-blacklist.coldkeys &#x3C;coldkey_0,coldkey_1,...>\u003C/code>: (Optional) List of coldkeys to blacklist. Default: [].\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">-whitelist.hotkeys &#x3C;hotkey_0,hotkey_1,...>\u003C/code>: (Optional) List of hotkeys to whitelist. Default: [].\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">-whitelist.coldkeys &#x3C;coldkey_0,coldkey_1,...>\u003C/code>: (Optional) List of coldkeys to whitelist. Default: [].\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"validator-options\">Validator options\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#validator-options\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Validator options”\u003C/span>\u003C/a>\u003C/div>\n\u003Chr>\n\u003Cp>Flags that you can use with the validator script.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">--validator.whitelist.unrecognized\u003C/code>: (Optional) Whitelist the unrecognized miners. Default: False.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">--validator.perform.hardware.query &#x3C;bool>\u003C/code>: (Optional) Perform the specs query - useful to register to a miner’s machine. Default: True.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">--validator.specs.batch.size &#x3C;size>\u003C/code>: (Optional) Batch size that perform the specs queries - For lower hardware specifications you might want to use a different batch_size than default. Keep in mind the lower is the batch_size the longer it will take to perform all challenge queries. Default: 64.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">--validator.force.update.prometheus\u003C/code>: (Optional) Force the try-update of prometheus version. Default: False.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">--validator.whitelist.updated.threshold\u003C/code>: (Optional) Total quorum before starting the whitelist. Default: 60. (%)\u003C/li>\n\u003C/ul>",{"headings":724,"localImagePaths":749,"remoteImagePaths":750,"frontmatter":751,"imagePaths":752},[725,728,731,734,737,740,743,746],{"depth":263,"slug":726,"text":727},"introduction","Introduction",{"depth":327,"slug":729,"text":730},"evaluation-process","Evaluation Process",{"depth":259,"slug":732,"text":733},"validator-script","Validator Script",{"depth":259,"slug":735,"text":736},"understanding-the-score-calculation-process","Understanding the Score Calculation Process",{"depth":327,"slug":738,"text":739},"example-1-miner-as-total-score","Example 1: Miner A’s Total Score",{"depth":259,"slug":741,"text":742},"resource-allocation-mechanism","Resource Allocation Mechanism",{"depth":259,"slug":744,"text":745},"options","Options",{"depth":259,"slug":747,"text":748},"validator-options","Validator options",[],[],{"title":714},[],"validator-system/ai_gpu_benchmarking_with_proof_of_gpu",{"id":753,"data":755,"body":760,"filePath":761,"digest":762,"rendered":763},{"title":756,"editUrl":15,"head":757,"template":17,"sidebar":758,"pagefind":15,"draft":19},"AI GPU Benchmarking with Proof-of-GPU",[],{"hidden":19,"attrs":759},{},"Building upon the foundational concepts introduced in \"[AI GPU Benchmarking with Proof-of-GPU (PoG)](https://x.com/tao_alignment/status/1874916194194801121),\" this technical overview delves deeper into the mechanisms and architecture of **Proof-of-GPU (PoG)** of the NI **Compute Subnet**. Aimed at developers, system architects, and technical stakeholders, this document elucidates the intricate processes that ensure robust GPU performance validation and efficient resource allocation within a decentralized AI compute ecosystem.\n\n---\n\n### Introduction\n\nIn the realm of artificial intelligence (AI), the computational demands have escalated beyond traditional benchmarks, necessitating sophisticated mechanisms to evaluate and validate GPU performance. **Proof-of-GPU (PoG)** emerges as a pivotal solution, designed to ensure that GPUs contribute effectively to AI workloads within a decentralized infrastructure. Coupled with NI **Compute Subnet**, PoG facilitates a seamless integration of diverse cloud platforms, fostering an ecosystem where computational resources are allocated efficiently and transparently.\n\n---\n\n### Proof-of-GPU (PoG) Mechanics\n\n**Proof-of-GPU (PoG)** is an advanced benchmarking system tailored specifically for AI workloads. Unlike conventional GPU benchmarks that focus on transient performance metrics, PoG emphasizes sustained computational capabilities, memory efficiency, and precision—key attributes for effective AI model training and inference.\n\n#### Real AI Workloads\n\nPoG employs benchmarks that replicate authentic AI training tasks. These benchmarks include operations such as matrix multiplications and tensor computations, which are fundamental to machine learning algorithms. By aligning tests with real-world AI workflows, PoG ensures that the performance metrics accurately reflect a GPU's suitability for prolonged AI tasks.\n\n#### Sustained Performance Measurement\n\nAI model training often spans extended periods—ranging from hours to weeks. PoG assesses a GPU's ability to maintain high floating-point operations per second (FLOPS) consistently over these durations. This sustained performance measurement is crucial to identify GPUs that might exhibit throttling or overheating under prolonged workloads, thereby ensuring reliability and efficiency in real-world applications.\n\n#### Memory Demands and Data Precision\n\nLarge-scale AI models necessitate substantial memory capacity and bandwidth. PoG benchmarks stress-test a GPU's memory limits by handling massive datasets, ensuring efficient data shuffling in and out of memory. Additionally, PoG evaluates a GPU's proficiency in handling mixed-precision computations (e.g., FP16, FP8), which are increasingly prevalent in machine learning to accelerate training without significantly compromising accuracy.\n\n#### Scalability and Multi-GPU Support\n\nModern AI training often leverages multi-GPU setups to expedite computational processes. PoG benchmarks the scalability of such configurations, measuring how effectively performance scales as additional GPUs are incorporated. This aspect ensures that the subnet can support large-scale AI projects by accurately reflecting the cumulative computational power of multi-GPU systems.\n\n---\n\n### NI Compute Subnet Architecture\n\nThe NI **Compute Subnet** serves as the backbone for integrating diverse cloud platforms into a unified computational network. It orchestrates resource sharing and allocation, enabling miners to contribute their GPU resources while validators ensure the integrity and efficiency of these contributions.\n\n#### Core Components\n\n* **Miners**: Nodes that contribute computational resources, primarily GPUs, to the network. They perform benchmarking tasks and provide performance data to validators.\n* **Validators**: Entities responsible for assessing and verifying the performance data provided by miners. They execute benchmarking challenges and update miners' scores based on their computational capabilities.\n* **Protocol Definitions**: The `compute/protocol.py` file outlines the communication protocols between miners and validators, ensuring standardized interactions and data exchanges.\n\n#### Miners and Validators\n\n* **Miners** operate by running the `miner.py` script, which handles the contribution of GPU resources and responds to validator requests. They must adhere to specific hardware specifications and ensure their systems remain performant and reliable.\n* **Validators** execute the `validator.py` script to request performance data from miners, run benchmarking tasks, and calculate comprehensive scores that reflect each miner's computational prowess. Validators play a crucial role in maintaining the network's integrity by ensuring only high-quality computational resources are rewarded and utilized.\n\n---\n\n### Scoring and Validation Mechanisms\n\nThe scoring system within NI's Compute Subnet is meticulously designed to provide a holistic evaluation of each miner's computational resources, ensuring that rewards are fairly distributed based on verified performance metrics.\n\n#### Holistic Hardware Scoring\n\nScores are calculated based on multiple hardware components, with a predominant emphasis on GPU performance. The scoring process involves:\n\n* **GPU Scores**: Each GPU model is assigned a base score reflecting its relative performance. The total GPU score is computed by multiplying the base score by the number of GPUs and applying a scaling factor to normalize the scores.\n* **CPU, RAM, and Hard Disk Scores**: Additional scores are calculated for CPU performance, RAM capacity and speed, and hard disk capacity and speed. These scores are weighted appropriately to contribute to the overall score.\n\n#### Merkle Tree Integration\n\nTo ensure the integrity and accuracy of computational results, PoG incorporates **Merkle trees** as a cryptographic proof mechanism. Here's how Merkle trees are utilized within the validation process:\n\n* **Construction**: After miners perform computational tasks, such as matrix multiplications, the results are organized into rows. Each row is hashed and integrated into a Merkle tree structure. This hierarchical hash structure enables efficient and secure verification of individual data points without necessitating the storage of the entire dataset.\n* **Proof Generation**: For specific computational results, miners generate Merkle proofs that validators use to verify the correctness and integrity of those results. These proofs ensure that the data has not been tampered with and accurately reflects the computations performed.\n* **Verification**: Validators utilize the provided Merkle proofs to confirm the authenticity of the computational results. This verification process is critical in maintaining trust within the decentralized network, ensuring that all contributions are both accurate and reliable.\n\n#### Dynamic Scoring Algorithm\n\nThe scoring algorithm dynamically adjusts miner scores based on verified performance data. The process involves:\n\n1. **Data Collection**: Validators gather performance metrics from miners, including GPU FLOPS, memory bandwidth, and efficiency metrics.\n2. **Normalization**: Scores are normalized to ensure comparability across different hardware configurations and performance levels.\n3. **Weight Application**: Predefined weights are applied to each hardware component's score, emphasizing GPU performance while still accounting for CPU, RAM, and storage capabilities.\n4. **Score Calculation**: The final score is computed by aggregating the weighted scores, incorporating any allocation bonuses, and ensuring the score reflects both the quantity and quality of the computational resources.\n5. **On-Chain Update**: The calculated scores are published on-chain, ensuring transparency and enabling miners to verify their standings within the network.\n\n---\n\n### Integration of PoG with NI Compute Subnet\n\n**Proof-of-GPU (PoG)** and NI's **Compute Subnet** forms the cornerstone of a trustworthy and efficient decentralized AI compute infrastructure. This synergy ensures that GPU performance is validated accurately, and computational resources are allocated optimally across the network.\n\n#### Workflow Integration\n\n1. **Resource Contribution**: Miners contribute their GPU resources to NI's Compute Subnet, making them available for AI workload processing.\n2. **Performance Validation**: Validators utilize PoG to benchmark and validate the performance of these GPUs, ensuring they meet the network's stringent requirements for sustained AI workloads.\n3. **Scoring and Rewards**: Based on the validated performance data, miners receive scores that directly influence their rewards. High-performing miners with verified GPU capabilities receive greater compensation, incentivizing the maintenance and upgrade of computational resources.\n4. **Transparent Verification**: All benchmarking results and scores are published on-chain, providing an immutable and transparent record that fosters trust and accountability within the network.\n\n#### Continuous Monitoring and Updates\n\nPoG's architecture supports continuous benchmarking and validation, adapting to changes in GPU drivers, hardware configurations, and AI workload demands. This ongoing monitoring ensures that the Compute Subnet remains resilient and responsive to evolving computational needs, maintaining high standards of performance and reliability.\n\n---\n\n### Codebase Insights\n\nUnderstanding the underlying code is essential for developers interacting with NI's Compute Subnet. Below, we explore key components and scripts that drive PoG and NI's Compute Subnet's functionality.\n\n#### Score Calculation Modules\n\n**Files Involved**:\n\n* `neurons/Validator/calculate_pog_score.py`\n* `neurons/Validator/calculate_score.py`\n\n**Functionality**:\n\n1. `calculate_pog_score.py`:\n\n   * **Purpose**: Computes the normalized PoG score for each miner based on their GPU specifications and performance data.\n   * **Key Functions**:\n\n     * `normalize(val, min_value, max_value)`: Normalizes a given value within a specified range.\n     * `calc_score_pog(gpu_specs, hotkey, allocated_hotkeys, config_data, mock=False)`: Calculates the PoG score using GPU performance data from configuration files.\n   * **Process**:\n\n     * Retrieves GPU scores from the configuration.\n     * Determines the maximum GPU score to establish a scaling factor.\n     * Calculates the score based on the GPU model, number of GPUs, and scaling factor.\n     * Applies an allocation bonus if the miner has allocated resources.\n     * Logs and returns the normalized score.\n2. `calculate_score.py`:\n\n   * **Purpose**: Aggregates scores across multiple hardware components to derive a comprehensive miner score.\n   * **Key Functions**:\n\n     * `score(data, hotkey)`: Combines CPU, GPU, hard disk, and RAM scores with respective weights to calculate the total score.\n     * `get_cpu_score(cpu_info)`, `get_gpu_score(gpu_info)`, `get_hard_disk_score(hard_disk_info)`, `get_ram_score(ram_info)`: Calculate individual component scores.\n     * `check_if_registered(hotkey)`: Verifies miner registration status using the WandB API.\n   * **Process**:\n\n     * Computes individual scores for CPU, GPU, hard disk, and RAM based on hardware metrics.\n     * Applies upper limits to each component score to maintain balance.\n     * Aggregates the scores using predefined weights.\n     * Adds a registration bonus if applicable.\n     * Returns the total score.\n\n#### Benchmarking and Proof Generation\n\n**Files Involved**:\n\n* `neurons/Validator/miner_script_m_merkletree.py`\n* `neurons/Validator/pog.py`\n\n**Functionality**:\n\n1. `miner_script_m_merkletree.py`:\n\n   * **Purpose**: Executes benchmarking tasks and generates cryptographic proofs using Merkle trees.\n   * **Key Functions**:\n\n     * `run_benchmark()`: Performs system benchmarking using matrix multiplication tasks to simulate AI workloads.\n     * `run_compute()`: Conducts computational tasks across all available GPUs, generating performance data and Merkle trees.\n     * `run_proof()`: Generates Merkle proofs based on computational results for validation purposes.\n     * Helper functions for GPU information retrieval, matrix generation, and Merkle tree construction.\n   * **Process**:\n\n     * Detects GPU availability and estimates available VRAM.\n     * Adjusts matrix sizes based on VRAM to optimize benchmarking tasks.\n     * Executes matrix multiplications and measures computation time.\n     * Constructs Merkle trees from computation results to create cryptographic proofs.\n     * Outputs root hashes and timing information for validators to use in score calculations.\n2. `pog.py`:\n\n   * **Purpose**: Validates GPU performance data and integrates benchmarking results with the PoG system.\n   * **Key Functions**:\n\n     * `identify_gpu()`: Identifies GPU models based on measured TFLOPS and VRAM, applying tolerance checks for accuracy.\n     * `verify_responses()`: Validates computational responses from miners using Merkle proofs and performance data.\n     * `verify_merkle_proof_row()`: Confirms the integrity of specific data rows using Merkle proofs.\n   * **Process**:\n\n     * Loads GPU performance configurations from YAML files.\n     * Identifies GPUs by comparing measured performance metrics against theoretical values.\n     * Verifies computational results using Merkle proofs to ensure data integrity.\n     * Updates miner scores based on verified performance metrics, influencing their network standing and rewards.\n\n#### Merkle Tree Verification\n\n**File Involved**:\n\n* `neurons/Validator/pog.py`\n\n**Functionality**:\n\n* **Merkle Tree Construction**: After computational tasks, results are organized into rows and hashed to form the leaves of a Merkle tree. Validators use these trees to verify the integrity of specific computational results efficiently.\n* **Proof Verification**: Validators receive Merkle proofs from miners and utilize cryptographic functions to verify that the provided data matches the expected results. This process ensures that computational contributions are both accurate and unaltered.\n\n---\n\n### Benefits and Impact\n\n**Proof-of-GPU (PoG)** yields numerous advantages, fostering a robust and efficient decentralized AI compute ecosystem.\n\n* **Transparency**: On-chain validation of benchmarking results ensures that GPU performance data is publicly verifiable, eliminating reliance on third-party claims and fostering trust within the network.\n* **Sustainability**: Miners are incentivized to maintain and upgrade high-performance hardware, as rewards are directly tied to verified GPU capabilities. This promotes a continually improving network of computational resources.\n* **Scalability**: The subnet efficiently manages multi-GPU setups and large-scale computational tasks, accurately reflecting and utilizing the collective power of the network's GPUs.\n* **Fair Rewards**: The dynamic scoring system ensures that miners with more capable and verified hardware receive proportionate rewards, encouraging equitable participation and resource contribution.\n* **Data Integrity**: The use of Merkle trees for proof generation and verification ensures that computational results are accurate and tamper-proof, maintaining the integrity of the network's operations.\n* **Industry Adoption Potential**: PoG's rigorous, AI-centric benchmarking and decentralized verification position it as a potential standard for future decentralized AI compute networks, setting high performance and reliability benchmarks.\n\n---\n\n**Proof-of-GPU (PoG)**, establishes a sophisticated framework for decentralized AI computation. By emphasizing realistic AI benchmarking, sustained performance evaluations, and transparent on-chain validation, PoG ensures that computational resources are both reliable and efficiently utilized. The comprehensive scoring and resource allocation mechanisms incentivize high-performance hardware contributions, fostering a scalable and trustworthy AI compute ecosystem.","src/content/docs/validator-system/ai_gpu_benchmarking_with_proof_of_gpu.md","a39f2329cd4ea0a0",{"html":764,"metadata":765},"\u003Cp>Building upon the foundational concepts introduced in “\u003Ca href=\"https://x.com/tao_alignment/status/1874916194194801121\">AI GPU Benchmarking with Proof-of-GPU (PoG)\u003C/a>,” this technical overview delves deeper into the mechanisms and architecture of \u003Cstrong>Proof-of-GPU (PoG)\u003C/strong> of the NI \u003Cstrong>Compute Subnet\u003C/strong>. Aimed at developers, system architects, and technical stakeholders, this document elucidates the intricate processes that ensure robust GPU performance validation and efficient resource allocation within a decentralized AI compute ecosystem.\u003C/p>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"introduction\">Introduction\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#introduction\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Introduction”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>In the realm of artificial intelligence (AI), the computational demands have escalated beyond traditional benchmarks, necessitating sophisticated mechanisms to evaluate and validate GPU performance. \u003Cstrong>Proof-of-GPU (PoG)\u003C/strong> emerges as a pivotal solution, designed to ensure that GPUs contribute effectively to AI workloads within a decentralized infrastructure. Coupled with NI \u003Cstrong>Compute Subnet\u003C/strong>, PoG facilitates a seamless integration of diverse cloud platforms, fostering an ecosystem where computational resources are allocated efficiently and transparently.\u003C/p>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"proof-of-gpu-pog-mechanics\">Proof-of-GPU (PoG) Mechanics\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#proof-of-gpu-pog-mechanics\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Proof-of-GPU (PoG) Mechanics”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>\u003Cstrong>Proof-of-GPU (PoG)\u003C/strong> is an advanced benchmarking system tailored specifically for AI workloads. Unlike conventional GPU benchmarks that focus on transient performance metrics, PoG emphasizes sustained computational capabilities, memory efficiency, and precision—key attributes for effective AI model training and inference.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"real-ai-workloads\">Real AI Workloads\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#real-ai-workloads\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Real AI Workloads”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>PoG employs benchmarks that replicate authentic AI training tasks. These benchmarks include operations such as matrix multiplications and tensor computations, which are fundamental to machine learning algorithms. By aligning tests with real-world AI workflows, PoG ensures that the performance metrics accurately reflect a GPU’s suitability for prolonged AI tasks.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"sustained-performance-measurement\">Sustained Performance Measurement\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#sustained-performance-measurement\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Sustained Performance Measurement”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>AI model training often spans extended periods—ranging from hours to weeks. PoG assesses a GPU’s ability to maintain high floating-point operations per second (FLOPS) consistently over these durations. This sustained performance measurement is crucial to identify GPUs that might exhibit throttling or overheating under prolonged workloads, thereby ensuring reliability and efficiency in real-world applications.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"memory-demands-and-data-precision\">Memory Demands and Data Precision\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#memory-demands-and-data-precision\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Memory Demands and Data Precision”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Large-scale AI models necessitate substantial memory capacity and bandwidth. PoG benchmarks stress-test a GPU’s memory limits by handling massive datasets, ensuring efficient data shuffling in and out of memory. Additionally, PoG evaluates a GPU’s proficiency in handling mixed-precision computations (e.g., FP16, FP8), which are increasingly prevalent in machine learning to accelerate training without significantly compromising accuracy.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"scalability-and-multi-gpu-support\">Scalability and Multi-GPU Support\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#scalability-and-multi-gpu-support\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Scalability and Multi-GPU Support”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Modern AI training often leverages multi-GPU setups to expedite computational processes. PoG benchmarks the scalability of such configurations, measuring how effectively performance scales as additional GPUs are incorporated. This aspect ensures that the subnet can support large-scale AI projects by accurately reflecting the cumulative computational power of multi-GPU systems.\u003C/p>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"ni-compute-subnet-architecture\">NI Compute Subnet Architecture\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#ni-compute-subnet-architecture\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “NI Compute Subnet Architecture”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The NI \u003Cstrong>Compute Subnet\u003C/strong> serves as the backbone for integrating diverse cloud platforms into a unified computational network. It orchestrates resource sharing and allocation, enabling miners to contribute their GPU resources while validators ensure the integrity and efficiency of these contributions.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"core-components\">Core Components\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#core-components\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Core Components”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Miners\u003C/strong>: Nodes that contribute computational resources, primarily GPUs, to the network. They perform benchmarking tasks and provide performance data to validators.\u003C/li>\n\u003Cli>\u003Cstrong>Validators\u003C/strong>: Entities responsible for assessing and verifying the performance data provided by miners. They execute benchmarking challenges and update miners’ scores based on their computational capabilities.\u003C/li>\n\u003Cli>\u003Cstrong>Protocol Definitions\u003C/strong>: The \u003Ccode dir=\"auto\">compute/protocol.py\u003C/code> file outlines the communication protocols between miners and validators, ensuring standardized interactions and data exchanges.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"miners-and-validators\">Miners and Validators\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#miners-and-validators\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Miners and Validators”\u003C/span>\u003C/a>\u003C/div>\n\u003Cul>\n\u003Cli>\u003Cstrong>Miners\u003C/strong> operate by running the \u003Ccode dir=\"auto\">miner.py\u003C/code> script, which handles the contribution of GPU resources and responds to validator requests. They must adhere to specific hardware specifications and ensure their systems remain performant and reliable.\u003C/li>\n\u003Cli>\u003Cstrong>Validators\u003C/strong> execute the \u003Ccode dir=\"auto\">validator.py\u003C/code> script to request performance data from miners, run benchmarking tasks, and calculate comprehensive scores that reflect each miner’s computational prowess. Validators play a crucial role in maintaining the network’s integrity by ensuring only high-quality computational resources are rewarded and utilized.\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"scoring-and-validation-mechanisms\">Scoring and Validation Mechanisms\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#scoring-and-validation-mechanisms\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Scoring and Validation Mechanisms”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The scoring system within NI’s Compute Subnet is meticulously designed to provide a holistic evaluation of each miner’s computational resources, ensuring that rewards are fairly distributed based on verified performance metrics.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"holistic-hardware-scoring\">Holistic Hardware Scoring\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#holistic-hardware-scoring\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Holistic Hardware Scoring”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Scores are calculated based on multiple hardware components, with a predominant emphasis on GPU performance. The scoring process involves:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>GPU Scores\u003C/strong>: Each GPU model is assigned a base score reflecting its relative performance. The total GPU score is computed by multiplying the base score by the number of GPUs and applying a scaling factor to normalize the scores.\u003C/li>\n\u003Cli>\u003Cstrong>CPU, RAM, and Hard Disk Scores\u003C/strong>: Additional scores are calculated for CPU performance, RAM capacity and speed, and hard disk capacity and speed. These scores are weighted appropriately to contribute to the overall score.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"merkle-tree-integration\">Merkle Tree Integration\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#merkle-tree-integration\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Merkle Tree Integration”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>To ensure the integrity and accuracy of computational results, PoG incorporates \u003Cstrong>Merkle trees\u003C/strong> as a cryptographic proof mechanism. Here’s how Merkle trees are utilized within the validation process:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Construction\u003C/strong>: After miners perform computational tasks, such as matrix multiplications, the results are organized into rows. Each row is hashed and integrated into a Merkle tree structure. This hierarchical hash structure enables efficient and secure verification of individual data points without necessitating the storage of the entire dataset.\u003C/li>\n\u003Cli>\u003Cstrong>Proof Generation\u003C/strong>: For specific computational results, miners generate Merkle proofs that validators use to verify the correctness and integrity of those results. These proofs ensure that the data has not been tampered with and accurately reflects the computations performed.\u003C/li>\n\u003Cli>\u003Cstrong>Verification\u003C/strong>: Validators utilize the provided Merkle proofs to confirm the authenticity of the computational results. This verification process is critical in maintaining trust within the decentralized network, ensuring that all contributions are both accurate and reliable.\u003C/li>\n\u003C/ul>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"dynamic-scoring-algorithm\">Dynamic Scoring Algorithm\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#dynamic-scoring-algorithm\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Dynamic Scoring Algorithm”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>The scoring algorithm dynamically adjusts miner scores based on verified performance data. The process involves:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>Data Collection\u003C/strong>: Validators gather performance metrics from miners, including GPU FLOPS, memory bandwidth, and efficiency metrics.\u003C/li>\n\u003Cli>\u003Cstrong>Normalization\u003C/strong>: Scores are normalized to ensure comparability across different hardware configurations and performance levels.\u003C/li>\n\u003Cli>\u003Cstrong>Weight Application\u003C/strong>: Predefined weights are applied to each hardware component’s score, emphasizing GPU performance while still accounting for CPU, RAM, and storage capabilities.\u003C/li>\n\u003Cli>\u003Cstrong>Score Calculation\u003C/strong>: The final score is computed by aggregating the weighted scores, incorporating any allocation bonuses, and ensuring the score reflects both the quantity and quality of the computational resources.\u003C/li>\n\u003Cli>\u003Cstrong>On-Chain Update\u003C/strong>: The calculated scores are published on-chain, ensuring transparency and enabling miners to verify their standings within the network.\u003C/li>\n\u003C/ol>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"integration-of-pog-with-ni-compute-subnet\">Integration of PoG with NI Compute Subnet\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#integration-of-pog-with-ni-compute-subnet\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Integration of PoG with NI Compute Subnet”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>\u003Cstrong>Proof-of-GPU (PoG)\u003C/strong> and NI’s \u003Cstrong>Compute Subnet\u003C/strong> forms the cornerstone of a trustworthy and efficient decentralized AI compute infrastructure. This synergy ensures that GPU performance is validated accurately, and computational resources are allocated optimally across the network.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"workflow-integration\">Workflow Integration\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#workflow-integration\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Workflow Integration”\u003C/span>\u003C/a>\u003C/div>\n\u003Col>\n\u003Cli>\u003Cstrong>Resource Contribution\u003C/strong>: Miners contribute their GPU resources to NI’s Compute Subnet, making them available for AI workload processing.\u003C/li>\n\u003Cli>\u003Cstrong>Performance Validation\u003C/strong>: Validators utilize PoG to benchmark and validate the performance of these GPUs, ensuring they meet the network’s stringent requirements for sustained AI workloads.\u003C/li>\n\u003Cli>\u003Cstrong>Scoring and Rewards\u003C/strong>: Based on the validated performance data, miners receive scores that directly influence their rewards. High-performing miners with verified GPU capabilities receive greater compensation, incentivizing the maintenance and upgrade of computational resources.\u003C/li>\n\u003Cli>\u003Cstrong>Transparent Verification\u003C/strong>: All benchmarking results and scores are published on-chain, providing an immutable and transparent record that fosters trust and accountability within the network.\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"continuous-monitoring-and-updates\">Continuous Monitoring and Updates\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#continuous-monitoring-and-updates\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Continuous Monitoring and Updates”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>PoG’s architecture supports continuous benchmarking and validation, adapting to changes in GPU drivers, hardware configurations, and AI workload demands. This ongoing monitoring ensures that the Compute Subnet remains resilient and responsive to evolving computational needs, maintaining high standards of performance and reliability.\u003C/p>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"codebase-insights\">Codebase Insights\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#codebase-insights\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Codebase Insights”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>Understanding the underlying code is essential for developers interacting with NI’s Compute Subnet. Below, we explore key components and scripts that drive PoG and NI’s Compute Subnet’s functionality.\u003C/p>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"score-calculation-modules\">Score Calculation Modules\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#score-calculation-modules\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Score Calculation Modules”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>\u003Cstrong>Files Involved\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">neurons/Validator/calculate_pog_score.py\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">neurons/Validator/calculate_score.py\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Functionality\u003C/strong>:\u003C/p>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Ccode dir=\"auto\">calculate_pog_score.py\u003C/code>:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Purpose\u003C/strong>: Computes the normalized PoG score for each miner based on their GPU specifications and performance data.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Key Functions\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">normalize(val, min_value, max_value)\u003C/code>: Normalizes a given value within a specified range.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">calc_score_pog(gpu_specs, hotkey, allocated_hotkeys, config_data, mock=False)\u003C/code>: Calculates the PoG score using GPU performance data from configuration files.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Process\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Retrieves GPU scores from the configuration.\u003C/li>\n\u003Cli>Determines the maximum GPU score to establish a scaling factor.\u003C/li>\n\u003Cli>Calculates the score based on the GPU model, number of GPUs, and scaling factor.\u003C/li>\n\u003Cli>Applies an allocation bonus if the miner has allocated resources.\u003C/li>\n\u003Cli>Logs and returns the normalized score.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Ccode dir=\"auto\">calculate_score.py\u003C/code>:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Purpose\u003C/strong>: Aggregates scores across multiple hardware components to derive a comprehensive miner score.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Key Functions\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">score(data, hotkey)\u003C/code>: Combines CPU, GPU, hard disk, and RAM scores with respective weights to calculate the total score.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">get_cpu_score(cpu_info)\u003C/code>, \u003Ccode dir=\"auto\">get_gpu_score(gpu_info)\u003C/code>, \u003Ccode dir=\"auto\">get_hard_disk_score(hard_disk_info)\u003C/code>, \u003Ccode dir=\"auto\">get_ram_score(ram_info)\u003C/code>: Calculate individual component scores.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">check_if_registered(hotkey)\u003C/code>: Verifies miner registration status using the WandB API.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Process\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Computes individual scores for CPU, GPU, hard disk, and RAM based on hardware metrics.\u003C/li>\n\u003Cli>Applies upper limits to each component score to maintain balance.\u003C/li>\n\u003Cli>Aggregates the scores using predefined weights.\u003C/li>\n\u003Cli>Adds a registration bonus if applicable.\u003C/li>\n\u003Cli>Returns the total score.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"benchmarking-and-proof-generation\">Benchmarking and Proof Generation\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#benchmarking-and-proof-generation\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Benchmarking and Proof Generation”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>\u003Cstrong>Files Involved\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">neurons/Validator/miner_script_m_merkletree.py\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">neurons/Validator/pog.py\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Functionality\u003C/strong>:\u003C/p>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Ccode dir=\"auto\">miner_script_m_merkletree.py\u003C/code>:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Purpose\u003C/strong>: Executes benchmarking tasks and generates cryptographic proofs using Merkle trees.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Key Functions\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">run_benchmark()\u003C/code>: Performs system benchmarking using matrix multiplication tasks to simulate AI workloads.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">run_compute()\u003C/code>: Conducts computational tasks across all available GPUs, generating performance data and Merkle trees.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">run_proof()\u003C/code>: Generates Merkle proofs based on computational results for validation purposes.\u003C/li>\n\u003Cli>Helper functions for GPU information retrieval, matrix generation, and Merkle tree construction.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Process\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Detects GPU availability and estimates available VRAM.\u003C/li>\n\u003Cli>Adjusts matrix sizes based on VRAM to optimize benchmarking tasks.\u003C/li>\n\u003Cli>Executes matrix multiplications and measures computation time.\u003C/li>\n\u003Cli>Constructs Merkle trees from computation results to create cryptographic proofs.\u003C/li>\n\u003Cli>Outputs root hashes and timing information for validators to use in score calculations.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Ccode dir=\"auto\">pog.py\u003C/code>:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Purpose\u003C/strong>: Validates GPU performance data and integrates benchmarking results with the PoG system.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Key Functions\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">identify_gpu()\u003C/code>: Identifies GPU models based on measured TFLOPS and VRAM, applying tolerance checks for accuracy.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">verify_responses()\u003C/code>: Validates computational responses from miners using Merkle proofs and performance data.\u003C/li>\n\u003Cli>\u003Ccode dir=\"auto\">verify_merkle_proof_row()\u003C/code>: Confirms the integrity of specific data rows using Merkle proofs.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Process\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Loads GPU performance configurations from YAML files.\u003C/li>\n\u003Cli>Identifies GPUs by comparing measured performance metrics against theoretical values.\u003C/li>\n\u003Cli>Verifies computational results using Merkle proofs to ensure data integrity.\u003C/li>\n\u003Cli>Updates miner scores based on verified performance metrics, influencing their network standing and rewards.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ol>\n\u003Cdiv class=\"sl-heading-wrapper level-h4\">\u003Ch4 id=\"merkle-tree-verification\">Merkle Tree Verification\u003C/h4>\u003Ca class=\"sl-anchor-link\" href=\"#merkle-tree-verification\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Merkle Tree Verification”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>\u003Cstrong>File Involved\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode dir=\"auto\">neurons/Validator/pog.py\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>Functionality\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Merkle Tree Construction\u003C/strong>: After computational tasks, results are organized into rows and hashed to form the leaves of a Merkle tree. Validators use these trees to verify the integrity of specific computational results efficiently.\u003C/li>\n\u003Cli>\u003Cstrong>Proof Verification\u003C/strong>: Validators receive Merkle proofs from miners and utilize cryptographic functions to verify that the provided data matches the expected results. This process ensures that computational contributions are both accurate and unaltered.\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Cdiv class=\"sl-heading-wrapper level-h3\">\u003Ch3 id=\"benefits-and-impact\">Benefits and Impact\u003C/h3>\u003Ca class=\"sl-anchor-link\" href=\"#benefits-and-impact\">\u003Cspan aria-hidden=\"true\" class=\"sl-anchor-icon\">\u003Csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\">\u003Cpath fill=\"currentcolor\" d=\"m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\">\u003C/path>\u003C/svg>\u003C/span>\u003Cspan class=\"sr-only\">Section titled “Benefits and Impact”\u003C/span>\u003C/a>\u003C/div>\n\u003Cp>\u003Cstrong>Proof-of-GPU (PoG)\u003C/strong> yields numerous advantages, fostering a robust and efficient decentralized AI compute ecosystem.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Transparency\u003C/strong>: On-chain validation of benchmarking results ensures that GPU performance data is publicly verifiable, eliminating reliance on third-party claims and fostering trust within the network.\u003C/li>\n\u003Cli>\u003Cstrong>Sustainability\u003C/strong>: Miners are incentivized to maintain and upgrade high-performance hardware, as rewards are directly tied to verified GPU capabilities. This promotes a continually improving network of computational resources.\u003C/li>\n\u003Cli>\u003Cstrong>Scalability\u003C/strong>: The subnet efficiently manages multi-GPU setups and large-scale computational tasks, accurately reflecting and utilizing the collective power of the network’s GPUs.\u003C/li>\n\u003Cli>\u003Cstrong>Fair Rewards\u003C/strong>: The dynamic scoring system ensures that miners with more capable and verified hardware receive proportionate rewards, encouraging equitable participation and resource contribution.\u003C/li>\n\u003Cli>\u003Cstrong>Data Integrity\u003C/strong>: The use of Merkle trees for proof generation and verification ensures that computational results are accurate and tamper-proof, maintaining the integrity of the network’s operations.\u003C/li>\n\u003Cli>\u003Cstrong>Industry Adoption Potential\u003C/strong>: PoG’s rigorous, AI-centric benchmarking and decentralized verification position it as a potential standard for future decentralized AI compute networks, setting high performance and reliability benchmarks.\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Cp>\u003Cstrong>Proof-of-GPU (PoG)\u003C/strong>, establishes a sophisticated framework for decentralized AI computation. By emphasizing realistic AI benchmarking, sustained performance evaluations, and transparent on-chain validation, PoG ensures that computational resources are both reliable and efficiently utilized. The comprehensive scoring and resource allocation mechanisms incentivize high-performance hardware contributions, fostering a scalable and trustworthy AI compute ecosystem.\u003C/p>",{"headings":766,"localImagePaths":828,"remoteImagePaths":829,"frontmatter":830,"imagePaths":831},[767,768,771,774,777,780,783,786,789,792,795,798,801,804,807,810,813,816,819,822,825],{"depth":259,"slug":726,"text":727},{"depth":259,"slug":769,"text":770},"proof-of-gpu-pog-mechanics","Proof-of-GPU (PoG) Mechanics",{"depth":327,"slug":772,"text":773},"real-ai-workloads","Real AI Workloads",{"depth":327,"slug":775,"text":776},"sustained-performance-measurement","Sustained Performance Measurement",{"depth":327,"slug":778,"text":779},"memory-demands-and-data-precision","Memory Demands and Data Precision",{"depth":327,"slug":781,"text":782},"scalability-and-multi-gpu-support","Scalability and Multi-GPU Support",{"depth":259,"slug":784,"text":785},"ni-compute-subnet-architecture","NI Compute Subnet Architecture",{"depth":327,"slug":787,"text":788},"core-components","Core Components",{"depth":327,"slug":790,"text":791},"miners-and-validators","Miners and Validators",{"depth":259,"slug":793,"text":794},"scoring-and-validation-mechanisms","Scoring and Validation Mechanisms",{"depth":327,"slug":796,"text":797},"holistic-hardware-scoring","Holistic Hardware Scoring",{"depth":327,"slug":799,"text":800},"merkle-tree-integration","Merkle Tree Integration",{"depth":327,"slug":802,"text":803},"dynamic-scoring-algorithm","Dynamic Scoring Algorithm",{"depth":259,"slug":805,"text":806},"integration-of-pog-with-ni-compute-subnet","Integration of PoG with NI Compute Subnet",{"depth":327,"slug":808,"text":809},"workflow-integration","Workflow Integration",{"depth":327,"slug":811,"text":812},"continuous-monitoring-and-updates","Continuous Monitoring and Updates",{"depth":259,"slug":814,"text":815},"codebase-insights","Codebase Insights",{"depth":327,"slug":817,"text":818},"score-calculation-modules","Score Calculation Modules",{"depth":327,"slug":820,"text":821},"benchmarking-and-proof-generation","Benchmarking and Proof Generation",{"depth":327,"slug":823,"text":824},"merkle-tree-verification","Merkle Tree Verification",{"depth":259,"slug":826,"text":827},"benefits-and-impact","Benefits and Impact",[],[],{"title":756},[],"validator-system/proof-of-gpu",{"id":832,"data":834,"body":839,"filePath":840,"digest":841,"deferredRender":15},{"title":835,"editUrl":15,"head":836,"template":17,"sidebar":837,"pagefind":15,"draft":19},"Proof of GPU",[],{"hidden":19,"attrs":838},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"compute/utils/math.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/math.py\" />\n\n  \u003CSourceLink text=\"config.yaml\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/config.yaml\" />\n\n  \u003CSourceLink text=\"neurons/Miner/specs.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Miner/specs.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/app_generator.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/app_generator.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/calculate_score.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_score.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/database/miner.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/miner.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/database/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/miner_script_m_merkletree.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/miner_script_m_merkletree.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n  \u003CSourceLink text=\"neurons/Validator/script.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/script.py\" />\n\n  \u003CSourceLink text=\"neurons/validator.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\n## Overview\n\nProof of GPU (PoG) is a critical verification mechanism in the NI Compute system (Subnet 27) that enables validators to verify the GPU hardware capabilities of miners in the network. This verification is essential to ensure miners possess the computational resources they claim, maintaining the integrity of the decentralized GPU marketplace.\n\nThis document details how validators implement Proof of GPU verification, the benchmarking process, and how these results affect miner scoring within the subnet.\n\n*For information about how scores are calculated based on PoG results, see [Scoring System](/validator-system/scoring-system#2.2).*\n\n## Core Concepts\n\nProof of GPU employs a series of technical tests to verify:\n\n1. Actual existence of GPU hardware\n2. Number of GPUs available on the miner\n3. Type/model of GPUs (e.g., RTX 3090, A100, etc.)\n4. Performance capabilities of the GPUs\n\nThe system uses a combination of direct hardware information queries, benchmarking performance tests, and cryptographic verification methods to ensure miners cannot falsify their hardware capabilities.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:638-762\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L638-L762\" />, \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n## Verification Process\n\n```mermaid\nsequenceDiagram\n    participant V as \"Validator\"\n    participant M as \"Miner\"\n    participant SSH as \"SSH Connection\"\n    participant GPU as \"Miner's GPU\"\n    \n    V->>M: Request allocation (for testing)\n    M-->>V: Return SSH connection details\n    V->>SSH: Establish connection\n    \n    Note over V,SSH: Integrity Verification Phase\n    V->>SSH: Hash script\n    SSH-->>V: Return script hash\n    V->>V: Verify script integrity\n    \n    Note over V,SSH: Hardware Detection Phase\n    V->>SSH: Request GPU information\n    SSH->>GPU: Query with nvidia-smi\n    GPU-->>SSH: Return GPU count and types\n    SSH-->>V: Return GPU information\n    \n    Note over V,SSH: Benchmarking Phase\n    V->>SSH: Execute benchmark operations\n    SSH->>GPU: Run matrix operations (FP16/FP32)\n    GPU-->>SSH: Return computation timings\n    SSH-->>V: Return benchmark results\n    V->>V: Calculate TFLOPS\n    V->>V: Identify GPU model based on performance\n    \n    Note over V,SSH: Merkle Proof Verification\n    V->>SSH: Send random seeds\n    SSH->>GPU: Run matrix calculations with seeds\n    GPU-->>SSH: Calculate Merkle tree of results\n    SSH-->>V: Return root hashes\n    V->>SSH: Challenge with random indices\n    SSH->>GPU: Generate proofs for indices\n    GPU-->>SSH: Return proofs\n    SSH-->>V: Return proofs\n    V->>V: Verify Merkle proofs\n    \n    V->>M: Deallocate resources\n    V->>V: Record GPU specifications and score\n```\n\nThe above diagram illustrates the complete PoG verification process between a validator and miner.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:774-923\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L774-L923\" />, \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n## Technical Implementation\n\n### 1. Allocation and Connection\n\nThe validator first allocates the miner's resources temporarily for testing purposes:\n\n1. The validator generates an RSA key pair for secure communication\n2. It requests allocation from the miner with minimal resource requirements\n3. If allocation succeeds, the validator receives SSH connection details\n4. An SSH connection is established to the miner's container\n\nThis process ensures validators can perform tests in a controlled environment.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:924-987\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L924-L987\" />\n\n### 2. Script Integrity Verification\n\nTo prevent miners from tampering with the benchmarking script:\n\n1. The validator computes a hash of the local benchmarking script\n2. The script is sent to the miner and a hash is computed remotely\n3. The validator compares the local and remote hashes\n4. If they don't match, the verification fails immediately\n\n```mermaid\nflowchart TD\n    A[\"Compute local hash\n    (compute_script_hash)\"] --> B[\"Send script to miner\n    (send_script_and_request_hash)\"]\n    B --> C[\"Receive remote hash\"]\n    C --> D{\"Hashes match?\"}\n    D -->|\"Yes\"| E[\"Continue verification\"]\n    D -->|\"No\"| F[\"Fail verification\"]\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:820-827\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L820-L827\" />, \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n### 3. GPU Detection and Benchmarking\n\nThe validator performs direct hardware detection and benchmarking tests:\n\n1. Query GPU information using NVIDIA tools on the miner\n2. Execute matrix multiplication benchmarks in both FP16 and FP32 precision\n3. Measure execution time and calculate TFLOPS (Tera Floating-Point Operations Per Second)\n4. Identify GPU model based on performance metrics and reported hardware information\n\nThe benchmarking uses specially designed tests that:\n- Must run on GPUs (cannot be efficiently faked with CPUs)\n- Produce consistent results for specific GPU models\n- Scale with the available GPU memory\n\nSources: \u003CSourceLink text=\"neurons/validator.py:828-858\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L828-L858\" />, \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n### 4. Merkle Proof Verification\n\nTo cryptographically verify that the benchmarking was actually performed:\n\n1. The validator sends random seeds to the miner\n2. The miner computes large matrices using these seeds\n3. The miner builds a Merkle tree from the computation results\n4. The miner returns the Merkle root hashes\n5. The validator requests proofs for random elements in the matrices\n6. The miner provides Merkle proofs for these elements\n7. The validator verifies the proofs against the root hashes\n\n```mermaid\nflowchart TD\n    A[\"Send random seeds to miner\n    (send_seeds)\"] --> B[\"Miner computes matrices\n    (execute_script_on_miner)\"]\n    B --> C[\"Miner builds Merkle trees\"]\n    C --> D[\"Receive root hashes\n    (parse_merkle_output)\"]\n    D --> E[\"Send challenge indices\n    (send_challenge_indices)\"]\n    E --> F[\"Receive Merkle proofs\n    (receive_responses)\"]\n    F --> G[\"Verify proofs\n    (verify_responses)\"]\n    G --> H{\"Verification successful?\"}\n    H -->|\"Yes\"| I[\"Record GPU specifications\"]\n    H -->|\"No\"| J[\"Fail verification\"]\n```\n\nThis cryptographic verification ensures the miner cannot precompute results or falsify benchmarks.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:859-908\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L859-L908\" />, \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n## System Architecture\n\nThe PoG system is implemented across several components in the codebase:\n\n```mermaid\ngraph TD\n    subgraph \"Validator System\"\n        V[\"validator.py\"] --> POG[\"Validator/pog.py\"]\n        V --> CALC[\"Validator/calculate_pow_score.py\"]\n        POG --> SH[\"send_script_and_request_hash()\"]\n        POG --> BM[\"parse_benchmark_output()\"]\n        POG --> MK[\"verify_merkle_proof_row()\"]\n        CALC --> CS[\"calc_score_pog()\"]\n    end\n    \n    subgraph \"Miner Node\"\n        MS[\"Miner Script\"] --> GPU[\"GPU Hardware\"]\n        MS --> CUDA[\"CUDA Operations\"]\n        MS --> MT[\"Merkle Tree Generation\"]\n    end\n    \n    subgraph \"Database\"\n        DB[\"ComputeDb\"] --> PST[\"POG Stats Table\"]\n        V --> DB\n    end\n    \n    V \u003C--> SSH[\"SSH Connection\"]\n    SSH \u003C--> MS\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:638-762\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L638-L762\" />, \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />, \u003CSourceLink text=\"neurons/Validator/database/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py\" />\n\n## GPU Scoring and Identification\n\nThe system identifies GPU models based on their performance characteristics:\n\n1. Benchmark results produce FP16 and FP32 TFLOPS measurements\n2. VRAM capacity is detected and reported\n3. These metrics are compared against known values for different GPU models\n4. A tolerance system allows for some variation in benchmark results\n5. The identified GPU type and count are stored in the database\n\nThe identification process uses a configuration file that defines performance expectations for different GPU models. The `identify_gpu` function matches the measured performance against these known profiles.\n\nSources: \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />, \u003CSourceLink text=\"neurons/Validator/calculate_pow_score.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py\" />\n\n## Database Integration\n\nProof of GPU results are stored in a database for:\n\n1. Persistent tracking of miner capabilities\n2. Input into the scoring system\n3. Historical analysis of network hardware\n\nKey database functions:\n- `get_pog_specs`: Retrieves stored GPU specifications for a specific miner\n- `update_pog_stats`: Updates the database with new proof results\n- `retrieve_stats`: Gets statistics for all miners\n\nSources: \u003CSourceLink text=\"neurons/validator.py:343-349\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L343-L349\" />, \u003CSourceLink text=\"neurons/Validator/database/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/database/pog.py\" />\n\n## Scheduling and Resource Management\n\nThe PoG system includes intelligent scheduling to avoid overwhelming the network:\n\n1. Tests are performed periodically (approximately every ~360 blocks)\n2. Random delays are added to prevent network congestion\n3. Concurrent testing is limited to a configurable number of miners\n4. Allocated miners are excluded from testing to avoid disrupting active services\n5. Failed tests can be retried a configurable number of times\n\n```mermaid\nflowchart TD\n    A[\"Validator Start\"] --> B[\"Block check \n    (current_block % block_next_pog == 0)\"]\n    B -->|\"Yes\"| C[\"Schedule next POG\n    (block_next_pog = current_block + 360)\"]\n    C --> D[\"Create async task\n    (proof_of_gpu)\"]\n    D --> E[\"Random delay\n    (0-1200 seconds)\"]\n    E --> F[\"Initialize worker pool\"]\n    F --> G[\"Queue miners for testing\"]\n    G --> H{\"Queue empty?\"}\n    H -->|\"No\"| I[\"Test miner GPU\n    (test_miner_gpu)\"]\n    I --> J{\"Test successful?\"}\n    J -->|\"Yes\"| K[\"Record results\"]\n    J -->|\"No\"| L{\"Retry limit reached?\"}\n    L -->|\"No\"| M[\"Add to retry queue\"]\n    L -->|\"Yes\"| N[\"Record failure\"]\n    M --> H\n    K --> H\n    N --> H\n    H -->|\"Yes\"| O[\"Complete POG cycle\"]\n    O --> P[\"Sync scores\"]\n```\n\nSources: \u003CSourceLink text=\"neurons/validator.py:638-762\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L638-L762\" />, \u003CSourceLink text=\"neurons/validator.py:1169-1176\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1169-L1176\" />\n\n## Integration with Scoring System\n\nThe PoG results directly influence miner scoring:\n\n1. Successful verification stores GPU type and count in the database\n2. The scoring system retrieves this data when calculating miner scores\n3. Miners with more powerful/numerous GPUs receive higher scores\n4. These scores influence the weights set on the blockchain\n5. Weights determine reward distribution in the subnet\n\nIf a miner fails PoG verification or has no GPUs detected, they receive a score of 0 for GPU capabilities.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:343-366\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L343-L366\" />, \u003CSourceLink text=\"neurons/Validator/calculate_pow_score.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py\" />\n\n## Security Considerations\n\nThe PoG system includes several security measures:\n\n1. Script integrity verification prevents tampering with the benchmarking code\n2. Random seeds prevent precomputation of results\n3. Merkle proofs cryptographically verify computational results\n4. Performance-based verification makes it difficult to simulate GPUs with CPUs\n5. SSH connections are secured with proper authentication\n\nThese measures collectively ensure that miners cannot easily falsify their hardware capabilities.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:774-923\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L774-L923\" />, \u003CSourceLink text=\"neurons/Validator/pog.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/pog.py\" />\n\n## Table of GPU Identification Parameters\n\nThe following table illustrates examples of how different GPU models might be identified (actual values may vary):\n\n| GPU Model | Typical FP16 TFLOPS | Typical FP32 TFLOPS | VRAM (GB) | Score Multiplier |\n|-----------|---------------------|---------------------|-----------|------------------|\n| RTX 3090  | 35-40               | 18-22               | 24        | High             |\n| RTX 3080  | 28-33               | 14-18               | 10        | Medium-High      |\n| A100      | 75-85               | 18-22               | 40/80     | Very High        |\n| V100      | 55-65               | 14-18               | 16/32     | High             |\n| T4        | 8-12                | 4-6                 | 16        | Medium           |\n| K80       | 4-6                 | 2-3                 | 12        | Low              |\n\nThe system uses tolerance pairs to account for variations in benchmark results across different environments and configurations.\n\nSources: \u003CSourceLink text=\"neurons/Validator/calculate_pow_score.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py\" />, \u003CSourceLink text=\"neurons/validator.py:856-858\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L856-L858\" />\n\n## Conclusion\n\nProof of GPU is a critical component of the NI Compute subnet that provides cryptographic assurance of miners' hardware capabilities. By combining hardware detection, performance benchmarking, and cryptographic verification, the system maintains the integrity of the marketplace and ensures that rewards are distributed fairly based on actual GPU resources contributed to the network.","src/content/docs/validator-system/proof-of-gpu.mdx","7ca3103bbab524e5","validator-system/scoring-system",{"id":842,"data":844,"body":849,"filePath":850,"digest":851,"deferredRender":15},{"title":845,"editUrl":15,"head":846,"template":17,"sidebar":847,"pagefind":15,"draft":19},"Scoring System",[],{"hidden":19,"attrs":848},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/Validator/calculate_pow_score.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py\" />\n\n  \u003CSourceLink text=\"neurons/validator.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\nThe scoring system evaluates miner performance based on their GPU capabilities and proof verification results. It calculates normalized scores that determine each miner's weight in the network's incentive mechanism. The system integrates Proof-of-GPU validation results with configurable GPU performance metrics to assign fair scores across the network.\n\nFor information about the Proof-of-GPU validation process that generates the data used in scoring, see [Proof of GPU](/validator-system/proof-of-gpu#2.1). For details about how scores are stored and retrieved, see [Database Operations](/validator-system/database-operations#2.3).\n\n## Score Calculation Architecture\n\nThe scoring system operates through several interconnected components that collect GPU performance data, calculate scores, and synchronize results across the network.\n\n```mermaid\nflowchart TD\n    subgraph \"Data Sources\"\n        POG[\"Proof-of-GPU Results\u003Cbr/>test_miner_gpu()\"]\n        WANDB[\"WandB Distributed State\u003Cbr/>get_stats_allocated()\"]\n        CONFIG[\"GPU Performance Config\u003Cbr/>config.yaml\"]\n        LOCALDB[\"Local Database\u003Cbr/>pog_stats table\"]\n    end\n    \n    subgraph \"Score Calculation Engine\"\n        CALC[\"calc_score_pog()\u003Cbr/>calculate_pow_score.py\"]\n        SYNC[\"sync_scores()\u003Cbr/>validator.py\"]\n        NORM[\"Score Normalization\u003Cbr/>normalize()\"]\n    end\n    \n    subgraph \"Score Storage & Distribution\"\n        STATS[\"self.stats dict\u003Cbr/>Miner Statistics\"]\n        SCORES[\"self.scores Tensor\u003Cbr/>PyTorch Tensor\"]\n        WEIGHTS[\"Network Weights\u003Cbr/>set_weights()\"]\n    end\n    \n    POG --> CALC\n    CONFIG --> CALC\n    LOCALDB --> SYNC\n    WANDB --> SYNC\n    \n    CALC --> NORM\n    SYNC --> STATS\n    STATS --> SCORES\n    \n    SCORES --> WEIGHTS\n    \n    NORM -.-> STATS\n```\n\n**Score Calculation Flow**\nThe system processes GPU specifications through multiple stages to produce final network weights that determine miner rewards.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:312-442\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L312-L442\" />, \u003CSourceLink text=\"neurons/Validator/calculate_pow_score.py:35-63\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py#L35-L63\" />\n\n## GPU Performance Scoring\n\nThe core scoring mechanism evaluates miners based on their GPU hardware capabilities using predefined performance benchmarks and real-time validation results.\n\n### Score Calculation Formula\n\nThe `calc_score_pog` function implements the primary scoring algorithm:\n\n```mermaid\nflowchart LR\n    subgraph \"Input Parameters\"\n        GPU_SPECS[\"gpu_specs\u003Cbr/>{gpu_name, num_gpus}\"]\n        CONFIG[\"config_data\u003Cbr/>gpu_performance.gpu_scores\"]\n        ALLOCATED[\"allocated_hotkeys\u003Cbr/>Active Allocations\"]\n    end\n    \n    subgraph \"Score Calculation\"\n        MAX_CALC[\"Calculate max_score\u003Cbr/>max_gpu * 8\"]\n        FACTOR[\"score_factor : 100/max_score\"]\n        BASE_SCORE[\"base_score : gpu_score * num_gpus * factor\"]\n        ALLOCATION[\"Allocation Multiplier\u003Cbr/>1.0 (no bonus)\"]\n        NORMALIZE[\"normalize(score, 0, 100)\"]\n    end\n    \n    GPU_SPECS --> BASE_SCORE\n    CONFIG --> MAX_CALC\n    CONFIG --> BASE_SCORE\n    MAX_CALC --> FACTOR\n    FACTOR --> BASE_SCORE\n    BASE_SCORE --> ALLOCATION\n    ALLOCATED --> ALLOCATION\n    ALLOCATION --> NORMALIZE\n```\n\n**GPU Score Calculation Pipeline**\nThe scoring formula normalizes GPU performance against the maximum possible score in the network.\n\n| Parameter | Description | Range |\n|-----------|-------------|-------|\n| `gpu_name` | GPU model identifier | String key from config |\n| `num_gpus` | Number of GPUs (capped) | 1-8 |\n| `gpu_score` | Base performance score | From config.yaml |\n| `score_factor` | Normalization factor | 100/max_possible_score |\n\nSources: \u003CSourceLink text=\"neurons/Validator/calculate_pow_score.py:35-63\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py#L35-L63\" />\n\n### GPU Performance Configuration\n\nThe system uses a configuration-driven approach to define GPU performance scores:\n\n```mermaid\ngraph TD\n    subgraph \"config.yaml Structure\"\n        GPU_PERF[\"gpu_performance\"]\n        GPU_SCORES[\"gpu_scores\u003Cbr/>{GPU_MODEL: score}\"]\n        GPU_TOL[\"gpu_tolerance_pairs\u003Cbr/>Performance Tolerances\"]\n    end\n    \n    subgraph \"Score Processing\"\n        MAX_GPU[\"max(gpu_scores.values())\"]\n        MAX_SCORE[\"max_score : max_gpu * 8\"]\n        FACTOR[\"score_factor : 100 / max_score\"]\n    end\n    \n    GPU_PERF --> GPU_SCORES\n    GPU_PERF --> GPU_TOL\n    GPU_SCORES --> MAX_GPU\n    MAX_GPU --> MAX_SCORE\n    MAX_SCORE --> FACTOR\n```\n\n**Configuration-Based Scoring**\nGPU performance scores are defined in configuration files and used to normalize miner capabilities.\n\nSources: \u003CSourceLink text=\"neurons/Validator/calculate_pow_score.py:37-42\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/Validator/calculate_pow_score.py#L37-L42\" />\n\n## Score Synchronization Process\n\nThe validator synchronizes scores across multiple data sources to maintain consistent network state and handle distributed validation scenarios.\n\n```mermaid\nsequenceDiagram\n    participant V as Validator\n    participant DB as ComputeDb\n    participant W as WandB\n    participant S as self.stats\n    participant T as self.scores\n    \n    Note over V,T: Score Synchronization Cycle\n    V->>DB: retrieve_stats()\n    V->>W: get_allocated_hotkeys()\n    V->>W: get_stats_allocated()\n    V->>W: get_penalized_hotkeys_checklist_bak()\n    \n    loop For each UID\n        V->>V: Check if uid in queryable_uids\n        alt UID not queryable\n            V->>S: Set score = 0, own_score = True\n            V->>T: scores[uid] = 0\n            V->>DB: DELETE FROM pog_stats\n        else UID queryable\n            V->>DB: get_pog_specs(hotkey)\n            alt Local specs found\n                V->>V: calc_score_pog(gpu_specs)\n                V->>S: Set own_score = True\n            else No local specs\n                V->>W: Use stats_allocated fallback\n                V->>S: Set own_score = False\n            end\n            V->>S: Apply penalty if hotkey penalized\n            V->>T: scores[uid] = calculated_score\n        end\n    end\n    \n    V->>DB: write_stats(stats)\n```\n\n**Score Synchronization Sequence**\nThe validator coordinates between local database, WandB distributed state, and in-memory score tracking.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:312-442\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L312-L442\" />\n\n### Local vs External Score Sources\n\nThe system handles both local Proof-of-GPU results and external score data from other validators:\n\n| Score Source | Priority | Indicator | Data Source |\n|--------------|----------|-----------|-------------|\n| Local PoG Results | High | `own_score: True` | Local `pog_stats` table |\n| External Validator Data | Low | `own_score: False` | WandB `stats_allocated` |\n| Penalized Miners | Override | `score: 0` | WandB penalized list |\n| Non-queryable Miners | Override | `score: 0` | Filtered queryable set |\n\nSources: \u003CSourceLink text=\"neurons/validator.py:360-382\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L360-L382\" />\n\n## Weight Setting and Network Integration\n\nThe scoring system converts calculated scores into network weights that determine miner rewards in the Bittensor incentive mechanism.\n\n```mermaid\nflowchart TD\n    subgraph \"Score Processing\"\n        SCORES[\"self.scores\u003Cbr/>PyTorch Tensor\"]\n        CLAMP[\"scores[scores \u003C 0] : 0\u003Cbr/>Remove Negatives\"]\n        NORMALIZE[\"torch.nn.functional.normalize\u003Cbr/>p:1.0, dim:0\"]\n    end\n    \n    subgraph \"Weight Setting\"\n        WEIGHTS[\"Normalized Weights\u003Cbr/>Sum : 1.0\"]\n        SUBTENSOR[\"subtensor.set_weights()\"]\n        BLOCKCHAIN[\"Bittensor Blockchain\u003Cbr/>Network State\"]\n    end\n    \n    subgraph \"Alternative: Burn Weights\"\n        BURN_UID[\"get_burn_uid()\u003Cbr/>Subnet Owner UID\"]\n        BURN_WEIGHT[\"Single Weight : 1.0\u003Cbr/>100% to Burn Account\"]\n        BURN_SET[\"set_burn_weights()\"]\n    end\n    \n    SCORES --> CLAMP\n    CLAMP --> NORMALIZE\n    NORMALIZE --> WEIGHTS\n    WEIGHTS --> SUBTENSOR\n    SUBTENSOR --> BLOCKCHAIN\n    \n    BURN_UID --> BURN_WEIGHT\n    BURN_WEIGHT --> BURN_SET\n    BURN_SET --> BLOCKCHAIN\n```\n\n**Weight Setting Architecture**\nThe system converts scores to blockchain weights through normalization and clamping operations.\n\n### Weight Setting Process\n\nThe validator implements two weight setting strategies:\n\n1. **Standard Weight Setting** (`set_weights`):\n   - Clamps negative scores to zero\n   - L1-normalizes scores to sum to 1.0\n   - Distributes weights across all miners\n\n2. **Burn Weight Setting** (`set_burn_weights`):\n   - Assigns 100% weight to subnet owner (burn account)\n   - Used as alternative to standard distribution\n\nSources: \u003CSourceLink text=\"neurons/validator.py:1132-1154\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1132-L1154\" />, \u003CSourceLink text=\"neurons/validator.py:1101-1131\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1101-L1131\" />\n\n### Weight Update Schedule\n\nWeights are updated periodically based on the `weights_rate_limit` configuration:\n\n| Event | Block Interval | Description |\n|-------|----------------|-------------|\n| Weight Setting | `weights_rate_limit` | Update network weights |\n| Score Sync | Every weight update | Recalculate all scores |\n| Block Tracking | Continuous | Prevent duplicate operations |\n\nThe validator tracks the last update block and ensures weights are only set once per interval to comply with network rate limits.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:1240-1247\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L1240-L1247\" />\n\n## Score Statistics and Monitoring\n\nThe system provides comprehensive statistics and monitoring for score calculation and distribution across the network.\n\n```mermaid\ngraph TD\n    subgraph \"Statistics Collection\"\n        STATS_DICT[\"self.stats Dictionary\u003Cbr/>Per-UID Statistics\"]\n        MINER_DETAILS[\"Miner Details\u003Cbr/>{hotkey, allocated, score, gpu_specs}\"]\n        RELIABILITY[\"reliability_score\u003Cbr/>Historical Performance\"]\n    end\n    \n    subgraph \"Monitoring Output\"\n        LOG_SUMMARY[\"Miner Stats Summary\u003Cbr/>Formatted Log Output\"]\n        WANDB_METRICS[\"WandB Metrics\u003Cbr/>Chain Data Logging\"]\n        ALLOCATION_SYNC[\"Allocation Synchronization\u003Cbr/>update_allocation_wandb()\"]\n    end\n    \n    subgraph \"Score Distribution Analysis\"\n        GPU_PARSING[\"GPU Spec Display\u003Cbr/>num_gpus x gpu_name\"]\n        SCORE_FORMAT[\"Score Formatting\u003Cbr/>2 decimal places\"]\n        SOURCE_TRACKING[\"Source Identification\u003Cbr/>Local vs External\"]\n    end\n    \n    STATS_DICT --> MINER_DETAILS\n    MINER_DETAILS --> LOG_SUMMARY\n    MINER_DETAILS --> GPU_PARSING\n    MINER_DETAILS --> SCORE_FORMAT\n    \n    LOG_SUMMARY --> MONITORING_OUTPUT\n    WANDB_METRICS --> MONITORING_OUTPUT\n    ALLOCATION_SYNC --> MONITORING_OUTPUT\n```\n\n**Statistics and Monitoring Pipeline**\nThe system provides detailed visibility into score calculation and distribution across miners.\n\n### Statistics Structure\n\nEach miner's statistics include comprehensive performance and allocation data:\n\n```python\nself.stats[uid] = {\n    \"hotkey\": hotkey,\n    \"allocated\": hotkey in allocated_hotkeys,\n    \"own_score\": bool,  # True if local PoG, False if external\n    \"score\": calculated_score * 100,  # Scaled to 0-100\n    \"gpu_specs\": gpu_specifications,\n    \"reliability_score\": historical_performance\n}\n```\n\nThe system outputs formatted statistics showing miner performance across the network with fixed-width columns for easy reading.\n\nSources: \u003CSourceLink text=\"neurons/validator.py:406-442\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/validator.py#L406-L442\" />","src/content/docs/validator-system/scoring-system.mdx","b138801301b914eb","resource-allocation-api/api-endpoints",{"id":852,"data":854,"body":859,"filePath":860,"digest":861,"deferredRender":15},{"title":855,"editUrl":15,"head":856,"template":17,"sidebar":857,"pagefind":15,"draft":19},"API Endpoints",[],{"hidden":19,"attrs":858},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/register_api.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\n## Overview\n\nThis document details the API endpoints available in the NI Compute (Subnet 27) RegisterAPI system. The API provides interfaces for allocating GPU resources, managing Docker containers, and retrieving information about available resources in the compute marketplace. For information about the overall resource management approach, see [Resource Management](/resource-allocation-api/resource-management#4.2).\n\nThe RegisterAPI runs as a FastAPI web service that exposes HTTP endpoints for client applications to interact with the compute subnet. It serves as the primary interface for allocating, deallocating, and checking the status of GPU resources.\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:228-342\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L228-L342\" />\n\n## API Architecture\n\nThe RegisterAPI is implemented using FastAPI and follows a RESTful design pattern. It provides endpoints for different operations categorized by functionality.\n\n```mermaid\nflowchart TD\n    subgraph \"RegisterAPI\"\n        direction TB\n        API[\"FastAPI Service\"]\n        \n        subgraph \"Allocation Endpoints\"\n            AE1[\"POST /service/allocate_spec\"]\n            AE2[\"POST /service/allocate_hotkey\"]\n            AE3[\"POST /service/deallocate\"]\n        end\n        \n        subgraph \"Management Endpoints\"\n            ME1[\"POST /service/check_miner_status\"]\n            ME2[\"POST /service/restart_docker\"]\n            ME3[\"POST /service/pause_docker\"]\n            ME4[\"POST /service/unpause_docker\"]\n            ME5[\"POST /service/exchange_docker_key\"]\n        end\n        \n        subgraph \"Resource Listing Endpoints\"\n            RL1[\"POST /list/allocations_sql\"]\n            RL2[\"POST /list/resources_sql\"]\n            RL3[\"POST /list/resources_wandb\"]\n            RL4[\"POST /list/count_all_gpus\"]\n            RL5[\"POST /list/count_all_by_model\"]\n            RL6[\"POST /list/all_runs\"]\n        end\n        \n        API --> AE1 & AE2 & AE3\n        API --> ME1 & ME2 & ME3 & ME4 & ME5\n        API --> RL1 & RL2 & RL3 & RL4 & RL5 & RL6\n    end\n    \n    DB[(ComputeDb)] \u003C-- \"Store/Retrieve\" --> API\n    WS[\"WebSocket\\n/connect\"] \u003C-- \"Status updates\" --> API\n    Metagraph[\"Bittensor\\nMetagraph\"] \u003C-- \"Retrieve miner info\" --> API\n    Dendrite[\"Bittensor\\nDendrite\"] \u003C-- \"Communicate with miners\" --> API\n    WandB[\"Weights & Biases\"] \u003C-- \"Resource metrics\" --> API\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:344-377\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L344-L377\" />, \u003CSourceLink text=\"neurons/register_api.py:400-406\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L400-L406\" />\n\n## Data Models\n\nThe API uses several data models to structure request and response data:\n\n```mermaid\nclassDiagram\n    class DeviceRequirement {\n        +int cpu_count\n        +str gpu_type\n        +int gpu_size\n        +int ram\n        +int hard_disk\n        +int timeline\n    }\n    \n    class DockerRequirement {\n        +str base_image\n        +str ssh_key\n        +str volume_path\n        +str dockerfile\n    }\n    \n    class Allocation {\n        +str resource\n        +str hotkey\n        +str regkey\n        +str ssh_ip\n        +int ssh_port\n        +str ssh_username\n        +str ssh_password\n        +str ssh_command\n        +str ssh_key\n        +str uuid_key\n        +int miner_version\n    }\n    \n    class Resource {\n        +str hotkey\n        +int cpu_count\n        +str gpu_name\n        +str gpu_capacity\n        +int gpu_count\n        +str ram\n        +str hard_disk\n        +str allocate_status\n    }\n    \n    class ResourceQuery {\n        +str gpu_name\n        +int cpu_count_min\n        +int cpu_count_max\n        +float gpu_capacity_min\n        +float gpu_capacity_max\n        +float hard_disk_total_min\n        +float hard_disk_total_max\n        +float ram_total_min\n        +float ram_total_max\n    }\n    \n    class Response {\n        +bool success\n        +str message\n        +dict data\n    }\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:136-226\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L136-L226\" />\n\n## Allocation Endpoints\n\n### Allocate by Specification\n\nAllocates GPU resources based on hardware specifications.\n\n**Endpoint:** `POST /service/allocate_spec`\n\n**Request Parameters:**\n- `requirements`: DeviceRequirement - Hardware specifications for the allocation\n- `docker_requirement`: DockerRequirement - Docker container configuration\n\n**Response:** \n- Success: Returns an Allocation object with SSH access details\n- Error: Returns an error message if allocation fails\n\n**Example Flow:**\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant RegisterAPI\n    participant Dendrite\n    participant Miner\n    participant ComputeDb\n\n    Client->>RegisterAPI: POST /service/allocate_spec\n    RegisterAPI->>RegisterAPI: Generate RSA key pair\n    RegisterAPI->>RegisterAPI: Find suitable miner\n    RegisterAPI->>Dendrite: Send Allocate request\n    Dendrite->>Miner: Forward Allocate request\n    Miner->>Miner: Create Docker container\n    Miner->>Dendrite: Return SSH access details\n    Dendrite->>RegisterAPI: Forward SSH access details\n    RegisterAPI->>ComputeDb: Update allocation DB\n    RegisterAPI->>Client: Return allocation information\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:407-547\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L407-L547\" />\n\n### Allocate by Hotkey\n\nAllocates resources from a specific miner by its hotkey.\n\n**Endpoint:** `POST /service/allocate_hotkey`\n\n**Request Parameters:**\n- `hotkey`: String - The miner's hotkey\n- `ssh_key`: Optional[String] - SSH public key for secure access\n- `docker_requirement`: Optional[DockerRequirement] - Docker container configuration\n\n**Response:**\n- Success: Returns an Allocation object with SSH access details\n- Error: Returns an error message if allocation fails\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:549-695\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L549-L695\" />\n\n### Deallocate\n\nDeallocates previously allocated GPU resources.\n\n**Endpoint:** `POST /service/deallocate`\n\n**Request Parameters:**\n- `hotkey`: String - The miner's hotkey\n- `uuid_key`: String - The UUID of the allocation\n- `notify_flag`: Boolean - Whether to send a notification about deallocation\n\n**Response:**\n- Success: Confirmation of successful deallocation\n- Error: Error message if deallocation fails\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:697-850\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L697-L850\" />\n\n## Management Endpoints\n\n### Check Miner Status\n\nChecks the status of specified miners.\n\n**Endpoint:** `POST /service/check_miner_status`\n\n**Request Parameters:**\n- `hotkey_list`: List[String] - List of miner hotkeys to check\n- `query_version`: Boolean - Whether to query miner version\n\n**Response:**\n- List of status objects for each miner, containing hotkey and status information\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:852-905\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L852-L905\" />\n\n### Docker Container Management\n\nThe following endpoints allow management of allocated Docker containers:\n\n1. **Restart Docker:**\n   - **Endpoint:** `POST /service/restart_docker`\n   - **Parameters:** `hotkey`, `uuid_key`\n\n2. **Pause Docker:**\n   - **Endpoint:** `POST /service/pause_docker`\n   - **Parameters:** `hotkey`, `uuid_key`\n\n3. **Unpause Docker:**\n   - **Endpoint:** `POST /service/unpause_docker`\n   - **Parameters:** `hotkey`, `uuid_key`\n\n4. **Exchange Docker SSH Key:**\n   - **Endpoint:** `POST /service/exchange_docker_key`\n   - **Parameters:** `hotkey`, `uuid_key`, `ssh_key`, `key_type`\n\nAll these endpoints follow a similar pattern:\n1. Verify the allocation exists and the UUID matches\n2. Send the appropriate command to the miner via Dendrite\n3. Return success/failure response\n\n```mermaid\nflowchart TB\n    Start[\"Docker Management Request\"] --> GetAllocation[\"Retrieve allocation\\ndetails from DB\"]\n    GetAllocation --> ValidateUUID{\"UUID valid?\"}\n    ValidateUUID -- \"Yes\" --> FindMiner[\"Find miner in metagraph\"]\n    ValidateUUID -- \"No\" --> Error1[\"Return Error:\\nInvalid UUID\"]\n    FindMiner --> SendCommand[\"Send command to miner\\nvia Dendrite\"]\n    SendCommand --> Response{\"Miner\\nresponded?\"}\n    Response -- \"Yes\" --> Success[\"Return Success\"]\n    Response -- \"No\" --> Error2[\"Return Error:\\nNo response\"]\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:907-1311\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L907-L1311\" />\n\n## Resource Listing Endpoints\n\n### List Allocations\n\nLists all current resource allocations.\n\n**Endpoint:** `POST /list/allocations_sql`\n\n**Response:**\n- List of Allocation objects describing currently allocated resources\n- Error if retrieving allocations fails\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:1313-1413\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1313-L1413\" />\n\n### List Resources\n\nLists available GPU resources from the SQLite database.\n\n**Endpoint:** `POST /list/resources_sql`\n\n**Request Parameters:**\n- `query`: ResourceQuery - Optional filter criteria\n- `stats`: Boolean - Return statistics instead of full list\n- `page_size`: Optional[Integer] - Number of items per page\n- `page_number`: Optional[Integer] - Page number to return\n\n**Response:**\n- List of Resource objects or statistics about available resources\n- Error if retrieving resources fails\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:1415-1638\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1415-L1638\" />\n\n### List Resources from W&B\n\nLists available GPU resources from Weights & Biases data.\n\n**Endpoint:** `POST /list/resources_wandb`\n\n**Request Parameters:**\n- Same as `/list/resources_sql`\n\n**Response:**\n- Similar to `/list/resources_sql` but using W&B as data source\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:1822-2047\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1822-L2047\" />\n\n### Count GPUs\n\nCounts all available GPUs on the compute subnet.\n\n**Endpoint:** `POST /list/count_all_gpus`\n\n**Response:**\n- Total count of GPUs available on the subnet\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:1698-1749\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1698-L1749\" />\n\n### Count GPUs by Model\n\nCounts GPUs of a specific model with optional filtering.\n\n**Endpoint:** `POST /list/count_all_by_model`\n\n**Request Parameters:**\n- `model`: String - GPU model name\n- `cpu_count`: Optional[Integer] - CPU count filter\n- `ram_size`: Optional[Float] - RAM size filter\n\n**Response:**\n- Count of GPUs matching the specified criteria\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:1750-1819\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1750-L1819\" />\n\n### List All Runs\n\nLists all running miners from Weights & Biases.\n\n**Endpoint:** `POST /list/all_runs`\n\n**Request Parameters:**\n- `hotkey`: Optional[String] - Filter by specific hotkey\n- `page_size`: Optional[Integer] - Number of items per page\n- `page_number`: Optional[Integer] - Page number to return\n\n**Response:**\n- List of run information objects from W&B\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2049-2154\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2049-L2154\" />\n\n## Resource Allocation Flow\n\nThe following diagram illustrates the complete flow of the resource allocation process:\n\n```mermaid\nflowchart TD\n    Client[\"Client\"] --> SpecRequest[\"POST /service/allocate_spec\\nor\\nPOST /service/allocate_hotkey\"]\n    \n    subgraph \"RegisterAPI Allocation Process\"\n        SpecRequest --> GenerateKey[\"Generate RSA Key Pair\"]\n        GenerateKey --> FindMiner[\"Find Suitable Miner\"]\n        FindMiner --> SendAlloc[\"Send Allocation Request\\nvia Dendrite\"]\n    end\n    \n    subgraph \"Miner Allocation Process\"\n        SendAlloc --> MinerProc[\"Miner Processes Request\"]\n        MinerProc --> CreateCont[\"Create Docker Container\"]\n        CreateCont --> GenerateSSH[\"Generate SSH Credentials\"]\n        GenerateSSH --> ReturnInfo[\"Return Container Info\"]\n    end\n    \n    ReturnInfo --> DecryptInfo[\"Decrypt Container Info\"]\n    DecryptInfo --> UpdateDB[\"Update Allocation Database\"]\n    UpdateDB --> UpdateWandB[\"Update WandB Metrics\"]\n    UpdateWandB --> ReturnDetails[\"Return Allocation Details\\nto Client\"]\n    \n    ReturnDetails --> Client\n    \n    Client --> DeallocRequest[\"POST /service/deallocate\"]\n    DeallocRequest --> VerifyUUID[\"Verify UUID\"]\n    VerifyUUID --> SendDealloc[\"Send Deallocation Request\"]\n    SendDealloc --> StopContainer[\"Stop Container\"]\n    StopContainer --> UpdateDBDealloc[\"Update Database\"]\n    UpdateDBDealloc --> NotifyDealloc[\"Send Deallocation Notification\"]\n    NotifyDealloc --> ReturnSuccess[\"Return Success to Client\"]\n    ReturnSuccess --> Client\n```\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:407-547\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L407-L547\" />, \u003CSourceLink text=\"neurons/register_api.py:549-695\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L549-L695\" />, \u003CSourceLink text=\"neurons/register_api.py:697-850\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L697-L850\" />\n\n## API Connection Validation\n\nThe RegisterAPI includes a utility function to check port availability and connectivity:\n\n```mermaid\nflowchart LR\n    Start[\"check_port(host, port)\"] --> CreateSocket[\"Create Socket Connection\"]\n    CreateSocket --> Attempt[\"Attempt Connection\"]\n    Attempt --> Result{\"Connection\\nResult\"}\n    Result -- \"Success (0)\" --> ReturnTrue[\"Return True\"]\n    Result -- \"Failure (non-zero)\" --> ReturnFalse[\"Return False\"]\n    CreateSocket -- \"Socket Error\" --> ReturnNone1[\"Return None\"]\n    Attempt -- \"Hostname Error\" --> ReturnNone2[\"Return None\"]\n```\n\nSources: \u003CSourceLink text=\"compute/utils/socket.py:1-18\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/compute/utils/socket.py#L1-L18\" />\n\n## Authentication and Security\n\nThe RegisterAPI includes several security features:\n\n1. **IP Whitelisting**: Optional middleware to restrict access to specific IP addresses\n2. **UUID Validation**: All resource management endpoints require a valid UUID that matches the allocation record\n3. **RSA Encryption**: Container access credentials are encrypted with RSA\n4. **Miner Blacklist**: Prevents allocation to known problematic miners\n\nWhen enabled, IP whitelisting restricts API access to a predefined list of IPs, with configuration available through environment variables.\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:85-97\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L85-L97\" />, \u003CSourceLink text=\"neurons/register_api.py:119-134\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L119-L134\" />, \u003CSourceLink text=\"neurons/register_api.py:321-322\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L321-L322\" />\n\n## Error Handling\n\nThe API implements custom error handling to provide clear, structured error responses:\n\n1. **Validation Errors**: Returns details about which fields failed validation\n2. **Not Found Errors**: When resources or allocations can't be found\n3. **Authorization Errors**: When security checks fail\n4. **General Errors**: For other unexpected failures\n\nAll error responses follow a consistent format with `success: false`, an error message, and optional details.\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:345-358\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L345-L358\" />\n\n## Background Tasks\n\nThe RegisterAPI maintains two background tasks:\n\n1. **Metagraph Refresh**: Periodically updates the metagraph to keep miner information current\n2. **Allocation Check**: Regularly checks allocated resources to ensure they're still available and properly managed\n\nThese tasks run asynchronously in the background while the API handles requests.\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:367-368\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L367-L368\" />","src/content/docs/resource-allocation-api/api-endpoints.mdx","442d45dfe72f656f","resource-allocation-api/resource-management",{"id":862,"data":864,"body":869,"filePath":870,"digest":871,"deferredRender":15},{"title":865,"editUrl":15,"head":866,"template":17,"sidebar":867,"pagefind":15,"draft":19},"Resource Management",[],{"hidden":19,"attrs":868},{},"import CollapsibleAside from '@components/CollapsibleAside.astro';\n\nimport SourceLink from '@components/SourceLink.astro';\n\n\n\n\u003CCollapsibleAside title=\"Relevant Source Files\">\n\n  \u003CSourceLink text=\"neurons/register_api.py\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py\" />\n\n\u003C/CollapsibleAside>\n\n\n\n\n\n\n\n\n## Purpose and Scope\n\nThe Resource Management system implements the core allocation logic within the Resource Allocation API, handling resource discovery, allocation strategies, health monitoring, and state synchronization. This system operates within the `RegisterAPI` class and provides the intelligence for matching compute requirements with available miners, managing resource lifecycle, and maintaining distributed state.\n\nFor information about the API endpoints that expose these capabilities, see [API Endpoints](/resource-allocation-api/api-endpoints#4.1). For broader context about the validator system that validates miner capabilities, see [Validator System](/validator-system#2).\n\n## Resource Discovery System\n\nThe resource discovery system identifies and evaluates candidate miners for allocation requests through a multi-stage process involving database queries, network validation, and scoring algorithms.\n\n### Candidate Selection Process\n\n```mermaid\nflowchart TD\n    DevReq[\"DeviceRequirement\"] --> SelectCandidates[\"select_allocate_miners_hotkey()\"]\n    SelectCandidates --> CandidateList[\"candidates_hotkey[]\"]\n    CandidateList --> FilterAxons[\"Filter metagraph.axons\"]\n    FilterAxons --> AxonCandidates[\"axon_candidates[]\"]\n    \n    AxonCandidates --> CheckAvailability[\"dendrite(Allocate(checking:True))\"]\n    CheckAvailability --> ValidateResponses[\"Validate responses\"]\n    ValidateResponses --> FinalCandidates[\"final_candidates_hotkey[]\"]\n    \n    FinalCandidates --> ScoreSort[\"Score-based sorting\"]\n    ScoreSort --> SortedCandidates[\"sorted_hotkeys[]\"]\n    SortedCandidates --> AttemptAllocation[\"Attempt allocation\"]\n```\n\n**Candidate Discovery Flow**\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2741-2805\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2741-L2805\" />\n\nThe system uses a two-phase approach for candidate selection:\n\n1. **Database Filtering**: The `select_allocate_miners_hotkey()` function queries the local database to find miners matching hardware requirements\n2. **Network Validation**: Available candidates are validated through the Bittensor network using `Allocate` synapse with `checking=True`\n3. **Scoring and Prioritization**: Valid candidates are sorted by their network scores to prioritize higher-performing miners\n\n### WandB-Based Resource Discovery\n\n```mermaid\nflowchart TD\n    WandbQuery[\"get_wandb_running_miners()\"] --> FilterRuns[\"Filter running miner runs\"]\n    FilterRuns --> CheckPenalized[\"Check penalized_hotkeys\"]\n    CheckPenalized --> ValidateAge[\"miner_is_older_than(48h)\"]\n    ValidateAge --> ValidatePog[\"miner_pog_ok(2.5h)\"]\n    ValidatePog --> CheckActive[\"Verify in metagraph.axons\"]\n    CheckActive --> ExtractSpecs[\"Extract specs from run.config\"]\n    ExtractSpecs --> SpecsDetails[\"specs_details{}\"]\n    SpecsDetails --> ResourceList[\"resource_list[]\"]\n```\n\n**WandB Resource Discovery Flow**\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:1646-1702\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1646-L1702\" />, \u003CSourceLink text=\"neurons/register_api.py:1881-1884\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1881-L1884\" />\n\nThe system implements an alternative discovery mechanism using WandB for distributed miner information:\n\n| Validation Check | Function | Purpose |\n|------------------|----------|---------|\n| Age Verification | `miner_is_older_than()` | Ensures miners have been active for 48+ hours |\n| PoG Validation | `miner_pog_ok()` | Confirms recent Proof-of-GPU completion within 2.5 hours |\n| Penalty Check | `get_penalized_hotkeys_checklist()` | Excludes blacklisted or penalized miners |\n| Network Presence | `metagraph.axons` lookup | Verifies miner is active on network |\n\n## Allocation Strategies\n\nThe system implements two primary allocation strategies: specification-based allocation and hotkey-specific allocation, each optimized for different use cases.\n\n### Specification-Based Allocation\n\n```mermaid\nflowchart TD\n    AllocateSpec[\"_allocate_container()\"] --> BuildDevReq[\"Build device_requirement{}\"]\n    BuildDevReq --> FindCandidates[\"select_allocate_miners_hotkey()\"]\n    FindCandidates --> CheckAvailable[\"dendrite(checking:True)\"]\n    CheckAvailable --> ScoreSort[\"torch.ones_like(metagraph.S)\"]\n    ScoreSort --> TryAllocation[\"Loop through sorted_hotkeys\"]\n    TryAllocation --> SendAllocate[\"dendrite(Allocate(checking:False))\"]\n    SendAllocate --> ValidateResponse[\"Check response.status\"]\n    ValidateResponse --> Success[\"Return allocation info\"]\n    ValidateResponse --> NextCandidate[\"Try next candidate\"]\n    NextCandidate --> TryAllocation\n```\n\n**Specification-Based Allocation Flow**\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2733-2805\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2733-L2805\" />\n\nThe specification-based strategy processes `DeviceRequirement` objects containing:\n- CPU count requirements\n- GPU type and memory specifications  \n- RAM and storage capacity needs\n- Timeline duration for allocation\n\n### Hotkey-Specific Allocation\n\n```mermaid\nflowchart TD\n    AllocateHotkey[\"_allocate_container_hotkey()\"] --> FindAxon[\"Locate axon by hotkey\"]\n    FindAxon --> SetDefaults[\"Set default device_requirement{}\"]\n    SetDefaults --> SetBaseImage[\"docker_requirement.base_image\"]\n    SetBaseImage --> DirectAllocate[\"dendrite(Allocate())\"]\n    DirectAllocate --> ValidateResponse[\"Check response.status\"]\n    ValidateResponse --> ReturnInfo[\"Return allocation + miner_version\"]\n    ValidateResponse --> ReturnError[\"Return error message\"]\n```\n\n**Hotkey-Specific Allocation Flow**\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2807-2889\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2807-L2889\" />\n\nThis strategy targets specific miners by hotkey, bypassing the discovery phase and applying default resource requirements with configurable Docker base images.\n\n## Health Monitoring System\n\nThe health monitoring system continuously tracks allocated resources and manages their lifecycle through automated checks and notifications.\n\n### Allocation Health Check Process\n\n```mermaid\nflowchart TD\n    CheckTask[\"_check_allocation()\"] --> QueryDB[\"SELECT * FROM allocation\"]\n    QueryDB --> LoopAllocations[\"For each allocation\"]\n    LoopAllocations --> ValidateHotkey[\"Check hotkey in metagraph\"]\n    ValidateHotkey --> HealthCheck[\"dendrite_check(Allocate(checking:True))\"]\n    HealthCheck --> EvaluateResponse[\"Evaluate response\"]\n    \n    EvaluateResponse --> Online[\"response.status :: False\"]\n    EvaluateResponse --> Offline[\"No response or timeout\"]\n    \n    Online --> NotifyOnline[\"_notify_allocation_status(ONLINE)\"]\n    Online --> RemoveFromCheck[\"Remove from checking_allocated[]\"]\n    \n    Offline --> AddToCheck[\"Add to checking_allocated[]\"]\n    Offline --> NotifyOffline[\"_notify_allocation_status(OFFLINE)\"]\n    Offline --> CountChecks[\"checking_allocated.count(hotkey)\"]\n    \n    CountChecks --> MaxReached[\"count >: ALLOCATE_CHECK_COUNT\"]\n    MaxReached --> Deallocate[\"update_allocation_db(False)\"]\n    Deallocate --> NotifyDealloc[\"_notify_allocation_status(DEALLOCATION)\"]\n```\n\n**Health Monitoring Process Flow**\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:3002-3101\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L3002-L3101\" />\n\nThe health monitoring system operates with the following parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `ALLOCATE_CHECK_PERIOD` | 180 seconds | Interval between health checks |\n| `ALLOCATE_CHECK_COUNT` | 20 | Maximum failed checks before deallocation |\n| Health Check Timeout | 10 seconds | Maximum wait time for miner response |\n\n### Status Transition Management\n\nThe system tracks miner status transitions and maintains allocation state through the `checking_allocated` list and database updates.\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:3039-3081\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L3039-L3081\" />\n\n## State Management\n\nThe Resource Management system maintains both local and distributed state through SQLite database operations and WandB synchronization.\n\n### Database State Operations\n\n```mermaid\nflowchart TD\n    AllocDB[\"allocation table\"] --> UpdateLocal[\"update_allocation_db()\"]\n    UpdateLocal --> ExtractHotkeys[\"Extract hotkey_list[]\"]\n    ExtractHotkeys --> UpdateWandB[\"_update_allocation_wandb()\"]\n    UpdateWandB --> WandbSync[\"wandb.update_allocated_hotkeys()\"]\n    \n    QueryState[\"List operations\"] --> QueryDB[\"SELECT hotkey, details FROM allocation\"]\n    QueryDB --> ParseJSON[\"json.loads(details)\"]\n    ParseJSON --> BuildAllocation[\"Build Allocation objects\"]\n    \n    HealthCheck[\"Health monitoring\"] --> StateUpdate[\"Allocation state changes\"]\n    StateUpdate --> NotifyRetry[\"notify_retry_table[]\"]\n    StateUpdate --> UpdateLocal\n```\n\n**State Management Architecture**\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2891-2919\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2891-L2919\" />, \u003CSourceLink text=\"neurons/register_api.py:1346-1419\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L1346-L1419\" />\n\n### Distributed State Synchronization\n\nThe system maintains consistency across validators through WandB-based state sharing:\n\n| State Component | Storage | Synchronization Method |\n|-----------------|---------|----------------------|\n| Active Allocations | SQLite `allocation` table | `_update_allocation_wandb()` |\n| Allocated Hotkeys | WandB runs | `wandb.update_allocated_hotkeys()` |\n| Retry Notifications | In-memory `notify_retry_table` | Periodic retry processing |\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2915-2919\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2915-L2919\" />\n\n## Notification System\n\nThe notification system provides external webhook integration for allocation lifecycle events and status changes.\n\n### Notification Event Types\n\n```mermaid\nflowchart TD\n    EventTrigger[\"Allocation Event\"] --> EventType{\"Event Type\"}\n    \n    EventType --> Deallocation[\"DEALLOCATION\"]\n    EventType --> Online[\"ONLINE\"] \n    EventType --> Offline[\"OFFLINE\"]\n    \n    Deallocation --> BuildDeallocationMsg[\"Build deallocation message\"]\n    Online --> BuildStatusMsg[\"Build status change message\"]\n    Offline --> BuildStatusMsg\n    \n    BuildDeallocationMsg --> DeallocationURL[\"deallocation_notify_url\"]\n    BuildStatusMsg --> StatusURL[\"status_notify_url\"]\n    \n    DeallocationURL --> SignAndSend[\"HMAC signature + POST\"]\n    StatusURL --> SignAndSend\n    \n    SignAndSend --> RetryLogic[\"Retry up to MAX_NOTIFY_RETRY\"]\n    RetryLogic --> Success[\"200/201 response\"]\n    RetryLogic --> AddToRetryTable[\"Add to notify_retry_table[]\"]\n```\n\n**Notification System Flow**\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:2940-3000\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2940-L3000\" />\n\n### Notification Configuration\n\nThe notification system operates with these key parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `MAX_NOTIFY_RETRY` | 3 | Maximum notification attempts |\n| `NOTIFY_RETRY_PERIOD` | 15 seconds | Delay between retry attempts |\n| Webhook Signature | HMAC-SHA256 | Request authentication |\n| SSL Certificates | Required | Secure communication |\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:92-93\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L92-L93\" />, \u003CSourceLink text=\"neurons/register_api.py:2972-2976\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L2972-L2976\" />\n\n## Data Models and Configuration\n\nThe Resource Management system uses several key data models and configuration parameters to define resource requirements and allocation responses.\n\n### Core Data Models\n\n```mermaid\nclassDiagram\n    class DeviceRequirement {\n        +int cpu_count\n        +str gpu_type\n        +int gpu_size\n        +int ram\n        +int hard_disk\n        +int timeline\n    }\n    \n    class Allocation {\n        +str resource\n        +str hotkey\n        +str regkey\n        +str ssh_ip\n        +int ssh_port\n        +str ssh_username\n        +str ssh_password\n        +str ssh_command\n        +str ssh_key\n        +str uuid_key\n        +int miner_version\n    }\n    \n    class ResourceQuery {\n        +str gpu_name\n        +int cpu_count_min\n        +int cpu_count_max\n        +float gpu_capacity_min\n        +float gpu_capacity_max\n        +float hard_disk_total_min\n        +float hard_disk_total_max\n        +float ram_total_min\n        +float ram_total_max\n    }\n    \n    class DockerRequirement {\n        +str base_image\n        +str ssh_key\n        +str volume_path\n        +str dockerfile\n    }\n    \n    DeviceRequirement --> Allocation : \"Generates\"\n    ResourceQuery --> Allocation : \"Filters\"\n    DockerRequirement --> Allocation : \"Configures\"\n```\n\n**Resource Management Data Models**\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:147-175\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L147-L175\" />, \u003CSourceLink text=\"neurons/register_api.py:156-167\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L156-L167\" />, \u003CSourceLink text=\"neurons/register_api.py:204-213\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L204-L213\" />\n\n### System Configuration Constants\n\nThe system behavior is controlled through key configuration constants:\n\n| Constant | Value | Purpose |\n|----------|-------|---------|\n| `DATA_SYNC_PERIOD` | 600 seconds | Metagraph refresh interval |\n| `ALLOCATE_CHECK_PERIOD` | 180 seconds | Health check frequency |\n| `ALLOCATE_CHECK_COUNT` | 20 | Max timeout count before deallocation |\n| `MAX_ALLOCATION_RETRY` | 3 | Maximum allocation attempt retries |\n| `VALID_VALIDATOR_HOTKEYS` | Array of 19 hotkeys | Authorized validator addresses |\n\nSources: \u003CSourceLink text=\"neurons/register_api.py:85-116\" href=\"https://github.com/neuralinternet/SN27/blob/6261c454/neurons/register_api.py#L85-L116\" />\n\nThe Resource Management system integrates these components to provide robust, scalable compute resource allocation with comprehensive monitoring and state management capabilities.","src/content/docs/resource-allocation-api/resource-management.mdx","223887afca9ea068"]